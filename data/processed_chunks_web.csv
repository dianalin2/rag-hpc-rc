source,chunk
rc-website-fork/content/thank-you.md,"Thank you for contacting UVA's Research Computing group (RC). We will make every effort to respond to your request within 1 business day (9am5pm EST, MF, excluding holidays). System status updates are available on www.rc.virginia.edu."
rc-website-fork/content/resources.md,Hello Allocations Review and manage your HPC allocations on Rivanna. Name Type SUs Remain % Remain S = Standard P = Purchased I = Instructional D = DeanAllocation counts are refreshed 1x per day. Request Allocations Storage Review and manage your Research Project or Research Standard storage shares. Group Name Type Capacity P = Project V = ValueStorage quotas are refreshed 1x per day. Request Storage Container Services Review and manage your microservices. Service Image Count Container inventories are refreshed 1x per day. Request Microservices Databases Review and manage your databases. Database Type Host Database inventories are refreshed 1x per day. Request a Database
rc-website-fork/content/name-contest.md,Hello Allocations Review and manage your HPC allocations on Rivanna. Request Allocations
rc-website-fork/content/support.md,"{{< formcookies }} Request Help Open a support ticket with your specific questions or issues. Open a Support Ticket Allocations Request, purchase, or modify your Afton/Rivanna HPC allocations. Request Allocations Storage Manage your shared Research Project or Research Standard storage volumes. Request Storage Ivy Projects Perform your research on a HIPAAcompliant computing platform. Set up a Project ACCORD Request information or support for an ACCORD project. Learn more. ACCORD Support Container Services Run your containerbased service in our Kubernetes cluster. Learn more. Request a Microservice User Guides Learn about systems, storage, data transfer, image analysis or tools. Read the User Guides FAQ / Knowledgebase Search topics or post your own related to research computing. Have a Question? Pricing Read more about pricing for SUs, Ivy virtual machines, storage, and more. Learn More Office Hours Research Computing staff host weekly office hours. Tell us about a project idea or talk to us about our high performance computing platforms, storage and services. We're here to help you solve your computational problems. Examples of the type of support we can provide are: Data Transfer/Access Parallel Coding in Fortran, C, Python, R, Matlab, Mathematica Bioinformatics Computational Chemistry Software Installation and Containers Image Processing Writing Slurm Job Scripts Maximizing Job Efficiency Managing Computational Workflows We offer office hours as online Zoom sessions twice a week. No appointments required. Tuesdays 3:005:00pm Join us via Zoom Thursdays 10:0012:00pm Join us via Zoom New to HighPerformance Computing? We offer orientation sessions to introduce you to the Afton & Rivanna HPC systems on Wednesdays (appointment required). Wednesdays 3:004:00pm Sign up for an ""Intro to HPC"" session"
rc-website-fork/content/signup.md,"Keep up to date with our latest news, updates, workshops and service announcements. indicates required Email Address First Name Last Name"
rc-website-fork/content/form/storage.md,"{{% jiramsg %}} {{< formcookies }} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} Type of Request Create new storage share Increase size of existing share Decrease size of existing share Retire existing share Space (TB) The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments. Storage Platform Research Project Storage ({{< extractstoragecost type=""project"" }}) Research Standard Storage ({{< extractstoragecost type=""standard"" }}) HighSecurity Research Standard Storage ({{< extractstoragecost type=""hsz standard"" }}) None of these storage options offer data backups or replication. Research Project storage provides week long snapshots of data. Snapshots are not available on Research Standard storage Billing information is required. However, if you are within the 10TB of free Research Standard Storage, no charges will apply. Internal Use / Public DataThis storage platform is appropriate for public or internal use data. Sensitive / Highly Sensitive DataThis storage platform is appropriate for highly sensitive data such as HIPAA, FERPA, CUI, etc. Grouper/MyGroups Ownership Grouper or MyGroups group name under your Eservices user ID. You will have access to Grouper management and will be able to add/remove users for your project. Legacy MyGroups groups created before November 28th, 2023, can be accessed through the “Legacy MyGroups” folder on Grouper. Shared Space Name This is the name to be applied to your shared storage space. By default, the space will be named according to the Grouper/MyGroups group associated with the storage request. If you would prefer a different identifier, indicate the name for the space. {{% groupcreationtip %}} Project Title {{% billingfdm %}} Data Agreement The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended"
rc-website-fork/content/form/storage.md,"and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a webbased tool that automatically tracks when data can be safely transferred or destroyed. I understand Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/support-request-orig.md,"{{% formuserinfo %}} Department/Organization Academic Discipline Select Astronomy Biochemistry Bioinformatics Biology Chemistry Commerce Computer Science Data Science Economics Environmental Science Engineering Health Sciences Informatics Physics Social Sciences Other Other Academic Discipline Support Category Select General research computing question HPC (Afton & Rivanna) Ivy Secure Computing Storage Omero Image Analysis Containerized Service Consultation request CHASE Accounts/Data Sentinel System/Software Other Use this form for general HPC support questions. Or submit an Allocation Request. Use this form for storage questions. Or submit a storage request. Use this form for general Omero questions. Or request Omero access. Use this form for general queries about containerized services. Or request a container service. Use this form for general Ivy questions. Or submit an Ivy Project Request. Brief description of your request Details of your request Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/form/skyline.md,"{{% jiramsg %}} {{< formcookies }} {{< enabledisableform }} Skyline Virtual Machines (VMs) are designated for computation that involves public and moderatelysensitive data. Processing of highly sensitive data is not permitted. Learn about our Ivy environment for processing and storage of highly sensitive data that have HIPAA, ITAR, or CUI requirements. {{% getstatus keyword=""jira"" %}} {{% formuserinfo %}} Classification Select FacultyStaffPostdoctoral AssociateOther Affiliation Select College of Arts & Sciences School of Data Science School of Engineering and Applied Sciences School of Medicine Darden School of Business UVA Health System Other Name of MyGroups Account (lowercase only, no spaces). This group is used to define all members with access to the requested VM. You can use an existing group or set up a new MyGroup. Project Summary Please describe your project and the software you intend to use on your Skyline VM. VM Configuration 2 cpu cores / 2GB memory ($4/month) 4 cpu cores / 16GB memory ($12/month) 8 cpu cores / 32GB memory ($48/month) 16 cpu cores / 64GB memory ($96/month) 16 cpu cores / 124GB memory ($176/month) Operating System CentOS 7.8: Unlimited number of concurrent user logins. Billing Tiers are selected and paid for by the PI. Submit this form again if you wish to change your VM configuration. PTAO Financial Contact Please enter the name and email address of your financial contact. Data Agreement The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a webbased tool that automatically tracks when data can be safely transferred or destroyed. I understand Please submit the form only"
rc-website-fork/content/form/skyline.md,"once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/combined-request-form.md,"{{% jiramsg %}} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} PI/Owner UVA ID Requestor ID (if different from User ID above) Your Current Resources Type Project Group Resource Name Tier Size/Count Status Update Date No resources found to display. Resource Type Allocation's (SU) Storage Allocation's (SU) Request New or Renewal New Update/Renewal If this is your first request, select New. Otherwise select Renewal. Name of Grouper/MyGroups Account Select a group Storage Grouper/MyGroups Account Select a group Your Existing Service Units Select Project Group Resource Name Tier Size/Count Status Update Date Project Nick Name Description of Research Project Briefly describe how you have used Rivanna/Afton in your research. Please include conference presentations, journal articles, other publications, or grant proposals that cite Rivanna. Tier Options For detailed information about each allocation tier option, please visit our Allocation Types Documentation. Standard Paid Instructional Additional SU's Requested The number of SU's requested.(Note: SU's cannot be requested for Standard and Instructional resources but will be automatically applied/updated once submitted) Storage Request New or Change Existing Create new storage share Update existing share Retire existing share Name of Grouper/MyGroups Account Select a group Group names can only contain letters, numbers, dashes, and underscores (e.g., researchlab1, datascience2) Your Existing Storage Select Project Group Resource Name Tier Size/Count Status Update Date Project Nick Name Description of Research Project Briefly describe how you have used Rivanna/Afton in your research. Please include conference presentations, journal articles, other publications, or grant proposals that cite Rivanna. Tier Options For detailed information about each storage tier option, please visit our Storage Documentation. SSZ Research Project ({{< extractstoragecost type=""project"" }}) SSZ Research Standard ({{< extractstoragecost type=""standard"" }}) HighSecurity Research Standard Storage ({{< extractstoragecost type=""hsz standard"" }}) Internal Use / Public DataThis storage platform is appropriate for public or internal use data."
rc-website-fork/content/form/combined-request-form.md,"Sensitive / Highly Sensitive DataThis storage platform is appropriate for highly sensitive data such as HIPAA, FERPA, CUI, etc. Total Space (TB) The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments. Free Space (TB) You have 10TB of free space, how much would you like to apply for this share? Existing FDM's Company Cost Center Business Unit Funding Number Fund Function Program Activity Assignee Delete + New FDM Payment Information {{% billingfdm %}} Add to FDM Details Cancel Data Agreement The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a webbased tool that automatically tracks when data can be safely transferred or destroyed. I understand Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit Cancel {{< /enabledisableform }}"
rc-website-fork/content/form/test-form.md,"{{% formuserinfo %}} Name of PI Is the PI of your account a UVA faculty member? Yes No (NonUVA personnel are charged $0.07/SU) I agree that this allocation will be used for research purposes only Agree Disagree Title of Award (if applicable) Total number of SUs requested Total amount to be charged to FDM $ SU expiration date (if applicable) Apply this purchase to which allocation {{% billingfdm %}} Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/form/README.md,"🏛 Combined Request Form Research Computing 📌 Overview The Combined Request Form is a dynamic webbased UI for Service Unit (SU) Requests and Storage Requests. The form handles new allocations and renewals, validates user inputs, generates API payloads, and submits requests using POST (New) and PUT (Renewal) methods. 📖 Table of Contents Form Structure JavaScript Architecture Event Handling & Interactivity Fetching and Processing User Data Payload Construction (New & Renewal Requests) Form Submission & API Communication Validation & Error Handling UI Behaviors & Additional Features 📑 1. Form Structure The form dynamically adjusts based on the selected request type. Service Unit (SU) Requests New Request: Users select a Group, Tier, and input billing details. Renewal: Users select from an Existing SU Table, and only the updatedate is changed. Storage Requests New Storage: Users pick a storage tier and specify allocation details. Modify Storage: Users choose an existing project from the Existing Storage Table. Billing Information Displayed dynamically if a selected request requires billing. ⚙️ 2. JavaScript Architecture The core JavaScript file combinedrequestform.js manages: Dynamic form updates based on user selections. Fetching and displaying user resources. Validating inputs and preventing invalid submissions. Building and submitting API payloads. Key JavaScript Functions | Function | Description | ||| | fetchMetadata() | Fetches storage tier limits and billing rules from API. | | fetchAndPopulateGroups() | Retrieves user groups and resources dynamically. | | updatePayloadPreview() | Shows realtime request preview before submission. | | validatePayload(payload) | Ensures API payload structure is correct. | | submitForm(formData, payload) | Sends request via POST (New) or PUT (Renewal). | | processUserResources(apiResponse) | Populates user’s Existing SU Table. | 🖱 3. Event Handling & Interactivity JavaScript dynamically updates the UI based on user actions. Key Event Listeners | Event | Function Triggered | Description | |||| | Change on"
rc-website-fork/content/form/README.md,"requesttype | toggleRequestFields() | Toggles between Service Unit and Storage Requests. | | Change on neworrenewal | toggleAllocationFields() | Shows correct fields for New vs Renewal. | | Change on existingprojectallocation | updatePayloadPreview() | Updates the API payload preview. | | Change on any form input | updateBillingVisibility() | Shows/hides Billing Information. | | Form Submit | handleFormSubmit(event) | Collects data, validates, and submits the request. | 🔄 4. Fetching and Processing User Data At page load, fetchAndPopulateGroups() retrieves: User Groups Existing Service Unit Allocations Storage Requests Fetched data is stored in consoleData and updates: 1. Existing SU Table (populateExistingServiceUnitsTable()) 2. Existing Storage Requests Table (if applicable) 🛠 5. Payload Construction (New & Renewal Requests) New SU Requests (POST) User selects Group and Tier. Billing details are included if applicable. Request count defaults to 1000 unless specified. Payload structure: json [ { ""groupname"": ""RCStaff"", ""projectname"": ""Test Project"", ""projectdesc"": ""This is free text"", ""dataagreementsigned"": true, ""piuid"": ""UVAComputingID"", ""resources"": { ""hpcserviceunits"": { ""CACSStaff"": { ""tier"": ""sszproject"", ""requestcount"": ""1000"", ""billingdetails"": { ""fdmbillinginfo"": [ { ""financialcontact"": ""First Name Last Name"", ""company"": ""234324"", ""businessunit"": ""3"", ""costcenter"": ""224"", ""fund"": ""a Fund"", ""gift"": """", ""grant"": """", ""designated"": """", ""project"": """", ""programcode"": ""a program"", ""function"": ""A Function"", ""activity"": ""an activity"", ""assignee"": ""an assignee"" } ] } } } }, ""userresources"": [] } ] Renewal Requests (PUT) The user selects an existing SU instead of filling in a new group. The only change allowed is updating the updatedate. The API payload includes the existing group, tier, and updated timestamp. json [ { ""groupname"": ""RCStaff"", ""projectname"": ""Existing Project"", ""resources"": { ""hpcserviceunits"": { ""CACSStaffsszstandard"": { ""tier"": ""sszstandard"", ""requestcount"": ""50000"", ""updatedate"": ""20250212T10:30:00Z"" ""billingdetails"": { ""fdmbillinginfo"": [ { ""financialcontact"": ""First Name Last Name"", ""company"": ""234324"", ""businessunit"": ""3"", ""costcenter"": ""224"", ""fund"": ""a Fund"", ""gift"": """", ""grant"": """", ""designated"": """", ""project"": """", ""programcode"": ""a program"","
rc-website-fork/content/form/README.md,"""function"": ""A Function"", ""activity"": ""an activity"", ""assignee"": ""an assignee"" } ] } } } } } ] 6. Form Submission & API Communication The submitForm() function determines whether to POST or PUT based on the selected New vs Renewal option. New Requests → POST to: https://uvarcunifiedservice.pods.uvarc.io/uvarc/api/resource/rcwebform/user/{userId} Renewal Requests → PUT to: https://uvarcunifiedservice.pods.uvarc.io/uvarc/api/resource/rcwebform/user/{userId}/{resourceid} 7. Validation & Error Handling Before submission, validatePayload(payload) checks for: Missing required fields (group, tier, request count). Duplicate group/tier combinations. Correct formatting of billing details (when required). Errors trigger showErrorMessage(), and invalid fields are marked red. 8. UI Behaviors & Additional Features ✅ RealTime Payload Preview The function updatePayloadPreview() shows the request payload before submission. ✅ Billing Visibility The form automatically hides/shows billing details based on the selected SU or Storage option. ✅ Sorting of Existing SU Table Newest allocations appear first in the Existing SU table."
rc-website-fork/content/form/allocation-purchase.md,"{{% jiramsg %}} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} Name of PI Is the PI of your account a UVA faculty member? Yes No I agree that this allocation will be used for research purposes only Agree Disagree Title of Award (if applicable) Total number of SUs requested Total amount to be charged to FDM $ SU expiration date (if applicable) Apply this purchase to which allocation {{% groupcreationtip %}} {{% billingfdm %}} Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/allocation-standard.md,"{{% jiramsg %}} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} Name of Grouper/MyGroups Account Lowercase only, no spaces, PI must create his/her Grouper group for new allocations. {{% groupcreationtip %}} New or Renewal New Renewal If this is your first request, select New. Otherwise select Renewal. Standard allocations expire 12 months after they are disbursed. Description of Research Project Briefly describe how you have used Rivanna/Afton in your research. Please include conference presentations, journal articles, other publications, or grant proposals that cite Rivanna. Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/support-request.md,"{{% jiramsg %}} {{< formcookies }} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} Support Category Select General research computing question HPC (Afton & Rivanna) Ivy Secure Computing Storage Container Service Data Analytics Center Digital Technology Core Other Use this form for general Rivanna support questions. Or submit an Allocation Request. Use this form for storage questions. Or submit a storage request. Use this form for general queries about containerized services. Or request a container service. Use this form for general Ivy questions. Or submit an Ivy Project Request. Use this form for questions related to services offered by our Data Analytics Center. Learn more about the Data Analytics Center. Use this form for questions related to services offered by our Digital Technology Core. Learn more about the Digital Technology Core. Brief description of your request Details of your request Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/support-request-vRedesign.md,"{{% formuserinfov2 %}} Select a support category: General Support HPC (Afton & Rivanna) Ivy Consultation Reset Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm whether the submission completed or failed. Submit"
rc-website-fork/content/form/allocation-instructional.md,"{{% jiramsg %}} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} Name of Grouper/MyGroup Account Lowercase only, no spaces, PI must create his/her Grouper group for new allocations. {{% groupcreationtip %}} Instructors are responsible for creating the class Grouper group and updating the roster for the chosen account through the Grouper portal. New or Renewal New Renewal If you have taught this same class before using Rivanna/Afton, select Renewal. Class ID Academic Term Class Size How many students are in your class? Class Schedule What days/times does this class meet? Enter “n/a” if students will use the cluster at different times. Cores/Memory Required Estimate how many cores and how much memory each student will need to process his/her jobs. General descriptions are fine. A member of our user services team will contact you if we need additional information. PIs are eligible for 10TB of Research Standard storage at no charge. Instructors are encouraged to utilize this 10TB of storage for both research and teaching activities. Read the full policy and guide for instructors. Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/containers.md,"{{% jiramsg %}} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} Project Summary Please describe your project and the container images you want to run. Tier of Service <= 5 containers ({{< extractmicroservicescost tier=light }} total) 6 15 containers ({{< extractmicroservicescost tier=medium }} total) 15 containers ({{< extractmicroservicescost tier=heavy }} total) Billing Tiers are selected and paid for by the PI. Submit this form again if you wish to change your tier. Stopped containers do not incur charges, nor does local cluster storage or remote NFS mounts to /project storage. Project storage pricing can be found here. Storage No storage required Persistent cluster storage required NFS mount of project storage is required Storage Capacity (GB) The size of storage if required. Specify in 1GB increments. SSL/HTTPS Required No Yes Netbadge Authentication No Yes {{% billingfdm %}} Data Agreement The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a webbased tool that automatically tracks when data can be safely transferred or destroyed. I understand Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/accord.md,"{{% jiramsg %}} {{< getstatus keyword=""jira"" }} {{< enabledisableform }} Name Email University/Institution Department/Organization Brief description of your request Details of your request Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/sns-test.md,here is a response {{% formuserinfo %}} Group Family Submit
rc-website-fork/content/form/dedicated-computing.md,"{{% jiramsg %}} {{< enabledisableform }} {{% getstatus keyword=""jira"" %}} {{% formuserinfov2 %}} Name of Grouper/MyGroup Account Lowercase only, no spaces, PI must create his/her Grouper group for new allocations. {{% groupcreationtip %}} Type and quantity of hardware requested (Check out the specifications) {{< dedicatedoptionsform }} Proposed Start Date (Subject to Resource Availability) Start Date End Date Please select a valid start and end date. {{% billingfdm %}} Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit {{< /enabledisableform }}"
rc-website-fork/content/form/support-request-attachments.md,"{{% formuserinfo %}} Support Category Select General research computing question HPC (Afton & Rivanna) Ivy Secure Computing Storage Omero Image Analysis Containerized Service Consultation request CHASE Accounts/Data Sentinel System/Software Other Use this form for general HPC support questions. Or submit an Allocation Request. Use this form for storage questions. Or submit a storage request. Use this form for general Omero questions. Or request Omero access. Use this form for general queries about containerized services. Or request a container service. Use this form for general Ivy questions. Or submit an Ivy Project Request. Brief description of your request Department/Organization Details of your request Attachments Upload files Select or drag files or screenshots here Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/post/2025-may-maintenance.md,"{{< alertgreen }}The UVA HPC systems, Afton/Rivanna, will be down for maintenance on Tuesday, May 27, 2025 beginning at 6 am.{{< /alertgreen }} All systems are expected to return to service by Wednesday, May 28 at 6 am. How should I prepare and what to expect on May 27, 2025? You may continue submitting jobs to the HPC system until the maintenance period begins. However, if the system determines your job won't finish in time, it will not start until the system is back online. The upcoming maintenance will include upgrades to the HPC image (upgrading to Rocky 8.10), the HPC scheduler (Slurm 24.11), the NVIDIA driver and Open OnDemand. To complete these tasks, all compute nodes and login nodes including the Open OnDemand and FastX portals – will need to be taken offline. However, Research Project Storage and Research Standard Storage will remain accessible via SMB and NFS mounts. Additionally, the UVA Standard Security Storage Data Transfer Node (DTN) will stay operational throughout the maintenance period. If you have any questions about the upcoming HPC system maintenance, you may contact our user services team. IMPORTANT MAINTENANCE NOTES System upgrades Operating system: Rocky 8.10 Slurm: 24.11.3 NVIDIA driver: 570.124.06 (CUDA 12.8) Open OnDemand: 4.0 Modules The software stack is migrated from /sfs/applications to /sfs/gpfs/tardis/applications and the /apps symbolic link is updated accordingly. Unless the full path is required, users should simply use /apps. Default version changes include: cuda/12.4.1 → 12.8.0 miniforge/24.3.0py3.11 → 24.11.3py3.12 namd/2.14 → 3.0.1 nvhpc/24.5 → 25.3 R/4.4.1 → 4.5.0 (The original defaults are not removed.) {{< table title=""replacement"" class=""table tablestriped"" }} | Module | Remove | Replace with | |||| |afni | 23.1.10 | 25.0.12 | |bracken | 2.9 | 3.1 | |cellranger | 8.0.0 | 9.0.1 | |codeserver| 4.92.2 | 4.99.1 | |cmake | 3.23.3, 3.24.3"
rc-website-fork/content/post/2025-may-maintenance.md,"| 3.28.1, 4.0.0 | |cuda | 10.2.89, 11.8.0 | 12.8.0 | |cudnn | 8.2.4.15, 8.9.7 | 8.9.4.25, 9.8.0CUDA12.8.0 | |cutadapt | 3.4 | 4.9 | |fastqc | 0.11.5 | 0.12.1 | |fmriprep | 23.1.4 | 25.0.0 | |fsl | 6.0.7.6 | 6.0.7.17 | |gcc | 13.3.0 | 14.2.0 | |globuscli | 3.11.0 | 3.34.0 | |go | 1.21.4 | 1.23.6 | |intel | 2024.0 | 2025.0 | |jcuda | 11.4.1 | | |jupyterlab | 3.6.3py3.11 | 4.4.1py3.12 (see note below) | |kraken2 | 2.1.3 | 2.1.5 | |mathematica| 11.2 | 14.2 | |matlab | R2022b | R2024b | |multiqc | 1.14 | 1.27.1 | |nextflow | 23.04.1 | 24.10.5 | |openmpi | 4.1.4nofabric[withoutverbs] | 4.1.4 | |openmpi | 4.1.5intel | 4.1.5 | |perl | 5.36.0 | 5.40.2 | |pytorch | 1.12.0, 2.0.1 | 2.4.0, 2.7.0 | |ruby | 3.1.2 | 3.4.3 | |smrtlink | 13.1.0.221970 | 25.2.0 | |spaceranger| 3.1.1 | 3.1.3 | |sumo | 1.14.1 | 1.22.0 | |texlive | 2023 | 2025 | |tmux | 2.5 | 3.4 | |tree | 1.8.0 | 2.2.1 | |trimgalore | 0.6.4 | 0.6.10 | {{< /table }} PostMaintenance Note Due to issues with accessing subfolders in the new JupyterLab, we have reverted to the premaintenance version on Open OnDemand. However, the issue is not observed when Jupyter is launched manually from the command line, and so the new version is still kept as a module for those who wish to launch it manually (e.g. within a Desktop session)."
rc-website-fork/content/post/2022-04-women-in-hpc.md,"Please join us for a lively panel discussion of the highperformance computing infrastructure and support resources available to researchers in the Commonwealth of Virginia. Panelists from Virginia's top research universities will provide an overview of the user base at their home institutions and discuss strategies for helping researchers make better use of existing HPC resources. Attendees are encouraged to share their own experiences and engage with panelists during this interactive Q&A session. Topic: Highperformance Computing Resources in the Commonwealth of Virginia When: April 27, 2022 01:00 PM, Eastern Time (US and Canada) Featured panelists: Matthew Brown (VT) Jayshree Sarma (GMU) Jacalyn Huband (UVA) Eric Walter (W&M) Carol Parish (U. Richmond) Mike Davis (VCU) Virginia WHPC is committed to increasing diversity and inclusion by promoting and encouraging the participation of women in highperformance computing and related fields."
rc-website-fork/content/post/2019-summer-workshops.md,"RC staff are teaching a series of free handson workshops this summer that are open to all UVA researchers. Space is limited, so register today! Topics include: Programming in Python (June 3June 5) R (June 3June 4) MATLAB (June 5June 7 and June 13) Compiled Languages, C++ and Fortran (June 6June 7) Scientific Image Processing with Fiji/ImageJ (June 10) Introduction to HighPerformance Computing (June 10) Software Design and Testing (June 11) HPC Data Analytics (June 11) Parallel Programming Using MPI (June 12June 13) Bioinformatics (June 12) Scientific Visualization (June 14) OpenMP and Accelerators (June 14) Register through the new CADRE Academy portal. You can register for classes, follow tracks or workshops that are not yet scheduled, and find other related workshops. For a comprehensive list of all such educational opportunities across Grounds, visit the Research & Data Services Training Portal. Learn More Would you like to learn the basics of highthroughput and parallel computing? Do you want handson high performance training that you can apply to your research? If so, then we invite you to attend one or more sessions in ARCS' Summer Education Series. The sessions are free for all UVA faculty, staff, and students. Topics include how to program in Python, scientific visualization, modern Fortran, data analytics, MPI for distributed systems, OpenMP for multicore systems, and more! Lectures on relevant topics will be delivered during the morning sessions; afternoon sessions offer handson practice. June 3June 5: Programming in Python Go from beginner to proficient in three days. Day 1: Basics of programming in Python. Day 2: Modules and packages. NumPy, SciPy, and Pandas. Day 3: Dictionaries and classes. Some computing experience is helpful but not required. Mornings are lectures and handson exercises, with afternoon labs focused on programming applications to cement your skills. June 3June 4: R Day"
rc-website-fork/content/post/2019-summer-workshops.md,"1 (R for data science): R is one of the most ubiquitous statistical computing languages. In this session, you will be introduced to the language's fundamental data structures as well as how to load packages and use functions. Topics include: composing scripts, reading data into R, and basic data manipulation and visualization techniques using the ""tidyverse"" set of packages (dplyr,tidyr,ggplot2). Day 2 (R as a programming language): Need to improve your skills with manipulating data in R? In R as a programming language, you will learn how to make R do more work for you. Topics include decisionmaking, repetition, functions, and optimizations (such as vectorization). June 5June 7: MATLAB Day 1: MATLAB fundamentals (no prior MATLAB experience required). Day 2: MATLAB programming techniques. Provides handson experience using the features in the MATLAB language to write efficient, robust, and wellorganized code. These concepts form the foundation for writing full applications, developing algorithms, and extending builtin MATLAB capabilities. Details of performance optimization, as well as tools for writing, debugging, and profiling code are covered. Day 3: MATLAB for data visualization. Focuses on importing and preparing data for data analytics applications. The workshop is intended for data analysts and data scientists who need to automate the processing, analysis, and visualization of data from multiple sources. June 6June 7: Compiled Languages The basics of C++ and Fortran will be taught in this accelerated twoday course. Prerequisite: experience programming in some language. Mornings will be handson and lecture, with afternoon programming labs. June 10: Intro to HighPerformance Computing This allday session is an indepth introduction to using the Rivanna HPC cluster. Topics covered will include an introduction to the Unix command line, writing Slurm scripts, and highthroughput computing. June 10: Scientific Image Processing with Fiji/ImageJ This handson workshop is an introduction to using Fiji, an"
rc-website-fork/content/post/2019-summer-workshops.md,"opensource and enhanced version of the popular ImageJ program used for scientific image processing. During the morning session, participants will be introduced to image processing filters, strategies for image background correction, as well as identification and analysis of image objects of interest using segmentation masks. During the afternoon session, participants will learn how to write scripts for automated execution of image processing pipelines and batch processing of multiple image files in Fiji. Example scripts will be provided using the Python and BeanShell languages. Participants should bring their own laptop with Fiji preinstalled. Instructions for installations of Fiji on Windows, Mac OSX, and Linux can be found on this website: https://fiji.sc/ June 11: HPC Data Analytics An overview of data analytics using an HPC Cluster. Topics include data transfer and storage; introduction to machine learning, including tensorflow. June 11: Software Design Learn to write code that is readable, reusable, and extensible. June 12June 13: Parallel Programming Using MPI The MessagePassing Interface is the standard for distributed parallel programming. MPI programs can run on multiple nodes of a cluster. Day 1: Basics of parallel programming. Collective Communications. Day 2: Pointtopoint communications. June 12: Bioinformatics Nextgeneration sequencing technology has evolved dramatically to enable investigation of genome/transcriptome/epigenome of any organism. Sessions in this track will introduce researchers to basic skills needed to analyze sequencing data using Rivanna, UVA's HPC cluster. June 13: Accelerating and Parallelizing MATLAB Code Covers a variety of techniques for making your MATLAB code run faster. You will identify and remove computational bottlenecks using techniques like preallocation and vectorization. On top of that, you will take advantage of multiple cores on your computer by parallelizing forloops with Parallel Computing Toolbox, and scale up across multiple computers using MATLAB Parallel Server. June 14: Parallel Programming with OpenMP and Accelerators OpenMP is a"
rc-website-fork/content/post/2019-summer-workshops.md,"programming model based upon threads that run over multiple cores of one node. OpenMP can also be used to program generalpurpose graphical processing units (GPGPUs). OpenACC, another compilerbased model for GPGPUs, will also be discussed. June 14: Scientific Visualization An exploration of tools used for visualization, including ParaView for scientific visualization, ITKSNAP for medical visualization, and Python for remote sensor data visualization."
rc-website-fork/content/post/job-postings.md,Data Analytics Center Positions Research Scientist (AI Specialist) Research Scientist (Image Processing Specialist)
rc-website-fork/content/post/2024-women-in-hpc-20240903.md,"When: Sep 03, 2023, 01:00 PM EST (US and Canada). What: Join us for a first of its kind Women in High Performance Computing (WHPC) event, cohosted by the Northeast, Purdue, and Virginia WHPC chapters to engage in discussions about current challenges and opportunities for fostering a more diverse and inclusive WHPC community. Our panelists will provide an overview of their chapters’ community activities and offer insights into navigating hurdles and fostering professional growth within HPC. The format for this event will be a brief presentation by the hosts followed by themefocused breakout rooms where participants will be invited to share their experience and personal perspective. Please join us for this critical dialogue! Panelists: Heather Baier, Graduate Student – Arts & Sciences, William & Mary Krista Valladares, MSM, Associate Director of Technology Strategy and Planning – Harvard University Paula Sanematsu, Ph.D., Sr. Research Computing Facilitator – FAS Research Computing, Harvard University Kaylea Nelson, Ph.D., Director, Arts & Sciences Research Computing – Yale University Laura Theademan, Director, Center Operations and Visualization – Rosen Center for Advanced Computing, Purdue University Discussion Topics: The role of mentors. Engaging allies. What topics/types of events are we covering (or want to cover) within our chapters? What gaps do you see in our programming or outreach efforts, and how can we fill them? What would you like to see from the WHPC chapters? Current challenges in gender diversity and inclusivity within HPC. Code of conduct: We welcome and value the diverse perspectives and experiences of all participants, ensuring a respectful and inclusive environment for meaningful dialogue."
rc-website-fork/content/post/2024-february-maintenance.md,"{{< alertgreen }}Rivanna, Research Project storage, and Research Standard storage will be down for maintenance on Tuesday, February 6 beginning at 6 a.m. {{< /alertgreen }} You may continue to submit jobs to Rivanna until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All systems are expected to return to service by 6 a.m. on Wednesday, February 7. UVA’s Facilities Management (FM) group will be updating the data center power grid during the maintenance period. This work is expected to be completed by 6 a.m. on 2/7."
rc-website-fork/content/post/2024-women-in-hpc-20241105.md,"On November 12, 2024 from 12 p.m. EST, Virginia Women in High Performance Computing (VAWHPC) will be hosting 8 students as they present research lightning talks to members of the HPC community from across Virginia. See here for details. Interested in giving a lightning talk? You’ll get 3 minutes and 12 slides to tell us all about your research. Don’t miss this chance to practice your presentation skills and share your research with a diverse audience! Sign up here by the end of today, October 24th! A helpful guide on how to give a successful lightning talk is available here This is a fantastic opportunity to explore a diverse range of topics in HighPerformance Computing (HPC) and engage with the next generation of researchers. Don’t miss out on this insightful event!"
rc-website-fork/content/post/2025-DAC-Fellowship.md,"The Scholars’ Lab, the UVA Library Digital Humanities Center (DHC), and the Data Analytics Center (DAC) are delighted to announce a new fellowship opportunity. The 2025 Data Analytics CenterDigital Humanities Center Fellowship is open for proposals from UVA faculty and graduate students. This is an experimental iteration encouraging the use of highperformance computing resources in the humanities, as well as identifying digital humanities research and teaching that could benefit from support from DHC and DAC. Possible projects might utilize AI, gaming platforms, imaging tools, geospatial technologies, use of new tools, and more. The awarded team will consist of one faculty and one graduate student collaborating on humanities research in the University of Virginia. For more information, see the complete CFP. Applications are due April 1, 2025. We highly recommend all interested applicants (either individually or as a team) email us (lbdacdhcfellowship@virginia.edu) to discuss ideas, budget possibilities, and proposed collaborations."
rc-website-fork/content/post/2024-rivanna-maintenance-dates.md,"Rivanna/Afton will be taken down for maintenance in 2024 on the following days: Tuesday, February 6 Tuesday & Wednesday, May 28 & 29 Tuesday, July 2 Tuesday, October 15 Thursday, December 12 Postponed to Tuesday, January 7, 2025 Please plan accordingly. Questions about the 2024 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2022-08-accord-commmunity-meeting.md,"When: Aug 5, 2022 11:00AM3:00PM, Eastern Time (US and Canada) UVA is proud to sponsor a community meeting to discuss ACCORD, how far we have come and where we are headed. The event is all virtual, and you may attend any or all of the topics. Access the Event: You are welcome to invite colleagues to attend the Community Meeting, especially researchers who are new to ACCORD. Agenda 11:00 – 11:10 Welcome to ACCORD 11:10 – 12:00 Technical Overview of ACCORD 12:00 – 12:15 Break 12:15 – 12:45 Demo of ACCORD 12:45 – 1:30 Brown bag lunch with users sharing their experience 1:30 – 1:50 Breakout Room Topic Discussions Session 1 1:50 – 1:55 Switch Breakout Rooms 1:55 – 2:15 Breakout Room Topic Discussions Session 2 2:15 – 2:30 Break 2:30 – 3:00 Peek into the Future of ACCORD The same topics will be covered in the two breakout sessions so that you may attend two of the topics. Breakout Room Topics: Security Architecture RUDAs/MOUs Software Applications Send your questions to ACCORDSUP@virginia.edu. See you on August 5th!"
rc-website-fork/content/post/2020-june-r_updates.md,"During the June maintenance, we made changes to R which will affect how your R programs run on Rivanna. A brief description of the changes is as follows: The gccbuilt versions of R were updated to goolfbuilt versions. The locations of the R libraries were updated. The versions of R have been streamlined to 3.4.4, 3.5.3, 3.6.3, and 4.0.0. 1. The gccbuilt versions of R have been updated to goolfbuilt versions. Instead of loading gcc before loading R, you will need to load goolf or gcc openmpi. For example: module load goolf R/4.0.0. Remember to update any Slurm scripts that have module load gcc R or module load gcc R/3.x.x. 2. The locations of the R libraries have been updated. We are changing the locations of the R libraries (i.e., the folders where local packages are installed). This change will create separate folders for different compiler versions of R, which will prevent package corruption. As a result, R will not see the packages that you had installed before the maintenance. (The only exception would be gcc openmpi R/4.0.0, which already uses the new library location). You will need to reinstall your R packages. To help with this effort, we are providing a script that will scrape the list of packages installed in an older library and will attempt to install these packages in the new library. Details are provided at ""New Libraries"". 3. The versions of R have been streamlined to 3.4.4, 3.5.3, 3.6.3, and 4.0.0. If you had hardcoded another version of R in your scripts (e.g., R/3.6.1), you will need to update your scripts to specify one of these newer versions. To see what modules would need to be loaded prior to loading R, you can use the module spider command (e.g.,module spider R/3.6.3)."
rc-website-fork/content/post/scratch-policy.md,Beginning 10/14/2019 RC system engineers will begin actively clearing /scratch files that have not been accessed for 90 days. /scratch is intended as a temporary work directory (90 days maximum). It is not backed up and needs to be purged periodically in order to maintain a stable HPC environment. We encourage users to back up their important data. RC offers several lowcost storage options to researchers. For more information about research computing storage options: Visit our Storage Overview page. Learn more about specific storage features of Rivanna HPC.
rc-website-fork/content/post/2025-04-RCExhibition-Awards.md,"{{% lead %}} Research Computing, a team within UVA Information Technology Services, recently held its second annual Research Computing Exhibition on April 23. This event showcased how researchers at UVA are pushing the boundaries of innovation using Research Computing’s services and resources. {{% /lead %}} Featuring research presentations, faculty lightning talks, and a poster competition, the exhibition underscored the growing significance and impact of computational research across the University. More than 30 students and scholars presented posters highlighting research powered by UVA Research Computing’s resources. Their work spanned a wide range of fields — including environmental science, astronomy, cancer genomics, digital health, and chemical engineering — demonstrating the breadth of disciplines benefiting from advanced computational tools. Projects included analyzing forest dieback along the Atlantic Coast, predicting health outcomes for newborns in intensive care, and investigating the chemistry of stars. In addition to the posters, faculty from across UVA delivered lightning talks—brief, highlevel presentations on how they are integrating Research Computing tools into their research. Topics ranged from exploring the universe to advancing healthcare and molecular biology. The poster competition was judged by a crossdisciplinary panel of UVA faculty, who evaluated submissions in two categories: Biological & Health Science and Engineering & Physical Sciences. Evaluations were based upon three criteria: 1) the extent to which Research Computing resources contributed to the research, 2) how clearly the use of those resources was presented on the poster, and 3) the clarity and significance of the scientific findings. Below are this year's winners of the poster competition. The winners receive a travel voucher to attend a conference of their choice where they can present their work. Thank you to all the researchers for sharing your brilliant insights and compelling results with us! Biological & Health Science 1: Henry Yeung Graduate Student, Department of Environmental"
rc-website-fork/content/post/2025-04-RCExhibition-Awards.md,"Sciences Poster: Extensive Ghost Forest Formation Across the US Atlantic Coast Abstract: Rising sea levels and climatedriven stressors are transforming coastal forests, reducing their ability to sequester carbon, protect against storms, and provide other services (e.g. timber) to coastal communities. Yet, the true extent and drivers on coastal forest loss remain poorly quantified. By mapping over 10 million individual dead trees across the US Atlantic region, we reveal pervasive mortality even in areas previously deemed resistant to sealevel rise –– an extent far greater than previous assessments had recognized. Previous studies only based their findings on a few wellstudied sites with mass dieback, largely overlooking landscapescale increase in tree mortality. By integrating deep learning and submeter imagery with the computing capacity of the Rivanna highperformance computing, we, for the first time, expose finescale vulnerability patterns that are invisible to traditional satellites across 1.2 million hectares of lowlying forests (< 5m), resolving untested assumptions on the roles of salinity versus flooding at regional scale. We find that these atrisk areas experience mortality fourfold higher than upland ecosystems, driven mainly by salinization. Our findings highlight an alarming yet underestimated scale of coastal ecosystem loss, offering scientists and policymakers urgentlyneeded insights and tangible pathways to preserve these vital habitats. 2: Navya Annapareddy Graduate Student, UVA School of Data Science (PhD) Poster: A New NICU: High Performance Compute Enabled Digital Twins for Real Time Infant Monitoring Abstract: 1 in 10 infants are born prematurely globally and receive specialized care in environments like the Neonatal Intensive Care Unit. Preterm infants are at exponentially higher risk for developmental disorders, such as cerebral palsy and autism at rates tens to hundreds of the general population. The most common diagnostic for such disorders is the general motion assessment (GMAs) heuristic carried out manually by clinicians for preterm infants"
rc-website-fork/content/post/2025-04-RCExhibition-Awards.md,"to assess neurodevelopmental (NVD) risk. As formal screening and diagnosis methods are limited by availability of trained clinicians and NICU size, we propose using computer assisted digital twins for automatic, even remote, risk assessment using a computer vision machine learning (CVML) model. We successfully develop the first clinical pose estimation framework for infants in clinical care settings meant for realtime streaming contexts. Our twostep sequential framework is comprised of two distinct models: (1) a CNN model for pose estimation of anatomic key points, and (2) a digital twin model prototype to reproduce the pose estimation CNN results in high fidelity. Each frame of a video is processed by the model pipeline and is only enabled by the vastly different capabilities of the HPC ecosystem, ranging from data storage, labeling, manipulation, training, and even validation and low latency deployment onto edge devices such as clinical tablets and computer devices. 3 Jisu Shin Graduate Student, Biochemistry and Molecular Genetics, Biomedical Sciences Graduate Program Poster: Multimodal Singlecell Sequencing Reveals Cellular Heterogeneity in LGL Leukemia Abstract: Large granular lymphocytic leukemia (LGLL) is a rare lymphoproliferative disorder characterized by clonal expansion of cytotoxic lymphocytes. To investigate the cellular heterogeneity and molecular mechanisms of LGLL, we performed singlecell multiomics analysis, integrating transcriptomic, proteomic, and TCR sequencing data from patients treated at UVA, a national referral center for this rare disease. Given the scale and complexity of the data, our analyses required UVA’s highperformance computing (HPC) resources, Rivanna and Afton. Parallelization and batch corrections in preprocessing steps significantly accelerated data analysis, making largescale singlecell processing feasible. However, downstream analyses required careful stepbystep validation and could not be automated through job submissions. The ability to run interactive jobs with large memory and multiple cores was essential for ensuring accuracy and efficiency in data integration and interpretation, enabling insights"
rc-website-fork/content/post/2025-04-RCExhibition-Awards.md,"into LGLL pathogenesis that would be computationally infeasible on standard systems. Engineering & Physical Sciences 1: Mélisse BonfandCaldeira PostDoctoral Fellow, Departments of Astronomy and Chemistry Poster: Unlocking the chemistry of stars with highperformance computing Abstract: Complex organic molecules, the elemental building blocks of life, are known to form in space within giant clouds of interstellar gas and dust that eventually give rise to stars and planets. To understand how these molecules form and evolve, surviving the harsh conditions of space, until they eventually become incorporated into planetary systems, we harness the computing power of Rivanna/Afton to run thousands of numerical simulations, modeling the chemistry of various starforming environments. We have developed an automated pipeline that then transforms astrochemical model outputs into synthetic data, mimicking real astronomical observations. By comparing observations of starforming regions with a large grid of synthetic data, we are able to identify the models that best reproduce the observations, unveiling the region’s key physical properties and the chemical reactions that shape its molecular complexity. 2: Md Jakir Hossen Graduate Student, Chemical Engineering Poster: Elucidating Biomolecular Surface Hydrophobicity Abstract: Many biophysical processes are driven by the removal of water molecules, known as hydrophobic interactions, near complex biomolecular surfaces such as proteins, enzymes, and membranes. While the hydrophobicity of individual nonpolar, polar, and charged amino acids is relatively simple to predict, collective watermediated interactions on heterogeneous surfaces remain challenging due to nonadditive effects. In this study, we designed selfassembled monolayers (SAMs) with varying spacings of hydroxyl, ammonium, and guanidinium groups—common in amino acid sequences—to investigate hydrophobic behavior in a nonpolar environment. Using molecular dynamics (MD) simulations on Rivanna's highperformance computing resources, we analyzed the binding affinity of a model hydrophobic protein to these surfaces and employed indirect umbrella sampling (INDUS) simulations to compare their hydrophobicity. Our findings reveal the"
rc-website-fork/content/post/2025-04-RCExhibition-Awards.md,"role of hydrogen bonding dynamics, binding affinity, and the free energy of water removal, emphasizing the impact of functional group spacing. These insights provide a foundation for studying more realistic peptidefunctionalized surfaces, where such functional groups are prevalent in a nonpolar background."
rc-website-fork/content/post/2020-dcos-maintenance.md,The DCOS cluster maintenance scheduled for 6/4/2020 has been postponed. A new date and time for this system outage will be announced in the near future.
rc-website-fork/content/post/2018-spring-workshops.md,"School of Medicine Research Computing provides training opportunities covering a variety of data analysis, basic programming and computational topics. Workshops break roughly into the three main areas relevant to computationallyintensive research: code, data, and computing. All of the classes are taught by RC experts and are freely available to UVa faculty, staff and students. R / R package development Python Matlab Biomedical Image Processing Bioinformatics on HPC Data manipulation Data visualization Databases Machine Learning Cloud Computing Containers Rivanna (HPC) Ivy (Secure Computing) View Workshops"
rc-website-fork/content/post/2022-knl.md,"Rivanna has included Intel KNL (Knight's Landing) nodes for several years. This was a unique architecture, not well suited for general use, and the manufacturer has stopped producing or supporting this type of hardware. As a result, the KNL nodes will be removed from Rivanna on June 30, 2022. Rivanna System Details"
rc-website-fork/content/post/2020-september22-matlab-seminar.md,"MathWorks engineers will offer a free live webinar on September 22nd from 2:00 to 3:30 Eastern time. Overview Deep learning can achieve humanlike accuracy at tasks such as naming objects in a scene or recognizing optimal paths in an environment. Sometimes it can even exceed human performance, recognizing nonobvious patterns in image or signal data. In this new neuroscience seminar, we’ll illustrate the fundamentals of deep learning in MATLAB. Using an agelabeled BIDS dataset from the OpenNeuro repository, we’ll train a deep network to accurately classify the age range of normalized human MRI brain images, not obviously discernible by human inspection. Highlights Along the way, participants will learn many aspects of the deep learning workflow: Load and manage large sets of images Import pretrained models such as ResNet Set up transfer learning via network modification Get to network training quickly with apps for preprocessing and augmenting training image data Configure network training parameters Validation of convergence during deep model training Interoperability with open source deep learning frameworks (i.e.,TensorFlowKeras, Caffe, PyTorch, etc.,) using ONNX Accelerate algorithms on NVIDIA® GPUs, cloud, and datacenter resources without specialized programming."
rc-website-fork/content/post/2020-december-maintenance-announcement.md,"{{< alertgreen }}Rivanna will be down for maintenance on Wednesday, December 16 & Thursday, December 17, beginning at 6 a.m. on December 16.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Rivanna is expected to return to service later in the day on December 17. The instructional queue will not be available until January 10. IMPORTANT MAINTENANCE NOTES Modules The following software modules will be removed from Rivanna during the maintenance period: gcc/8.3.0 (replaced by 9.2.0) The following upgrades will take place during the maintenance period: Changes to the default compiler and related toolchain versions: gcc/7.1.0 9.2.0 intel/18.0 20.0 cuda/10.2.89 11.0.228 Most dependent modules have been installed under the new default, except for R. R users must specify the previous goolf/iintelmpi version explicitly as goolf/7.1.03.1.4 or iintelmpi/18.018.0. Some dependent modules have been removed from the previous default compiler/toolchain version. If you need to use a module under the previous version, please contact our user services team at hpcsupport@virginia.edu. Users are reminded to recompile their code when switching to a different compiler/toolchain. Upgrades to default versions of applications: cellranger/3.1.0 4.0.0 go/1.8.1 1.13.4 julia/1.3.1 1.5.3 ase/3.17.0py3 3.20.1 ffmpeg/4.3.1 (under gcc/9.2.0) with x264 and x265 codecs If you need to use a nondefault version of an application, please specify the version when you load the module. Use module spider to find prerequisites. New nodes and tools: Two RTX 2080 Ti nodes (10 GPU devices each) in gpu partition use gres:rtx2080 in Slurm script Visual Studio Code Server on Open OnDemand nvhpc/20.9 NVIDIA HPC SDK (CUDA 11.0) awscli/2.1.10 command line interface to Amazon Web Services texlive/2020 LaTeX jq/1.6 JSON processor qiime2/2020.8 microbiome bioinformatics platform with Empress"
rc-website-fork/content/post/2020-december-maintenance-announcement.md,"and PICRUSt2 plugins cellrangeratac/1.2.0 lightgbm/2.3.1 CLI for gradient boosting framework Docker Hub container registry We are now using Docker Hub as our official container registry. Details are available here. Many of these container images are very new (e.g. pytorch:1.7.0, tensorflow:2.4.0) and have not been installed as modules on Rivanna, but you are welcome to use them by following the instructions on the Docker Hub repository page. If you have any questions or concerns about maintenance day, please contact our user support team at hpcsupport@virginia.edu prior to 12/16."
rc-website-fork/content/post/2023-women-in-hpc-20230919.md,"VAWHPC September Event Leadership Journeys Time: Sep 19, 2023, 01:00 PM EST (US and Canada). Join us for our next community event featuring Dr. Neena Imam as she shares her personal view of challenges and successes experienced throughout her inspiring leadership journey in research, HPC and AI computing. Come learn about career strategies, ask questions, and contribute to our discussion of how the playing field may be leveled to offer equitable IT & HPC leadership opportunities for women and minorities. Dr. Imam earned a PhD in Electrical Engineering and has been engaged in research and computing in a variety of roles. She was a Science Fellow to Senator Lamar Alexander, a Research Scientist and Deputy Director of Research Collaboration, Computing and Computational Sciences at Oak Ridge National Laboratory, and currently leads research engagement at NVIDIA for the Americas regions. Featured speaker: Neena Imam Director of Researcher Engagement at NVIDIA Attendees are invited to share their own experiences and engage with the speaker during this interactive Q&A session. This virtual event is jointly hosted by Virginia Commonwealth University, George Mason University, Virginia Tech, William & Mary, University of Richmond, Virginia Institute of Marine Science, Old Dominion University, ACCESS, and the University of Virginia in cooperation with NVIDIA. VAWHPC October Event Student Lightning Talks Time: Oct 31, 2023, 01:00 PM EST (US and Canada). VAWHPC will be hosting 1012 students as they give lightning talks on their research to members of the HPC community from across Virginia. Interested in giving a lightning talk? You’ll get 3 minutes and 12 slides to tell us all about your research. Don’t miss this chance to practice your presentation skills and share your research with a diverse audience! Send us the title of your presentation before September 29. A helpful guide on how to give a"
rc-website-fork/content/post/2023-women-in-hpc-20230919.md,successful lightning talk is available here.
rc-website-fork/content/post/2023-december-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on Monday, December 18, 2023 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All systems are expected to return to service by 6 a.m. on Tuesday, December 19. IMPORTANT MAINTENANCE NOTES The operating system will be upgraded to Rocky 8.7 with system glibc 2.28 and GCC 8.5.0. Due to fundamental changes in system libraries, the entire software stack is rebuilt. Users should rebuild all selfcompiled codes and R packages. Starting Nov 21, users who want early access to the development environment to rebuild/test codes against the new software stack can log in by running ssh udcba3336 on the frontend. (Please be sure not to overwrite your existing codes for the production environment.) Contact us here if you need assistance. Modules Compilers and toolchains have been consolidated to the following: GCC: gcc/11.4.0, goolf/11.4.04.1.4 Intel: intel/2023.1 (default), intel/2024.0 (experimental), intel/18.0 (legacy) NVIDIA: nvhpc/23.7, nvompi/23.74.1.5 Singularity has been renamed to Apptainer. Load the apptainer/1.2.2 module for containers. (The singularity command is provided as an alias.) All users can now build containers directly on Rivanna; see here for details. There are many module version upgrades and deprecation of older versions. Run module spider NAME to check the available versions and the corresponding load command. Contact us here if you need a different version. Only the most important changes are listed below: {{< table title=""Replacements"" class=""table tablestriped"" }} |Name |Default version|Other versions|Removed| ||||| |OOD JupyterLab | 3.6.3 | | 2.2.9 | |OOD RStudio Server | 2023.06.2 | | 1.0.143, 1.1.463, 1.3.1073, 2023.03.0 | |anaconda |2023.07py3.11 | | 2019.10py2.7, 2020.11py3.8| |clang |15.0.7 | | 10.0.1 |"
rc-website-fork/content/post/2023-december-maintenance.md,"|cuda |12.2.2 |10.2.89, 11.4.2| 10.1.168, 11.0.228 | |gcc |11.4.0 | | 7.1.0, 9.2.0, 11.2.0 | |go |1.21.4 | | 1.18.4, 1.19.4 | |intel |2023.1 | 18.0, 2024.0 | 20.0, 2022.11 | |julia |1.9.2 | | 1.5.3, 1.6.0 | |llvm |15.0.7 | | 4.0.0 | |netcdf |4.9.2 | | 4.6.2, 4.7.3, 4.7.4 | |nvhpc |23.7 | | 21.9 | |perl |5.36.0 | | 5.24.0 | |python |3.11.4 | 2.7.18, 3.9.16 | 2.7.16, 3.6.6, 3.6.8, 3.7.7, 3.8.8 | |pytorch |2.0.1 | 1.12.0 | 1.8.1 | |R |4.3.1 | | 3.5.3, 3.6.3, 4.0.3, 4.1.1, 4.2.1 | |ruby |3.1.2 | | 2.3.4 | |rust |1.66.1 | | 1.38.0, 1.41.0 | |spark |3.4.1 | | 3.1.2 | |tensorflow |2.13.0 | | 2.7.0, 2.10.0 | |texlive |2023 | | 2020 | {{< /table }} Special reminders C/C++/Fortran users who must build code with GCC 7 or older should containerize the application starting with the official GCC base image. Contact us if you need assistance. Intel 18.0 modules are either migrated to the newer version (2023.1) or dropped. Intel users should rebuild code with intel/2023.1 if possible. RStudio Server is now backed by a native module with R as a dependency. R packages installed via the R module will be detected automatically in RStudio Server, and vice versa. All R packages will need to be rebuilt. Python 2.7dependent modules are completely removed from the software stack. Users of legacy Python code can create a custom environment using the anaconda or mamba (recommended) module. Code Server is backed by a native module instead of a container. This allows usage of compilers and interpreters on Rivanna. Python users please see instructions here. Mamba is separated from anaconda into its own module. Java module versions are standardized to 7, 8, 11, 12 (previously 1.7.0, etc.). Rebuilding R Libraries Due to"
rc-website-fork/content/post/2023-december-maintenance.md,"changes in the operating system and compilers on Rivanna, your existing R libraries will not work. We have created a command that will help you to rebuild your library for the new version of R. The command is updateRlib and requires two pieces of information: i) How you run your R code ii) The version of R that you have been using. For example, if you have been using with RStudio 1.3.1073 R 4.1.1, you can type: updateRlib OOD 4.1.1 This command will capture your packages that were used in your R/4.1 library for Open OnDemand and rebuild to a new library. The three options for how you run your code are: OOD, goolf, or intel. Rebuilt libraries will be installed in ~/R/goolf/4.3 for both module and OOD versions."
rc-website-fork/content/post/2022-rivanna-maintenance-dates.md,"Rivanna will be taken down for maintenance in 2023 on the following days: Tuesday, March 7 Tuesday, May 30 Tuesday, October 3 Monday, December 18 Please plan accordingly. Questions about the 2023 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2020-december-maintenance-update-2.md,"In order to expedite users' access to the system, Rivanna will be returned to service in 2 separate phases: During phase 1, which is expected to be completed in fewer than 72 hours from now, the standard, GPU, largemem, and frontend nodes will be reactivated. Rivanna's parallel nodes will be released in phase 2 which should be finished within the 72hour timeframe."
rc-website-fork/content/post/2019-september-scratch-notes.md,"{{% callout %}} As part of the September 2019 maintenance for Rivanna, the Research Computing team replaced the /scratch file system with newer hardware. The systems engineers did transfer files that were no older than 90 days to the new system. If you are missing files, you will be able to retrieve them until the next maintenance (planned for December 2019). As a reminder, /scratch is temporary storage and files older than 90 days are subject to purging. {{% /callout %}} Transferring your files The path to your folder on the old scratch file system is /oldscratch/$USER. This folder has been changed to readonly. You do not have permission to write or execute files from that location. To transfer your files, we recommend using the rsync command within a shell (i.e., Terminal Window). If you normally use Open onDemand or JupyterLab, you can open a Terminal Window by logging into Open onDemand (https://ood.hpc.virginia.edu); clicking on Files Home Directory ; clicking on Open in Terminal Transferring a single file {{% callout %}} To copy a file from /oldscratch to /scratch, you can type (for example): rsync av /oldscratch/$USER/somefolder/myFile /scratch/$USER/somefolder {{% /callout %}} Transferring a folder {{% callout %}} To copy a folder and its contents from /oldscratch to /scratch, you can type (for example): rsync av /oldscratch/$USER/somefolder/ /scratch/$USER/somefolder {{% /callout %}} Notice the trailing slash at the end of the first somefolder, and the lack of a slash at the end of the second somefolder. The placement of the slash is important for how the transfer is done. The slash at the end of the first folder refers to the contents of the folder (in this case, all files within somefolder). Whereas, no slash at the end of the second folder instructs the computer to place the files directly in that"
rc-website-fork/content/post/2019-september-scratch-notes.md,"folder. If you do include a slash at the end of the second folder, the computer will create a new folder under the existing folder. So, you would have /scratch/$USER/somefolder/somefolder. Need Help? If you have questions or need help with transferring your files, Research Computing will hold a ""Clinic on Scratch"" on Wednesday, September 18 in Brown Library, room 145 from 3 to 5 pm. More about scratch"
rc-website-fork/content/post/2022-globus-scratch-transfer.md,"Globus is a simple, reliable, and fast way to access and move your research data between systems. Researchers can transfer data from their old scratch directories to new scratch via a single web interface no software download or installation is necessary. Old scratch (/oldscratch) directories are now readonly. You will not be able to write new files to /oldscratch or copy files from /scratch to /oldscratch. {{% callout %}} Users who need help using Globus to transfer their /scratch files are invited to attend one of the following online tutorial sessions: Thursday, September 29 (2 to 4 p.m.) Friday, September 30 (10 to 11 a.m.) Monday, October 3 (10 to 11 a.m.) Friday, October 7 (10 to 11 a.m.) The old scratch system will be permanently retired on October 31. {{% /callout %}} Transferring Data Selecting data from old scratch Go to https://app.globus.org/filemanager. Choose ""University of Virginia"" as your institution and log in using Netbadge. Click the ""Collection"" field. Search for and select ""UVA MainDTN"". Doubleclick the ""/oldscratch"" folder and then doubleclick the folder with your computing ID (only your own computing ID will be visible). Select the files and folders you want to transfer. Select destination for files on /scratch Click ""Transfer or Sync to..."" in the menu to open up the second collection panel. You can also do this by clicking the twopanel icon in the Panels menu in the top righthand corner of the app. Click the ""Collection"" field in the newly opened panel. Search for and select ""UVA MainDTN"". It should now be in your ""Recent"" collections since you selected it previously. Doubleclick the ""scratch"" folder and then doubleclick the folder with your computing ID. If you have already created new scratch folders you can doubleclick them to select them as a destination for your old"
rc-website-fork/content/post/2022-globus-scratch-transfer.md,"scratch files. If you want the files in the top level of your scratch folder then do not select anything. Transfer Click the highlighted ""Start"" button to begin the transfer. This should be the ""Start"" button on the /oldscratch Panel. Your transfer will begin. You can monitor your transfer in the ""Activity"" tab of the Globus app. You will receive an email when the transfer is complete. {{% callout %}} To transfer data from /oldscratch to local storage like your laptop or lab/departmental storage, you will need to install Globus on your workstation. Please see our documentation for Globus installation and data transfer between systems. {{% /callout %}}"
rc-website-fork/content/post/2020-september15-matlab-seminar.md,"MathWorks engineers will offer a free live webinar on September 15th from 2:00 to 3:00 Eastern time. Overview Automated image labeling helps to reduce the cost and time that it takes to label your dataset. MATLAB significantly reduces the time required to preprocess and label datasets with domainspecific apps for audio, video, images and text data. Whether you train your models in MATLAB or Python, we support your entire image processing and AI workflow, from acquisition to deployment. In this session we show you how to use apps for labeling image and video data to build AI models. We explore preprocessing to facilitate feature extraction and present approaches to building models in an iterative fashion, validating predicted labels and incorporating onthefly models to label large datasets. We also discuss an approach to automating pixellevel labeling for semantic segmentation workflows. Highlights Preprocess and label datasets faster with domainspecific apps for audio, video, images and text data Use interactive apps to label, crop and identify important features and automate the process of labeling Create, visualize and edit deep learning networks with our easytouse Deep Network Designer app Incorporate deep learning models without having to create complex network architectures from scratch Accelerate development and training of deep learning networks with GPUs, clusters and cloud resources"
rc-website-fork/content/post/featured-projects.md,"UVA Research Computing strives to empower researchers to achieve more through the use of cuttingedge computational resources. This has led to fruitful collaborations with researchers and staff across grounds, including these groups and departments: Astronomy Biochemistry and Molecular Genetics Biomedical Engineering Center for Applied Biomechanics Center for Advanced Medical Analytics Center for Behavioral Health and Technology Center for Diabetes Technology Center for Public Health Genomics Economics Emergency Medicine Environmental Sciences Infectious Diseases Public Health Sciences Materials Science & Engineering Pediatrics–Neonatology Radiology and Medical Imaging Surgery UVA Sinklab To browse a gallery of recent projects, visit our Projects page below. Browse Projects"
rc-website-fork/content/post/2025-workshop-survey.md,Please fill out the survey below to help us shape the future of Research Computing’s training initiatives. Your insights ensure the continued delivery of relevant and impactful learning opportunities for the research community. Link to Survey : https://virginia.az1.qualtrics.com/jfe/form/SV6PDQmrw5AbCrVR4
rc-website-fork/content/post/2025-jan-maintenance.md,"{{< alertgreen }}The HPC cluster will be down for maintenance on Tuesday, Jan 7, 2025 beginning at 6 am.{{< /alertgreen }} All systems are expected to return to service by Wednesday, Jan 8 at 6 am. IMPORTANT MAINTENANCE NOTES How should I prepare and what to expect on Jan 7, 2025? You may continue submitting jobs to the HPC system until the maintenance period begins. However, if the system determines your job won't finish in time, it will not start until the system is back online. The maintenance will involve upgrading the storage client, requiring all compute and login nodes, including the Open OnDemand and FastX portals, to be taken offline. Additionally, Research Project Storage and home directories will be inaccessible. Research Standard Storage will remain accessible via SMB and NFS mounts. The UVA Standard Security Storage Data Transfer Node (DTN) will remain operational throughout the maintenance period. Reminder: New Service Unit pricing and consumption rates Effective January 8, 2025, Research Computing will implement a new Service Unit (SU) and pricing schedule for HPC services. Standard and instructional SU allocations will remain free of charge. Jobs that start after the maintenance period will be charged based on the updated SU consumption rates determined by the type of hardware utilized. For details on the changes, see the pricing table and SU consumption rates. Additionally, the default memory allocation per CPU core will decrease to 4GB, reflecting typical usage patterns and aligning with the updated SU model. This change provides more granular control, as SU charges will now account separately for CPU, memory, and specialty hardware like GPUs. If your job encounters an ""OutofMemory"" error, adjust your memory request accordingly. Reminder: “Dedicated Computing” as a new service model This model allows researchers to lease hardware managed by Research Computing (RC) as an"
rc-website-fork/content/post/2025-jan-maintenance.md,"alternative to purchasing their own equipment. It provides dedicated access to HPC resources with no wait times. See here. Removed old DNS names The old Domain Name System (DNS) entries for logging into Rivanna/Afton HPC have been removed. Please refer to the table below for the updated login names. |Old|New| ||| |rivanna.hpc.virginia.edu |login.hpc.virginia.edu| |rivannadesktop.hpc.virginia.edu |fastx.hpc.virginia.edu| |rivannaportal.hpc.virginia.edu | ood.hpc.virginia.edu| Modules Apptainer will be upgraded from 1.2.2 to 1.3.4. There is no change to the containers themselves, and users do not need to rebuild their own containers. The following modules will be removed from Rivanna during the maintenance period. {{< table title=""replacement"" class=""table tablestriped"" }} | Module | Removed version | Replacement | |||| |amber | 22.0 | 24CUDA12.2.2 | |apptainer| 1.2.2 | 1.3.4 | |blender | 3.2.1, 3.4.1 | 3.6.17 | |diamond | 2.0.14 | 2.1.6 | |freesurfer| 6.0.1 | 7.2.0 | |gatk | 4.3.0.0, 4.5.0.0 | 4.6.0.0 | |irfinder | 1.3.1 | 2.0.1 | |kraken2 | 2.1.2 | 2.1.3 | |ncbivdb | 3.0.2 | 3.1.1 | |orca | 5.0.2 | 5.0.4, 6.0.0 | |rapidsai | 23.10 | 24.06 | |rust | 1.66.1 | 1.79.0 | |scons | 4.2.0 | 4.5.2 | |smrtlink | 12.0.0.177059 | 13.1.0.221970 | |spaceranger| 2.0.1 | 3.1.1 | |sratoolkit| 3.0.2, 3.0.3 | 3.1.1 | {{< /table }}"
rc-website-fork/content/post/2020-september22-maintenance.md,"{{< alertgreen }} Rivanna will be down for maintenance on Tuesday, September 22, beginning at 8:30 a.m. It is expected to return to service later in the day. {{< /alertgreen }} RC engineers will be installing new hardware that is needed to stabilize the /scratch filesystem. You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. If you have any questions or concerns about the maintenance period, please contact our user support team at hpcsupport@virginia.edu."
rc-website-fork/content/post/2025-hpc-maintenance-dates.md,"Rivanna/Afton will be taken down for maintenance in 2025 on the following days: Winter: Tuesday, January 7 Spring: May 27, 2025 Summer: August 12, 2025 Fall: Oct 14, 2025 Please plan accordingly. Questions about the 2025 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2022-women-in-hpc-202208.md,"Topic: Women in HPC & IT Leadership Roles. When: October 12, 2022 01:00 PM, Eastern Time (US and Canada). Join us for our Fall community meeting to hear from female leaders in the HPC & IT field sharing challenges and successes experienced throughout their careers. Don’t miss this fantastic opportunity to learn about career strategies, share your experience, and contribute to our discussion of how the playing field may be leveled to offer equitable HPC & IT leadership opportunities for women and minorities. Attendees are invited to share their own experiences and engage with panelists during this interactive Q&A session. Featured Panelists: Robin Bryan AVP for Information Technology/CIO at James Madison University. Corinne Picataggi Chief Technology Officer at the College of William and Mary. Moderator Gladys Andino, Senior Computational Scientist at the University of Virginia. Virginia WHPC is committed to increasing diversity and inclusion by promoting and encouraging the participation of women in highperformance computing and related fields. https://vawhpc.org/"
rc-website-fork/content/post/2020-march-rivanna-software.md,"{{< alertgreen }}The Rivanna maintenance has been completed on March 11 and the system is back in service.{{< /alertgreen }} The following software modules have been removed from Rivanna during the maintenance period. Please use the suggested newer versions: gcc/5.4.0 & toolchains 7.1.0 All modules that depend on gcc/5.4.0 are now available under gcc/7.1.0. The only exception is cushaw3/3.0.3. Please contact us if you need to use it. pgi/19.7 & toolchains 19.10 All modules that depend on pgi/19.7 are now available under pgi/19.10. anaconda/5.2.0py2.7 2019.10py2.7 All modules that depend on anaconda/5.2.0py2.7 are now available under anaconda/2019.10py2.7. tensorflow/1.6.0py27, py36 1.12.0, 2.0.0, or 2.1.0 If you must use version 1.6.0, please pull the image from our repository on Singularity Library. singularity/3.1.1 3.5.2 boost/1.66.0 1.68.0 julia/1.0.2, 1.0.3 1.3.1 cushaw3/3.0.3 no replacement The following upgrades have taken place during the maintenance period: JupyterLab v1.2.6 Python 3.7 Jupyter kernel based on anaconda/2019.10py3.7 The previous ""Python 3"" kernel (based on anaconda/5.2.0py3.6) has been renamed as ""Python 3.6"". anaconda/2019.10py2.7, py3.7 gcc/9.2.0 & toolchains gcc/7.1.0 remains the default singularity/3.5.2 now default version gurobi/9.0.1 tensorflow/2.1.0py37 Singularity container module & Jupyter kernel julia/1.3.1 module & Jupyter kernel ansys/2020r1 samtools/1.10 rust/1.41.0 cmake/1.16.5 New tools: LibreOffice through FastX Web desktop environment pytorch/1.4.0 Singularity container module & Jupyter kernel openfoam 7 (version 1909) opensource CFD software goolfc/6.5.03.1.410.1.168 GOOLF toolchain (GCC + OpenMPI + OpenBLAS + ScaLAPACK + FFTW) with CUDA support atom/1.43.0 Atom text editor rclone/1.51.0 Rclone for cloud file syncing (supports Google Drive) nodejs/12.14.1 Node.js JavaScript runtime environment ninja/1.10.0py3.6 Ninja build system meson/0.53.1py3.6 Meson build system gtk+/3.24.14 GTK+ 3 libraries for GUI applications fribidi/1.0.8 Free Implementation of the Unicode Bidirectional Algorithm atk/2.28.1 Accessibility Toolkit tree/1.8.0 Tree structure of file system gnupg/2.2.19 GnuPG encrypt and sign data"
rc-website-fork/content/post/afton-dedication.md,"﻿+++ images = [""""] author = ""Staff"" description = """" date = ""20241015T00:00:0005:00"" title = ""Afton Cluster Dedicated to Prof. John Hawley"" draft = false tags = [""afton"",""hpc""] categories = [""feature""] +++ On September 16, 2024, RC dedicated the new Afton computing cluster to the memory of John F. Hawley (19582021), late Professor of Astronomy who was a leading researcher in computational astrophysics. He also served in the Office of the Dean of the College and Graduate School of Arts and Sciences for nine years, first as Associate Dean for the Sciences and later as Senior Associate Dean for Academic Affairs. The ceremony featured some remarks by Josh Baller, Associate Vice President for Research Computing, and Scott Ruffner, Director of Infrastructure for Research Computing, along with a recorded message from Provost Ian Baucom. John received his PhD in Astronomy from UIUC in 1984 with a dissertation on numerical studies of accretion disks around black holes. He moved to the California Institute of Technology as a Bantrell Fellow postdoc in the summer of that year. He was deeply involved with the national supercomputing centers from the earliest days. He returned to Illinois in October 1985 to prepare some “showcase” applications for the January 1986 opening of the National Center for Supercomputing Applications, housed at the University of Illinois at UrbanaChampaign. John served on advisory committees and review panels for the national centers throughout his career. He moved to UVA in the fall of 1987 as an assistant professor in the Astronomy Department, where he continued his research on numerical simulations of blackhole accretion disks. At UVA he met Steven Balbus, with whom he collaborated on an important series of papers from 1991 to 1992 that unraveled one of the fundamental mysteries of accretiondisk dynamics. They were jointly awarded the Shaw Prize"
rc-website-fork/content/post/afton-dedication.md,"in Astronomy in 2013 for these discoveries. Even though he primarily used the national centers for his own research, John was devoted to promoting computational science from the beginning of his time at UVA. In April of 2001, he chaired a Universitywide committee whose outcome was the acquisition of the University’s first “Beowulf” cluster in May of 2002. From 20062008, John worked with other faculty to repurpose part of what at the time was research computing software support within ITS into the first dedicated group for more advanced research computing support. The new group was given the name UVACSE (University Alliance for Computational Science and Engineering) and Andrew Grimshaw, Professor of Computer Science, was its first director. James Hilton, then VP for IT, provided budgetary support and made the group answer to a facultyled Computational Science Advisory Council. John continued to serve on committees such as UCIT (University Committee for Information Technology), other advisory committees, and multiple task forces, working with other faculty on projects related to computational education and hardware acquisition. This culminated in the purchase in 2014 of the University’s first highend computational resource, Rivanna, a goal he had sought for years. Ivy, the first shared resource for sensitive data processing, was also initiated through the work of these committees. Even with his strong advocacy for hardware, John was an even stronger champion of developing a professional support group for research computing. He often stated that while hardware was important, what really mattered was people. The University has responded by building Research Computing into the acclaimed group it is today. {{< rawhtml }} Left: Cray1 at NCSA, October 1985. Right: Frontera at TACC, August 2019. {{< /rawhtml }}"
rc-website-fork/content/post/2020-december-maintenance-update-3.md,"Phase 1 of the maintenance period is now complete. Rivanna’s standard, GPU, largemem, and frontend nodes have been returned to service. Pending jobs that were submitted before the maintenance began need to be resubmitted. Rivanna’s parallel nodes will be released in phase 2 which we anticipate being finished by 5 p.m. on Tuesday, December 22."
rc-website-fork/content/post/2024-july-scratch-purge.md,"On Sep 1, 2024 RC system engineers will reinstate a file purging policy for personal /scratch folders on the Afton and Rivanna highperformance computing (HPC) systems. From Sep 1 forward, scratch files that have not been accessed for over 90 days will be permanently deleted on a daily rolling basis. This is not a new policy; it is a reactivation of an established policy that follows general HPC best practices. The /scratch filesystem is intended as a temporary work directory. It is not backed up and old files must be removed periodically to maintain a stable HPC environment. {{% callout %}} Key Points: Purging of personal scratch files will start on Sep 1, 2024. Files that have not been accessed since Jun 3, 2024 will be deleted on that day. Directories that have been emptied as part of the file purging process will be removed as well. The /scratch filesystem is not backed up. Users should back up important scratch data to other storage options on a regular basis. Eligible PIs can request 10TB of Research Standard storage for their groups at no charge. Learn more about all storage options. {{% /callout %}} {{% highlight %}} Do you have additional questions? Please contact our user services team, or join us for our virtual office hours every Tuesday, 35 p.m. and Thursday, 1012 p.m.. {{% /highlight %}} Communications {{% accordiongroup title=""Emails"" id=""emailgroup"" %}} {{% accordionitem title=""Reinstatement of purging policy for personal scratch files on Afton and Rivanna"" id=""email3"" %}} Dear HPC user, On Sep 1, 2024 RC system engineers will reinstate a file purging policy for personal /scratch folders on the Afton and Rivanna highperformance computing (HPC) systems. From Sep 1 forward, files that have not been accessed for over 90 days will be permanently deleted on a daily rolling basis."
rc-website-fork/content/post/2024-july-scratch-purge.md,"This is not a new policy; it is a reactivation of an established policy that follows general HPC best practices. The /scratch filesystem is intended as a temporary work directory. It is not backed up and old files must be removed periodically to maintain a stable HPC environment. How should I prepare? We encourage you, if you are using scratch, to identify and move important files to more persistent storage solutions on a regular basis. The FAQ “How can I find out what files will be purged?” provides instructions to identify the scratch files marked for deletion. RC offers several lowcost storage options to researchers, including 10TB of Research Standard storage for each eligible PI at no charge. For more detailed descriptions of our storage options, visit [https://www.rc.virginia.edu/userinfo/storage/]. What to expect on Sep 1, 2024? Starting on Sep 1, 2024, a script will be launched on the Afton and Rivanna systems to monitor on a daily basis if scratch files have been accessed within the past 90 days. Last access times are determined as the last time a file was opened (read) or modified (written to). Scratch files that have not been accessed since Jun 3 (90 days before Sep 1) will be removed from the filesystem. The 90day file purging window will move forward on a daily basis. If you have any questions about the file purging policy or process, you may contact our user services team. With regards, Karsten Siller Director, Research Computing User Services Information Technology Services University of Virginia {{% /accordionitem %}} {{% /accordiongroup %}} FAQ {{% accordiongroup title=""Group"" id=""faqgroup""%}} {{% accordionitem title=""1. Why are files being deleted? "" id=""faq1"" %}} Scratch is intended as a temporary work directory, not for longterm storage. It is not backed up, and old files need to be purged periodically"
rc-website-fork/content/post/2024-july-scratch-purge.md,"to maintain system stability and filesystem performance. This is generally an established best practice at HPC centers. {{% /accordionitem %}} {{% accordionitem title=""2. How does the file purging work? "" id=""faq2"" %}} Starting Sep 1, 2024, the Afton and Rivanna systems execute a daily script that identifies the last access time for each scratch file. Each day the script will permanently delete those files identified with an access time older than 90 days. Directories that are left empty as a result of the file purging process will be removed as well. {{% /accordionitem %}} {{% accordionitem title=""3. How is a file’s last access time being determined? "" id=""faq3"" %}} A file’s last access corresponds to the date and time that file was last opened (read) or modified (written to). {{% /accordionitem %}} {{% accordionitem title=""4. How can I find out what files will be purged? "" id=""faq4"" %}} Log in to Afton/Rivanna and from the command line, run: checkscratchforpurge outfile where outfile is the path of the file to which you wish to save the results. This will save to your outfile a list of files ordered from the oldest last accessed to the most recently accessed that will be purged. Alternatively, you can access the Check Scratch For Purge tool available in the Utilities dropdown on Open OnDemand. From there, you can view a list of files being purged and save the list to /home, /scratch, or download it locally. {{% /accordionitem %}} {{% accordionitem title=""5. What should I do with files that I still need? "" id=""faq5"" %}} We encourage users to back up their important data. Data can be transferred to either your home storage (50G) or leased storage (if applicable). For more details, see ""What storage options does RC provide?"" Learn more about available data transfer"
rc-website-fork/content/post/2024-july-scratch-purge.md,"tools. {{% /accordionitem %}} {{% accordionitem title=""6. Can deleted files be restored if needed later? "" id=""faq6"" %}} No. Scratch is a highperformance filesystem without any snapshots or backups. Deleted files cannot be restored. {{% /accordionitem %}} {{% accordionitem title=""7. What storage options does RC provide? "" id=""faq7"" %}} RC offers several lowcost storage options to researchers. Home directory storage provides up to 50G with daily and weekly snapshots of data. Eligible PIs can request 10TB of Research Standard storage at no charge. PIs also have the option to purchase additional leased storage. Learn more about storage options and how to purchase storage. {{% /accordionitem %}} {{% accordionitem title=""8. How can I get help? "" id=""faq8"" %}} If you have any questions, please contact our user services team, or join us for our virtual office hours every Tuesday, 35 p.m. and Thursday, 1012 p.m.. {{% /accordionitem %}} {{% /accordiongroup %}}"
rc-website-fork/content/post/2023-may-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on May 30, 2023 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All systems are expected to return to service by 6 a.m. on Wednesday, May 31. IMPORTANT MAINTENANCE NOTES Five RTX3090 nodes (4 GPU devices each) have been added to the gpu partition use gres=gpu:rtx3090 in Slurm script. Modules The toolchains gompic gcccuda goolfc will be removed from Rivanna during the maintenance period, since we now have CUDAaware toolchains based on gcc/11.2.0. If you need assistance with rebuilding your own code, please contact us here. The following affected modules have been migrated: {{< table title=""Replacements"" class=""table tablestriped"" }} | Module | Previous toolchain | Replacement | |||| |gpunufft/2.1.0 | gcccuda/9.2.011.0.228 | same version under gcc/11.2.0 | |gromacs/2021.2 | goolfc/9.2.03.1.611.0.228 | 2022.4 under goolf/11.2.04.1.4 | |mumax3/3.10 | gcccuda/9.2.011.0.228 | same version under gcc/11.2.0 | {{< /table }} The following modules will be removed from Rivanna during the maintenance period. {{< table title=""Replacements"" class=""table tablestriped"" }} | Module | Removed version | Replacement | |||| |alphafold | 2.2.0 | 2.1.2, 2.2.2, 2.3.0 | |awscli | 2.4.12 | 2.9.17 | |cellprofiler | 3.1.8 | 4.2.5 | |gurobi | 9.5.0 | 10.0.1 | |imagemagick | 7.0.70 | 7.1.057 | |pytorch | 1.10.0 | 1.8.0, 1.12.0 | |rapidsai | 21.10 | 23.02 | |tensorflow | 2.4.1, 2.8.0 | 2.7.0, 2.10.0 | {{< /table }} Archived containers can be found in /share/resources/containers/singularity/archive. Upgraded modules: clang/15.0.7 Default version changes: nextflow/20.10.0 → 23.04.1 New modules: bazel/6.1.1 claraparabricks/4.0.3 gpumd/3.7 kubectl/1.26.2 optix/6.5.0, 7.3.0"
rc-website-fork/content/post/2024-february-exhibition.md,"About This Event: The Research Computing Exhibition will be held on Tuesday, April 23, 2024 in the Newcomb Hall Ballroom. The event will include: A panel discussion made up of academic scientific computing experts and research computing faculty and staff. Judged poster session with prizes: First Place: $3,000 travel voucher Second Place: $2,000 travel voucher Third Place: $1,000 travel voucher Light refreshments If you would like to participate in the poster session, please fill out the Intent to Participate Form by Friday, March 15. While all are welcome to present a poster during the exhibition, only UVA affiliated nonfaculty submissions will be eligible for the award prizes. Key dates for poster session participants: March 15: Intent to Participate Form Due April 5: Final PDF version of Poster Due April 23: Research Computing Exhibition (finalists will be notified prior to the event) Event Schedule Time Activity 10:0011:15 Poster Setup 11:3011:45 Opening Remarks, Kelly Doney, Vice President & Chief Information Officer, UVA 11:4512:45 Panel Discussion; ""Employment in HPC"" 12:452:15 Event attendees and participants circulate among posters. Participants will be assigned specific time slots they need to be next to their posters. Judges circulate and interview finalists about their posters.Will break down into 3 timeslots 2:152:45 Judges cluster to decide 1st, 2nd, and 3rd place winners 2:453:00 Announcement of poster competition winners and farewell remarks"
rc-website-fork/content/post/2020-june-rivanna-accounting.md,"Research Computing will be activating a new accounting management package for Rivanna on June 17, 2020. The software was purchased from Adaptive Computing, which specializes in advanced management applications for highperformance systems. Rivanna users can expect to see more accurate reporting on their Service Unit (SU) balances and burn rates. Information on usage by individual members of an allocation group will also be available. Commands such as allocations will remain but will reflect the new accounting. Users should be aware that the new accounting system implements ""liens"" on running jobs, and that the SUs requested for each job will be held in a reserved pool until the job completes. When the job completes the lien is released and the actual SUs consumed are deducted from the allocation balance. That means that fewer SUs will be available while jobs are running. Details are explained in our FAQ section. Learn more: HPC Overview"
rc-website-fork/content/post/2020-december-maintenance-update-4.md,"Phase 2 of the maintenance period is complete. A majority of Rivanna's nodes, including its parallel nodes, can now be accessed by users."
rc-website-fork/content/post/2021-december-maintenance.md,"{{< alertgreen }}Rivanna and the Globus data transfer nodes (DTNs) will be down for maintenance on Tuesday, December 14, 2021 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Users will be unable to transfer data using Globus during the maintenance period. Rivanna and the Globus DTNs are expected to return to service by 6 a.m. on Wednesday, December 15. IMPORTANT MAINTENANCE NOTES New GPU We are pleased to announce the addition of DGX A100 GPU to the gpu partition. To request an A100 GPU in Slurm scripts, use gres=gpu:a100. Attention PyTorch/TensorFlow users: We are removing all the nondefault PyTorch and TensorFlow versions, together with the corresponding Jupyter kernels, as they are not compatible on the A100, and adding a new version that will replace the current default (1.8.1 1.10.0 for PyTorch; 2.4.1 2.7.0 for TensorFlow). For the sake of reproducibility/continuity of ongoing projects, the deprecated containers will be accessible from /share/resources/containers/singularity/archive and can be used to install your own Jupyter kernel. You may use them on other GPUs by excluding the A100 via the Slurm option x udcan28[1,7]. Modules The following software modules will be removed from Rivanna during the maintenance period: pytorch/1.4.0py37, 1.5.1 (see section above) tensorflow/1.12.0py27, 1.12.0py36, 2.0.0py36, 2.1.0py37 (see section above) cuda/9.2.148.1 cudnn/7.4.1.5 python/3.8.8 under gompic moved to goolfc matlab/R2018b, 2019a, R2019b mathematica/12.0, 12.1 epacts/3.3.0 replaced by 3.3.2 under goolf/7.1.03.1.4 R/3.2.1, 3.4.4, 3.5.3, 4.0.0, 4.1.0 Attention R users: We will streamline the R modules to include the following versions: 3.6.3, 4.0.3, and 4.1.1. The default version will be 4.0.3. If you have hardcoded an older version of R in your scripts (e.g., R/3.5.3), you will"
rc-website-fork/content/post/2021-december-maintenance.md,"need to update your scripts to specify one of the newer versions. If you need to switch to a newer version of R, your library containing the packages that you have installed will have to be updated. You can attempt this manually, or you can contact hpcsupport@virginia.edu for help with automating the installation of your packages. The following upgrades will take place during the maintenance period. gcc/11.2.0 and libraries (openmpi/3.1.6, openblas/0.3.17, scalapack/2.1.0, fftw/3.3.10, boost/1.77.0) The default version is still 9.2.0. gromacs/2021.2 with GPU support; please load goolfc first Upgrades to default versions of applications: R/3.6.3 4.0.3 matlab/R2021a R2021b mathematica/12.2 12.3 nvhpc/20.9 21.9 cuda/11.0.228 11.4.2 cudnn/7.6.5.32 8.2.4.15 pytorch/1.8.1 1.10.0 tensorflow/2.4.1 2.7.0 alphafold/2.0.0 2.1.1; note changes to flags! amptorch/20210308 0.1 freebayes/0.9.9 1.3.4 salmon/1.2.1 1.5.1 rapids/0.19 21.10 New modules: spark/3.1.2 rosetta/3.13 computational modeling and analysis of protein structures namd/2.14 Nanoscale Molecular Dynamics vmd/1.9.4 Visualization software for NAMD cc3d/4.2.5 CompuCell3D deeplabcut/2.2 animal pose estimation mirdeep2/0.1.3 multiqc/1.11 pbwt/3.0 ocaml/3.12.1 povray/3.7.0 3D graphics with raytracing unrar/6.0.2"
rc-website-fork/content/post/2019-december-mainenance.md,"Rivanna will be taken down for routine maintenance on Wednesday, December 18, beginning at 6 a.m. You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Rivanna is expected to return to service by 6 a.m. on Thursday, December 19."
rc-website-fork/content/post/2023-04-computational-scientist-position.md,"To see the full job posting, please visit the UVA job board. UVA Research Computing (RC) within Information Technology Services (ITS) is seeking a Research Computing Associate/Computational Scientist to join their team. Our team strives to create innovative computational and data analysis solutions for researchers. We build and maintain the University's high performance computing platforms while educating the next generation of researchers on the power of advanced computing. ITS at UVA is a phenomenal place to lead, grow, and deliver impact. It's an organization that values results and teamwork. We strive to create a welcoming, supportive workplace in an agile group where everyone feels empowered to be their authentic selves and share ideas. We embrace a commitment to diversity, equity, and inclusion. ITS values worklife balance and provides flexible work location options where possible. Please see additional information about joining our team. This is an entrylevel position. As a member of the RC User Services team, you will further our organization’s mission by teaching research computing best practices, directly working with university faculty and researchers, and engage with them to produce new computing solutions within their fields of research. As an ideal candidate, you have an outgoing personality with a passion for providing highquality services, enjoy supporting and educating academic researchers, have some experience with academic computing, and are able to apply those skills to UVA’s research computing environment. We seek an energetic selfstarter and quick learner who enjoys technical problemsolving and working on multiple projects simultaneously, has strong communication skills, and is a team player. Funding is available for a limited amount of travel for professional development. This position is a hybrid role requiring occasional travel to UVA campus (Grounds) for office work, educational outreach and to meet with researchers. Responsibilities: Provide technical support for use of RC’s computing"
rc-website-fork/content/post/2023-04-computational-scientist-position.md,"and storage services. Create internal and userfacing technical documentation. In coordination with senior staff, develop and deliver educational and consulting offerings in their field of expertise. Support senior staff with organizing outreach events to strengthen the research computing community at UVA. Engage with and enable researchers to use RC’s computing resources for processing of research data, including highly sensitive data. Requirements: Bachelor’s degree in engineering, mathematics, science, or a related area. Familiarity with data analysis in a Linux environment. At least one year of relevant experience. Relevant experience may be considered in lieu of a degree Preferred Education and Experience: Master of Science degree in engineering, mathematics, natural or life science, data science or research experience in related area is preferred. At least 2 years of experience with R, Python, or Matlab programming. Teaching or user training experience in any computational research domain is a plus. Experience with highperformance computing and basic shell scripting. Familiarity with using database management systems (e.g. SQL, PostgreSQL, MongoDB, etc.) is a plus. Familiarity with software container technology such as Docker and an understanding of the lifecycle of containers. Experience with version control software such as Git/GitHub. Experience with Python or R Shiny app development is a plus. Benefits Include: The choice between 3 different health plans; vision and dental insurance; life insurance; benefits savings accounts; starting with 22 days of paid time off a year in addition to 12 or more paid holidays; 8 weeks of paid parental leave; short term disability; up to $4,360 after your first year for combined use of tuition toward a degreeseeking program or up to $2,000 for professional development including classes, certification training and conferences; and more! The selected applicant will be required to complete a background check prior to their first day of employment per university policy."
rc-website-fork/content/post/2023-04-computational-scientist-position.md,"TO APPLY: Complete an application online and attach: Cover Letter Resume PROCESS FOR INTERNAL UVA APPLICANTS: Please apply through your Workday Home page, search “Find Jobs”, and search for R0046849 PROCESS FOR EXTERNAL APPLICANTS: Please visit UVA job board: https://uva.wd1.myworkdayjobs.com/UVAJobs and search for R0046849 The position will remain open until filled. Please note that you MUST upload ALL documents into the CV/Resume box. Applications that do not contain all of the required documents will not receive full consideration. For questions about the application process, please contact Bill Crane Senior IT Recruiter at xer5ff@virginia.edu For more information about UVA and the Charlottesville community please see www.virginia.edu/life/charlottesville and https://embarkcva.com/ Physical Demands: This is primarily a sedentary job involving extensive use of desktop computers. The job does occasionally require traveling some distance to attend meetings, and programs. COVID Vaccination Requirement and Guidelines Please visit the UVA COVID19 Job Requirements and Guidelines webpage prior to applying for current information regarding vaccination requirements and guidelines for employment at UVA. The University of Virginia, including the UVA Health System which represents the UVA Medical Center, Schools of Medicine and Nursing, UVA Physician’s Group and the Claude Moore Health Sciences Library, are fundamentally committed to the diversity of our faculty and staff. We believe diversity is excellence expressing itself through every person's perspectives and lived experiences. We are equal opportunity and affirmative action employers. All qualified applicants will receive consideration for employment without regard to age, color, disability, gender identity or expression, marital status, national or ethnic origin, political affiliation, race, religion, sex (including pregnancy), sexual orientation, veteran status, and family medical or genetic information."
rc-website-fork/content/post/alphafold.md,"We are pleased to announce that AlphaFold is now available on Rivanna! Here are some of our user's first few protein structure prediction calculations on Rivanna. Simple ZFc protein: Similar results for AlphaFold (brown) and ITASSER (blue). (This figure was created with the RCSB Pairwise Structure Alignment tool.) 146PduDlinkerZFc protein: AlphaFold's (left) superior ability to predict secondary structure, a βsheet in its greenyellow region, whereas ITASSER (right) is not sufficiently refined to feature any βstrands. (This figure was created with NGL Viewer.) (Credits: David Bass and Prof. Keith Kozminski, Department of Biology) FAQ What is AlphaFold? AlphaFold is an AI for protein structure prediction developed by Google DeepMind. Watch DeepMind's video or read the Nature article. How do I use AlphaFold on Rivanna? See here for installation and usage details. If you are a PI interested in using AlphaFold on Rivanna but do not have an account, please request an allocation."
rc-website-fork/content/post/2019-september-maintenance-notes.md,"Rivanna was down for maintenance on Tuesday, September 17. The items below summarize the changes that may impact the users of Rivanna. I. Changes to scratch System engineers have installed a new /scratch file system, and have transferred to the new system any files/data that were less than 90 days old on the former scratch system. II. Updates to software modules New and updated modules: The following software modules either replace older versions or are new to Rivanna: pgi/19.7 openmpi/3.1.4 (for all GCC and PGI compilers) cuda/10.1.168 For openmpi, be sure to remove any reference to 2.1.5 in your scripts. Removed modules: The following software modules were removed from Rivanna during the maintenance period: cellranger/2.1.1 (replaced with cellranger/3.1.0) exonerate/2.2.0 (replaced with exonerate/2.4.0) fenics/20180 fluent/18.2 (is now part of the ansys/18.2 module) fiji/1.51 miniconda/4.3.21py3.6 (replaced with anaconda/5.2.0py3.6 openmpi/2.1.5 (replaced with openmpi/3.1.4) pgi/17.5 & pgi/18.10 (replaced with pgi/19.7) povray/3.7.0.7 rstudio/0.98.1103 III. Other important changes {{% callout %}} The loading of some software modules now requires preloading of a dependency, such as a compiler or version of mpi. {{% /callout %}} Run the command module spider <YOURMODULE to view module load instructions for a particular application module. For example, module spider abinit/8.2.2 states that You will need to load all module(s) on any one of the lines below before the ""abinit/8.2.2"" module is available to load. intel/18.0 intelmpi/18.0 This statement tells you that both intel and intelmpi must be loaded in order to load abinit. {{% callout %}} The operating system was updated, and (as usual) users who compile their own code may need to recompile. This also applies to anyone who installed R packages which are dependent on openMPI. Those packages will need to be reinstalled. {{% /callout %}} {{% callout %}} Libraries and applications built with the Intel 18.0 compiler and"
rc-website-fork/content/post/2019-september-maintenance-notes.md,"IntelMPI libraries have been recompiled to enable execution on compute nodes with Knights Landing ManyCore processors in the knl queue. {{% /callout %}} If you have any questions or concerns about these changes, please contact our user support team at hpcsupport@virginia.edu. {{< button buttonclass=""primary"" buttontext=""About HPC"" buttonurl=""/userinfo/hpc"" }}"
rc-website-fork/content/post/2022-may-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on May 17, 2022 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Users will not be able to access the Globus data transfer node (UVA MainDTN) or Research Project storage during the maintenance period. All systems are expected to return to service by 6 a.m. on Wednesday, May 18. IMPORTANT MAINTENANCE NOTES The operating system will be upgraded from CentOS 7.8 to 7.9. This should have no impact on the software built on Rivanna, whether it be modules or your own compiled codes. If you need assistance to rebuild your code, please contact hpcsupport@virginia.edu. Slurm will be upgraded to 21.08.8, which requires us to rebuild the OpenMPI module. If your code was built with OpenMPI and it no longer works after maintenance, you may need to rebuild it. NVIDIA Driver will be upgraded to 470.103.01. You should not need to rebuild CUDA programs. Modules The following software modules will be removed from Rivanna during the maintenance period: | Module | Removed version | Replacement | |||| |gcc |6.5.0 | 7.1.0, 9.2.0 | |mvapich2 |2.3.1, 2.3.3 | Please use gcc openmpi or intel intelmpi. | |nvhpc |20.9 | 21.9 | |alphafold |2.0.0, 2.1.1 | 2.1.2, 2.2.0 | |awscli |2.1.10 | 2.4.12 | |cellranger|2.2.0, 3.0.2, 3.1.0 | 4.0.0, 5.0.0, 6.0.1 | |cmake | 3.5.2, 3.12.3 | 3.6.1, 3.16.5 | |gatk |3.8.1.0, 4.0.0.0, 4.1.6.0 | 4.2.3.0 | |metamorpheus|0.0.311dev, 0.0.317 | 0.0.320 | |mpi4py |3.0.0py2.7, 3.0.3 | Load any MPI toolchain (e.g. gcc openmpi) plus anaconda and run pip install user mpi4py; see here | |picard |2.1.1, 2.18.5, 2.20.6 | 2.23.4 | |rapidsai |0.19"
rc-website-fork/content/post/2022-may-maintenance.md,"| 21.10 | Archived containers can be found in /share/resources/containers/singularity/archive. Upgrades: Addition of Matplotlib widget ipympl/0.8.7 to JupyterLab tensorflow/2.8.0 swig/4.0.2 Default version changes: alphafold/2.1.1 → 2.2.0 cellprofiler/3.1.8 → 4.2.1 cuda/11.0.228 → 11.4.2 diamond/0.9.13 → 2.0.14 igvtools/2.8.9 → 2.12.0 matlab/R2021b → R2022a totalview/2019.0.4linuxx8664 → 2021.4.10 zlib/1.2.11 → 1.2.12 hidden module; load zlib/.1.2.12 New modules: nvompic/21.93.1.611.4.2 toolchain (nvhpc/21.9 + openmpi/3.1.6 + cuda/11.4.2) libraries: scalapack, fftw, hdf5 berkeleygw/3.0.1 quantumespresso/7.0 yambo/5.0.4 pandoc/2.17 trinity/2.13.2 cufflinks/2.2.1 rediscli/6.2.6"
rc-website-fork/content/post/2025-april-ssz-h200-announcement.md,"﻿+++ images = [""""] author = ""Staff"" description = """" date = ""20250505T00:00:0005:00"" title = ""New NVIDIA H200 GPU Node Added to Afton"" draft = false tags = [""afton"",""hpc""] categories = [""feature""] +++ We are excited to announce the expansion of UVA’s AI computing capabilities. On April 3, we added a NVIDIA HGX H200 GPU node to the Afton highperformance computing (HPC) cluster, the first of many planned for the coming year. Each node provides 2TB of node CPU memory and 8way connected Tensor Core GPUs with 141GB of VRAM memory per device. These devices offer higher performance than the current Afton GPU nodes, opening new possibilities for the most challenging deep learning and large language model computations. How can I get access to the new hardware? On April 3, the new GPU nodes became available as a betarelease. During this phase the new nodes were accessible for all users with active Afton HPC allocations. Please follow these instructions to use the HGX H200 nodes in your jobs. Jobs running on the new hardware will consume service units based on charge rates reflective of the actual hardware and service cost. Visit our webpage for a complete list of service unit charge rates here. On May 1, the new HGX H200 node was released into full production. Access and service unit charge rates remain in effect as posted. Additional hardware upgrades or configuration updates, if needed, will be handled as part of Afton’s preannounced regular maintenance cycles. Where can I learn more? More detailed descriptions of the new hardware’s capabilities and how to use it are available on our website. In addition, you may reach out to our User Services team during virtual office hours or by submitting a support request."
rc-website-fork/content/post/2019-jira-downtime.md,"The JIRA ticketing system will be taken offline on Friday, December 13 from 6 p.m. to 9 p.m. while our system engineers continue the process of migrating the ticketing system from a local environment to the cloud. Please avoid submitting requests during this period if possible. Although moving to a cloudbased ticketing system will improve the speed and efficiency of our customer service in the long run, in the shortterm it may cause disruptions for some users. If you are unable to log in to JIRA after the migration is completed, you will need to change your password using your UVA email address. Please note: Eservices passwords will not work in JIRA cloud."
rc-website-fork/content/post/2021-october-maintenance.md,"{{< alertgreen }}The Globus data transfer nodes (DTNs) and Rivanna's parallel nodes will be down for maintenance on Tuesday, October 12, 2021, between 8 a.m. and 4 p.m.{{< /alertgreen }} Users will be unable to transfer data using Globus or run parallel jobs on Rivanna during this period. All other systems and services—including storage—are expected to continue operating normally. IMPORTANT MAINTENANCE NOTES The following Rivanna software changes will be implemented during the maintenance period: IDL/8.4 replaced by 8.8 (8.7.2 still available) ANSYS default version changing from 2021r1 to 2021r2"
rc-website-fork/content/post/2020-december-maintenance-update-1.md,"The maintenance period has been extended for another 72 hours minimum. We apologize for the inconvenience. Rivanna was expected to return to service on December 17, but ongoing electrical work in the data center and other obstacles have delayed our engineering team's progress. We will send out an email announcement as soon as the maintenance period ends. Updates will also be posted on our website."
rc-website-fork/content/post/what-is-research-computing.md,"UVA Research Computing (RC) is a new program that aims to support computational biomedical research by providing advanced cyberinfrastructure and expertise in data analysis at scale. Our mission is to foster a culture of computational thinking and promote interdisciplinary collaboration in various datadriven research domains. We offer services related to high performance computing, cloud architecture, scientific programming and big data solutions. We also aim to promote computationally intensive research at UVA through collaborative efforts such as UVA's own CADRE (Computation And Data Resource Exchange) and XSEDE (Extreme Science and Engineering Discovery Environment). One of our driving philosophies is that researchers already have medical and scientific expertise, and should not have to become computing experts on top of that. To that end, our approach is service and projectdriven. Rather than approach your research with a specific technology or platform in mind beforehand, we want to understand your research goals and how computational support will help you get there. Ways to get started or learn more: Our Services User Support Bioinformatics & Genomics Cloud Solutions Data Analysis High Performance Computing Image Processing Collaboration Citations"
rc-website-fork/content/post/2023-women-in-hpc-20230404.md,"What: Join us in welcoming 11 undergraduate and graduate students from across Virginia to talk about their research. The talks will be lightning style format allowing 3 minutes for students to present and 12 questions and 12 questions from the audience. Don’t miss out on this fantastic opportunity to hear about a variety of research topics within HPC! Event Time: April 4, 2023, 01:00 PM EST (US and Canada). Featured Speakers: Lakshmi Miller Graduate Student Aerospace and Ocean Engineering, Virginia Tech “CFD Informed Maneuvering of AUVs” Rashmi Chawla Graduate Student Aerospace Engineering, Virginia Tech “Multiphysics Modeling of UHTCs using Material Point Method” Naina Pisharoti Graduate Student Aerospace Engineering, Virginia Tech “Highfidelity Computational Analysis of UAV Propellers” Liza Harold Undergraduate Student Biomedical Engineering, University of Virginia “Investigating the Role of Stereocomplexation in Peptide Assembly via Molecular Dynamics Simulations” Clare Cocker Undergraduate Student Chemical Engineering, University of Virginia “Role of Amino Acid sSereochemistry in the Assembly of Peptide Hydrogels for Tissue engineering” Marion LoPresti Undergraduate Student Biochemistry, Virginia Tech “Utilizing HPC to Explore the Dynamics of the Druggable Dengue Virus Protease” Nhi Huynh Graduate Student Engineering and Technology, Old Dominion University “RSM: To Increasing the Capacity of the Deep GCN (Graph Convolution Neural) Image” Sarah Patterson Undergraduate Student Developmental Biology, William & Mary “Single Cell RNA Sequencing in Xenopus Laevis Embryology” Cynthia Sias Graduate Student Plant and Environmental Sciences, Virginia Tech “Evaluating the Effect of Cover Crop Termination Management on Palmer Amaranth (Amaranthus palmeri) Suppression in Soybean” Afrina Tabassum Graduate Student Computer Science, Virginia Tech “Multimodal Learning: Representation and Generation” Mahshid Ahmadian Graduate Student Systems Modeling and Analysis, Virginia Commonwealth University “Modeling Salmon Shark’s Location in the Pacific Ocean Using Stochastic Process approach” Moderators: Mark Gardner Network Research Manager, Advanced Research Computing, Virginia Tech Heather Baier Ph.D. Student in Computational Geography, William"
rc-website-fork/content/post/2023-women-in-hpc-20230404.md,"& Mary This virtual event is jointly hosted by Virginia Commonwealth University, George Mason University, Virginia Tech, William & Mary, University of Richmond, Virginia Institute of Marine Science, Old Dominion University, and the University of Virginia, {{% vawhpc %}}"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"Due to the new licensing restrictions by Anaconda on research usage, the licensed Anaconda distribution was removed from the system on October 15, 2024. The current anaconda/2023.07py3.11 module will redirect to the miniforge/24.3.0py3.11 module, switching to condaforge as the default package installation channel with fewer preinstalled packages. Existing environments will not be affected. However, using Anaconda default channels for research without a personal license will violate the Anaconda license. For instructional use, package installation from licensed channels is still allowed Maintenance: Oct 15, 2024 {{< alertgreen }}The UVA highperformance computing (HPC) system will be down for maintenance on Tuesday, Oct 15, 2024, beginning at 6 a.m. The HPC systems are expected to return to full service by 6 a.m. on Wednesday, Oct 16.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until the HPC systems are returned to service. Questions: Please contact our user services team, or join us for our virtual office hours every Tuesday, 35 p.m. and Thursday, 1012 p.m.. {{% callout %}} What to expect after the Oct 15 maintenance? The licensed Anaconda distribution and base environment provided by the anaconda module will be removed from our systems on Oct 15, 2024. The anaconda module will redirect to the new miniforge/24.3.0py3.11 module, effectively switching to condaforge as the only default package installation channel and with a reduced number of preinstalled packages in the base environment. The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the licensed Anaconda default channels as shown in the example above. However, any use of such environment for research purposes is a violation of the Anaconda"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"license unless you obtained your own license. We understand that these changes may cause inconvenience, but these changes are mandated by the Anaconda licensing condition which we cannot control. If you have any questions or concerns, please feel free to reach out. What to expect after the December 12? We will be removing the dummy module anaconda/2023.07py3.11 from our system on December 12. The Conda package manager, along with a base environment, will only be available by loading the miniforge/24.3.0py3.11 module. Please refer to the FAQ for further instructions on how to use the Miniforge module."" {{% /callout %}} FAQ {{% accordiongroup title=""Group"" id=""faqgroup""%}} {{% accordionitem title=""1. How miniforge is different from Anaconda?"" id=""faq1"" %}} Miniforge and Anaconda are both popular tools for managing Python environments and packages, but they differ in a few key ways: 1. Size and Preinstalled Packages: Anaconda base environment came with a large number of preinstalled data science libraries. However, miniforge only includes the essential Conda and Mamba package managers along with commonly used packages such as numpy, pandas, matplotlib, etc., from the condaforge channel. 2. Default Package Channels: Anaconda: Uses Anaconda’s proprietary channels (Anaconda repository) for package installations by default. These packages may have specific licensing restrictions. Miniforge: Uses condaforge as its default channel, an opensource communitydriven repository, ensuring more transparency and flexibility without proprietary limitations. 3. Licensing: Anaconda: The default Anaconda distribution has licensing restrictions for commercial and research use, requiring a paid license for certain types of usage. Miniforge: Has no such restrictions since it uses condaforge, which provides fully opensource packages. {{% /accordionitem %}} {{% accordionitem title=""2. Can I still use my conda environements?"" id=""faq2"" %}} The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"licensed Anaconda default channels as shown in the example below. conda install n pathtomycondaenf seaborn c anaconda However, any use of such environment for research purposes is a violation of the Anaconda license unless you obtained your own license. Therefore, for research use, it is expected to replace packages installed through the anaconda restricted channels with packages from nonproprietary channels such as condaforge. {{% /accordionitem %}} {{% accordionitem title=""3. How to use miniforge to create conda envs?"" id=""faq3"" %}} The process and commands for creating conda environments using Miniforge are exactly the same. The only difference is that you need to load the Miniforge module instead of the Anaconda module on our system.Basically, module load miniforge conda create n yourenvnamegoeshere (default Python version: use conda info to find out) {{% /accordionitem %}} {{% accordionitem title=""4. Why are we switching from Anaconda to Miniforge?"" id=""faq4"" %}} Miniforge avoids violating Anaconda's Terms of Service because it pulls packages from the condaforge channel by default. Condaforge is a community led collection of recipes, build infrastructure and distributions for the conda package manager and is free to use. {{% /accordionitem %}} {{% accordionitem title=""5. Will I lose access to any packages that I had with Anaconda?"" id=""faq5"" %}} Existing environments will not be removed, however, any packages installed in your conda environments via the Anaconda default (proprietary) channel will need to be reinstalled through a different channel or tool such as condaforge or pip. {{% /accordionitem %}} {{% accordionitem title=""6. How do I install Anaconda packages if I need them (e.g., licensed or proprietary ones)?"" id=""faq6"" %}} The Miniforge module includes the conda package management system. You can use conda install <packagename as you may have done previously using the Anaconda module. Miniforge uses the condaforge channel by default. If you are using Anaconda"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"for instructional use or have your own license, you can install packages from the anaconda channel using conda install c anaconda <packagename {{% /accordionitem %}} {{% accordionitem title=""7. Can I still use pip to install nonConda packages with Miniforge?"" id=""faq7"" %}} Yes. Miniforge supports installation of nonConda packages with pip and uses the same syntax. {{% /accordionitem %}} {{% accordionitem title=""8. Will my existing Conda environments work with Miniforge?"" id=""faq8"" %}} Yes. Miniforge supports installation of nonConda packages with pip and uses the same syntax. {{% /accordionitem %}} {{% accordionitem title=""9. How do I update packages and environments in Miniforge?"" id=""faq9"" %}} Once the environment is activated you an update packages and environments in the same fashion as you would with Anaconda. To activate all packages run: conda update all To activate a single package you would run: conda update <name {{% /accordionitem %}} {{% accordionitem title=""10. Will my scripts that depend on specific Anaconda packages break when switching to Miniforge?"" id=""faq10"" %}} Scripts using Anaconda packages that are also available within Miniforge may not break. However, any Anaconda packages that are not used soley for instructional purposes would be violating the Anaconda license terms, so they would need to be reinstalled through a different channel or tool such as condaforge or pip. {{% /accordionitem %}} {{% accordionitem title=""11. What channels are available by default in Miniforge?"" id=""faq11"" %}} The condaforge channel is set as the default (and only) channel for Miniforge. {{% /accordionitem %}} {{% accordionitem title=""12. Can I still use the Anaconda repository with Miniforge?"" id=""faq12"" %}} The Anaconda repository can only be used with miniforge provided that the packages are used strictly for instructional purposes. {{% /accordionitem %}} {{% accordionitem title=""13. How do I migrate my existing Anaconda environments to Miniforge?"" id=""faq13"" %}} First, you'll need to"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"load the Miniforge module and activate the environment module load miniforge source activate <envname Next, you'll need to export the existing environment to a yaml which will be used for rebuilding, then remove the existing environment. You'll need to deactivate the environment prior to removing: conda env export f <envname.yml conda deactivate conda env remove name <envname {{% callout %}} Note: Use an editor to remove the proprietary channels (e.g. defaults or anaconda) from the yaml file. {{% /callout %}} You might encounter issues with resolving dependencies in which case you might need to leave out the versions of the packages in the yaml file and automatically install the most recent compatible versions. You can use the command conda env export | cut f 1 d ""="" | grep v ""prefix"" <envname.yml in place of the above command to remove the versioning information of packages while exporting the environment into a yaml file. You can then recreate the environment with Miniforge using the following: conda env create f <envname.yml {{% /accordionitem %}} {{% accordionitem title=""14. How do I get help if I encounter problems during the transition?"" id=""faq14"" %}} You can either submit a support request on our website or you can attend one of our office hour sessions. We meet virtually over Zoom on Tuesdays (35PM) and Thursdays (10AM12PM) via Zoom. Links to the sessions can be found at the bottom of this page on our website. {{% /accordionitem %}} {{% /accordiongroup %}} Announcements {{% accordiongroup title=""Comms"" id=""commsgroup"" %}} {{% accordionitem title=""Aug 27, 2024 Change to Anaconda Module on Our System"" id=""comm1"" %}} Dear PI, This message is important if you intend to use the anaconda/2023.07py3.11 for your fall classes. Due to the recent licensing restrictions by Anaconda on research usage, we will be removing the licensed Anaconda distribution"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"from our system on October 15, 2024. As an alternative we have installed the miniforge/24.3.0py3.11 module, which includes the essential Conda and Mamba package managers along with commonly used packages such as numpy, pandas, matplotlib, etc., from the condaforge channel. How should I prepare? Between now and October 15, you may load the existing anaconda module or the newly installed miniforge/24.3.0py3.11 module. Both modules provide the same conda commands to manage and use your conda environments. Important: By default, the miniforge distribution will only provide packages from the condaforge channel. Therefore, if you require packages from channels that are covered by the Anaconda repository Terms of Service (main/anaconda, r, msys2) you may specify this in your installation command but only for environments that are restricted to educational use, i.e., instructional work in your classes. For example, to install the seaborn package from the Anaconda default channels, you would use: conda install –n pathtomycondaenv seaborn –c anaconda What to expect on October 15, 2024? The licensed Anaconda distribution and base environment provided by the anaconda module will be removed from our systems on Oct 15, 2024. The anaconda module will redirect to the new miniforge/24.3.0py3.11 module, effectively switching to condaforge as the only default package installation channel and with a reduced number of preinstalled packages in the base environment. The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the licensed Anaconda default channels as shown in the example above. However, any use of such environment for research purposes is a violation of the Anaconda license unless you obtained your own license. We understand that these changes may cause inconvenience, but these changes are mandated by the Anaconda licensing condition which we cannot control. If you have"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"any questions or concerns, please feel free to reach out. At your service, RC staff Research Computing E hpcsupport@virginia.edu P 434.243.1107 University of Virginia P.O. Box 400231 Charlottesville 22902 {{% /accordionitem %}} {{% /accordiongroup %}}"
rc-website-fork/content/post/2021-remaining-maintenance-days-2021.md,"Rivanna will be down for maintenance on Tuesday, October 12 and Tuesday, December 14. Please plan accordingly. We do not anticipate any additional maintenance days in 2021."
rc-website-fork/content/post/2025-Call-for-Posters.md,"Important Dates New Submission Date: March 31, 2025 UVA RC Exhibition: April 23, 2025: ` Everyone is welcome to attend the Exhibition on April 23rd, at the Newcomb Hall Ballroom, 10:00am 2:00pm. Our Exhibition includes a judged poster competition with travel awards presented to the winning finalists. If you have used Research Computing resources to support your research, we invite you to submit a poster! Posters should reflect how you have used these resources to support your research. Three finalists will be recognized with a travel award that can be used for professional travel such as conferences, meetings of professional associations, or other travel related to your research. First Place: $3,000 travel voucher Second Place: $2,000 travel voucher Third Place: $1,000 travel voucher Although all researchers using Research Computing resources are invited to participate, the lead author must be someone other than a faculty member to be eligible for an award. Resources used may include systems, such as Rivanna, Afton, or Ivy, as well as collaboration with Research Computing staff. Submit your poster PDF on the Poster Competition Submission Form on or before March 31st in one of the following categories: Biological and Health Sciences, Physical and Health Sciences, Social Sciences and Humanities, or Other. Participants must print their poster for the event day. Judging Information The judges will review the PDFs prior to the event to select the finalists for the awards. During the event, judges will visit your poster and ask questions related to your research. Prizes will be awarded at the end of the Exhibition. To select our finalists, judges will consider the following criteria: How much did using RC resources contribute to the outcome of the research presented? How well is the use of RC resources communicated on the poster? How well is the importance of"
rc-website-fork/content/post/2025-Call-for-Posters.md,this scientific investigation communicated? We look forward to your participation in this event! Please contact rcevents@virginia.edu with questions.
rc-website-fork/content/post/2024-oct-ivy-maintenance.md,"{{< alertgreen }}The Ivy Virtual Machines (VMs) and high security zone HPC system will be down for storage maintenance on Tuesday, Oct 15, 2024, beginning at 6 a.m. The system is expected to return to full service by 6 a.m. on Wednesday, Oct 16..{{< /alertgreen }} IMPORTANT MAINTENANCE NOTES During the maintenance all VMs will be down as well as the UVA Ivy Data Transfer Node (DTN) and Globus services. The HighSecurity HPC cluster will also be unavailable for all job scheduling and viewing. If you have any questions about the upcoming Ivy system maintenance, you may contact our user services team. Ivy Central Storage transition to HSZ Research Standard To transition from old storage hardware, we have retired the Ivy Central Storage and replaced it with the new High Security Zone Research Standard storage. There will be new filesystem locations for Ivy Linux VMs and updated shortcuts on Windows VMs. Filesystem locations can be found on the RC website Research Data Storage Page."
rc-website-fork/content/post/2019-sept17-notes.md,"Rivanna was down for maintenance on Tuesday, September 17. The items below summarize the changes that may impact the users of Rivanna. I. Changes to scratch System engineers have installed a new /scratch file system, and have transferred to the new system any files/data that were less than 90 days old on the former scratch system. II. Updates to software modules New and updated modules: The following software modules either replace older versions or are new to Rivanna: pgi/19.7 openmpi/3.1.4 (for all GCC and PGI compilers) cuda/10.1.168 For openmpi, be sure to remove any reference to 2.1.5 in your scripts. Removed modules: The following software modules were removed from Rivanna during the maintenance period: cellranger/2.1.1 (replaced with cellranger/3.1.0) exonerate/2.2.0 (replaced with exonerate/2.4.0) fenics/20180 fluent/18.2 (is now part of the ansys/18.2 module) fiji/1.51 miniconda/4.3.21py3.6 (replaced with anaconda/5.2.0py3.6 openmpi/2.1.5 (replaced with openmpi/3.1.4) pgi/17.5 & pgi/18.10 (replaced with pgi/19.7) povray/3.7.0.7 rstudio/0.98.1103 III. Other important changes {{% callout %}} The loading of some software modules now requires preloading of a dependency, such as a compiler or version of mpi. {{% /callout %}} Run the command module spider <YOURMODULE to view module load instructions for a particular application module. For example, module spider abinit/8.2.2 states that You will need to load all module(s) on any one of the lines below before the ""abinit/8.2.2"" module is available to load. intel/18.0 intelmpi/18.0 This statement tells you that both intel and intelmpi must be loaded in order to load abinit. {{% callout %}} Recompiling & Reinstalling The operating system was updated, and (as usual) users who compile their own code may need to recompile. This also applies to anyone who installed R packages which are dependent on openMPI. Those packages will need to be reinstalled. Libraries and applications built with the Intel 18.0 compiler and IntelMPI libraries have"
rc-website-fork/content/post/2019-sept17-notes.md,"been recompiled to enable execution on compute nodes with Knights Landing ManyCore processors in the knl queue. {{% /callout %}} If you have any questions or concerns about these changes, please contact our user support team. {{< button buttonclass=""primary"" buttontext=""About HPC"" buttonurl=""/userinfo/hpc"" }} {{< supportbutton }}"
rc-website-fork/content/post/2023-july-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on July 18, 2023 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All systems are expected to return to service by 6 a.m. on Wednesday, July 19. IMPORTANT MAINTENANCE NOTES New scratch RC engineers will be installing a new /scratch storage filesystem that can be accessed at /scratch/$USER after the end of maintenance. Modified queue limits will be implemented to provide maximum read/write performance of the new /scratch filesystem. Users are encouraged to consult our updated documentation and adjust their job scripts accordingly. The current /scratch filesystem will be permanently retired on October 17, 2023 and all the data it contains will be deleted. We have prepared a sample script for users who wish to transfer files to the new scratch system. Users should clean up their current /scratch directory in preparation, to minimize the load. A sample script is posted below. Example script to copy files {{< pullcode file=""/static/scripts/democopyscratch.slurm"" lang=""bash"" }} The script will also be available through the Open OnDemand Job Composer: Go to Open OnDemand Job Composer Click: New Job From Template Select democopyscratch In the right panel, click ""Create New Job"" This will take you to the ""Jobs"" page. In the ""Submit Script"" panel at the bottom right, click ""Open Editor"" Enter your own allocation. You may edit the script as needed. Click ""Save"" when done. Going back to the ""Jobs"" page, select democopyscratch and click the green ""Submit"" button. As we expect a high volume of data migration, please refrain from doing so directly on the login nodes but instead submit it as a job"
rc-website-fork/content/post/2023-july-maintenance.md,"via the provided Slurm script as described above. The new scratch is subject to the same 10 TB quota and 90day purge policy. There is no restriction on the number of files. A friendly reminder that scratch is intended as a temporary work directory, not longterm storage space. It is not backed up and old files need to be purged periodically for system stability. RC offers a number of lowcost storage options to researchers. For more information, visit our storage page. Modules The following software modules will be removed from Rivanna during the maintenance period: | Module | Removed version | Replacement | |||| |abinit |8.2.2 (intel/18.0), 8.10.3 (intel/20.0) | 8.10.3 (intel/2022.11) | |maven | 3.3.9 | 3.9.0 | |postgresql | 11.3 | 14.5 |"
rc-website-fork/content/post/lolaweb.md,"From his lab in the Center for Public Health Genomics at UVa, Nathan Sheffield seeks to develop a deeper understanding of functional genomics. Dr. Sheffield and his collaborators study epigenetic mechanisms, including DNA methylation, which can involve analyzing enrichment of genomic region set data. By identifying patterns of enriched genomic regions, one can differentiate between normal and diseased gene regulation. Dr. Sheffield builds on this research focus as well as a history of opensource software development with the publication of LOLAweb: A containerized web server for interactive genomic locus overlap enrichment analysis as part of a special web server issue of Nucleic Acids Research. The LOLAweb methodology for locus overlap analysis is derived from the LOLA R package, which was written and developed by Dr. Sheffield. Since its publication in 2016, LOLA has been cited in several dozen publications, and is currently in the top 20% of packages downloaded from the Bioconductor repository. These citations and downloads demonstrate the variety of scientific usecases for quantifying overlap and enrichment for regions of interest. According to Dr. Sheffield, restricting the locus overlap analysis methods to a purely programmatic interface may prove to be a barrier for some biologists. However, by teaming with RC to create LOLAweb, the functionality from LOLA will become more accessible to the research community, thus allowing more researchers to develop new hypotheses. LOLAweb closely mirrors LOLA, providing users graphical dropdowns and inputs to upload their data. The application also features data visualizations that are not available in the original package. RC has also helped design LOLAweb with a flexible deployment, which includes a feature to cache results so that users can bookmark or share output with colleagues. The collaboration between Dr. Sheffield and RC staff is not the first, nor will it be the last. In 2017, RC"
rc-website-fork/content/post/lolaweb.md,staff contributed to the simpleCache R package and coauthored a resulting publication in the Journal of Open Source Software. RC is also currently working with Sheffield Lab members to optimize containerized data analysis pipelines and develop new software packages. Read more: LOLAweb: A containerized web server for interactive genomic locus overlap enrichment analysis. Visit LOLAweb: http://lolaweb.databio.org/
rc-website-fork/content/post/2026-hpc-maintenance-dates.md,"Rivanna/Afton will be taken down for maintenance in 2026 on the following days: [Winter: Tuesday, January 6, 2026] Spring: Summer: Fall: Please plan accordingly. Questions about the 2026 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2025-DAC-Awards.md,"The Data Analytics Center (DAC) is accepting proposals for the Small Analytics Resource Award. If you have a computationallyintensive project and would like a collaborator, consider applying for a Small Analytics Resource Award. The DAC team is ready to support your research in AI, Bioinformatics, Image Processing, Data Analysis, Data Management, and more. The next proposal deadline is 30 June 2025. For details on how to apply for the Small Award, visit the Awards website"
rc-website-fork/content/post/2019-fall-workshops.md,"RC staff are teaching a series of free handson workshops this fall that are open to all UVA researchers. Space is limited, so register today! Topics include: Image Processing with Fiji/ImageJ (Sept 11) MATLAB Fundamentals (Sept 12) Programming in MATLAB (Sept 19) Optimizing R Code (Sept 24) Parallel Computing in MATLAB (Sept 26) Introduction to Parallel R (Oct 1) Machine Learning with MATLAB (Oct 3) Automation of Image Processing with Fiji/ImageJ (Oct 9) Deep Learning with MATLAB (Oct 17) Moving R Programs to Rivanna (Oct 17) Browse Workshops"
rc-website-fork/content/post/2020-march-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on Wednesday, March 11, beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Rivanna is expected to return to service later in the day. The following software modules will be removed from Rivanna during the maintenance period (please use the suggested newer versions): gcc/5.4.0 & toolchains 7.1.0 All modules that depend on gcc/5.4.0 will be available under gcc/7.1.0. The only exception is cushaw3/3.0.3. Please contact us if you need to use it. pgi/19.7 & toolchains 19.10 All modules that depend on pgi/19.7 will be available under pgi/19.10. anaconda/5.2.0py2.7 2019.10py2.7 All modules that depend on anaconda/5.2.0py2.7 will be available under anaconda/2019.10py2.7. tensorflow/1.6.0py27, py36 1.12.0, 2.0.0, or 2.1.0 If you must use version 1.6.0, please pull the image from our repository on Singularity Library. singularity/3.1.1 3.5.2 boost/1.66.0 1.68.0 julia/1.0.2, 1.0.3 1.3.1 cushaw3/3.0.3 no replacement The following upgrades will take place during the maintenance period: JupyterLab v1.2.6 Python 3.7 Jupyter kernel based on anaconda/2019.10py3.7 The previous ""Python 3"" kernel (based on anaconda/5.2.0py3.6) has been renamed as ""Python 3.6"". anaconda/2019.10py2.7, py3.7 gcc/9.2.0 & toolchains singularity/3.5.2 now default version gurobi/9.0.1 tensorflow/2.1.0py37 Singularity container module & Jupyter kernel julia/1.3.1 module & Jupyter kernel ansys/2020r1 samtools/1.10 rust/1.41.0 cmake/1.16.5 New tools: LibreOffice through FastX Web desktop environment pytorch/1.4.0 Singularity container module & Jupyter kernel openfoam 7 (version 1909) opensource CFD software goolfc/6.5.03.1.410.1.168 GOOLF toolchain (GCC + OpenMPI + OpenBLAS + ScaLAPACK + FFTW) with CUDA support atom/1.43.0 Atom text editor rclone/1.51.0 Rclone for cloud file syncing (supports Google Drive) nodejs/12.14.1 Node.js JavaScript runtime environment ninja/1.10.0py3.6 Ninja build system meson/0.53.1py3.6 Meson build system gtk+/3.24.14 GTK+ 3 libraries for GUI applications"
rc-website-fork/content/post/2020-march-maintenance.md,fribidi/1.0.8 Free Implementation of the Unicode Bidirectional Algorithm atk/2.28.1 Accessibility Toolkit tree/1.8.0 Tree structure of file system gnupg/2.2.19 GnuPG encrypt and sign data
rc-website-fork/content/post/2021-its-home-dirs.md,"ITS has completed phase 1 of its network reconfiguration and ITS NAS storage volumes have been remounted on Rivanna. RC managed Research Standard and Research Project storage are also fully available. ITS home1 directories, including /nv/t mounts, remain unavailable on Rivanna. Users will still be able to mount ITS home1 storage to their local workstations and transfer their data via Globus and the UVA Main DTN. We regret the inconvenience."
rc-website-fork/content/post/2024-oct-maintenance.md,"{{< alertgreen }}The HPC cluster will be down for maintenance on Tuesday, Oct 15, 2024 beginning at 6 am.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until the cluster is returned to service. All systems are expected to return to service by Wednesday, Oct 16 at 6 am. IMPORTANT MAINTENANCE NOTES Expansion of /home To transition away from the Qumulo filesystem, we will migrate all /home directories to the GPFS filesystem and automatically increase each user’s /home directory limit to 200GB. Transition from Anaconda to Miniforge module Due to the recent licensing restrictions by Anaconda on research usage, we will be removing the licensed Anaconda distribution from our system on October 15, 2024. The anaconda module will redirect to the new miniforge/24.3.0py3.11 module, with a reduced number of preinstalled packages in the base environment, but includes the essential Conda and Mamba package managers along with commonly used packages such as numpy, pandas, matplotlib, etc., from the condaforge channel. By default, the miniforge distribution will only provide packages from the condaforge channel. Therefore, if you require packages from channels that are covered by the Anaconda repository Terms of Service (main/anaconda, r, msys2) you may specify this in your installation command but only for environments that are restricted to educational use, i.e., instructional work in your classes. The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the licensed Anaconda default channels however, any use of such environment for research purposes is a violation of the Anaconda license unless you obtained your own license. More information on preparing for this change is available here. We"
rc-website-fork/content/post/2024-oct-maintenance.md,"understand that these changes may cause inconvenience, but these changes are mandated by the Anaconda licensing condition which we cannot control. If you have any questions or concerns, please feel free to reach out. Modules The following modules will be removed during the maintenance period. {{< table title=""replacement"" class=""table tablestriped"" }} | Module | Removed version | Replacement | |||| |anaconda | 2023.07py3.11 | miniforge/24.3.0py3.11 | |anvio | 6.2 | 8 | |codeserver| 4.16.1 | 4.92.2 | |deeplabcut | 2.2.1.1anipose| 3.0.0rc4 | |deeptools | 3.5.1 | 3.5.5 | |maestro | 1.3.0 | 1.5.1 | |mamba | 22.11.14 | miniforge/24.3.0py3.11 | |matlab | R2020a | R2024a | |matlab | R2020b | R2024a | |matlab | R2021a | R2024a | |matlab | R2021b | R2024a | |matlab | R2022a | R2024a | |matlab | R2022b | R2024a | {{< /table }}"
rc-website-fork/content/post/2024-july-afton-release.md,"Our new supercomputer, “Afton,” is now available for general use. This represents the first major expansion of RC’s computing resources since Rivanna's last hardware refresh in 2019. Afton represents a substantial increase in the HighPerformance Computing (HPC) capabilities available at UVA, more than doubling the available compute capacity. Each of the 300 compute nodes in the new system has 96 compute cores, an increase from a maximum of 48 cores per node in Rivanna. The increase in core count is augmented by a significant increase in memory per node. Each Afton node boasts a minimum of 750GB of memory, with some supporting up to 1.5TB. The large amount of memory per node allows researchers to efficiently work with the everexpanding datasets we are seeing across diverse research disciplines. Maintenance: July 2, 2024 {{< alertgreen }}The HPC system in the standard security zone, including Rivanna, will be down on Tuesday, July 2, 2024 beginning at 6 a.m. During the downtime RC engineers will implement final configuration changes in preparation of the full production release of the new Afton HPC system.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until the HPC systems are returned to service. The Rivanna and Afton production systems are expected to return to service by Wednesday, July 3 at 6 a.m. Questions: Please contact our user services team, or join us for our virtual office hours every Tuesday, 35 p.m. and Thursday, 1012 p.m.. {{% callout %}} What to expect after the maintenance? New hardware: On May 28, a total of 300 compute nodes, 96 cores each, based on the AMD EPYC 9454 architecture have been added to UVA's HPC environment as the new Afton"
rc-website-fork/content/post/2024-july-afton-release.md,"system. The new Afton hardware provides additional capacity for serial, parallel and GPU computing sidebyside with the existing Rivanna system. Configuration: The hardware partition definitions will be reconfigured to optimize effective use of the new Afton and existing Rivanna systems. (The Weka scratch filesystem will be mounted in nondedicated mode, which means all cores will be available. Previously 3 cores per node were dedicated to Weka.) Access: The Rivanna and Afton systems are accessible via the existing and shared Open OnDemand, FastX and ssh access points. Software, Code, and Job Submissions: The shared software stack and modules have been tested during the prerelease phase. In most cases users can utilize the system without any changes to their job submission scripts. In some instances users may need to update their Slurm job scripts or recompile their own code. The RC team is available to help with the transition. Policy: A new charge rate policy will be implemented during Fall 2024 (tentative) to reflect more closely the actual hardware cost. {{% /callout %}} FAQ {{% accordiongroup title=""Group"" id=""faqgroup""%}} {{% accordionitem title=""1. Is Afton replacing the older Rivanna system?"" id=""faq1"" %}} No, the new Afton system exists sidebyside with the existing Rivanna system. Both systems are accessible through shared login nodes, see ""How do I log in to the Afton system?"". {{% /accordionitem %}} {{% accordionitem title=""2. What compute capabilities does the new Afton hardware offer?"" id=""faq2"" %}} On May 28, a total of 300 compute nodes with 96 cores each, based on the AMD EPYC 9454 architecture, have been added to UVA’s HPC environment. The added nodes expand UVA's HPC capabilities in the following areas: A complete hardware refresh of the parallel partition with 96core nodes that roughly doubles its capacity (based on aggregated cpu core count). Expanded capacity of the standard"
rc-website-fork/content/post/2024-july-afton-release.md,"partition for single node jobs and highthroughput computing with up to 96 cores and 1.5TB of memory per node. Addition of new nodes with NVIDIA A40 general purpose graphics processing units (GPUs) to accommodate more ML/DL computing in the gpu partition. {{% /accordionitem %}} {{% accordionitem title=""3. How can I get an account to access the Afton system? Can I use my Rivanna allocation?"" id=""faq3"" %}} The service unit allocations are shared for Rivanna and Afton. If you already have an active Rivanna allocation, no action is required. If you'd like to start using Afton or Rivanna, please follow the instructions here. {{% callout %}} Please note: {{% pieligibility %}} {{% /callout %}} {{% /accordionitem %}} {{% accordionitem title=""4. How do I log in to the Afton system?"" id=""faq4"" %}} The login access points are shared for the Afton and Rivanna systems. Option 1: Web access via Open OnDemand: https://ood.hpc.virginia.edu Option 2: Remote Desktop via FastX: https://fastx.hpc.virginia.edu Option 3: Secure Shell (ssh) session: @login.hpc.virginia.edu See here for details. You must be a member of an active HPC allocation before you can log in. {{% /accordionitem %}} {{% accordionitem title=""5. Can I still use Rivanna?"" id=""faq5"" %}} Yes, login access points are shared for the Rivanna and Afton systems. We added new hardware feature tags that allow you to specifically request Rivanna resources for your compute jobs once logged in. See ""What are the changes to the hardware partitions?"" and ""What are hardware features? What are the hardware feature defaults for each partition?"". {{% /accordionitem %}} {{% accordionitem title=""6. What are the changes to the hardware partitions?"" id=""faq6"" %}} The following partition changes are taking place on July 2: The prerelease afton partition will be removed. The nodes will be placed in other partitions. The nodes making up the parallel partition"
rc-website-fork/content/post/2024-july-afton-release.md,"will be completely replaced with 200 Afton nodes. The original nodes will be placed into standard. The largemem partition will be removed. All 750GB nodes will be placed in the standard partition. All RTX3090 nodes from the gpu partition will be placed in the interactive partition. New partition configuration: {{< table title=""partitionconfig"" class=""table tablestriped"" }} | Partition | Rivanna Nodes | Afton Nodes | Use Cases | | | | | | | standard | yes | yes | For jobs on a single compute node, including those with large memory requirements. | | parallel | no| yes | For large parallel multinode jobs. | | gpu | yes | yes | For jobs using general purpose graphical processing units, e.g. for machine learning/deep learning. | | interactive | yes | yes | For quick interactive sessions, code development, and instructional use. It includes a small number of lowerend GPU nodes. | {{< /table }} {{% /accordionitem %}} {{% accordionitem title=""7. What happened to the largemem, dev, and instructional partitions?"" id=""faq7"" %}} Nodes of the largemem partition have been moved to the standard partition. See ""What are the changes to the hardware partitions?"" The dev and instructional partitions were merged and replaced with a single interactive partition during the Afton prerelease on May 30. {{% /accordionitem %}} {{% accordionitem title=""8. What are hardware feature constraints? What are the default hardware feature constraints for each partition?"" id=""faq8"" %}} Features constraints and generic resources (GRES) allow you to request specific hardware within a given partition. Through feature constraints you can specify if a job should be scheduled on the new Afton hardware or the older Rivanna system. Feature constraints are optional; you may submit jobs without feature constraints. If no feature constraint is specified, the Slurm scheduler will place your job on"
rc-website-fork/content/post/2024-july-afton-release.md,"available partition hardware following a default priority order. Note: Not all features are available in every partition. This table lists the available features for each partition, including the default if no feature is specified. {{< table title=""featureconstraintsandgres"" class=""table tablestriped"" }} | Partition | Available Features Constraints | GRES | Default Priority Order | Notes | | | | | | | | standard | afton, rivanna | None | rivanna afton | If not specified, the scheduler will attempt to place the job on Rivanna hardware first or Afton hardware as second alternative. | | parallel | None | None | n/a | The entire partition is configured with new Afton nodes. No feature constraint is required. | gpu | None | v100, a40, a6000, a10040gb, a10080gb | v100 a6000 a40 a10040gb a10080gb | If no GRES request is specified, the scheduler will attempt to place the job on a V100 node first and A100 80GB nodes (i.e. the BasePOD) hardware as last alternative. The A40 nodes were purchased along with the new Afton hardware. | | interactive | afton, rivanna | rtx2080, rtx3090 | rivanna afton | If not specified, the scheduler will attempt to place the job on Rivanna hardware first or Afton hardware as second alternative. | {{< /table }} See ""How do I use Afton for my Slurm job? Do I need to update my job scripts?"" and ""How can I use the new Afton hardware in Open OnDemand?"" for instructions on using these feature constraints in your job submission scripts or Open OnDemand. {{% /accordionitem %}} {{% accordionitem title=""9. How do I use Afton for my Slurm job? Do I need to update my job scripts?"" id=""faq9"" %}} Most users should be able to submit jobs without changing their Slurm job scripts, unless: invalid request"
rc-website-fork/content/post/2024-july-afton-release.md,"due to partition changes (see ""What are the changes to the hardware partitions?"") Example: A job submitted to will become invalid since the partition has been removed. One should submit to with (up to 1462G) to specify the memory. cost considerations (see How is compute time charged on the Rivanna and Afton systems?) Example: Instead of running a light GPU job on an A100 in gpu, request an RTX2080 or RTX3090 in interactive via gres=gpu. a need for specific Rivanna vs Afton hardware for performance/reproducibility/benchmarking reasons (only relevant for standard and interactive) Example: To restrict a standard job to run on the new Afton hardware, provide a constraint (constraint= or C): {{% /accordionitem %}} {{% accordionitem title=""10. How can I use the new Afton hardware in Open OnDemand?"" id=""faq10"" %}} When setting up an Interactive App session in Open Ondemand you may enter the constraint=afton or constraint=rivanna feature constraint in Optional: Slurm Option ( Reservation, Constraint ) field. Available feature constraints are listed here: ""What are hardware features? What are the default hardware features for each partition?"". {{% /accordionitem %}} {{% accordionitem title=""11. Do I need to recompile my code?"" id=""faq11"" %}} If you have already done this for the Afton prerelease testing then no. Otherwise please use the following flowchart. Which compiler did you use to build your code? Not Intel (e.g. GCC gcc, NVIDIA nvhpc) → no Intel → continue Do you intend to run your code on Afton hardware? (Please note the parallel partition will be completely replaced by Afton hardware.) No → no Yes → continue Did you use the x flag (e.g. xavx)? No → no Yes → yes, rebuild with march=skylakeavx512 instead of x... {{% /accordionitem %}} {{% accordionitem title=""12. How is compute time charged on the Rivanna and Afton systems?"" id=""faq12"" %}} Starting"
rc-website-fork/content/post/2024-july-afton-release.md,"Spring 2025, a new service unit (SU) charge rate policy will be implemented to reflect more closely the actual hardware cost. For all nonGPU jobs, the SU charge rate will be based on the amount and type of CPU cores (Intel on Rivanna, AMD on Afton) plus memory allocated. For GPU jobs (in gpu and interactive), the SU charge rate will be based on the number and type of GPU devices allocated. Detailed information about new SU charge rates can be found here. {{% /accordionitem %}} {{% accordionitem title=""13. Why are there different charge rates for Rivanna, Afton, and GPU hardware?"" id=""faq13"" %}} Starting Fall 2024 (tentative), a new charge rate policy will be implemented to reflect more closely the actual hardware cost. For all nonGPU jobs, the charge rate will be based on the amount of CPU cores and memory allocated. For GPU jobs (in gpu and interactive), the charge rate will be based on the amount of GPU devices allocated. Use of Afton hardware may allow jobs to complete faster but may consume more SUs overall due to a higher burn rate. {{% /accordionitem %}} {{% accordionitem title=""14. What is fairshare?"" id=""faq14"" %}} To ensure fair access to the HPC environment for all research groups, we utilize Slurm's job accounting and fairshare system. This system influences job placement priority, with a higher fairshare value typically resulting in a higher queue priority. However, the fairshare value decreases as more service units are consumed. Crucially, fairshare values are linked to the Principal Investigator (PI) of the allocation being utilized. This connection prevents any single group from dominating the resources and maintains fairness across PI groups, especially those who have not utilized their fairshare allocation for an extended period. Paid service units place fairshare values and job priority above those of"
rc-website-fork/content/post/2024-july-afton-release.md,"users utilizing instructional or standard allocations. {{% /accordionitem %}} {{% accordionitem title=""15. How do use of different hardware and service unit burn rates affect my fairshare?"" id=""faq15"" %}} The high performance new Afton hardware as well as the higherend GPU hardware incur higher service unit (SU) burn rates. For example, allocation of 40 cores and 256GB of memory on a new Afton node consumes more service units per hour than the same cpu core and memory allocation on an older Rivanna node. Similarly, use of an NVIDIA A100 80GB GPU device incurs a higher SU charge per hour compared to a lowerend NVIDIA A6000 GPU device. The more SUs have been consumed, the lower the fairshare value drops. This will impact the user's priority when submitting new jobs with the same allocation. Use of Afton hardware may allow jobs to complete faster but may consume more SUs overall due to a higher burn rate. See ""How is compute time charged on the Rivanna and Afton systems?"". {{% /accordionitem %}} {{% accordionitem title=""16. How can I get help?"" id=""faq16"" %}} Please contact our user services team, or join us for our virtual office hours every Tuesday, 35 p.m. and Thursday, 1012 p.m.. {{% /accordionitem %}} {{% /accordiongroup %}} Afton Release Announcements {{% accordiongroup title=""Comms"" id=""commsgroup"" %}} {{% accordionitem title=""May 30, 2024 Afton is available in prerelease configuration"" id=""comm3"" %}} Effective May 30, the new Afton HPC hardware is now available in a prerelease configuration as part of the HPC environment. During this prerelease phase the number of available Afton nodes may fluctuate as the RC team completes final configurations. The full production release of the Afton cluster with stable service of all 300 nodes is planned for Tuesday, July 2. Learn more. {{% /accordionitem %}} {{% accordionitem title=""May 20, 2024 Reminder:"
rc-website-fork/content/post/2024-july-afton-release.md,"Rivanna maintenance and Afton prerelease"" id=""comm1"" %}} Dear Rivanna user: A friendly reminder that Rivanna and Research Project storage will be down for maintenance from Tuesday, May 28 at 6 a.m. through Thursday, May 30 6 a.m.. How to prepare and what to expect during the maintenance? You may continue to submit jobs to Rivanna until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All Rivanna compute nodes and login nodes will be offline, including the Open OnDemand and FastX portals. Research Project storage will be unavailable. The UVA Standard Security Storage data transfer node (DTN) and Research Standard storage remain online throughout the maintenance period. Prerelease of the new Afton cluster after the maintenance All systems are expected to return to service by 6 a.m. on Thursday, May 30. The new Afton HPC hardware will become available in a prerelease configuration at that time, with the addition of 300 new compute nodes, 96 cores each, based on the AMD EPYC 9454 architecture. The new Afton hardware will provide additional capacity for serial, parallel and GPU computing sidebyside with the existing Rivanna system. During this prerelease phase the number of available Afton nodes may fluctuate as the RC team completes final configurations. The full production release of the Afton cluster with stable service of all 300 nodes is planned for Tuesday, July 2. A detailed description of the maintenance plan and instructions for using the new Afton resources is available on the RC website. If you have any questions about the Rivanna maintenance or Afton prerelease, you may contact our user services team. At your service, RC staff Research Computing E hpcsupport@virginia.edu P 434.243.1107 University of Virginia P.O. Box 400231 Charlottesville"
rc-website-fork/content/post/2024-july-afton-release.md,"22902 {{% /accordionitem %}} {{% accordionitem title=""May 14, 2024 Rivanna maintenance and Afton prerelease"" id=""comm2"" %}} Dear Rivanna user: A reminder that Rivanna and Research Project storage will be down for maintenance from Tuesday, May 28 at 6 a.m. through Thursday, May 30 6 a.m.. How to prepare and what to expect during the maintenance? You may continue to submit jobs to Rivanna until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All Rivanna compute nodes and login nodes will be offline, including the Open OnDemand and FastX portals. Research Project storage will be unavailable. The UVA Standard Security Storage data transfer node (DTN) and Research Standard storage remain online throughout the maintenance period. Prerelease of the new Afton cluster after the maintenance All systems are expected to return to service by 6 a.m. on Thursday, May 30. The new Afton HPC hardware will become available in a prerelease configuration at that time, with the addition of 300 new compute nodes, 96 cores each, based on the AMD EPYC 9454 architecture. The new Afton hardware will provide additional capacity for serial, parallel and GPU computing sidebyside with the existing Rivanna system. During this prerelease phase the number of available Afton nodes may fluctuate as the RC team completes final configurations. The full production release of the Afton cluster with stable service of all 300 nodes is planned for Tuesday, July 2. A detailed description of the maintenance plan and instructions for using the new Afton resources is available on the RC website. If you have any questions about the Rivanna maintenance or Afton prerelease, you may contact our user services team. At your service, RC staff Research Computing E hpcsupport@virginia.edu P"
rc-website-fork/content/post/2024-july-afton-release.md,434.243.1107 University of Virginia P.O. Box 400231 Charlottesville 22902 {{% /accordionitem %}} {{% /accordiongroup %}}
rc-website-fork/content/post/2024-may-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on Tuesday, May 28, 2024 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. While drive mapping and project storage will be unavailable, other storage will remain accessible through Globus. All systems are expected to return to service by Thursday, May 30 at 6 a.m. IMPORTANT MAINTENANCE NOTES Hardware and partition changes afton: We are pleased to announce the addition of 300 nodes, 96 cores each, based on the AMD EPYC 9454 architecture. For the time being, these new nodes are placed into their own partition afton, in which each user can request up to 6000 cores in aggregate. There is some chance that 100 nodes will be available initially, in which case the core limit will be adjusted to 2000. (The full release is scheduled for July, where the nodes will be integrated into the other existing partitions. More information to follow.) interactive: The dev and instructional partitions will be combined into interactive, which will have a maximum time limit of 12 hours and a maximum core limit of 24. The SU charge rate will be 1. Two RTX 2080Ti nodes (10 GPU devices each) will be added from the gpu partition. gpu: Four NVIDIA A40 (48 GB) nodes, 8 GPUs each, will be added to the gpu partition. The RTX 2080Ti nodes (2 x 10 GPU) will be moved into the interactive partition. Please contact us if you have questions about the new partitions or if you experience issues with the new hardware. System upgrades Operating System: Rocky 8.7 → 8.9. There is no need to rebuild your own"
rc-website-fork/content/post/2024-may-maintenance.md,"code. (Intel users may need to do so but for a different reason; see below.) Slurm: 21.08.8 → 23.02.7. Jobrelated commands remain the same. NVIDIA driver: 535.104.12 → 550.54.14. Modules Attn Intel users: With the addition of the Afton nodes based on the AMD EPYC architecture, we have reorganized and rebuilt all modules under the intel toolchain. If you used x (e.g. xavx) to build your own code, you should rebuild it with march=skylakeavx512 for it to run on both AMD and Intel hardware. Below we list all the modules that are upgraded or moved to a different toolchain. (Intel modules not listed can be loaded the same way as before.) The toolchain needs to be loaded before the module, e.g. module load gcc gmp. The gompi toolchain is equivalent to gcc openmpi. There is no impact on the existing modules built with GCC. {{< table title=""intel"" class=""table tablestriped"" }} | Module | New version | Toolchain| |||| |abinit/8.10.3, 9.8.3| 10.0.3 | intel | |chemps2/1.8.12 | (removed) | | |cesm/2.1.3 | 2.2.2 | intel | |cp2k/2023.1 | 2024.1 | intel | |gmp/6.2.0 | | gcc | |kimapi/2.3.0 | | gcc | |mpfr/4.2.0 | | gcc | |ncview/2.1.7 | 2.1.10 | intel | |neuron/8.2.2 | | gompi | |p3dfft/2.7.9 | | gompi | |pcmsolver/1.3.0| | gompi | |pcre2/10.42 | | gcc | |raxml/8.2.12 | | gompi | |readosm/1.1.0a | | gcc | |scotch/7.0.3 | | gompi | |shapelib/1.5.0 | | gcc | |viennarna/2.5.1| | gcc | |voro++/0.4.6 | | gcc | {{< /table }} Attn NVHPC users: The compiler toolchain nvhpc and nvompi will be upgraded to 24.1 and 24.14.1.6, respectively. The previous versions 23.7 and 23.74.1.4 will be removed. All modules under this toolchain will be rebuilt. There should be no need to rebuild your own code. The following modules"
rc-website-fork/content/post/2024-may-maintenance.md,"will be removed from Rivanna during the maintenance period. {{< table title=""replacement"" class=""table tablestriped"" }} | Module | Removed version | Replacement | |||| |aocc |4.1.0 | 4.2.0 | |cellranger|6.0.1, 7.2.0| 8.0.0 | |fiji |1.53t | 2.14.0 | |fsl |6.0.5 | 6.0.7.6| |gatk |4.2.3.0 | 4.3.0.0 | |gpumd |3.7 | 3.9.1 | |picard |2.23.4 | 2.27.5 | {{< /table }}"
rc-website-fork/content/post/2025-RC-event.md,"Join us for our annual UVA Research Computing Exhibition: Wednesday, April 23, 10am 2pm in the Newcomb Hall Ballroom Drop by anytime! The UVA Research Computing Exhibition showcases the incredible research happening across UVA. Whether you are interested in sharing your research or discovering what others are working on, this exhibition offers the opportunity to: See and discuss research posters Hear Lightning Talks from UVA faculty Learn about resources for research Explore diverse research methods Find potential collaborators Connect with the broader research community Enjoy free food and beverages Exhibition Agenda Attendees can drop by anytime. 8:309:30am — Participant SignIn and Poster Setup Participants check in, set up their posters, and get ready to share their research. 10am — Group A Poster Session Meet participants, discuss their research, exchange ideas, and explore potential collaborations. 11am — Welcome and Lightning Talks Hear short, engaging talks from UVA faculty about their research and techniques. 12:30pm — Group B Poster Session Meet participants, discuss their research, exchange ideas, and explore potential collaborations. 1:30pm — Awards Ceremony Celebrate outstanding research and recognize the winners of the poster competition. 2:00pm — Event WrapUp"
rc-website-fork/content/post/2021-june-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on Tuesday, June 15, 2021, beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Rivanna is expected to return to service by 6 a.m. on Wednesday, June 16. IMPORTANT MAINTENANCE NOTES Globus Some Globus users may need to rebuild their shared connections after the maintenance period has ended. Users who require assistance with this task are invited to join us for office hours between 10 a.m. and 12 p.m. on Thursday, June 17. The Zoom link is available here. Modules The following software modules will be removed from Rivanna during the maintenance period: singularity/2.6.1, 3.5.2, 3.6.1 replaced by 3.7.1 (details) anaconda/5.2.0py3.6, 2019.10py3.7 replaced by 2020.11py3.8 (details) julia/0.6.0, 1.1.0, 1.3.1 replaced by 1.5.3, 1.6.0 vscode/1.50.1, 1.53.2 replaced by Code Server on Open OnDemand cellprofiler/2.2.0 replaced by 3.1.8 meme/4.10.2 replaced by 5.1.0, 5.3.3 nextflow/0.26.0.4715 replaced by 20.10.0 phono3py/1.19.1 1.22.3 included in phonopy/2.9.3 qiime2/2020.6 replaced by 2020.8 salmon/0.11.2 replaced by 1.2.1 samtools/0.1.20, 1.4.1, 1.7 replaced by 1.9, 1.10, 1.12 lftp/4.8.4 replaced by 4.9.2 The following upgrades will take place during the maintenance period. Upgrades to default versions of applications: JupyterLab backed by Anaconda 2020.11 with Python 3.8.8 python/3.7.7 3.8.8 pytorch/1.5.1 1.8.1 tensorflow/2.1.0py37 2.4.1 cellrangeratac/1.2.0 2.0.0 lammps/20200615 20201029 meme/5.1.0 5.3.3 samtools/1.10 1.12 For anaconda/pythondependent modules, please see below. New modules: rapidsai/0.19 NVIDIA data science libraries pipenv/2020.11.15 automatically create and manage a virtualenv Changes to Singularity modules All Singularity modules are now under 3.7.1. If you hardcoded older Singularity versions, e.g. bash module load singularity/2.6.1 or 3.5.2, 3.6.1 please change it to bash module load singularity The containers themselves have not been modified. We have not"
rc-website-fork/content/post/2021-june-maintenance.md,"encountered backwards compatibility issues; please let us know if you do. If you need to know the Singularity version that was used to create a container, run: bash singularity inspect /path/to/container Changes to Anaconda/Python modules Many of our Anaconda/Python modules have been upgraded to Python 3.8.8 in light of security vulnerabilities. If you need assistance with migrating python packages from one version to another, please visit here. Note that conda environments created by one anaconda module version can be activated by another. The following table shows the detailed version changes for all affected modules. Please note: The Python version is upgraded to 3.8.8 unless otherwise stated. The new version replaces the current default. If the new version is , that means the module version remains the same. In some cases, the module load command is different. Check module spider <module/<version if you cannot load a module. If you must use a particular module with an older Python version, please create your own conda environment. | Module | Version | Python| NEW version | NEW Python | Removed versions | ||||||| |anaconda | 2020.11py3.8 | 3.8.5 | | | 5.2.0py3.6, 2019.10py3.7 | | ase | 3.20.1 | 3.7.9 | | | 3.17.0py3 | | bart | 2.0 | 3.7.8 | | | 1.0.1 | |bioconda | py3.8 | 3.8.5 | | | py3.6, py3.7 | |biopython | 1.70py2 | 2.7.17| 1.78py3 | | | |cudatoolkit | 10.1.168py3.6| 3.6.10| 11.0.3py3.8 | | 10.1.168py3.6 | | cutadapt | 2.5 | 3.7.4 | 3.4 | | 1.16, 2.5 | | deeptools | 3.3.1 | 3.6.6 | 3.5.1 | | 2.5.3, 3.3.1 | |gcloudsdk | 196.0.0 | 2.7.17| 334.0.0 | | 196.0.0 | |gdcclient | 1.5.0 | 3.7.7 | 1.6.0 | 3.7.10 | 1.3.0, 1.5.0 | |globuscli | 1.12.0 | 3.7.7 | 2.0.0 | |"
rc-website-fork/content/post/2021-june-maintenance.md,"1.11.0, 1.12.0 | |googleapi | 1.9.6 | 2.7.17| 2.0.2 | | 1.9.6 | |gpustat | 0.6.0 | 3.7.7 | | | | | hexrd | 0.6.12 | 2.7.17| 0.8.4 | | jb0.3.x, jb0.5.6, 0.6.12 | | hoomd | 2.9.4 | 3.7.7 | 2.9.6 | | 2.9.4 | | idr | 2.0.2py3 | 3.6.6 | | | | | iqtree | 2.0.3 | 3.7.9 | 2.1.2 | | 2.0.3 | | intervene | 0.6.4 | 3.7.3 | 0.6.5 | | 0.6.4 | | kallisto | 0.44.0 | 3.7.3 | 0.46.2 | | 0.44.0 | | marge | 1.0 | 3.6.7 | | | | | mayavi | 4.5.0 | 2.7.15| 4.7.2 | | 4.5.0 | | meson | 0.53.1 | 3.7.7 | 0.57.1py3.8 | | 0.53.1, 0.54.3 | | mrtrix3 | rc3 | 2.7.17| 3.0.2 | | rc3 | |mrtrix3tissue| 5.2.8 | 2.7.17| 5.2.9 | | 5.2.8 | | mysqlclient | 1.4.6py3.7 | 3.7.4 | 2.0.3py3.8 | | 1.4.4py3.6, 1.4.6py3.7 | | ninja | 1.10.0 | 3.7.7 | 1.10.2py3.8 | | 1.10.0 | |openslidepython| 1.1.1py3 | 3.6.6 | 1.1.2py3 | | 1.1.1py3 | | phonopy | 2.6.1 | 3.7.7 | 2.9.3 | | 2.6.1 | | pybind11 | 2.2.4py3.7 | 3.7.4 | 2.6.2py3.8 | | 2.2.4py3, 2.2.4py3.7 | | reframe | 2.17 | 3.6.7 | | | | |snakemake | 5.2.2 | 3.6.6 | 6.0.5 | | 5.2.2 | |snapstanford| 5.0.9py3.6 | 3.6.6 | 5.0.9py3.8 | | 4.1, 5.0.9py3.6, snapstanfordpy/4.1 | |spades | 3.15.0 | 3.7.4 | 3.15.2 | | 3.15.0 | |thirdorder | 1.1.1py3 | 3.6.6 | | | | |trimgalore | 0.6.4 | 3.6.8 | | | 0.4.5 | | wasp | 0.3.4 | 3.7.7 | | | |"
rc-website-fork/content/post/2020-june-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on Wednesday, June 17, beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Rivanna is expected to return to service later in the day. The following software modules will be removed from Rivanna during the maintenance period: intel/16.0 & toolchains replaced by intel/18.0, 20.0 imsl/7.1.0 expired license The following upgrades will take place during the maintenance period: intel/20.0 & toolchains default 18.0 goolfc/8.3.03.1.610.2.89 GCC 8 toolchain with OpenMPI, CUDA support, and numerical libraries cuda/10.2.89 R/3.6.3 (default), 3.5.3, 3.4.4; removed 3.6.[02], 3.5.1, 3.4.3 (For more details, see ""R Updates"" and ""New Libraries"") matlab/R2020a hdf/4.2.14 added shared libraries but disabled Fortran (if you need Fortran please contact us) netcdf/4.7.3 sagemath/9.0 removed 8.0 salmon/1.2.1 removed 1.0.0 and 1.1.0 due to segfault bug snapstanford/5.0.9py3.6 New tools: R/4.0.0 under intel/18.0 python/3.7.7 under intel/20.0 Intel Distribution for Python gpustat/0.6.0 GPU monitoring tool orca/4.2.1 quantum chemistry package alamode/1.1.0 anvio/6.2 atat/3.36"
rc-website-fork/content/post/2023-october-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on Tuesday, October 3, 2023 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All systems are expected to return to service by 6 a.m. on Wednesday, October 4. IMPORTANT MAINTENANCE NOTES New largemem nodes RC engineers will be adding 36 nodes, each with 40 cores and 750 GB total memory, to the largemem partition on Rivanna. Jobs that need more memory than 9 GB per core should be submitted to the largemem partition rather than the standard partition. Some examples are given below. I need 4 cores and 100 GB memory. Since this amounts to 25 GB memory per core, the job should be submitted to largemem. I need 10 cores and 50 GB memory. Since this amounts to 5 GB memory per core, the job should be submitted to standard without specific memory requests. By default 9 GB per core will be allocated. I am not sure how much memory I need. First submit the job to the standard partition without specific memory requests. If the job runs out of memory, resubmit to the largemem partition. To check the memory usage of a completed job, you may either run the seff command or add to your Slurm script: and check the report in your email. NVIDIA driver upgrade and modules The NVIDIA driver will be upgraded to version 535.104.12 (CUDA 12.2). The default CUDA module version will remain at 11.4.2. New modules will be added: cuda/12.2.2 cudnn/8.9.4.25 pytorch/2.0.1 tensorflow/2.13.0 The corresponding Jupyter kernels for PyTorch and TensorFlow will be provided as well. AlphaFold versions 2.1.2, 2.2.2, and their corresponding"
rc-website-fork/content/post/2023-october-maintenance.md,"database will be removed. The 2.3 database will be migrated off of the current /project storage and the ALPHAFOLDDATAPATH environment variable will be updated accordingly. QGIS (Open OnDemand) will be upgraded to 3.28.10. Old scratch permanently retired on October 17 A reminder that the /oldscratch (i.e. /gpfs/gpfs0/scratch) filesystem will be permanently retired on October 17 and all the data it contains will be deleted. A sample script for users who wish to transfer files to the new /scratch system can be found here. If you have any questions or concerns about the maintenance period, you may contact us here."
rc-website-fork/content/post/2022-women-in-hpc-202301.md,"The Data Analytics Center is UVA's new hub for the management and analysis of your large research data. Need help with your computational research? DAC staff specialize in key domain areas such as image processing, text analysis, bioinformatics, computational chemistry and physics, neural networks, and more. And because the DAC team is located within Research Computing, they can assist in getting your workflows running on the University’s highperformance cluster or secure data system. They can answer your basic computational questions or, through funded engagements, be embedded in your projects. Big data doesn’t have to be a big deal. Learn how DAC can assist with your computational research – schedule an initial consultation with one of their data analysts by submitting a consultation request. Information on DAC's full range of services is available on the DAC webpage."
rc-website-fork/content/post/2023-03-matlab-seminar.md,"Medical images come from multiple sources such as MRI, CT, Xray, ultrasound, and PET. Analysis of these images requires a comprehensive environment for data access, visualization, processing, and algorithm development. The main challenge is to extract clinically meaningful information based on advanced techniques such as Artificial Intelligence (AI). To achieve this, one needs to clean, segment, register, and label a large collection of images. For the AI analysis, there are many more challenges such as iteratively adjusting AI models or learning parameters. MATLAB provides tools such as Medical Imaging Toolbox and Deep Learning Toolbox and algorithms for endtoend medical image analysis and AI workflow. When: March 16, 2023, 34PM EDT Highlights In this presentation, you will learn how to: Easily import and visualize 2D images and 3D volumes interactively Segment, register, and label medical image and volume data Import and edit pretrained networks for processing Design, train, and test AI and deep learning models About the Presenter Dr. Elvira OsunaHighley Principal Education Application Engineer MathWorks eosunahi@mathworks.com Elvira OsunaHighley, Ph.D. is part of a global team supporting academic research and teaching at MathWorks. Before joining MathWorks, she was on the faculty of the Computational Biology Department at Carnegie Mellon University. She holds a doctorate in Biomedical Engineering from Carnegie Mellon University, where her research involved applying machine learning techniques to fluorescence microscope images."
rc-website-fork/content/post/2018-fall-workshops.md,"School of Medicine Research Computing provides training opportunities covering a variety of data analysis, basic programming and computational topics. Workshops break roughly into the three main areas relevant to computationallyintensive research: code, data, and computing. All of the classes are taught by RC experts and are freely available to UVa faculty, staff and students. R / R package development Python Matlab Biomedical Image Processing Bioinformatics on HPC Data manipulation Data visualization Databases Machine Learning Cloud Computing Containers Rivanna (HPC) Ivy (Secure Computing)"
rc-website-fork/content/post/2024-september-17-open-house.md,"UPDATE: The Research Computing Open House was held on a blustery, rainy day, but the spirits of the staff and attendees were not dampened. Turnout was above expectations despite the wet weather. Attendees enjoyed the buffet and their interactions with RC staff. The winners of the randomdrawing prizes were Maria Luana Morais, SOM Matt Panzer, SEAS Artun Duransoy, SEAS {{< rawhtml }} {{< /rawhtml }} Please join us at the Research Computing Open House on Tuesday, September 17, 2024, from 25 p.m. in the Commonwealth Room at Newcomb Hall. We are excited to host the UVA community to share updates on a new supercomputer and services that we are offering. Why Attend? Talk with research computing experts and staff. Have your questions answered. Receive the latest information on research computing at UVA, including Afton, UVA’s new supercomputer, and other highperformance computing resources Secure compute & storage solutions Research collaboration and grant support services the Data Analytics Center: dataset management and analytics including AI the Digital Technology Core: use of wearables, smartwatches, smartphones or IoT devices in your research Upcoming RC workshops Learn about our student workers program Enjoy light refreshments How to Attend You are welcome to drop in anytime during the event and stay as long as you would like. If you are planning to attend, please RSVP for the Open House in advance. Those who RSVP and attend will be entered into a drawing for for one of three prizes totaling $150! Time & Location: Tuesday, September 17 25 p.m. Commonwealth Room at Newcomb Hall Event Schedule: Time Activity 2:052:20 Opening Remarks: Presented by Joshua Baller, Associate Vice President for Research Computing 2:205:00 Networking and Information Tables: Explore information tables and interactive demos. Research Computing staff will be available to answer questions. Have a Question? Contact Research Computing"
rc-website-fork/content/post/2024-september-17-open-house.md,at hpcsupport@virginia.edu.
rc-website-fork/content/post/2022-01-women-in-hpc.md,"We are proud to announce the founding of Virginia's first Women in HighPerformance Computing (VAWHPC) program. Join us for our first event of 2022: Female research leaders of the Commonwealth sharing and discussing how HPC has facilitated their scientific research and professional careers. Topic: How does HPC help with your scientific research Faculty perspectives, Part II When: Jan 25, 2022 01:00 PM, Eastern Time (US and Canada) Our speakers: Anne Brown (VT) is an Assistant Professor of Biochemistry, Science Informatics Consultant and Health Analytics Coordinator at Virginia Tech. Her research interests include utilizing computational modeling to answer biological questions and aid in drug discovery and the application of computational molecular modeling to elucidate the relationship between structure, function, and dynamics of biomolecules. Jenna Cann (GMU) is a postdoctoral fellow at NASA Goddard Space Flight Center. Jenna received a PhD in Physics from George Mason University. Her research focuses on studying black holes in dwarf and low metallicity galaxies, in an effort to constrain the origins of supermassive black holes that can be up to billions of times the mass of our Sun. To do this, they use both theoretical modeling with the Cloudy spectral simulation code and infrared and Xray observations to determine the most effective ways to find these elusive objects. Jenna currently serves as a coofficer in the NASA Goddard Association for Postdoctoral and Early Career Scholars (NGAPS+) and was a cofounder of the GMU Physics and Astronomy department's diversity, equity, inclusion, and accessibility (DEIA) organization, SPECTRUM. Alexis Edwards (VCU) is an Associate Professor of Psychiatry at Virginia Commonwealth University. Her research focuses on understanding the etiology of substance use disorders, suicidal behavior, and internalizing problems, including how these outcomes are related to one another. Virginia WHPC is committed to increasing diversity and inclusion by promoting and encouraging"
rc-website-fork/content/post/2022-01-women-in-hpc.md,the participation of women in highperformance computing and related fields.
rc-website-fork/content/post/2023-july-scratch-transfer.md,"{{< alertgreen }}During the July 18th maintenance, RC engineers installed a new /scratch file storage system on Rivanna. We have created sample scripts and instructions to help you transfer your files from the previous file system to the new one.(Expand the link below for details.){{< /alertgreen }} The previous scratch filesystem, now called /oldscratch, will be permanently retired on October 17, 2023 and all the data it contains will be deleted. Users should clean up their /oldscratch directory in preparation, to minimize the load. A sample script is posted below. Modified queue limits have been implemented to provide maximum read/write performance of the new /scratch filesystem. Please refer to our updated documentation and adjust your job scripts accordingly. Transfer Instructions Example script to copy files {{< pullcode file=""/static/scripts/democopyscratch.slurm"" lang=""bash"" }} The script will also be available through the Open OnDemand Job Composer: Go to Open OnDemand Job Composer Click: New Job From Template Select democopyscratch In the right panel, click ""Create New Job"" This will take you to the ""Jobs"" page. In the ""Submit Script"" panel at the bottom right, click ""Open Editor"" Enter your own allocation. You may edit the script as needed. Click ""Save"" when done. Going back to the ""Jobs"" page, select democopyscratch and click the green ""Submit"" button. As we expect a high volume of data migration, please refrain from doing so directly on the login nodes but instead submit it as a job via the provided Slurm script as described above. The new scratch is subject to the same 10 TB quota and 90day purge policy. There is no restriction on the number of files. A friendly reminder that scratch is intended as a temporary work directory, not longterm storage space. It is not backed up and old files need to be purged periodically for"
rc-website-fork/content/post/2023-july-scratch-transfer.md,"system stability. RC offers a number of lowcost storage options to researchers. For more information, visit our storage page."
rc-website-fork/content/post/2022-december-maintenance.md,"{{< alertgreen }}Rivanna will be down for maintenance on December 19, 2022 beginning at 6 a.m.{{< /alertgreen }} You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Users will not be able to access the Globus data transfer node during the maintenance period. All systems are expected to return to service by 6 a.m. on Tuesday, December 20. Globus users may need to rebuild their shared collections. IMPORTANT MAINTENANCE NOTES Two new toolchains are now available: goolf/11.2.04.1.4 and intel/2022.11. The former consists of GCC 11.2.0, OpenMPI 4.1.4, and math libraries. The latter is Intel oneAPI that consists of the Intel compilers, MKL, and MPI. (Note that there is no need to load intelmpi from this version onwards.) The default versions have not been upgraded this time. Users with their own compiled codes are encouraged to build them with the new toolchains and report any issues. Modules The following software modules will be removed from Rivanna during the maintenance period: | Module | Removed version | Replacement | |||| |blender |2.78c | 3.2.1 | |cellranger |4.0.0 | 5.0.0, 6.0.1, 7.0.1 | |cellrangeratac |1.2.0 | 2.0.0 | |diamond |0.9.13| 2.0.14| |drmaa |1.1.2 | 1.1.3 | |fsl |6.0.0 | 6.0.5 | |go |1.8.1, 1.13.4 | 1.18.4 | |gparallel |20170822 | parallel/20200322 | |gurobi |9.0.1, 9.1.1 | 9.5.0 | |htslib |1.4.1 | 1.7, 1.9 | |igvtools |2.8.9 | 2.12.0 | |librmath |3.6.2 | 3.6.3 | |macs2 |2.1.2 | 2.2.7.1 | |micromamba |0.7.14| 0.24.0 | |nextflow |0.26.0.4715 | 20.10.0 | |salmon |1.2.1 | 1.5.1 | |seqoutbias |1.2.0 | 1.3.1 | |sratoolkit |2.8.0, 2.9.1 | 2.10.5 | |trimmomatic |0.36 | 0.39 | |vcftools |0.1.15| 0.1.16 | Archived containers can be found"
rc-website-fork/content/post/2022-december-maintenance.md,in /share/resources/containers/singularity/archive. Upgrades: eccodes/2.26.0 under gcc/9.2.0 openmpi/3.1.6 hicpro/3.1.0 kraken2/2.1.2 from kraken/0.10.5; note module name change openfoam/v2206 under goolf/9.2.03.1.6 R/4.2.1 under goolf/9.2.03.1.6 ruby/3.1.2 under gcc/9.2.0 Default version changes: alphafold/2.2.0 → 2.3.0 cellranger/5.0.0 → 7.0.1 cmake/3.16.5 → 3.23.3 deeplabcut/2.2 → 2.2.1.1anipose matlab/R2022a → R2022b pytorch/1.10.0 → 1.12.0 (includes PyTorch Geometric) qiime2/2020.8 → 2022.2 tensorflow/2.7.0 → 2.10.0 New modules: clang/10.0.1 crossftp/1.99.9 gawk/5.1.1 isaacgym/1.0.preview4 nibabies/22.1.3 rnaeditor/1.1a
rc-website-fork/content/post/2024-aug-maintenance.md,"{{< alertgreen }}The HPC cluster will be partially down for maintenance on Tuesday, Aug 13, 2024 beginning at 6 a.m.{{< /alertgreen }} The following nodes will be unavailable during this period: all of parallel afton nodes in standard and interactive A40 GPU nodes in gpu The nodes are expected to return to service by Wednesday, Aug 14 at 6 a.m. There is no impact on other nodes or services. Jobs on other nodes will continue to run."
rc-website-fork/content/education/courses.md,"In addition to providing free, inperson workshop training, UVA Research Computing staff teach forcredit courses. Below is a selection of courses that members of our group have taught, cotaught or provided guest lectures: BIMS 8382: Introduction to Biomedical Data Science Spring 2017, Spring 2018 This course introduces methods, tools, and software for reproducibly managing, manipulating, analyzing, and visualizing largescale biomedical data. Specifically, the course introduces the R statistical computing environment and packages for manipulating and visualizing highdimensional data, covers strategies for reproducible research, and culminates with analysis of data from a real RNAseq experiment using R and Bioconductor packages. CS 6501: Distributed & Cloud Computing Spring 2017, Spring 2018 This graduate course introduces a basic grounding in designing and implementing distributed and cloud systems. It aims to acquaint students with principles and technologies of server clusters, virtualized datacenters, Grids/P2P, Internet clouds, social networks, Internet of Things (IoT), and applications. Students will have the opportunity to gain handson experience on public cloud such as Amazon EC2. Selected scientific applications will also be used as case studies to gain handson experiences. CS 4740: Cloud Computing Spring 2018, Fall 2020, Fall 2021 Investigates the architectural foundations of the various cloud platforms, as well as examining both current cloud computing platforms and modern cloud research. Student assignments utilize the major cloud platforms. DS 3002: Data Science Systems Spring 2021 Exposes students to contemporary platforms, tools, and pipelines for data analysis through a series of steadily escalating use cases. The course will begin with simple local database construction and evolve to cloud based providers such as AWS or Google Cloud. Attention is given to data lakes and NoSQL as appropriate. Data Science Bootcamp: Computing, Storage & Data Analysis in the Cloud Summer 2017 This 1.5day course introduces MSDS students to the basics of cloud computing"
rc-website-fork/content/education/courses.md,"in AWS, and the independent management of code, data, and computing resources in a research environment. Particular concern is given to the concepts of programmable, reusable, scalable resources in the AWS cloud, through handson labs in EC2 and S3."
rc-website-fork/content/education/rivanna-instructional.md,"Instructors can request instructional allocations on Rivanna and Afton for classes and extended workshops. These allocations are timelimited and generally allow access to a restricted set of nodes and only one special Slurm partition, but are otherwise equivalent to any allocation. Resource Availability Hardware and Partition Instructional allocations may use interactive partition. The instructional allocation is 100,000 SUs for the semester during which the course is conducted. For workshops, the allocation will persist during the workshop and for two days afterwards. RC offers several lowcost storage options to researchers, including 10TB of Research Standard storage for each eligible PI at no charge. Instructors are encouraged to utilize this 10TB of storage for both research and teaching activities. For more detailed descriptions of our storage options, visit https://www.rc.virginia.edu/userinfo/storage/. Software & Storage Environment Research Computing's primary focus is supporting the direct research mission of the University. Instructional allocations are provided in recognition of the many areas where the educational and research missions of the University meet, and in recognition that there is value in providing UVA's diverse communities with experience in an HPC environment. However, staff time is a highly limited resource and instructional use of RC systems as a largely 'asis' service with standardized software and storage environments. We are unable to provide customization of the environment for specific classes. Interface For most classes, we recommend the Open OnDemand interface if it suits the expected usage. This does not require knowledge of Unix and greatly reduces the training burden. The Open OnDemand interface requires only Netbadge credentials and can be accessed without a VPN from off Grounds. If Open OnDemand is not adequate, the other recommended interface is FastX Web. This is a remote desktop application and requires the students to be able to navigate a Unix desktop system. Access from"
rc-website-fork/content/education/rivanna-instructional.md,"off Grounds via FastX requires a VPN connection. FastX connects only to a frontend. We significantly restrict the time, memory, and cores available to frontend jobs. If students are running anything but very short jobs, the Open OnDemand applications should be utilized. These access the compute nodes and are far less limited. Open OnDemand provides a remote desktop on compute nodes as well as direct access to JupyterLab, the Matlab Desktop, and Rstudio Server. How to Submit a Request Instructors planning to use HPC should fill out the form. You will need to create the Grouper (requires VPN connection) allocation group. We suggest a generic group name related to the course rubric, e.g. cs5014. Once the group is created, the instructor or a designated group administrator will need to add the student IDs. The instructor should empty the membership of the group after the class or workshop has terminated. Instructors will need to submit an instructional allocation renewal request at the start of each semester. Using the Allocation Prior to the first class use, instructors should test the allocation and the software applications required during class. Please do not wait until multiple students are attempting to use it. Passwords Students, particularly undergraduates, frequently experience password difficulties. Rivanna and Afton use the Eservices password to authenticate, but few students know this password. Instructors are urged to communicate to students that they should go to the ITS password page at least several hours in advance and change their Netbadge password before using the system. Changing the Netbadge password will sync the Eservices password with it. Partition and Reservations The allocation will have access to the interactive partition. Students can enter this with the p or partition options to Slurm. If students use the Open OnDemand interface, they will enter this into the"
rc-website-fork/content/education/rivanna-instructional.md,"appropriate textbox when starting their interactive job application. Instructors are urged to request reservations for their classes. The reservation will be created to coincide with the class meeting time. Students must add an option reservation=yourreservation in order to access the reserved resources. Students may still use the instructional partition outside the reservation, but those jobs will wait like any other queued job. Outside the dedicated reservation window jobs should be submitted without the reservation flag for immediate queueing; otherwise the job will be pending until the next reservation window opens. For batch jobs, the reservation can be entered on the command line sbatch reservation=yourreservation myscript.slurm or it can be added to the job script preamble For Open OnDemand interactive applications, it should be entered as an additional Slurm option. Training Research Computing staff are available to come to a class session to provide training to the students. This can be done inperson, when possible, or virtually through Zoom."
rc-website-fork/content/education/workshops.md,"UVA Research Computing provides training opportunities covering a variety of data analysis, basic programming and computational topics. All of the classes listed below are taught by experts and are freely available to UVa faculty, staff and students. New to HighPerformance Computing? We offer orientation sessions to introduce you to the Afton & Rivanna HPC systems on Wednesdays (appointment required). Wednesdays 3:004:00pm Sign up for an ""Intro to HPC"" session Upcoming Workshops {{% upcomingworkshopssmart %}} Research Computing is partnering with the Research Library and the Health Sciences Library to deliver workshops covering a variety of research computing topics. All Upcoming Workshops from UVA Library Research Data Services All Upcoming Workshops from UVA Health Sciences Library Workshop Material Course material and exercises are available through a companion site. Feel free to browse classes, tutorials and workshop material and learn at your own pace. https://learning.rc.virginia.edu New Tutorials Specifically, check out these new tutorials! {{% newtutorials %}} Previous Workshops Advanced Computing in the Cloud Advanced Data Manipulation with R Advanced Data Visualization with R Analyzing 16s RNA Amplicons Building Shiny Web Applications in R Conditionals and Iteration in R Data Analysis & Visualization with Python Data Visualization with Matlab Databases and How to Use Them Docker Containers for Scientific Research How to Work With Databases Image Processing with Matlab Introduction to Cloud Computing Introduction to Docker Containers Introduction to Git and GitHub Introduction to High Performance Computing (Rivanna) Introduction to HighlySensitive Data Analysis (Ivy) Introduction to Matlab Introduction to Python Introduction to R Introduction to Rivanna Introduction to the Command Line Introduction to UVA Research Computing Resources Machine Learning in the Cloud Machine Learning with MatLab Next Generation Sequence Alignment Optimizing R Parallel Computing with Matlab R For Beginners R Package Development Tools Writing in Functions in R {{% callout %}} Do you need"
rc-website-fork/content/education/workshops.md,a specific workshop and have a group of people to attend? Let us know.{{% /callout %}}
rc-website-fork/content/about/employment.md,We currently have no openings. Please check again in the future!
rc-website-fork/content/about/mission.md,"Research Computing empowers UVA researchers to achieve more with cuttingedge computational resources. Our support team strives to create innovative solutions for researchers who need help solving complex optimization, parallelization, workflow, and data analysis issues. We build and maintain the University's best computing platforms while educating the next generation of researchers on the power of advanced computing."
rc-website-fork/content/about/students.md,"The Research Computing Student Worker Program is dedicated to supporting RC staff and advancing computational research at UVA. Through this program, student workers will undertake shortterm projects, relieving RC staff of certain responsibilities and allowing them to devote more time to scaling support for complex, researchdomain endeavors. This initiative not only benefits RC staff but also provides students with valuable exposure to highperformance computing (HPC) and scientific computing early in their academic journey. Student Manager: Gladys K. Andino, PhD, Strategic Services and Education Manager For any questions, please email rcstudentjobs@virginia.edu. Check out our Student Workers! {{< rawhtml }} {{< currentstudents }} {{< /rawhtml }} Interested in Becoming a Student Worker? {{< button buttonurl=""https://uva.wd1.myworkdayjobs.com/UVAStudentJobs/job/CharlottesvilleVA/ResearchComputingStudentWorkerStudentWageR0066673"" buttonclass=""primary"" buttontext=""Apply for the Student Worker Position"" }}"
rc-website-fork/content/about/staff.md,"Rick Downs Director of Research Computing {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Andrew Bell, PhD Communications and Business Operations {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Ravi Kiran R. Chamakuri, PMP Full Stack Developer {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Michele Co, PhD HPC Systems Specialist {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Christina Gancayco Research Computing Associate {{< expertise subjects=""matlab,imageprocessing,datascience,python"" }} Katherine Holcomb, PhD Computational Research Consultant {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Ed Hall, PhD Computational Research Consultant {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Jacalyn Huband, PhD Computational Research Consultant {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} ByoungDo Kim, PhD Special Projects {{< expertise subjects=""hpc,parallelcomputing,storage,infiniband,data"" }} Neal Magee, PhD HPC/Cloud Solution Architect {{< expertise subjects=""globus,cloud,containers,databases,infrastructure,python"" }} Adam Munro HPC Systems Specialist {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} VP Nagraj Research Computing Associate {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Gisoo Park HPC Systems Specialist {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} David Parsley HPC Systems Specialist {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Alex Ptak HPC Systems Specialist {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Karsten Siller, PhD Computational Research Consultant {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }} Alden Stradling, PhD HPC Systems Specialist {{< expertise subjects=""hpc,rivanna,parallelcomputing,storage"" }}"
rc-website-fork/content/project/ed-triage.md,"Before patients are admitted to the emergency room, they are assigned a triage level based on the severity of their health problems. This is accomplished using the Emergency Severity Index (ESI), an emergency department triage algorithm that classifies patient cases into five different levels of urgency. Researchers are interested in using machine learning to develop a model to predict patient triage level. This model would not only analyze the typical vital signs that are used in the ESI, but also demographic data and patients’ history of health. Demographic and health data have been collected. RC is helping to prepare and normalize the data for use in a machine learning model. RC is currently developing preliminary machine learning models for predicting triage level. PI: Thomas Hartka (Department of Emergency Medicine)"
rc-website-fork/content/project/basic-science.md,"School of Medicine Research Computing is engaged in multiple collaborative projects in support of basic science research. Below is a list of some recent collaborations in this area. Microbiome Analysis of Hospital Sink Drains Sink drains are notoriously characterized as reservoirs of pathogens causing nosocomial transmissions in hospitals worldwide. Outbreaks where sinks have been implicated as source of antibiotic resistant bacteria have upsurged over the last few years. To understand transmission dynamics University of Virginia School of Medicine has established a unique ""Sink Lab"" for this research. This oneofthe kind laboratory establishes UVa as worldwide frontrunners in investigating sink related antibiotic resistant bacteria and how they spread. RC is working with the UVa Sink Lab for genomic analysis of the sink biomass. RC is contributing to: Comparative genomic analysis of gramnegative bacterial isolates: The analysis aims at tracking the mobile genetic element blaKPC gene, which encodes for Klebsiella pneumoniae carbapenemase (KPC) enzyme that confers resistance to all beta lactam agents including penicillins, cephalosporins, monobactams and carbapenems. As a part of this project, wholegenome shotgun sequencing data for about 1500 bacterial isolates will be analyzed to assess the risk of acquisition of Carbapenemase producing Enterobacteriaceae from exposure to contaminated waste water premise plumbing. Metagenomic analysis: This project, under a contract for the Center for Disease Control and Prevention (CDC), aims at understanding the temporal dynamics of hospital sink microbiome. Taxonomic and functional analysis of whole metagenomic shotgun sequencing data from longitudinal sampling will shed light on the transfer and sustenance of highrisk antibiotic resistance genes in the hospital environments. PI: Amy Mathers (Infectious Diseases & UVa Sink Lab) Genomic Locus Overlap Enrichment Analysis (LOLAweb) The past few years have seen an explosion of interest in understanding the role of regulatory DNA. This interest has driven largescale production of functional genomics data"
rc-website-fork/content/project/basic-science.md,"resources and analytical methods. One popular analysis is to test for enrichment of overlaps between a query set of genomic regions and a database of region sets. In this way, annotations from external data sources can be easily connected to new genomic data. SOM Research Computing is working with faculty in the UVA Center for Public Health Genomics to implement LOLAweb, an online tool for performing genomic locus overlap annotations and analyses. This project, written in the statistical programming language R, allows users to specify region set data in BED format for automated enrichment analysis. LOLAweb provides interactive plots and annotated data based on specific reference genomes and region databases. http://lolaweb.databio.org/ Manuscript under review PI: Nathan Sheffield (Center for Public Health Genomics) PHACTR1 and Smooth Muscle Cell Behavior Coronary artery disease (CAD) is the major cause of morbidity and mortality worldwide. Recent genome wide association studies (GWAS) have revealed more than 50 genomic loci that are associated with increased risk for CAD. However, the pathological mechanisms for the majority of the GWAS loci leading to increased susceptibility to this complex disorder are still unclear. RC is working with Redouane Aherrahrou (CPHG) who aims to study the impact of the CADassociated genetic factors on the cellular and molecular SMC phenotypes. Support for this project has included preparation of scripts for programmatic data analyses, data visualization, statistical modeling, and assistance with use of the Rivanna highperformance computing cluster. Preliminary results were presented as a poster at the 2016 International Vascular Biology Meeting. PI: Redouane Aherrarou (Center for Public Health Genomics) Functional Connectome Fingerprinting Functional magnetic resonance imaging (fMRI) can be used to assess functional activity in the brain and connectivity between different regions of interest (ROIs), and a functional connectome is a map of the interactions between ROIs. Previous research has shown"
rc-website-fork/content/project/basic-science.md,"that a functional connectome contains enough unique characteristics, not unlike a fingerprint, that it can be used for accurate identification of an individual subject from a large group. RC is working with the UVA Functional Neuroradiology Lab to perform this fingerprinting analysis for a wide variety of populations and to develop innovative ways to visualize the results. PI: Jason Druzgal (Radiology and Medical Imaging) Sonomicrometry Signal Classification Researchers are using sonomicrometry to study the biomechanics of the human brain. While at times the signals collected do not require any preprocessing, more frequently they do require some denoising or are too noisy to analyze. Currently, researchers are manually categorizing the quality of thousands of these sonomicrometry signals and preprocessing them individually. RC is helping researchers develop a machine learning model to classify the signals and to determine the necessary preprocessing steps. Preliminary sonomicrometry data have been collected, and RC is working to classify, prepare, and normalize the data for use in a machine learning model. RC is currently developing preliminary models to classify the data by signal quality and preprocess automation techniques that will later be applied to noisy signals. PI: Matthew Panzer (Center for Applied Biomechanics) epihet RC is working with researchers in the Center for Public Health Genomics to write an R package to calculate Relative Proportion of Sites with Intermediate Methylation (RPIM) scores, which represent the epigenetic heterogeneity in a bisulfite sequencing sample. https://github.com/databio/epihet PI: Nathan Sheffield (Center for Public Health Genomics) Transcription factorchromatin Binding Dynamics Two important measures of the in vivo interaction of transcription factors with chromatin are the search time and the residence time. The former refers to the time it takes a factor to find its binding location, while the latter is the time the factor physically attaches to the chromatin. By quantifying the"
rc-website-fork/content/project/basic-science.md,"interaction dynamics of transcription factors, researchers hope to understand the role of these factors in basic cellular processes such as transcription and gene regulation. The RC team is working with collaborators from UVA and the NIH to understand the dynamics of the Gal4 protein in yeast. The project involves quantitatively analyzing ChIPqPCR data, writing and running nonlinear regression and statistical routines in Mathematica, and developing numerical simulations to determine the error bounds on the kinetic parameters. PI: Stefan Bekiranov (Biochemistry and Molecular Genetics)"
rc-website-fork/content/project/nicu-bpd.md,Episodes of bradycardia and oxygen desaturation (BD) are common among preterm very low birthweight (VLBW) infants and their association with adverse outcomes such as bronchopulmonary dysplasia (BPD) is unclear. A better understanding of this relationship could lead to improved clinical interventions. RC is helping neonatologists describe BD events in a large singleNICU VLBW cohort and test the hypothesis that measures of BD in the neonatal period add to clinical variables to predict BPD or death and other adverse outcomes. RC has implemented statistical modeling and machine learning techniques to assess the primary outcome of BPD in the context of a combination of clinical characteristics (like birthweight and gestational age) and bedside monitor features. PI: Karen Fairchild (Department of Pediatrics–Neonatology) & Doug Lake (Center for Advanced Medical Analytics)
rc-website-fork/content/project/covid-saliva.md,"In cooperation with the UVA Saliva Testing Lab, the UVA Health System, and the Virginia Department of Health, the ""Be SAFE"" saliva testing program was launched in late 2020. Now a retired project, Be SAFE used saliva samples to detect the COVID19 virus through a diagnostic PCR test. Research Computing provided computational, storage, and data integration expertise to this project."
rc-website-fork/content/project/bartweb.md,"BART (Binding Analysis for Regulation of Transcription) Web Working with researchers in the Zang Lab in the Center for Public Health Genomics (CPHG), RC helped launch BARTweb, an interactive webbased tool for users to analyze their Genelist or ChIPseq datasets. BARTweb is a containerized Flask frontend (written in Python) that ingests files and submits them to a more robust Pythonbased genomics pipeline running on Rivanna, UVA's high performance computing cluster (HPC). This architecture of a public web application that uses a supercomputer to process data is a new model for UVA, and one that eases the learning curve for researchers who may not have access to an HPC system or the expertise to run a BART pipeline in the commandline. http://bartweb.org/ PI: Chongzhi Zang (Center for Public Health Genomics)"
rc-website-fork/content/project/bii-covid.md,"The Biocomplexity Institute at the University of Virginia has been at the forefront of epidemiological modeling to track the COVID19 pandemic and has developed a suite of COVID19 epidemic response resources including a series of dashboards to better help the public and the government better understand the pandemic. This is a static view of the Institute’s interactive COVID19 Surveillance Dashboard, which provides a visualization of COVID19 cases, recoveries, and deaths across the globe. In an effort to support the planning and response efforts for the recent Coronavirus pandemic, researchers prepared this visualization tool that provides a unique way of examining data curated by different data sources. https://nssac.bii.virginia.edu/covid19/dashboard/"
rc-website-fork/content/project/refgenie.md,"Reference genome assemblies are essential for highthroughput sequencing analysis projects. Typically, genome assemblies are stored on disk alongside related resources; e.g., many sequence aligners require the assembly to be indexed. The resulting indexes are broadly applicable for downstream analysis, so it makes sense to share them. However, there is no simple tool to do this. Refgenie is a reference genome assembly asset manager. Refgenie makes it easier to organize, retrieve, and share genome analysis resources. In addition to genome indexes, refgenie can manage any files related to reference genomes, including sequences and annotation files. Refgenie includes a command line interface and a server application that provides a RESTful API, so it is useful for both tool development and analysis. RC staff supported this project through its design phase, underlying infrastructure and final deployment of a Refgenie server within containers, which are attached to reference data in high performance storage. http://refgenie.databio.org/ PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/age-mvc.md,"Previous research has shown that older adults are more susceptible to severe injury than their younger counterparts after being involved in a motor vehicle collision. Dr. Hartka was interested in determining whether there are agerelated differences in the accuracy of severe injury prediction following a motor vehicle collision. Using R, Research Computing developed agespecific logistic regression models and assessed their accuracy, and generated unique graphs and animations to visualize the data more effectively. PI: Thomas Hartka"
rc-website-fork/content/project/dest.md,"Evolutionary biologists use populationbased DNA sequencing to gain insight into the nature of adaptation, genetic diversity, and organismal form and function. When collecting DNA data, scientists are often sample limited because of the logistical challenges of collecting DNA from wild individuals across large portions of a species range. This can be mitigated when groups of scientists work together to create data and then share it with the larger community. The Bergland Lab has been a central participant in developing and maintaining DEST (“Drosophila Evolution through Space and Time”), a large (~10TB) repository of Drosophila melanogaster population genomic data which has been processed and standardized. The DEST dataset is a unique, spatially resolved, genomic timeseries dataset for one of the premier model organisms in genetics. UVA’s Research Computing has been the primary host for the DEST dataset and bioinformatics pipeline since 2020. Users access data through a combination of an httppassthrough, Globus, and a website. The website has been viewed nearly 5,000 times by over 2,500 unique visitors since its launch in 2020 and has been used by members of the broader research community in dozens of published research projects. The Bergland Lab is working on a new version of the DEST dataset (DEST 2.0) that includes genomic data for over 50,000 flies from 500 populationbased samples collected at ~100 localities throughout the world, with many localities sampled through time for upwards of a decade. Research Computing’s Data Analytics Center supported this work by debugging and streamlining one of the main parallel processes in the bioinformatics pipeline to efficiently use UVA HPC. PI: Alan Bergland, PhD (Department of Biology)"
rc-website-fork/content/project/ncaa.md,"Bacteria are an important type of human pathogen that can cause lifethreatening infections. Increasingly, these microorganisms can survive the effects of antibiotics previously used to kill them. As bacteria become resistant to multiple kinds of antibiotics, the diseases they cause become ever more difficult to cure. Accordingly, infections caused by ‘multidrugresistant’ (MDR) pathogens are associated with frequent treatment failures, high hospitalization costs, and substantial mortality. New therapeutics are needed to treat infections caused by MDR bacteria. Towards developing these critical countermeasures, our group has discovered a unique peptide that efficiently kills many of the most challenging antibioticresistant pathogens and also demonstrates therapeutic efficacy in preclinical animal models of bacterial infection. Interested in investigating the effect of replacing the canonicalamino acids by noncanonical amino acids (NCAA) to increase the efficacy of the peptide, a DAC team member has created a computational strategy to perform the screening of multiplepeptide positions using a NCAA peptide library and the highperformance computing capabilities of Research Computing. With good agreement between computational predictions and benchtop experiments, our collaboration with the DAC led to the following key points: Determination of NCAAcontaining preferred peptides with better predicted binding (under experimental testing) Construction of the first structural model of the peptide (both canonical and noncanonical variants) with the potential bacterial target Structurefunction insights in search of optimized antimicrobial peptides towards better therapeutics (by utilizing NCAAs) These investigations have set the stage for our continued collaboration with the DAC team member in this area including multiple state and federal grants that we will be targeting in 2025. PIs: Matthew A Crawford, PhD (Department of Medicine, Division of Infectious Diseases & International Health) and Molly A Hughes, PhD (Department of Medicine, Division of Infectious Diseases & International Health)"
rc-website-fork/content/project/transcription-factor.md,"Two important measures of the in vivo interaction of transcription factors with chromatin are the search time and the residence time. The former refers to the time it takes a factor to find its binding location, while the latter is the time the factor physically attaches to the chromatin. By quantifying the interaction dynamics of transcription factors, researchers hope to understand the role of these factors in basic cellular processes such as transcription and gene regulation. The RC team is working with collaborators from UVA and the NIH to understand the dynamics of the Gal4 protein in yeast. The project involves quantitatively analyzing ChIPqPCR data, writing and running nonlinear regression and statistical routines in Mathematica, and developing numerical simulations to determine the error bounds on the kinetic parameters. PI: Stefan Bekiranov (Biochemistry and Molecular Genetics)"
rc-website-fork/content/project/sink-microbiome.md,"Sink drains are notoriously characterized as reservoirs of pathogens causing nosocomial transmissions in hospitals worldwide. Outbreaks where sinks have been implicated as source of antibiotic resistant bacteria have upsurged over the last few years. To understand transmission dynamics University of Virginia School of Medicine has established a unique ""Sink Lab"" for this research. This oneofthe kind laboratory establishes UVa as worldwide frontrunners in investigating sink related antibiotic resistant bacteria and how they spread. RC is working with the UVa Sink Lab for genomic analysis of the sink biomass. RC is contributing to: Comparative genomic analysis of gramnegative bacterial isolates: The analysis aims at tracking the mobile genetic element blaKPC gene, which encodes for Klebsiella pneumoniae carbapenemase (KPC) enzyme that confers resistance to all beta lactam agents including penicillins, cephalosporins, monobactams and carbapenems. As a part of this project, wholegenome shotgun sequencing data for about 1500 bacterial isolates will be analyzed to assess the risk of acquisition of Carbapenemase producing Enterobacteriaceae from exposure to contaminated waste water premise plumbing. Metagenomic analysis: This project, under a contract for the Center for Disease Control and Prevention (CDC), aims at understanding the temporal dynamics of hospital sink microbiome. Taxonomic and functional analysis of whole metagenomic shotgun sequencing data from longitudinal sampling will shed light on the transfer and sustenance of highrisk antibiotic resistance genes in the hospital environments. PI: Amy Mathers (Infectious Diseases & UVa Sink Lab)"
rc-website-fork/content/project/brodie-biology.md,"Ed Hall worked with the Brodie Lab in the Biology department, to set up a workflow to analyze videos of bug tracking experiments on the Rivanna Linux cluster. They wanted to use the community Matlab software (idTracker) for beetle movement tracking. Their two goals were to shorten the software runtime and to automate the process. There was a large backlog of videos to go through. Ed installed the idTracker software on Rivanna and modified the code to parallelize the bug tracking process. He wrote and documented shell scripts to automate their workflow on the cluster. PI: Edmund Brodie, PhD (Department of Biology)"
rc-website-fork/content/project/clinical-research.md,"Bringing expertise in data analysis and large scale computation, School of Medicine Research Computing is supporting clinical research at UVa. Several recent collaborations are listed below. Bradycardia and oxygen desaturation events in VLBW infants Episodes of bradycardia and oxygen desaturation (BD) are common among preterm very low birthweight (VLBW) infants and their association with adverse outcomes such as bronchopulmonary dysplasia (BPD) is unclear. A better understanding of this relationship could lead to improved clinical interventions. RC is helping neonatologists describe BD events in a large singleNICU VLBW cohort and test the hypothesis that measures of BD in the neonatal period add to clinical variables to predict BPD or death and other adverse outcomes. RC has implemented statistical modeling and machine learning techniques to assess the primary outcome of BPD in the context of a combination of clinical characteristics (like birthweight and gestational age) and bedside monitor features. Manuscript under review Karen Fairchild (Department of Pediatrics–Neonatology) & Doug Lake (Center for Advanced Medical Analytics) Predicting Triage Level in the Emergency Department with Machine Learning Before patients are admitted to the emergency room, they are assigned a triage level based on the severity of their health problems. This is accomplished using the Emergency Severity Index (ESI), an emergency department triage algorithm that classifies patient cases into five different levels of urgency. Researchers are interested in using machine learning to develop a model to predict patient triage level. This model would not only analyze the typical vital signs that are used in the ESI, but also demographic data and patients’ history of health. Demographic and health data have been collected. RC is helping to prepare and normalize the data for use in a machine learning model. RC is currently developing preliminary machine learning models for predicting triage level. PI: Thomas Hartka (Department of"
rc-website-fork/content/project/clinical-research.md,"Emergency Medicine) Customized Secure Computing Environment for Surgical Research RC is working with Dr. Eric Schneider to create a secure computing environment for the research of the Healthcare Surgical Outcome team. Data from this project will contain HIPAA identifiers, as well as Medicare information, and requires more security and control of data ingress/egress than projects previously hosted on the Ivy platform. After successful implementation of this project, RC will create a similar computing environment for DoD blast and traumatic brain injury data collected by Dr. Schneider before he joined UVA. PI: Eric Schneider (Department of Surgery) Heart Rate Ranges in Premature Neonates Using High Resolution Physiologic Data There are limited evidencebased published heart rate ranges for premature neonates. However, knowing heart rate reference ranges in the premature neonatal population can be beneficial for bedside assessment in the Neonatal Intensive Care Unit (NICU). RC is collaborating with clinical researchers in the Department of Pediatrics to establish baseline ranges for heart rate data in premature infants. These results are summarized from more than two billion data points collected via bedside monitoring in the NICU. RC staff has contributed data analysis and visualization expertise to aggregate the data, generate interactive heatmaps and produce tables of these ranges by gestational age. Manuscript under review PI: Corrie Alonzo (Department of Pediatrics–Neonatology) & Mike Spaeder (Department of Pediatrics)"
rc-website-fork/content/project/smooth-muscle-cells.md,"Coronary artery disease (CAD) is the major cause of morbidity and mortality worldwide. Recent genome wide association studies (GWAS) have revealed more than 50 genomic loci that are associated with increased risk for CAD. However, the pathological mechanisms for the majority of the GWAS loci leading to increased susceptibility to this complex disorder are still unclear. RC is working with Redouane Aherrahrou (CPHG) who aims to study the impact of the CADassociated genetic factors on the cellular and molecular SMC phenotypes. Support for this project has included preparation of scripts for programmatic data analyses, data visualization, statistical modeling, and assistance with use of the Rivanna highperformance computing cluster. Preliminary results were presented as a poster at the 2016 International Vascular Biology Meeting. PI: Redouane Aherrarou (Center for Public Health Genomics)"
rc-website-fork/content/project/functional-connectome.md,"Functional magnetic resonance imaging (fMRI) can be used to assess functional activity in the brain and connectivity between different regions of interest (ROIs), and a functional connectome is a map of the interactions between ROIs. Previous research has shown that a functional connectome contains enough unique characteristics, not unlike a fingerprint, that it can be used for accurate identification of an individual subject from a large group. RC is working with the UVA Functional Neuroradiology Lab to perform this fingerprinting analysis for a wide variety of populations and to develop innovative ways to visualize the results. PI: Jason Druzgal (Radiology and Medical Imaging)"
rc-website-fork/content/project/zhigilei-materialsci.md,"Dr. Zhigilei and his team are using Rivanna to perform largescale atomistic simulations aimed at revealing fundamental processes responsible for the modification of surface morphology and microstructure of metal targets treated by short pulse laser irradiation. The simulations are performed with a highlyoptimized parallel computer code capable of reproducing collective dynamics in systems consisting of up to billions of atoms. As a result, the simulations naturally account for the complexity of the material response to the rapid laser energy deposition and provide clear visual representations, or “atomic movies,” of laserinduced dynamic processes. The mechanistic insights revealed in the simulations have an immediate impact on the development of the theoretical understanding of laserinduced processes and assist in optimization of laser processing parameters in current applications based on laser surface modification and nanoparticle generation in laser ablation. PI: Leonid V. Zhigilei, PhD (Department of Materials Science & Engineering)"
rc-website-fork/content/project/periasamy-flim.md,"Multiphoton FLIM microscopy offers many opportunities to investigate processes in live cells, tissue and animal model systems. For redox measurements, FLIM data is mostly published by cell mean values and intensitybased redox ratios. Our method is based entirely on FLIM parameters generated by 3detector time domain microscopy capturing autofluorescent signals of NAD(P)H, FAD and novel FLIMFRET application of Tryptophan and NAD(P)Ha2%/FADa1% redox ratio. Furthermore, image data is analyzed in segmented cells thresholded by 2 × 2 pixel Regions of Interest (ROIs) to separate mitochondrial oxidative phosphorylation from cytosolic glycolysis in a prostate cancer cell line. Hundreds of data points allow demonstration of heterogeneity in response to intervention, identity of cell responders to treatment, creating thereby different subpopulations. Histograms and bar charts visualize differences between cells, analyzing whole cell versus mitochondrial morphology data, all based on discrete ROIs. This assay method allows to detect subtle differences in cellular and tissue responses, suggesting an advancement over meansbased analyses. RC staff supported this project with development of custom image analysis tools. PI: Ammasi Periasamy (Keck Center for Cellular Imaging)"
rc-website-fork/content/project/radiology-tustison-stone.md,"A powerful new technique for quantifying regions of the cerebral cortex was developed by Nick Tustison and James Stone at the University of Virginia along with collaborators from the University of Pennsylvania. It was evaluated using large data sets comprised of magnetic resonance imaging (MRI) of the human brain processed on a highperformance computing cluster at the University of Virginia. By making this technique available as opensource software, other neuroscientists are now able to investigate various hypotheses concerning the relationship between brain structure and development. Tustison’s and Stone’s software has been widely disseminated and is being actively incorporated into a variety of clinical research studies, including a collaborative effort between the Department of Defense and Department of Veterans Affairs, exploring the long term effects of traumatic brain injury (TBI) among military service members. Learn more about the ITK Insight Toolkit on GitHub. PIs: Nick Tustison and James Stone (Department of Radiology)"
rc-website-fork/content/project/calcium-oscillations.md,"Calcium oscillations signify communication between zona glomerulosa cells of the mouse adrenal gland. Researchers in the Barrett Lab can capture these oscillatory events with calcium imaging, but they had difficulty analyzing the results. The Barrett Lab was in need of a comprehensive MATLAB program for quantitative analysis of the intracellular calcium signals from their cell imaging experiments. Prior to Research Computing's involvement in the project, the Barrett Lab had been using fragments of code to analyze their data with little success. Research Computing developed a MATLAB application to create an efficient, centralized workflow that is also accessible to people who are new to MATLAB and programming. With this application, the Barrett Lab was able to analyze the characteristics of calcium oscillatory events for the first time in two years. The app allows researchers to: Choose data and analysis parameters View full or partial fluorescent reading traces Perform quantitative analysis of individual events and bursts of events PI: Paula Barrett"
rc-website-fork/content/project/cloud-migrations.md,"UVA Research Computing worked with researchers in the UVA Center for Behavioral Health and Technology to move many of its webbased research instruments to the cloud. CBHT ""has been involved in eHealth, specifically the development and testing of clinical interventions delivered via the Internet. We believe the Internet can be used to implement engaging, interactive, and comprehensive interventions. Our interventions are designed to improve outcome by tailoring the programs to the individual user. We were among the first to test the feasibility and effectiveness of delivering Internet interventions."" Their research depends upon secure, datadriven websites that both enable participants to make regular submissions via web form, but also to safeguard private health data for researchers to learn from. When their previous generation of hardware was ready to be retired, CBHT developers contacted Research Computing to help redesign their platform and plan their migration into the Amazon public cloud. Their setup includes private, encrypted databases and web servers, and a strictlycontrolled environment for highly sensitive data. Learn more about the Center for Behavioral Health & Technology."
rc-website-fork/content/project/guagliardo.md,"This proposal is to enhance the computational analysis of calcium imaging data from live adrenal tissue in the study of primary aldosteronism, a leading cause of hypertension. The significant computational challenges associated with large calcium imaging datasets and complex analyses are addressed by first improving the current workflow for greater efficiency and accessibility, and second, establishing a robust computaional workflow and data management strategy. PI: Nick Guagliardo, Dept. of Pharmacology"
rc-website-fork/content/project/reidenbach-envirosci.md,"Professor Reidenbach and his team are using Rivanna to run computational fluid dynamics simulations of wave and tide driven flows over coral reefs in order to determine how storms, nutrient inputs, and sediments impact reef health. This is an image of dye fluxing from the surface of the Hawaiian coral Porites compressa utilizing a technique known as planar laser induced fluorescence (PLIF). Reefs such as this one have been severely impacted by human alteration, both locally through additional inputs of sediments and nutrients, and globally through increased sea surface temperatures caused by climate change. Reidenbach is hopeful that his computational models will allow scientists to better predict the future health of reefs based on human activity and improve global reef restoration efforts. PI: Matthew Reidenbach, PhD (Department of Environmental Sciences)"
rc-website-fork/content/project/nicu-vital-signs.md,"There are limited evidencebased published heart rate ranges for premature neonates. However, knowing heart rate reference ranges in the premature neonatal population can be beneficial for bedside assessment in the Neonatal Intensive Care Unit (NICU). RC is collaborating with clinical researchers in the Department of Pediatrics to establish baseline ranges for heart rate data in premature infants. These results are summarized from more than two billion data points collected via bedside monitoring in the NICU. RC staff has contributed data analysis and visualization expertise to aggregate the data, generate interactive heatmaps and produce tables of these ranges by gestational age. PI: Corrie Alonzo (Department of Pediatrics–Neonatology) & Mike Spaeder (Department of Pediatrics)"
rc-website-fork/content/project/esfarjani-aladyn.md,"Prof. Esfarjani's group is using the HPC cluster to develop the Anharmonic LAttice DYNamics (ALADYN) software suite to calculate thermal transport properties and phase transitions from firstprinciples. The codes can extract force constants, solve the Boltzmann transport equation, predict thermal equilibrium based on the selfconsistent phonon theory, and run molecular dynamics simulations within an anharmonic force field. The figure shows the phonon density of states and dispersion curve of Ge obtained from ALADYN. PI: Keivan Esfarjani, PhD (Department of Materials Science & Engineering)"
rc-website-fork/content/project/shukla-nikhil.md,"The Computing Hardware Research Lab (CHRL) worked with the DAC to develop a pipeline connecting a web app to HPC resources for solving computationally hard combinatorial optimization problems such as computing the MaxCut in complex graphs using OmegaSync. The DAC created an RShiny app running on Kubernetes that collects user information and graph files. The app formats data, saves it to the HPC filesystem, and automates job submissions. It also triggers email notifications to users upon job start and completion, providing the results they need. This project highlights the DAC's role in supporting faculty with complex research workflows. PI: Nikhil Shukla, PhD (Department of Electrical and Computer Engineering)"
rc-website-fork/content/project/sonomicrometry-signal.md,"Researchers are using sonomicrometry to study the biomechanics of the human brain. While at times the signals collected do not require any preprocessing, more frequently they do require some denoising or are too noisy to analyze. Currently, researchers are manually categorizing the quality of thousands of these sonomicrometry signals and preprocessing them individually. RC is helping researchers develop a machine learning model to classify the signals and to determine the necessary preprocessing steps. Preliminary sonomicrometry data have been collected, and RC is working to classify, prepare, and normalize the data for use in a machine learning model. RC is currently developing preliminary models to classify the data by signal quality and preprocess automation techniques that will later be applied to noisy signals. PI: Matthew Panzer (Center for Applied Biomechanics)"
rc-website-fork/content/project/epihet.md,"RC is working with researchers in the Center for Public Health Genomics to write an R package to calculate Relative Proportion of Sites with Intermediate Methylation (RPIM) scores, which represent the epigenetic heterogeneity in a bisulfite sequencing sample. https://github.com/databio/epihet PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/arctic.md,"Understanding the Changing NaturalBuilt Landscape in an Arctic Community Navigating the New Arctic (NNA) is one of The National Science Foundation's 10 Big Ideas. NNA projects address convergence scientific challenges in the rapidly changing Arctic. Arctic research is needed to inform the economy, security and resilience of the Nation, the larger region and the globe. NNA empowers new research partnerships from local to international scales, diversifies the next generation of Arctic researchers, enhances efforts in formal and informal education, and integrates the coproduction of knowledge where appropriate. This award fulfills part of that aim by addressing interactions among social systems, natural environment, and built environment in the following NNA focus areas: Arctic Residents, Data and Observation, Education, and Resilient Infrastructure. Arctic communities face many challenges as they grow and develop in the context of a rapidly changing environment. These challenges include coastal erosion, permafrost thaw, and ecosystem change. This project is developing and deploying a network of environmental sensors collecting continuous information over a fiveyear period in terrestrial and aquatic locations within the community of Utqiagvik. The sensor network yields an unprecedented dataset for examining the interactive effects of the natural and built environments. This project is improving the health and economic wellbeing of Utqiagvik and potentially other North Slope Borough villages in Alaska. This research investigates two essential challenges for the Arctic city of Utqiagvik, Alaska: i) the impacts of existing community infrastructure practices on the surrounding tundra, coastal, and lagoon landscapes within and around the city, and ii) the impacts of a changing environment on the design and future planning of community infrastructure and buildings. The ultimate goal of the project is to understand how the natural and built environments interact with social systems in an Arctic city. UVA Research Computing is supporting this research by hosting computational"
rc-website-fork/content/project/arctic.md,"services to ingest, process, transform, store, and serve sensor data over the life of the project. A scalable data service named HSDS runs within a containerized environment in the HPC networks to aggregate the collected sensor data. This service is backed by object storage, and is then served internally for research using a variety of data analysis tools. Data: https://arcticdata.io/ Award: https://www.nsf.gov/awardsearch/showAward?AWDID=2022639 PI: Howard Epstein, Chair, Dept. of Environmental Sciences."
rc-website-fork/content/project/primed.md,"In their research around constant glucose monitoring and the automated maintenance of insulin for patients, the CDT is exploring data drawn from external data sources such as DexCom and FitBit. RC has assisted the CDT by designing a secure computing footprint in Amazon Web Services to pull in these data, parse and process them, in order to perform deeper analytics through machine learning. In January 2018, CDT sponsored a ski camp at Wintergreen Resort for a group of youth diagnosed with Type I diabetes with the goal of importing glucose, insulin, and exercise metrics at the end of each day through remote web APIs. This proof of concept has enabled the CDT to move forward with further monitoring efforts that draw upon existing devices and providers, rather than reinventing them in a singular system. PI: Marc Breton, PhD (Center for Diabetes Technology)"
rc-website-fork/content/project/ciliberto-economics.md,"While conducting research for a highlytechnical study of market behavior, Dr. Ciliberto realized that he needed to parallelize an integration over a sample distribution. RC staff member Ed Hall successfully parallelized Ciliberto’s Matlab code and taught him how to do production runs on the University’s highperformance clusters. “The second stage estimator was computationally intensive,” Ciliberto recalls. “We needed to compute the distribution of the residuals and unobservables for multiple parameter values and at many different points of the distribution, which requires parallelizing the computation. Ed Hall’s expertise in this area was crucial. In fact, without Ed’s contribution, this project could not have been completed.” PI: Federico Ciliberto, PhD (Department of Economics)"
rc-website-fork/content/project/lolaweb.md,"The past few years have seen an explosion of interest in understanding the role of regulatory DNA. This interest has driven largescale production of functional genomics data resources and analytical methods. One popular analysis is to test for enrichment of overlaps between a query set of genomic regions and a database of region sets. In this way, annotations from external data sources can be easily connected to new genomic data. SOM Research Computing is working with faculty in the UVA Center for Public Health Genomics to implement LOLAweb, an online tool for performing genomic locus overlap annotations and analyses. This project, written in the statistical programming language R, allows users to specify region set data in BED format for automated enrichment analysis. LOLAweb provides interactive plots and annotated data based on specific reference genomes and region databases. https://github.com/databio/LOLAweb/ PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/johnson-steven.md,"A team of UVA Commerce faculty partnered with the DAC to explore how information about personal health and finance propagates on online platforms and influences decisions. The DAC developed for them a process to merge and standardize data from CoreLogic and ComScore, two sources of consumer and real estate information. The DAC assisted by developing a method to map real estate data to Designated Market Areas (DMAs), which are geographic regions where the population receives similar types of information through the media. The DMA mapping allowed the researchers to link consumer behavior in the real estate market with demographic characteristics. To make the data analysis more efficient, the DAC automated the data ingestion and analysis processes using highperformance computing. This project demonstrates how the DAC can assist faculty in tackling complex and socially relevant research questions using big data. PI: Steven Johnson, PhD (UVA McIntire School of Commerce)"
rc-website-fork/content/project/cardiovascular-genomics.md,"Coronary artery disease (CAD) is the major cause of morbidity and mortality worldwide. Recent genome wide association studies (GWAS) have revealed more than 50 genomic loci that are associated with increased risk for CAD. However, the pathological mechanisms for the majority of the GWAS loci leading to increased susceptibility to this complex disorder are still unclear. Many of the CAD loci appear to act through the vessel wall, presumably affecting smooth muscle cell (SMC) function. UVA Research Computing (RC) is working with Redouane Aherrahrou from the Center for Public Health Genomics who aims to study the impact of the CADassociated genetic factors on the cellular and molecular SMC phenotypes, as well as the underlying biological pathways that are perturbed by these genetic factors. While providing scientific programming and data analysis support for this project, Research Computing has: Developed a series of scripts to programmatically normalize and summarize experimental data Created interactive and static data visualizations Performed statistical hypothesis tests Provided guidance on the use of the local highperformance computing cluster (Rivanna)"
rc-website-fork/content/project/political-sentiment.md,"The nature of political communication has been fundamentally altered by the emergence of social media. In earlier eras, social scientists, journalists, and citizens could focus on static statements by politicians and candidates in order to understand the nature of political discourse. Social scientists studying political communication would design surveys and focus groups to understand which messages were received by citizens, and with what effect. Today, as news moves to digital platforms and as political figures increasingly rely on social media, political communication is fundamentally dynamic. Studying patterns of communication among politicians, their supporters, and their critics requires scholarly focus on the content, sentiment, and framing of posts on various social media platforms. Similarly, making sense of the contours of an election campaign requires that scholars explore the interplay among online messages and realworld events such as endorsements, scandals, and standing in the polls. All of this means that the tools of political analysis must be combined with the tools of data science. Over the past several years a team of researchers from the Department of Politics, the School of Data Science, and Research Computing have collaborated to create a system to capture and store every Tweet to and from every major presidential candidate since the 2016 primaries, continuing with every Tweet to and from President Trump since Election Day. This collaboration has produced a unique database of approximately one billion tweets (currently 5 TB). While the analysis of tweets from candidates and office holders is commonplace, collecting, organizing, and analyzing tweets to these figures, both from supporters and opponents, requires significant effort and sustained collaboration. PI: Paul Freedman (Department of Politics)"
rc-website-fork/content/project/simpleCache.md,"In partnership with researchers in the Center for Public Health Genomics, School of Medicine Research Computing has contributed to the development of a novel package for computationally efficient caching and loading of data in R. simpleCache provides an interface to a series of functions to store and retrieve cached objects, including in the context batch processing or HPC environments. The package further extends base R functionality of saving and loading external representations of objects by enabling caching to predefined directories and timed cache operations. RC helped document and develop new functions for the package ahead of its release to the Comprehensive R Archive Network (CRAN). An accompanying article was selected for publication in the Journal of Open Source Software in early 2018. https://CRAN.Rproject.org/package=simpleCache PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/surgical-research.md,"RC is working with Dr. Eric Schneider to create a secure computing environment for the research of the Healthcare Surgical Outcome team. Data from this project will contain HIPAA identifiers, as well as Medicare information, and requires more security and control of data ingress/egress than projects previously hosted on the Ivy platform. After successful implementation of this project, RC will create a similar computing environment for DoD blast and traumatic brain injury data collected by Dr. Schneider before he joined UVA. PI: Eric Schneider (Department of Surgery)"
rc-website-fork/content/project/ercp.md,"Endoscopic retrograde cholangiopancreatography (ERCP) is a procedure in which an endoscope is guided into the first part of the small bowel for various instruments to be passed into the biliary and pancreatic ducts to remove obstructions, drain infectious collections or diagnose diseases of the bile ducts. ERCP is associated with higher rates of complications than other endoscopic procedures, one of the most severe being postERCP pancreatitis which is thought to take place in up to 10% of patients. Dr. Podboy and Internal Medicine Resident Physician Dr. Jason Erno are interested in factors that may contribute to reduced outcomes of postERCP pancreatitis, such as the use of dexmedetomidine as an anesthetic agent. Research Computing’s Data Analytics Center investigated this by performing statistical and machine learning analyses of data from a retrospective cohort study who underwent ERCP at UVA in the last five years. These analyses examined associations between postERCP pancreatitis and factors such as dexmedetomidine use, patient demographics, preprocedural medications, and specific intraoperative stent placements and procedures. Full image attribution: Davee T, Garcia JA, Baron TH. Precut sphincterotomy for selective biliary duct cannulation during endoscopic retrograde cholangiopancreatography. Annals of Gastroenterology (2012). PMCID: PMC3959408 PI: Alexander Podboy, MD (Division of Gastroenterology and Hepatology)"
rc-website-fork/content/userinfo/pricing.md,Below is a schedule of prices for Research Computing resources. HPC Service Unit Allocations {{< pricing allocations }} About Allocations HPC Dedicated Computing {{< pricing dedicatedcomputing }} Storage {{< rawhtml }} {{< pricing storage }} {{< /rawhtml }} Storage Details Request Storage Ivy Virtual Machines {{< pricing ivy }} Ivy Details Request Ivy Resources
rc-website-fork/content/userinfo/storage.md,"{{< getstatus keyword=""storage"" }} There are a variety of options for storing research data at UVA. Public, internal use, and sensitive data storage systems can be accessed from the Rivanna and Afton high performance computing systems. Highly sensitive data can be stored and accessed within the Ivy secure computing environment. University Information Security provides an overview of the data sensitivity classifications, while our Data Sensitivity and Research Computing Systems table specifies where each type of data can be stored or analyzed in compliance with regulations. {{% highlight %}} {{% pieligibility %}} {{% /highlight %}} {{< highlight }} Information Technology Services (ITS) also provides multiple tiers of data storage for personal and nonresearch storage needs. {{< /highlight }} Public, Internal Use, and Sensitive Data Storage {publicinternalusesensitivedatastorage} Public data are intentionally made available to the public. Examples of public data in research computing include, but are not limited to: Data intended for display on a public website Public Data Sets obtained from a publicly available source Open source code Internal use data are classified as public records available to anyone in accordance with the Virginia Freedom of Information Act (FOIA) but are not intentionally made public. Examples of internal use data within a research computing context include but are not limited to: audits models, scripts, and logfiles Preliminary analyses or reports Correspondence Sensitive data is the default classification for all data that is not explicitly defined as highly sensitive data, may be held from release under FOIA, or that is not intended to be made publicly available. Examples of sensitive data within a research computing context include, but are not limited to: University ID numbers FERPAprotected student information not covered by the definition of highly sensitive data Health information where all Protected Health Information (PHI) have been systematically removed (i.e., deidentified) or"
rc-website-fork/content/userinfo/storage.md,"aggregated, making identification impossible Personnel and financial information not covered by the definition of highly sensitive data, but not intended to be public Any information that doesn’t fit into public, internal use, or highly sensitive data categories {{< storagemainpagefirst }} 1For PIs with existing Research Standard Storage, the charges will be adjusted accordingly. PIs without existing Research Standard Storage will need to submit a request for the storage. 2Snapshot files are uneditable backup copies of all the files and folders in your account, taken at a daily interval. The Research Project Storage system keeps these snapshots for a week. Snapshot files are deleted sequentially after a week has passed. This saving method is useful for human error prevention as any accidentally deleted files may be recovered. Look to our FAQ page to learn how to access your snapshots. 3Replication is a data management process that stores copies of data fragments over a distributed cluster or database. By having replicated data across each node or server on a given database, data can be accessed more reliably than data that only resides on a single server. This saving method is useful for disaster scenarios where if data is stored on multiple disks, and one disk fails, the data is still accessible. 4Backup files are copies of files that are stored on a separate disk storage than that of the original copies. Files may be backed up on a separate disk storage or within cloud storage. Backed up files are not synced with their original, so any edits to the original are not reflected on the backup. This saving method is useful for disaster scenarios where if the original disk storage is unsalvageable, the backups may still be accessible. Highly Sensitive Data Storage Highly sensitive data (HSD) are data that require restrictions on"
rc-website-fork/content/userinfo/storage.md,"access under the law or that may be protected from release in accordance with applicable law or regulation. HSD includes personal information that can lead to identity theft or health information that reveals an individual's health condition and/or medical history. Examples of HSD include, but are not limited to: Personally identifying information (PII) is any information that can be used to identify a person. Examples of PII include social security number, passport number, driver’s license number, military identification number, or biometric records (e.g. photographic facial images, fingerprints, voice signature, etc.). Health information that reveals an individual’s health condition and/or medical history, including information defined by the Health Insurance Portability and Accountability Act (HIPAA) {{< storagemainpagesecond }} Researchers who request space on HighSecurity Research Standard must first request an Ivy account using the Ivy request form. Further information on Ivy and the HighSecurity Research storage can be found here. HighSecurity Research Standard Storage is accessible by using Globus and connecting to the HighSecurity DTN. Ivy Central Storage (ICS) Ivy Central Storage has been replaced by HighSecurity Research Standard storage. Request Storage Storage requests can be made via this form: Request / Purchase Storage"
rc-website-fork/content/userinfo/resources.md,Resources are defined here.
rc-website-fork/content/userinfo/transition_new_r_libraries.md,"The recommended steps for transitioning your R programs after the June maintenance are as follows: Determine which version of R you will be using (e.g., R/3.6.3). Open a terminal window on the HPC system and load the version of R that you chose in step 1 (e.g., module load goolf R/3.6.3). (Optional) Run our script to rebuild your existing R library for the newer version of R. For example, if you had been using R/3.5.1 and are switching to R/3.6.3, type the following in the terminal window: updateRlib 3.5.1 . Make sure that you have loaded any other modules (e.g., curl, gdal) that your packages may need. Update your Slurm scripts to load the newer version of R. We at Research Computing understand that you may have some issues during the transition. To help with the transition, we will have additional office hours specifically for your R questions: Thursday, 18 June, 3:005:00pm Join us via Zoom Friday, 19 June, 3:005:00pm Join us via Zoom"
rc-website-fork/content/userinfo/hipaa-compliance.md,"UVA School of Medicine Research Computing can assist medical researchers in both understanding what HIPAA compliance requires of their work, and how to implement technical solutions to achieve and verify such compliance. Review & Assessment From a compliance perspective, RC offers three levels of review/assessment: Security Plan Having a security plan in place is important to your success at UVA School of Medicine. If your lab or department doesn’t have a plan in place Research Computing Information Security will be happy to help you develop such a plan. Security Review If you have a security plan in place already we can help you verify that your computing resources are functioning as documented in your security plan. As part of security review, we will do a risk analyst and provide you a list of recommended enhancements. Risk Assessment This is the systematic process of evaluating the potential security risks/hazards and any business impact they could present. We analyze the likelihood of events occurring. As we conduct a risk assessment we look for vulnerabilities and weaknesses that could your system more susceptible to an event. We will provide list of finds and either work with your local support person or other resources to help mitigate the risk. Implementation For implementation, RC offers a number of skills and services: PHI and Deidentification Encryption best practices Encryption of files, databases, and systems PHI & DeIdentification Some research data are ""deidentified"" in order to remove them from HIPAA security requirements. This is particularly useful when researchers are processing across many hundreds or thousands of patients for trends and statistically meaningful insights that do not rely upon patientspecific data points. Here are some common examples of PHI research data that may need to be deidentified. Deidentification can mean the complete removal of such fields from your"
rc-website-fork/content/userinfo/hipaa-compliance.md,"dataset, or the complete replacement of these data with meaningless placeholder values. A. Names B. All geographic subdivisions smaller than a state, including street address, city, county, precinct, ZIP code, and their equivalent geocodes, except for the initial three digits of the ZIP code if, according to the current publicly available data from the Bureau of the Census: The geographic unit formed by combining all ZIP codes with the same three initial digits contains more than 20,000 people; and The initial three digits of a ZIP code for all such geographic units containing 20,000 or fewer people is changed to 000 C. All elements of dates (except year) for dates that are directly related to an individual, including birthdate, admission date, discharge date, death date, and all ages over 89 and all elements of dates (including year) indicative of such age, except that such ages and elements may be aggregated into a single category of age 90 or older D. Telephone numbers E. Vehicle identifiers and serial numbers, including license plate numbers F. Fax numbers G. Device identifiers and serial numbers H. Email addresses I. Web Universal Resource Locators (URLs) J. Social security numbers K. Internet Protocol (IP) addresses L. Medical record numbers M. Biometric identifiers, including finger and voiceprints N. Health plan beneficiary numbers O. Fullface photographs and any comparable images P. Account numbers Q. Any other unique identifying number, characteristic, or code, except as permitted as a ""reidentifier"" R. Certificate/license numbers Resources HIPAA for Professionals & Providers (HHS)"
rc-website-fork/content/userinfo/data-transfer.md,"Efficient and reliable data transfer is a critical component of research computing. A variety of useful tools is available for rapid data transfer, whether you are transferring data from an external site or within different computing environments at UVA. Common Scenarios Transfer public or internal use data between local workstation/laptop and UVA storage Transfer sensitive or highly sensitive data to Ivy storage Transfer data between external institutions/supercomputing centers and UVA Transfer between UVA HPC and Cloud storage The data transfer method you choose heavily relies on the data sensitivity classification, where the data are currently located and to where you want to transfer the data. Click on a row in the table below to learn more about the data transfer methods available for a specific scenario. System 1 System 2 Example Scenario Lab Workstation Storage for Public and Internal Use Data Copy data from a lab workstation to a /project storage share. Copy result files from a /scratch directory on Rivanna or Afton. Expand Local Computer Laptop Lab Workstation Remote System /home on Rivanna or Afton /scratch on Rivanna or Afton Research Project Storage Research Standard Storage Data Transfer Methods Globus Connect Graphical SFTP Clients Command Line Tools System 1 System 2 Example Scenario Secure Lab Workstation Storage for Sensitive and Highly Sensitive Data Transfer HIPAA data from a Health Systems workstation to HighSecurity Research Standard Storage. Expand Sources Lab Workstation Health Systems Workstation Destination HighSecurity Research Standard Storage Data Transfer Methods Globus Connect System 1 System 2 Example Scenario External Institution University of Virginia Transfer public or nothighly sensitive data collected at another institution to UVA storage. Transfer results from an analysis carried out on a remote supercomputer at a national lab or supercomputing center. Expand Sources Any institution that uses Globus, such as: Other universities Supercomputing facilities Destinations"
rc-website-fork/content/userinfo/data-transfer.md,"/home /scratch Research Project Storage Research Standard Storage HighSecurity Research Standard Storage Data Transfer Methods Globus Connect System 1 System 2 Example Scenario Rivanna/Afton Storage Cloud Storage Transfer public or nothighly sensitive data from Research Project & Research Standard storage or Rivanna/Afton home & scratch directories to AWS cloud storage. Expand Sources Any institution that uses Globus, such as: /home /scratch Research Project Storage Research Standard Storage Destinations AWS S3 AWS S3 Deep Glacier Data Transfer Methods AWS command line tools Data Transfer Methods Globus Connect Largescale research data transfer Transferring large amounts of research data is fast and simple with Globus Connect. Globus gives researchers unified access to their data through an easytouse web interface, and can be used to transfer data between your laptop and storage systems mounted on the HPC system. Globus can also be used to transfer data from other universities or supercomputing facilities. Learn more Access Globus For transferring highly sensitive data such as HIPAA or CUI data to the Ivy secure computing environment, researchers must use the secure Globus data transfer node (DTN). Learn more Graphical SFTP Clients Secure file transfer protocol (SFTP) Programs such as MobaXterm, Filezilla, and Cyberduck provide a graphical user interface to transfer data between a local computer and a remote storage location that permits scp or sftp. These applications allow draganddrop file manipulation. Learn more Command Line Tools Transferring Files from a Terminal Researchers who are comfortable with the command line can use a variety of command line tools to transfer their data between their laptops and storage systems. Programs such as scp, sftp, rsync and aws cli can be used to quickly transfer files. Learn more Local Data Transfer with the Command Line When using a Linux file system, users can invoke generic Linux commands to manage files"
rc-website-fork/content/userinfo/data-transfer.md,"and directories (mv, cp, mkdir), manage permissions (chmod) and navigate the file system (cd, ls, pwd). If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below: 10 Essential Linux Commands How To Manage Files From The Linux Terminal Shell Novice Transfering Data to Cloud Storage Several command line tools are available to transfer data from your UVA storage locations to the cloud. On the HPC system we provide the rsync and aws cli tools to transfer files from Research Project, Research Standard and Rivanna/Afton home & scratch directories to AWS storage. Learn more about the AWS CLI tools {{% callout %}} For more help, please feel free to contact RC staff to set up a consultation or visit us during office hours. {{% /callout %}}"
rc-website-fork/content/userinfo/computing-environments.md,"Research Computing (UVARC) serves as the principal center for computational resources and associated expertise at the University of Virginia (UVA). Each year UVARC provides services to over 433 active PIs that sponsor more than 2463 unique users from 14 different schools/organizations at the University, maintaining a breadth of systems to support the computational and data intensive research of UVA’s researchers. High Performance Computing Standard Security Zone UVARC’s High Performance Computing (HPC) systems are designed with highspeed networks, high performance storage, GPUs, and large amounts of memory in order to support modern compute and memory intensive programs. UVARC operates two HPC systems within the standard security zone, Rivanna and Afton. In total these systems are comprised of over 900 compute nodes, with a total of more than 48,000 X86 64bit compute cores. Scheduled using SLURM, these resources can support over 1.5 PFLOP of peak CPU performance. HPC nodes are equipped with between 375 GB and 1.5 TB of RAM to support applications that require small and large amounts of memory, and 55 nodes include various configurations of the NVIDIA general purpose GPU accelerators (RTX2080, RTX3090, A6000, V100, A40, A100 and H200), from 4 to 10way. UVARC also acquires and maintains capability systems focused on providing novel environments. This includes an 18node DGX BasePOD system with 8x A100 GPU devices per node, as well as newly added HGX H200 GPU nodes. The BasePOD provides a shared memory space across all GPUs in the system, allowing it to work collectively on models with memory needs larger than what can be held in a single node. The addition of H200 nodes further enhances UVARC’s support for largescale AI workloads and memoryintensive applications. More information can be found here. High Security Zone The HighSecurity HPC (Rio) cluster is a highperformance computing system specifically designed for"
rc-website-fork/content/userinfo/computing-environments.md,"the processing and analysis of controlledaccess and highly sensitive data. It features highspeed networks, highperformance storage, and GPUs including an NVIDIA HGX H200 GPU to support demanding computational tasks. Currently, Rio comprises 39 compute nodes, providing a total of 1,560 x86 64bit compute cores. Each HPC node is equipped with 375 GB of RAM to accommodate memoryintensive applications. Additional GPU nodes designed to support AI and machine learning workloads will be integrated in the near future. Situated within the highsecurity zone, Rio can only be accessed through Ivy Linux virtual machines, ensuring compliance with stringent security requirements for data storage and processing. Compute time is managed through service unit allocations and fairshare models, with SLURM serving as the job scheduler. This architecture provides a secure, efficient, and robust environment for handling sensitive research workloads. Interactive Computing and Scientific Visualization UVARC supports specialized interfaces (i.e., Open OnDemand, FastX) and hardware for remote visualization and interactive computing. Interactive HPC systems allow realtime user inputs in order to facilitate code development, realtime data exploration, and visualizations. Interactive HPC systems are used when data are too large to download to a desktop or laptop, software is difficult or impossible to install on a personal machine, or specialized hardware resources (e.g., GPUs) are needed to visualize large data sets. Expertise UVARC aggregates expertise to provide consulting and collaboration services to researchers addressing all levels of the Research Computing technology stack. UVARCs user support staff provide basic support and general onboarding through helpdesk and regularly scheduled tutorials. The Data Analytics Center (DAC) serves as a central hub for computational services and expertise in Data Analytics, including bioinformatics, image processing, text analysis, Artificial Intelligence. By offering specialized support and resources, the DAC connects UVA researchers with the advanced computing capabilities essential for their dataintensive research and analysis."
rc-website-fork/content/userinfo/computing-environments.md,"The DAC provides a comprehensive suite of services designed to enhance research efforts. The services are organized into three primary areas: Training and Technical Support, Consultations, and Collaborations. Training and technical support, offered as a general service, includes tutorials and technical assistance with research computing systems. Consultations, also free of charge, facilitate knowledge sharing and the design of analysis workflows through dedicated meetings. DAC team members are available for collaborations on grants. Collaborations involve more indepth work than consultations and in general require a DAC member to be embedded into a researcher’s grant. The DAC members, all holding advanced degrees, are adept at reviewing published techniques and adapting them to specific domain use cases, ensuring tailored and effective support for researchers. Senior support staff have advanced degrees in relevant research domains such as biology, imaging, physics, computer science and material science, enabling indepth collaboration on complex projects. For projects that require significant application development work, UVARC maintains a Solutions & DevOps team capable of rapid iteration while leveraging nontraditional HPC technologies. Lastly, UVARC's Infrastructure Services team enables projects that may require custom hardware or configurations outside of the standard images. Beyond their availability for direct project support, together these teams provide the R&D and operations expertise needed to ensure that UVARC is providing a modern research computing ecosystem for UVA researchers. Cloud Computing {ivy} Ivy is a secure computing environment for researchers consisting of virtual machines (Linux and Windows) backed by a total of 45 nodes and 2048 cores. Researchers can use Ivy to process and store sensitive data with the confidence that the environment is secure and meets HIPAA, FERPA, or CUI requirements. For standard security projects, UVARC supports microservices in a clustered orchestration environment that leverages Kubernetes to automate the deployment and management of many containers in an"
rc-website-fork/content/userinfo/computing-environments.md,"easy and scalable manner. This cluster has 876 cores and 4.9TB of memory allocated to running containerized services, including one node with 4 x A100 GPUs. It also has over 300TB of cluster storage and can attach to UVARC's broader storage offerings. ACCORD The ACCORD project (NSF Award: 1919667) offers flexible webbased interfaces for sensitive and highly sensitive data in a system focused on supporting crossinstitutional access and collaboration. The ACCORD platform consists of 8 nodes in a Kubernetes cluster, for a total of 320 cores and ~3.2TB of memory. Cluster storage is approximately 1PB of IBM Spectrum storage (GPFS). Researchers from nonUVA institutions can be brought into the ACCORD system through a memorandum of understanding between the researcher’s institution and UVA, security training for the researcher, and a posturechecking client installed on the researcher’s laptop/desktop. Data Storage All researchers on UVARC's systems have access to a highperformance parallel storage platform. This system provides 8PB (PetaBytes) of storage with sustained read and write speeds of up to 10 GB/sec. The integrity of the data is protected by daily snapshots. UVARC also supports a secondtier storage solution, 3 PB, designed to address the growing need for resources that support dataintensive research by offering a lower cost, scalable solution. The system is tightly integrated with other UVARC storage and computing resources in order to support a wide variety of research data life cycles and data analysis workflows. Data Centers, Network Connectivity, and Office Facilities UVARC enables interdisciplinary research through its robust data center facilities with over 1.5 MW of IT capacity to support leading edge computational and data storage systems. UVARC's equipment occupies a data center near campus, connected to the 10 Gbps campus network. Dedicated 10 and 100 Gbps links to our regional optical network and Internet2 give our researchers the"
rc-website-fork/content/userinfo/computing-environments.md,"network capacity and capability needed to collaborate with researchers from around the world. A Globus data transfer node enables data access and transfers to transcend institutional credentials. Located in the Michie North Building at 918 Emmet Street, UVARC’s offices are a short shuttle ride away from the central UVA grounds."
rc-website-fork/content/userinfo/xsede.md,"XSEDE's Mission was to substantially enhance the productivity of a growing community of scholars, researchers, and engineers through access to advanced digital services that support open research; and coordinate and add significant value to the leading cyberinfrastructure resources funded by the NSF and other agencies. The XSEDE project ended on August 31, 2022 and was succeeded by the ACCESS project. XSEDE Home: www.xsede.org"
rc-website-fork/content/userinfo/access-ci.md,"The NSF’s ACCESS (Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support) program builds upon the successes of the 11year XSEDE project, while also expanding the ecosystem with capabilities for new modes of research and further democratizing participation. ACCESS Home: accessci.org accessci.org/about Allocations Allocations: allocations.accessci.org Documentation Support: support.accessci.org Community Engagement ACCESS: support.accessci.org/affinitygroups Campus Champions: https://campuschampions.cyberinfrastructure.org UVa Research Computing has two Champions, Ed Hall and Katherine Holcomb {{% callout %}} For more help, please feel free to contact RC staff to set up a consultation or visit us during office hours. {{% /callout %}}"
rc-website-fork/content/userinfo/globus.md,"Globus is a simple, reliable, and fast way to access and move your research data between systems. Globus allows you to transfer data to and from systems such as: Laptops & personal workstations Rivanna/Afton HPC clusters HighSecurity Research Standard Storage Lab / departmental storage Tape archives Cloud storage Offcampus resources (ACCESS, National Labs) Globus can help you share research data with colleagues and coinvestigators, or to move data back and forth between a lab workstation and Rivanna/Afton or your personal computer. Are your data stored at a different institution? At a supercomputing facility? All you need is your institution's login credentials. Getting Started {{% callout %}} Before you are able to transfer files from or to your personal laptop/workstation, you must set up a Globus Collection on that computer, aka your local endpoint. A collection can be a storage volume or specific file folder on Your local workstation, A departmental server, A ""DTN"" (Data Transfer Node) connected to Rivanna, Afton, or HighSecurity Research Standard Storage, or A server operated by another university or by a national computing center. {{% /callout %}} Create a Personal Collection for your laptop {{% callout %}} In order to transfer data to/from a lab or personal computer, you must install the Globus Connect Personal application. {{% /callout %}} Open a browser and navigate to https://www.globus.org/globusconnectpersonal. Select your operating system in the ""Install Globus Connect Personal"" section and click ""install now"". Follow the installation instructions to install Globus Connect Personal. Launch the newly installed Globus Connect Personal. A box will appear and ask you to login. It will redirect you to a webpage where you will allow access and provide a label for your collection, i.e. your computer. We recommend using something very descriptive, such as mstk3laptop or smithgenlabworkstation. After clicking allow, a new page will"
rc-website-fork/content/userinfo/globus.md,"pop up asking you to provide a collection name and a description. Again, use something descriptive like mstk3laptop. Do not select the ""High Assurance"" checkbox. Your collection is now set up and ready to use. On Windows and Mac OSX, the agent will run in the background on your laptop or workstation and will restart when the machine is booted. Click on the agent icon (in the tray for Windows users, in the toolbar for macOS users) to change your preferences or to see the web console. On Linux you must start the agent manually upon rebooting. Your local computer is now able to serve as a Globus Collection. Check your new Collection Globus transfers data between two ""collections"" or endpoints. You must log in to the Globus website to initiate any transfers. Open a browser window to https://www.globus.org/ and click on Log In. Select “University of Virginia” from the dropdown list of organizations. You may also type the name into the textbox next to the down arrow. Click Continue. Next to you will be directed to sign in using UVA NetBadge. Once logged in you will see the File Manager page: Click the Collection box and you should see your newly created collection Transferring Files {{% callout %}} You can search for the collections to use for your transfer from the File Manager at the Globus website, then use their Web interface to initiate and monitor your transfers. {{% /callout %}} The official UVA managed collections are: UVA Standard Security Storage generally available; maps to Rivanna/Afton home directories, scratch, Research Standard & Research Project storage. uvaivyDTN available to Ivy secure platform users, for moving files into HighSecurity Research Standard Storage. Globus is the only permitted datatransfer protocol for highly sensitive data. To transfer data to HighSecurity Research Standard Storage,"
rc-website-fork/content/userinfo/globus.md,"please see the special instructions here. You can transfer files to or from your personal collection to a managed collection, one run either by UVA or by another institution. You can transfer files between two managed collections. You cannot transfer files from one personal collection to another personal collection. If you wish to do this, contact Research Computing to convert at least one personal collection to a Globus Plus collection. To transfer a file: From the File Manager page, select a collection by clicking on the ""Collection"" link near the top of the screen (""start here, select a collection""). Start typing the name of the collection to see the options containing the string as you type. Once the collection is found, click on its link. Wait while it finds your folders. When complete, click on ""Transfer or sync to..."" on the right sidebar. If you do not remember the exact name of the second collection, click the magnifying glass to search. If your second collection is one you have registered with Globus, you may also click Your Collections. This will open a second pane. Either pane may be the source or destination. Select a file or folder from your ""source"" pane. Click the Start or < Start button at the bottom of the pane to begin the transfer into the ""destination"" pane. After you initiate a transfer, it will be assigned a Task ID that you can use to reference that specific transfer. This is a useful way of identifying transfers when checking on the status of a job or viewing your past Globus activity. Your transfer may take several minutes or hours to complete depending upon the size of the data. Globus transfers are persistent, which means that if there is a network interruption, or one collection is turned"
rc-website-fork/content/userinfo/globus.md,"off, the transfer will resume whenever the connection is restored. The transfer takes place in the background, so once it is assigned an ID and you receive the notification that it has begun, you can log out from the Globus page. The Globus Personal Connection application will show only a limited default set of paths on your computer. If you need to use another folder, such as one on an external hard drive, as the source or destination, you will have to add it. With the Globus Personal Collection running, click on the g logo in your taskbar or tray. Mac: go to Preferences/Access. Click the + button to add a path. Windows: Options/Access, click + to add the path to the drive. Navigate as usual to the location you wish to add. Monitoring Transfer Activity You can check on the status of your transfer using the Task ID. From the lefthand navigation bar of the Globus Connect manager, click on ""Activity"". Or visit https://app.globus.org/activity Here you will see a list of your current and past transfer jobs. Click on a job and you will get details and status. Notifications Users are notified via email for both successful and failed transfers. The email looks something like this, and provides a URL for more information: TASK DETAILS Task ID: 7c0351b49c1c11eda29d8383522b48d9 Task Type: TRANSFER Status: SUCCEEDED Source: Gancayco Laptop (e6b14dc634a811edba40d5fb255a47cc) Destination: UVA Standard Security Storage (e6b338df213b4d31b02c1bc2c628ca07) Label: n/a https://app.globus.org/activity/7c0351b49c1c11eda29d8383522b48d9/overview Sharing Folders {{% callout %}} You can share folders to either specific individuals, or to groups that you create and manage within Globus. A group must be populated with at least one user. A shared folder must be created on a managed collection or on a Globus Plus collection; personal collection can receive shared folders but cannot create shares. {{% /callout %}} Open"
rc-website-fork/content/userinfo/globus.md,"the Globus web interface and log in using UVA Netbadge. From the Transfer Files interface, log in to the UVA Standard Security Storage collection as described above. Navigate in the folder structure of that collection until you find the folder you want to share. Highlight it. Next, select the Share link on the right side of the files window. Globus regards shared folders as collections, so you must create a new collection to share the folder. Clicking on Share allows you to ""Add a Guest Collection."" You can only create and manage guest collections for directories or files that you own and can access. Provide a Display Share Name (required) and a description (optional). Click ""Create Share"". Your new share will be created. Now click ""Add Permissions Share With"" in the upper right. You must go through this even if you do not change permissions from the default. Path Leave this set to / since it refers to the path relative to the directory you are sharing from. Share With Decide whether you want to share with individual users or with a group. Please do not set this to ""All Users"" or ""Public"". If you share with an individual user, follow the instructions below. If you choose to share with a group, you will first need to create that and add users to it by using the GROUPS tab at the top of the page. Identity/Email You can look up other Globus users by searching for a part of their name or institution. If you cannot find the individual, you should contact them to make sure they have signed in to Globus at least once. Generally, users at other colleges and universities can be identified with the simple form of their email address, like mst3k@virginia.edu or jdoe@mit.edu, etc. Users who"
rc-website-fork/content/userinfo/globus.md,"are unaffiliated with a university can still sign in to Globus using Google (identified as username@gmail.com) or by creating a username and password in Globus (identified as userid@globusid.org) If you enter your collaborator's email address, it must exactly match the one associated with the recipient's Globus ID. Permissions You can specify whether this user has access to read or write to your share. Keep in mind that permission to write to the folder also grants the recipient the ability to delete files within in. Add a message if you wish, then click ""Add Permission"" whether you made any changes or not. Since a share is a Globus collection, to manage it see the Managing Endpoints section below. You may delete the share to remove access, once your recipient has obtained the folder. Managing Endpoints {{% callout %}} The Globus interface makes it easy to manage and delete your endpoints. {{% /callout %}} You can view your collections, including your shared collections or other collections that have been shared with you, by clicking on the Endpoints submenu at the File Manager page. From this page you can see endpoints That are shared with you That are shareable by you That are administered by you Clicking on the name of each collection will allow you to review or modify settings. You can modify only collections that are administered by you. To modify or delete a collection, click the Administered By You tab, then click the endpoint you wish to manage. You can edit its properties, open it, or delete it. Security {{% callout %}} UVA permits faculty and researchers to manage data transfer and sharing with colleagues and collaborators themselves. However, with this ability comes the responsibility to share wisely and carefully. Therefore, we ask that you follow a few basic principles"
rc-website-fork/content/userinfo/globus.md,"when you share data using Globus: {{% /callout %}} Grant the least permissions necessary, not the most. If a colleague needs only to retrieve your data files, then grant readonly permissions. Grant access to specific individuals only, not to ""all users"" or to the public. These settings risk your information going to people and places that you have not designated, or being used in ways you do not control. Remove shared collections as soon as they are no longer needed. It is a good practice to revisit your endpoints page periodically to clean up and to cull unused resources. Finally, monitor and track your large file transfers. When someone is transferring large data sets to you, or you to them, monitor their progress and keep in touch with the person or group on the other end. This helps identify any unusual behavior. For Advanced Users Globus has a commandline interface. Globus also has an API and Python SDK. For other technical details, see Globus Documentation."
rc-website-fork/content/userinfo/r_updates.md,"During the June maintenance, we will make changes to R which will affect how your R programs run on Rivanna. Below is a list of the changes and how they will affect your code. 1. The gccbuilt versions of R will be updated to goolfbuilt versions. Instead of loading gcc before loading R, you will need to load goolf or gcc openmpi. For example: module load goolf R/4.0.0. Remember to update any Slurm scripts that have module load gcc R or module load gcc R/3.x.x. 2. The locations of the R libraries will be updated. We are changing the locations of the R libraries (i.e., the folders where local packages are installed). This change will create separate folders for different compiler versions of R, which will prevent package corruption. As a result, R will not see the packages that you had installed before the maintenance. (The only exception would be gcc openmpi R/4.0.0, which already uses the new library location). You will need to reinstall your R packages. To help with this effort, we are providing a script that will scrape the list of packages installed in an older library and will attempt to install these packages in the new library. Details are provided at ""New Libraries"". 3. The versions of R will be streamlined to 3.4.4, 3.5.3, 3.6.3, and 4.0.0. If you had hardcoded another version of R in your scripts (e.g., R/3.6.1), you will need to update your scripts to specify one of these newer versions. To see what modules would need to be loaded prior to loading R, you can use the module spider command (e.g.,module spider R/3.6.3)."
rc-website-fork/content/userinfo/user-guide.md,"High Performance Computing Standard and high security HPC to run your code, generally written in R, Python or shell scripts. Get Started › Secure Computing Secure virtual machines and interactive notebooks for processing HIPAA and other highly sensitive data. Get Started › Storage Need large, or extremely large storage offsite or on grounds? Can you count in GB, TB, or PB? Learn more about storage options and pricing. Get Started › Cloud Have an idea you'd like to test? Need an environment provisioned in shortorder? We can help you build in the AWS cloud. Get Started › Image Analysis Do you have a large imaging dataset to process? Do you want to automate your image processing pipeline? Learn more about tools and techniques to speed up your workflow. Get Started › Data Transfer Sometimes you have the right data, but in the wrong place. There are several paths available for researchers depending upon the size and destination of your data. Get Started ›"
rc-website-fork/content/userinfo/systems.md,"UVA Research Computing can help you find the right system for your computational workloads. From supercomputers to HIPAA secure systems to cloudbased deployments with advanced infrastructure, various systems are available to researchers. {{< systemsboilerplate }} High Performance Computing Rivanna and Afton A traditional high performance cluster with a resource manager, a large file system, modules, and MPI processing. Get Started with UVA HPC Secure High Performance Computing Rio A high performance cluster designed to process and store secure data accessed via ivy dedicated virtual machines. Get started on Rio Secure Computing for Highly Sensitive Data Ivy A multiplatform, HIPAAcompliant system for secure data that includes dedicated virtual machines (Linux and Windows), JupyterLab Notebooks, and Apache Spark. Get started on Ivy Secure Computing for Sensitive Data ACCORD A web based platform for researchers from different colleges and universities to collaborate, analyze, and store their sensitive data in a central location. ACCORD supports RStudio, JupyterLab, and Theia Python. Get started on ACCORD Virtual Machines Public/Private Cloud Cloudbased computing solutions are also available in Amazon Web Services, Google Cloud Platform, and our private cloud UVA Skyline. All three provide options for quick deployments, shortterm lifecycles, and unique requirements such as GPUs or clusters. Learn more Container Services Deploying software in containers has grown increasingly useful for research scenarios. Containers are portable, distributable, allowing developers to include both code and dependencies (libraries, modules, etc.) in a single bundle. We operate a Kubernetes cluster to support researchoriented microservices. Learn more NSF ACCESSCI The Advanced Cyberinfrastructure Coordination Ecosystem: Services and Support (ACCESS) is an NSFfunded virtual organization that integrates and coordinates the sharing of advanced digital services including supercomputers and highend visualization and data analysis resources with researchers nationally to support science. Learn more"
rc-website-fork/content/userinfo/microservices.md,"Containerbased architecture, also known as ""microservices,"" is an approach to designing and running applications as a distributed set of components or layers. Such applications are typically run within containers, made popular in the last few years by Docker. Containers are portable, efficient, reusable, and contain code and any dependencies in a single package. Containerized services typically run a single process, rather than an entire stack within the same environment. This allows developers to replace, scale, or troubleshoot portions of their entire application at a time. {{< highlight }} General Availability (GA) of Kubernetes Research Computing now manages microservice orchestration with Kubernetes, the opensource tool from Google. New deployments are now launched directly within Kuberenetes. » Read about Kubernetes and user deployments. {{< /highlight }} Microservices at UVA Research Computing runs microservices in a clustered orchestration environment that automates the deployment and management of many containers easy and scalable. This cluster has 1000 cores and ~1TB of memory allocated to running containerized services. It also has over 300TB of cluster storage and can attach to project and value storage. {{% highlightdanger %}} UVA's microservices platform is hosted in the standard security zone. It is suitable for processing public or internal use data. Sensitive or highly sensitive data are not permitted on this platform. {{% /highlightdanger %}} Basic Principles 1 Microservice architecture is a design approach, or a way of building things. Microservices can be considered the opposite of ""monolithic"" designs. A few guiding design principles: Separate components and services Availability and resilience Replaceable elements Easily distributable Reusable components Decentralized elements Easy deployment Here's a talk given by Martin Fowler explaining the idea: {{< youtube ""2yko4TbC8cI"" }} 2 The easiest and most common way to run microservices is inside of containers. We teach workshops on containers and how to use them. Browse"
rc-website-fork/content/userinfo/microservices.md,"the course overview for Building Containers for the HPC System at your own pace. Docker provides an excellent Getting Started tutorial. Users may inject ENV environment variables and encrypted secrets into containers at runtime. This means sensitive information does not need to be written into your container. Uses for Research Microservices are typically used in computational research in one of two ways: Standalone microservices or small stacks Such as interactive or datadriven web applications and APIs or scheduled task containers. Some examples: Simple web container to serve Project files to the research community or as part of a publication. Reference APIs can handle requests based either on static reference data or databases. Shiny Server presents users with interactive plots to engage with your datasets. A scheduled job to retrieve remote datasets, perform initial ETL processing, and stage them for analysis. Microservices in support of HPC jobs Some workflows in HPC jobs require supplemental services in order to run such as relational databases, keyvalue stores or reference APIs. Browse a list of recent UVA projects employing microservices. Common Deployments Service Accessibility Description NGINX Web Server Public A fast web server that can run Static HTML demo Flask or Django apps demo RESTful APIs demo Expose Project storage demo Apache Web Server Public An extremely popular web server that can run your static HTML, Flask or Django apps, RESTful APIs, or expose files stored in Project storage. Shiny Server Public Runs Rbased web applications and offers a dynamic, datadriven user interface. See a demo or try using LOLAweb Recurring Tasks n/a Schedule or automate tasks or data staging using the language of your choice (bash, Python, R, C, Ruby). Database Hosting Research computing may be able to provide support for your database hosting needs. Please schedule a consultation request on our website."
rc-website-fork/content/userinfo/microservices.md,"Follow the link here and fill out the form under ""Consultation Request"" on the right hand side of the page under ""All Forms"". Service Eligibility & Limitations To be eligible to run your microservice on our infrastructure, you must meet the following requirements: Microservices and custom containers must be for research purposes only. We do not run production systems outside the scope of academic research support. Your container(s) must pass basic security checks. Containers may not contain passwords, SSH keys, API keys, or other sensitive information. There are secure methods for passing sensitive information into containers. If bringing your own custom container, it must be ready to go! Unfortunately, we cannot create custom containers for you unless it is part of a funded project. Microservices may not run efficiently for all use cases. Some scenarios that cannot run successfully in containers include: Services (apart from webbased services over HTTP/HTTPS) that need to be accessed from outside the HPC network. Services that require licensing, such as Microsoft SQL Server, MATLAB, etc. Services that require GPU to run. Apptainer Want to run your container within an HPC environment? It can be done, using Apptainer! Apptainer is a container application targeted to multiuser, highperformance computing systems. It interoperates well with Slurm and with the Lmod modules system. It can be used to create and run its own containers, or it can import Docker containers. Learn more about Apptainer. Next Steps Have a containerized application ready for launch? Or want a consultation to discuss your microservice implementation? Request Access {{< consultbutton }}"
rc-website-fork/content/userinfo/lab-computing.md,"Lab Computing services are now available to research groups in the UVA School of Medicine to assist in providing the best in data storage for this specific environment. These services include: Data Migration & Storage Assist in migration of data from legacy and nonmanaged data storage equipment to managed infrastructure storage on UVA Secured and NonSecured networks. Ensure that labs, workers and managers have full access to their data from all locations in a secure and stable environment. Data storage options include: ES1 Data Storage ES3 Secured Data Storage Research Standard Storage (Option for Object Storage Backup) Training & Tools Train researchers in the use of data storage migration and management tools. These tools are a combination of tested Open Source technologies and longused tools. These tools provide for: Synchronized and itemized data transfers for file syncs of live data Multithreaded rapid moves of data from Lab Storage to Managed Central Storage options Legacy platform data migration from older operating system platforms Data storage comparison to ensure all data copies have been tallied between multiple points Requirements & Consultation As part of these services, we also provide an initial consultation to ensure that the data and laboratory requirements are met on all levels. There is more to data migration than just uploading data. Services may also include: Retirement of outdated legacy equipment Support the regular inventories of data, hardware and software Purchase and procurement of future storage options in line with longterm budgeting strategies These resources help ensure that your lab data is protected and secured in compliance with ISPro, Grant and Security requirements, and that your equipment, data management and software application needs are being met in a managed, professional manner."
rc-website-fork/content/userinfo/secure-computing.md,"UVA School of Medicine Research Computing can assist medical researchers in both understanding what HIPAA compliance requires of their work, and how to implement technical solutions to achieve and verify such compliance. Review & Assessment From a compliance perspective, RC offers three levels of review/assessment: Security Plan Having a security plan in place is important to your success at UVA School of Medicine. If your lab or department doesn’t have a plan in place Research Computing Information Security will be happy to help you develop such a plan. Security Review If you have a security plan in place already we can help you verify that your computing resources are functioning as documented in your security plan. As part of security review, we will do a risk analyst and provide you a list of recommended enhancements. Risk Assessment This is the systematic process of evaluating the potential security risks/hazards and any business impact they could present. We analyze the likelihood of events occurring. As we conduct a risk assessment we look for vulnerabilities and weaknesses that could your system more susceptible to an event. We will provide list of finds and either work with your local support person or other resources to help mitigate the risk. Implementation For implementation, RC offers a number of skills and services: PHI and Deidentification Encryption best practices Encryption of files, databases, and systems PHI & DeIdentification Some research data are ""deidentified"" in order to remove them from HIPAA security requirements. This is particularly useful when researchers are processing across many hundreds or thousands of patients for trends and statistically meaningful insights that do not rely upon patientspecific data points. Here are some common examples of PHI research data that may need to be deidentified. Deidentification can mean the complete removal of such fields from your"
rc-website-fork/content/userinfo/secure-computing.md,"dataset, or the complete replacement of these data with meaningless placeholder values. Names All geographic subdivisions smaller than a state, including street address, city, county, precinct, ZIP code, and their equivalent geocodes, except for the initial three digits of the ZIP code if, according to the current publicly available data from the Bureau of the Census: 1. The geographic unit formed by combining all ZIP codes with the same three initial digits contains more than 20,000 people; and 2. The initial three digits of a ZIP code for all such geographic units containing 20,000 or fewer people is changed to 000 All elements of dates (except year) for dates that are directly related to an individual, including birthdate, admission date, discharge date, death date, and all ages over 89 and all elements of dates (including year) indicative of such age, except that such ages and elements may be aggregated into a single category of age 90 or older Telephone numbers Vehicle identifiers and serial numbers, including license plate numbers Fax numbers Device identifiers and serial numbers Email addresses Web Universal Resource Locators (URLs) Social security numbers Internet Protocol (IP) addresses Medical record numbers Biometric identifiers, including finger and voice prints Health plan beneficiary numbers Fullface photographs and any comparable images Account numbers Any other unique identifying number, characteristic, or code, except as permitted as a ""reidentifier"" Certificate/license numbers Resources HIPAA for Professionals & Providers (HHS) University of Virginia Information Security"
rc-website-fork/content/userinfo/use-cases.md,"Web Distributions Donec fermentum eu tortor eu dignissim. Curabitur elit diam, tempor in dui non, tincidunt rhoncus risus. Praesent pharetra nisl elit, vitae commodo odio rutrum et. Praesent ac ligula pharetra, mollis lorem tristique, convallis leo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nulla facilisi. Cras sit amet euismod elit, et iaculis ipsum. Fusce aliquet mauris sit amet elit euismod, in varius justo suscipit. Genomics Pipelines Donec fermentum eu tortor eu dignissim. Curabitur elit diam, tempor in dui non, tincidunt rhoncus risus. Praesent pharetra nisl elit, vitae commodo odio rutrum et. Praesent ac ligula pharetra, mollis lorem tristique, convallis leo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nulla facilisi. Cras sit amet euismod elit, et iaculis ipsum. Fusce aliquet mauris sit amet elit euismod, in varius justo suscipit. Data Processing Morbi in nisl est. Nam accumsan elementum semper. Vivamus enim nunc, fringilla at nibh in, ornare mattis eros. Curabitur volutpat sapien at risus convallis finibus sed sit amet urna. Mauris nec nunc sit amet dui interdum commodo. Curabitur vitae mollis mauris. Aenean nec scelerisque neque, ac vehicula quam. Nulla viverra metus eget eros maximus, et dignissim ipsum egestas. Suspendisse mi nisi, efficitur ut nunc quis, consequat fringilla nibh. Integer in enim eget quam dignissim imperdiet. Curabitur placerat suscipit augue quis lobortis. Machine Learning Nam accumsan elementum semper. Vivamus enim nunc, fringilla at nibh in, ornare mattis eros. Curabitur volutpat sapien at risus convallis finibus sed sit amet urna. Mauris nec nunc sit amet dui interdum commodo. Curabitur vitae mollis mauris. Aenean nec scelerisque neque, ac vehicula quam. Nulla viverra metus eget eros maximus, et dignissim ipsum egestas. Suspendisse mi nisi, efficitur ut nunc quis, consequat fringilla nibh. Integer in enim eget quam dignissim imperdiet. Curabitur placerat suscipit augue"
rc-website-fork/content/userinfo/use-cases.md,"quis lobortis. Batch Analysis of Bulk Data Proin non volutpat diam, in imperdiet nulla. Sed quis lobortis elit, at lobortis turpis. Curabitur at eleifend ipsum. Etiam egestas consectetur massa nec dictum. Pellentesque feugiat ipsum massa, sed lobortis enim sodales nec. Aliquam erat volutpat. Nunc lobortis quam sit amet tellus interdum aliquam. Integer urna nisi, tempus vel fringilla a, dapibus non sapien. Alexa Voice Skills / Natural Language Processing Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin tincidunt viverra elit vitae mollis. Donec ipsum erat, ornare id suscipit non, lobortis in orci. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed ac ante eget purus ultrices cursus. Vivamus pretium erat in mattis feugiat. Nullam accumsan dignissim erat non auctor. Quisque elementum faucibus lacus pretium pretium. Mauris luctus, sapien id suscipit semper, eros ipsum fringilla odio, in scelerisque diam sem a libero. Ut accumsan non nibh in gravida. Pellentesque non ornare ipsum. Sed sed tellus eu arcu consectetur convallis. Aenean feugiat turpis id ex pretium ornare. Morbi sed odio sodales lorem tempus egestas ac at magna."
rc-website-fork/content/userinfo/tools.md,"Tools and software projects that UVA Research Computing has collaborated on: LOLAweb LOLAweb is a web server and interactive results viewer for enrichment of overlap between a userprovided query region set (a bed file) and a database of region sets. It provides an interactive result explorer to visualize the highest ranked enrichments from the database. LOLAweb is a web interface to the LOLA R package. Launch LOLAweb BARTweb There are a number of commercially licensed tools available to UVa researchers for free. These products, including UVa Box, Dropbox (Health System) and CrashPlan, are most suitable for smallscale storage needs. Learn more"
rc-website-fork/content/service/collaboration.md,"The RC is available for collaborative work with researchers, faculty, and students. These may include: Code and package development, such as R and Python packages, Matlab toolboxes, pipelines, or containers. Open source initiatives. Ongoing analysis of large data sets and publication of results. Architecture / design best practices for new research approaches."
rc-website-fork/content/service/acknowledgement.md,"Recognition and documentation of the contribution that Research Computing’s systems and support play in breakthrough research is essential to ensuring continued support for and availability of cuttingedge computing resources at The University of Virginia. Please cite UVA Research Computing in any research report, journal article, or other publication that requires citation of an author’s contributions. Suggested format: {{% callout %}} The authors acknowledge Research Computing at The University of Virginia for providing computational resources and technical support that have contributed to the results reported within this publication. URL: https://rc.virginia.edu {{% /callout %}}"
rc-website-fork/content/service/map.md,"From a security perspective, research data generally breaks down into two types: Moderately Sensitive and Highly Sensitive. Each must be handled appropriately. For the purposes of computing environments, that means conducting your research in the right security zone. Blue Zone Orange Zone Moderately Sensitive Data HPC Rivanna and Afton, our high performance clusters. Includes over 12k cores across Storage Virtual Machines Microservices Highly Sensitive Data HPC Storage Virtual Machines Microservices HPC Computing with over 12k cores. Storage All your data are belong to us. Virtual Machines Virtual Machines for all your data. Microservices Microservices for all your data."
rc-website-fork/content/service/high-performance-computing.md,"Research Computing supports all UVA researchers who are interested in writing code to address their scientific inquiries. Whether these programming tasks are implemented interactively, in a series of scripts or as an opensource software package, services are available to provide guidance and enable collaborative development. RC has specific expertise in objectoriented programming in Matlab, R, and Python. Examples of service areas include: Collaborating on package development Reviewing and debugging code Preparing scripts to automate or expedite tasks Developing web interfaces for interactive data exploration Advising on integration of existing software tools UVA has four local computational facilities available to researchers: Rivanna, Afton, Rio and Ivy. Depending upon your use case, privacy requirements, and the application(s) you need to run, we can help you create an account and start processing your data. Afton Standard Security HPC Cluster {{< getallocationblurb name=""Afton"" }} Read more about Afton Rivanna Standard Security HPC Cluster {{< getallocationblurb name=""Rivanna"" }} Read more about Rivanna Rio HighSecurity HPC Cluster Rio is one of the recent University of Virginia’s HighPerformance Computing (HPC) systems, specifically designed for the processing and analysis of controlledaccess and highly sensitive data. Currenlty, Rio consists of 39 HPC nodes, each equipped with 375 GB of RAM and offering a combined total of 1,560 x86 64bit compute cores. Researchers can use Rio to process and store sensitive data with the confidence that the environment is secure and meets HIPAA and FERPA requirements. Read more about Rio Ivy HighSecurity / HIPAA Computing Environment Ivy is a secure computing environment for researchers consisting of virtual machines (Linux and Windows) backed by a total of {{< ivynodecount }} nodes and approximately {{< ivycorecount }} cpu cores. Researchers can use Ivy to process and store sensitive data with the confidence that the environment is secure and meets HIPAA, FERPA, CUI"
rc-website-fork/content/service/high-performance-computing.md,or ITAR requirements. Read more about Ivy
rc-website-fork/content/service/grant-support.md,"RC is interested in participating in funding proposals related to advanced computing efforts. If you have a project or grant application in mind, feel free to contact us for more information."
rc-website-fork/content/service/imaging.md,"Image Processing and Scientific Visualization are two separate processes within the scientific research lifecycle, yet the two concepts often play off of one another. Image processing refers to the enhancement and transformation of images to prepare them for quantitative analysis. Scientific visualization is the graphical communication of data so that trends and anomalies can be more easily recognized. UVa Research Computing offers many services and resources to help researchers augment their work with image processing and scientific visualization techniques. Image Processing Overview Image processing encompasses a variety of techniques to prepare images for analysis. Researchers often need to remove noise artifacts from their imaging data, or they need to analyze particular regions of interest. While manual image manipulation can easily yield the desired results, this can be timeconsuming or even impossible with the amount of data we are able to collect with high throughput screening. By automating image processing steps such as noise filtering and segmentation, researchers are able to perform their work faster and for larger quantities of data. Common Image Processing Techniques The following techniques are commonly employed in imaging research. All of these processes can be automated and run locally on your computer or on UVa's high performance computing (HPC) cluster. With the parallelization capabilities of HPC, it is possible to fully process and analyze a large imaging data set in a few hours or less! Preprocessing Image preprocessing can help enhance the quality of your images. Common preprocessing techniques include adjusting brightness and contrast, removing noise, sharpening images, and performing geometric and color transformations. Segmentation Image segmentation is useful for determining one or multiple regions of interest. Segmentation can be used to identify foreground objects, cell boundaries, or tissue types. Registration Image registration is useful when comparing two or more objects of differing size or morphological"
rc-website-fork/content/service/imaging.md,"features. Registration can be used to align 2D or 3D images through linear or nonlinear algorithms. Analysis Image analysis is the measurement and statistical analysis of meaningful features in your imaging data, such as area or volume of a region of interest and mean intensity value throughout an image. Popular Software ImageJ/Fiji ImageJ is a Javabased image processing program developed at the NIH. ImageJ can be used interactively through a graphical user interface or automatically with Java. Fiji is ImageJ with common plugins preinstalled for scientific image analysis. MATLAB Matlab is a numerical computing environment with its own proprietary programming language. Matlab provides an extensive Image Processing Toolbox for with builtin functions for image registration, segmentation, and analysis. Python Python is a powerful highlevel programming language for general purpose programming. There are several open source packages available in Python for image processing, including: OpenCV, scikitimage, and Python Imaging Library. ANTs ANTs, or Advanced Normalization Tools, is a stateoftheart medical image registration and segmentation toolkit. ANTs works in conjunction with Insight Toolkit(ITK) to read and visualize multidimensional imaging data. R R is an open source programming language and computing environment for statistical analysis and data visualization. There are a variety of R packages available for image processing, such as ANTsR, EBImage, and magick. Additional Resources We currently offer an online short course for image processing with Fiji/ImageJ. Introduction to Image Processing with Fiji/ImageJ Stay tuned for additional online tutorials as well as inperson workshops listed on our workshops page Visualization Overview Visualization is the conversion of data into plots or images in order to view various features of the data. As humans, we are able to absorb large amounts of information through sight. We can use visualizations as an exploratory tool to gain insight into the data we collect and to"
rc-website-fork/content/service/imaging.md,"create hypotheses for relationships. We can also use visualizations to communicate ideas to others. Popular Software MATLAB MATLAB contains many builtin functions for data visualization, including those for 3D surfaces and meshes. MATLAB is also capable of medical image visualization and is compatible with DICOM and NIFTI filetypes. ParaView ParaView is an opensource application for visualization and analysis of data defined on meshes or grids. It allows for visualization of 2D or 3D data and is good for general purpose, rapid visualization. VisIt VisIt is software for the visualization of data defined on meshes or grids. It is compatible with file types that have an underlying HDF5 format. Blender Blender is a 3D graphics software that can be used for creating 3D objects and animations. It can be used for 3D modeling, rendering, motion tracking, and video editing. Unity Unity is a crossplatform software application for the creation of visualizations in augmented and virtual reality. Additional Resources We currently offer several online tutorials for data visualization. MATLAB Data Processing and Visualization Stay tuned for additional online tutorials as well as our workshops posted on our workshops page"
rc-website-fork/content/service/status.md,"Project Storage Data Migration The storage hardware serving the Research Project Storage file system has been experiencing intermittent disruptions since October 2023. In response, we acquired and installed new, upgraded hardware offering faster, more reliable access and capacity. However, because an Active File Management (AFM) connection between the old and new systems was used to facilitate automatic file transfers, the new system’s performance is being negatively impacted by the old hardware and causing accessibility issues for some users. To mitigate these issues, Research Computing engineers are switching to an alternate backend migration process on February 26, 2024, at 9:00 a.m. IMPORTANT: This action will make data that have not yet been transferred appear to have vanished from the new Project storage file system. Please be assured that all data will remain secure and intact throughout the migration process. Rivanna and RC’s other storage services, Scratch and Research Standard, continue to operate normally. {{% callout %}} Key Points: An alternate method of transferring data from old Research Project storage to new Research Project storage will be implemented on 2/26/24 at 9:00 a.m. EST. All data on old Research Project and new Research Project are secure and intact. Starting February 26, data will be transferred by RC staff to a new readonly share /stagedproject on the new storage system. Files that have already been migrated to /project remain intact. Though the new Project storage system is operating with expected performance, the transfer of all data from the old storage system will take several months. The severe performance degradation of the old storage system will remain a bottleneck regardless of the change in data transfer method. Rivanna and RC’s other storage services, Scratch and Research Standard, continue to operate normally. Update: 20240701 Before February 26: A total of 1.7 PB out of 4.3"
rc-website-fork/content/service/status.md,"PB were copied from old Project storage to /project folder on the new storage system using the automated migration process before February 26 (40%). Since February 26: 100% of the data from old /project has been copied and is now available in the /stagedproject or /project folders on the new storage system. {{% /callout %}} {{% highlight %}} Do you have additional questions? Please contact our user services team, or join us for our virtual office hours every Tuesday, 35 p.m. and Thursday, 1012 p.m. starting March 6. {{% /highlight %}} Incident Response Research Computing will reach out to all known users of this storage system with instructions for accessing data before and after February 26, and for assistance prioritizing files for transfer. {{% accordiongroup title=""Email Communications"" id=""commgroup"" %}} {{% accordionitem title=""Email Communications"" id=""emails"" %}} {{% accordiongroup title=""Emails"" id=""emailgroup"" %}} {{% accordionitem title=""Apr 22, 2024 Project Storage Update"" id=""email3"" %}} Dear Research Project storage user: We are pleased to report that our new Project storage filesystem is performing as expected. A majority of the data on the legacy Project filesystem has been successfully copied to /stagedproject on the new system, enabling researchers to access their data via /stagedproject or /project. We are emailing individual share owners as soon as their groups’ data have been transferred. The /stagedproject shares were provisioned as a temporary accommodation free of charge to expedite file transfers to the new storage system while allowing active work in /project. Please note: If you don’t have a /stagedproject share, no further action will be required. Otherwise users will need to consolidate all of their files from /stagedproject to /project storage. Stepbystep instructions are available on the RC website (“How can I consolidate my files in /stagedproject and /project?”). If you need assistance with consolidating your files, you may"
rc-website-fork/content/service/status.md,"reach out to us during office hours or contact our user services team. In addition, monthly billing for Research Project storage quotas will resume on May 1st. Billing was suspended in October 2023 due to the filesystem’s performance issues. These bills will be based on quotas on the ‘new’ /project storage space which has been working with expected performance for the past 2 months. Usage on /stagedproject and the legacy system will not be charged. Billing questions should be directed to RCBilling@virginia.edu. Detailed documentation on the Project storage incident, including previous email communications and frequently asked questions, is available on our Data Migration status page. We are committed to working diligently until the data transfer is complete and the legacy system is decommissioned. Our technical support teams will continue to be available to you to answer questions and address any concerns. With regards, Karsten Siller Director, Research Computing User Services Information Technology Services University of Virginia {{% /accordionitem %}} {{% accordionitem title=""Feb 22, 2024 Reminder: Upcoming Changes to Data Transfer Process for Project Storage File System"" id=""email2"" %}} Dear Colleagues, This email serves as a friendly reminder of the upcoming changes to the Research Project Storage data transfer process that take effect on Monday, February 26 at 9 a.m. EST. For your convenience, a copy of the initial announcement that was released on February 16 is included below. You can find detailed documentation of the planned changes, previous email communications, and sections for frequently asked questions on our Data Migration status page (this page). We are committed to working diligently until data transfer is complete and the legacy system is decommissioned. Our technical support teams will continue to be available to you to answer questions and address any concerns. Thank you for your continued patience and partnership. With regards, Karsten"
rc-website-fork/content/service/status.md,"Siller Director, Research Computing User Services Information Technology Services University of Virginia {{% /accordionitem %}} {{% accordionitem title=""Feb 16, 2024 Upcoming Changes to Data Transfer Process for Project Storage File System"" id=""email1"" %}} Dear Colleagues, As previously shared, efforts are still underway to transfer data in the Project Storage file system from the legacy GPFS hardware to the new, upgraded hardware. To date, we have successfully transferred about 35% of Project Storage files. These files were transferred because they were either needed for a scheduled research project, or they had been recently accessed. However, now we are finding that the Active File Management (AFM) connection being used to transfer files is causing too great of a system load for the legacy hardware, as the old system continues to degrade. This is causing additional disruptions and file access issues. Although there are issues with access, rest assured that all files remain safe and secure, and will be transferred to the new system by fall. What we are doing now On Monday, 2/26, we will move from the AFM connection to a new, manual process to transfer the remaining data. As part of this manual process, we have launched a high priority transfer request for files actively needed for your research. As part of this manual process, we are prioritizing files identified as actively needed for current research projects. If you need to access files on the legacy system for active project work, please indicate which directories or files should be prioritized for transfer using our data transfer request form. You can also use this form to request a list of your files that remain on the old system. Please note that we cannot guarantee a timeline for transferring prioritized files due to the uncertainty of the old hardware. Prioritized file transfer"
rc-website-fork/content/service/status.md,"may still take weeks to months to complete. If you have already reached out to prioritize file transfer or do not anticipate immediate use of these files, no further action is required. What you may experience now until the transfer is complete Today, when you log in to Project Storage, you will see your complete file list in your directory. Although the file list is complete, it is possible that some files in your directory have already been migrated to the new hardware and are readily available for use, while others remain on the old cluster. Accessing files that remain on the old cluster will likely result in excessively slow access speeds or a “file not found” error. This “file not found” error only indicates that your file has not yet been transferred. On 2/26, we will move to the new, manual file transfer process. Because this process is manual and no longer based on the file connection method, file names of files not yet transferred to the new system will be removed from your /Project storage directory. These file names will automatically repopulate in your directory under a new /stagedproject folder as they are transferred to the new hardware. Additional information about the file transfer efforts and system status is available on our new Data Migration status page (this page). We will provide ongoing updates at this location. I understand the impact these disruptions may have had on your work, and I share in your frustrations. Our mission is to provide excellent service and support for a seamless research computing experience. We are committed to working diligently until data transfer is complete and the legacy system is decommissioned. Our help desk and technical support teams will continue to be available to you to address any concerns. Thank you for"
rc-website-fork/content/service/status.md,"your continued patience and partnership. With regards, Joshua Baller, PhD Associate Vice President for Research Computing Information Technology Services University of Virginia {{% /accordionitem %}} {{% /accordiongroup %}} {{% /accordionitem %}} {{% /accordiongroup %}} What to expect on February 26 Before February 26, your /project folder contains a mix of files, including those that have already been transferred and those that still reside physically on the old Project storage system. Files that are still on the old system appear as empty ""stub files"" in the new system. Because the old and new systems are still connected, if you try to access a file that is still on the old system, the empty stub file is replaced by the original file as it is transferred ondemand to the new system. On February 26, the old and new Project storage systems will be disconnected. Researchers will not have any direct access to the old Project storage. The current /project folder on the new storage system should perform optimally without the tether to the old storage system. We will begin deleting the empty stub files on /project. These are empty files and are not needed for the new migration process. The original files are still intact and secure on the old system. A new filesystem /stagedproject will be mounted readonly on Rivanna login nodes, compute nodes, and the UVA Standard Security Storage data transfer node (DTN). This folder will be used as a target to stage your data as it is being transferred from the old system to the new system. Setting up a new destination for the not yet transferred files prevents potential interference with your active work in /project. Your Project storage folders on February 26: /project/MYSHARE: This is located on the new storage system. It contains files that have already"
rc-website-fork/content/service/status.md,"been transferred since Fall 2023 as well as newlycreated files. /stagedproject/MYSHARE: This is a new share set up on the new storage system. It will be empty initially. Files will begin to appear here as they are transferred, starting Feb 26. “MYSHARE” refers to your personal project name Note: The /stagedproject/MYSHARE folder will only be created for you if you have folders/files on the old storage system that still need to be migrated. FAQ {{% accordiongroup title=""Group"" id=""faqgroup""%}} {{% accordionitem title=""1. How should I prepare for the changes coming on February 26?"" id=""faq1"" %}} If you have already reached out to us to prioritize transfer of a specific subset of your folders or files, no further action is required. These files will be copied to samenamed folder in your active /project share on the new Project storage system. If you have not yet contacted us with a list of priority folders or files to transfer, or if there are additional folders and files that you urgently need for your active work, please reach out to RC with a specific list of those folders/files and we will add them to the file transfer queue. See ""How can I get help with migration of my data?"" for details. Questions about the data migration process should be directed to our user services team. {{% /accordionitem %}} {{% accordionitem title=""2. Some of my Project storage files disappeared. Where did they go?"" id=""faq2"" %}} Until February 26, all files are shown in the new Project storage system, including those that have already been transferred and those that are still on the old Project storage system. Files that are still on the old system present as empty stub files in the new system. When accessed for the first time, the empty stub file is replaced by"
rc-website-fork/content/service/status.md,"the original file as it is transferred ondemand from the old system to the new system. On February 26, the empty stub files will be deleted from the new system as they are not needed for the new migration process. This is a gradual process that may take a few weeks to complete, so you may see different files, depending on when you access the new system. However, the original files behind the stub files still exist and are secure on the Old Project system. See ""How do I find out what files are on the old Project storage system?"" {{% /accordionitem %}} {{% accordionitem title=""3. Where are you copying my files?"" id=""faq3"" %}} Until February 26, files that you have already requested be transferred will be copied to the samenamed directories in your active /project share on the new Project storage system. Beginning February 26, all your files, including files that are still on the old system and files that have already been transferred to the new system, will start being copied to the samenamed directories in a new /stagedproject share. The /stagedproject share was created for your tobemigrated files to prevent potential interference with your active work in /project. Note: Your folder in /stagedproject will be empty on February 26, but will gradually fill with your files as they are copied over. {{% /accordionitem %}} {{% accordionitem title=""4. How do I access the new /stagedproject folder?"" id=""faq4"" %}} On February 26, a new /stagedproject folder will become available in readonly mode on the Rivanna login nodes and the UVA Standard Security Storage data transfer node (DTN). It will not be available on compute nodes. This folder will be used as destination to stage data transferred from your old Project storage to the new storage system. {{% /accordionitem %}}"
rc-website-fork/content/service/status.md,"{{% accordionitem title=""5. How can I work with the files that have been transferred into my /stagedproject folder?"" id=""faq5"" %}} On Feb 26, your folder in /stagedproject is set up as readonly on the Rivanna login nodes, compute nodes, and the UVA Standard Security Storage data transfer node (DTN). Option 1 (preferred): For compute jobs we recommend you first copy files from /stagedproject into your /project or /scratch folder. For transfer of large folders see “How can I consolidate my files in /stagedproject and /project?”. {{% highlight %}} Note: A subset of files may not copy over because of existing ""stub files"" on the /project storage system. Stub files are ""placeholders"" for files that exist on the old project storage system but had not been copied over to the new project storage system. They are not needed for the new data migration process. We began with deletion of these empty placeholder stub files on February 26. This process is still ongoing. The original files are still intact and secure on the old system. {{% /highlight %}} If you do not need any of the files affected by any failed copy operation immediately, you may continue to work out of /project and /scratch folders as usual. We will inform you when all stub files have been deleted and you may consolidate the remaining files from /stagedproject to /project then, following the copy instructions one more time. See “How can I consolidate my files in /stagedproject and /project?” Option 2: If the copy of any needed files to /project fails, you can update your job scripts to read the necessary input files from your /stagedproject folder and write the output to a new folder in your existing /project share. We will inform you when all stub files have been deleted and you"
rc-website-fork/content/service/status.md,"may consolidate the remaining files from /stagedproject to /project then by following the copy instructions one more time, See “How to consolidate files from /stagedproject to /project?”. {{% /accordionitem %}} {{% accordionitem title=""6. Why can't I access the old Project storage system directly to copy my own files?"" id=""faq6"" %}} Performance of the old Project storage system is severely degraded. Any exploratory search for folders or file listings by users would create additional strain on the system, which would further reduce the already limited data transfer rates from the old to new Project storage system. Because of these performance issues, RC set up a managed process that transfers all files from the old Project storage system to a new /stagedproject folder for you in the new system. See “How do I access the new /stagedproject folder?”. You can reach out to RC to request a list of your files on the old Project storage system. See ""How do I find out what files are on the old Project storage system?"" RC will work with you to prioritize the list of your files so that those files most urgently needed for your active work can be transferred first. See ""How can I get help with the migration process?"" {{% /accordionitem %}} {{% accordionitem title=""7. How do I find out what files are on the old Project storage system?"" id=""faq7"" %}} After February 26, you will not be able to connect to the old Project storage system. See ""Why can't I access the old Project storage system directly to copy my own files?"" However, we have placed a list of your old Project storage files in the toplevel folder of your new share on /stagedproject (i.e. /stagedproject/myshare/oldprojectfilelist.txt). You may use this list to prioritize folders and files for your data migration (see"
rc-website-fork/content/service/status.md,"""Can I pick which of my files are transferred first?""). Please keep in mind that the list in oldprojectfilelist.txt represents all your files stored on the old Project storage system, some of which have already been transferred to the new system. Eventually all files will be transferred from the old to the new Project storage system. If you already have all the data needed for your active work on the new Project storage system, no action is required. {{% /accordionitem %}} {{% accordionitem title=""8. Are all files being transferred to the new Project storage system at once?"" id=""faq8"" %}} No. We are prioritizing transfer of files that you actively need for your research. You may reach out to us to provide a specific list of your high priority, essential folders and files. The severe performance degradation of the old storage system will remain a bottleneck regardless of the change in data transfer method. However, the more selective this list, the better we can help you with this transition. See ""Can I pick which of my files are transferred first?"" for details. Once the transfer process has been stabilized, engineers will begin transferring any remaining files that users did not explicitly request to be moved. {{% /accordionitem %}} {{% accordionitem title=""9. Can I pick which of my files are transferred first?"" id=""faq9"" %}} Yes. Please complete this web form to provide us with a list of specific, high priority folders and files for migration. Please indicate as precisely as possible which folders or files should be transferred first so our storage engineers can prioritize these items. The more selective this list, the better we can help you with the transition. If you need help with your file prioritization, you may reach out to RC to request a list of files that"
rc-website-fork/content/service/status.md,"you still have on the old Project storage system. {{% /accordionitem %}} {{% accordionitem title=""10. How can I get an estimate of when my files will be transferred?"" id=""faq10"" %}} Though the new Project storage system provides vastly improved performance, the overall transfer of files is limited by the degraded performance of the old system. This issue with the old storage system will remain a bottleneck regardless of the change in data transfer method. However, you can facilitate the data transfer process by providing us with a narrowed down list of files that you need for your research over the next few months. This will allow us to deprioritize less urgently needed files. See ""Can I pick which of my files are transferred first?"" for details. All data will be migrated eventually, but this process is expected to take several months to complete. We will post weekly progress of data migration on this page. We will notify the PI of the storage allocation when all their folders have been copied over. We will not purge any files on the old Project storage system until the PI has had an opportunity to verify that their files have been migrated to the new Project storage system. {{% /accordionitem %}} {{% accordionitem title=""11. Why is the transfer of folders to the new Project storage system taking longer than expected?"" id=""faq11"" %}} The old Project storage hardware (GPFS) is in a degraded state. Due to this state, transferring data from the system is unusually slow even in otherwise ideal conditions. With a wide range of researchers and research workflows attempting to simultaneously access the system, the system becomes overloaded resulting in the “IO Error” and similar errors that researchers have been experiencing. As the system gets overloaded the transfer rates drop even further. By"
rc-website-fork/content/service/status.md,"having RC manage the data transfer from the old to the new system we will be able to limit the load on the system and keep it closer to ideal conditions in order to maximize the transfer rate going forward. {{% /accordionitem %}} {{% accordionitem title=""12. What caused this issue with the old Project storage file system?"" id=""faq12"" %}} The old GPFS hardware was beyond its expected endoflife and due to historical sporadic financial investment in RC the hardware was not replaced. That changed recently with substantial University investments in RC and replacement hardware was immediately purchased. Unfortunately, we were not able to get the new GPFS hardware up and running before we started to experience failures on the old GPFS hardware. Given that the hardware was already endoflife, we prioritized completing installation of the new hardware quickly and transferring from the old hardware to the new. Because the old GPFS hardware is still experiencing failure, these transfers have been slow with ~35% of the data transferred todate. {{% /accordionitem %}} {{% accordionitem title=""13. What is RC doing to ensure increased reliability and improved service?"" id=""faq13"" %}} With recent University investments in Research Computing we have planned annual purchases of new hardware with a model that allows for the seamless addition of new hardware and decommissioning of old hardware as it reaches its endoflife. This will reduce the need for downtimes and manual data migrations as we ensure the integrity of our infrastructure. Research Computing is also working to improve its approach to user communications. If a major disruption occurs to an RC system, we will be using pages like this one to make sure there is a single location researchers can go to see both the current status and the history of the incident. Looking forward, Research Computing’s service"
rc-website-fork/content/service/status.md,"roadmap includes filling in some services currently missing from our portfolio. Critically, there should be options available for researchers to elect for redundant access to their key datasets. This means not only additional levels of protection in case of a disaster but also an alternate location where data can be retrieved. {{% /accordionitem %}} {{% accordionitem title=""14. How were files prioritized for transfer for the new system?"" id=""faq14"" %}} At this time, about 35% of files have been successfully migrated. These files were moved as part of prioritized research datasets or were transferred over by AFM when accessed by a researcher. The prioritized datasets were known to be actively used and were prime candidates to transfer when extra transfer capacity was available. {{% /accordionitem %}} {{% accordionitem title=""15. What are stub files and how can I find them?"" id=""faq15"" %}} Stub files are ""placeholders"" for files that exist on the old project storage system but had not been copied over to the new project storage system. They are not needed for the new data migration process. We began with deletion of these empty placeholder stub files on February 26. This process is still ongoing. The original files are still intact and secure on the old system. You may create a list of stub files currently present in your /project folder by running this command: find /project/MYSHARE ls regularfiles.log 2 stubfiles.log This produces two files, regularfiles.log and stubfiles.log. The stubfiles.log contains all files that the system cannot list which is indicative of being stub files. {{% /accordionitem %}} {{% accordionitem title=""16. Why do I get File Not Found Errors when accessing some of my files in my /project subfolders?"" id=""faq16"" %}} Stub files may be present which are placeholders that linked the new storage system to the legacy storage system. As"
rc-website-fork/content/service/status.md,"a part of the data migration process, stub files linking to the legacy system were also attempted to be deleted. A subset of these stub files remains visible on the new system, but attempting to access them will result in File Not Found Errors, as they are no longer coupled with the old system. These files are scheduled for deletion through an automated process eventually. {{% /accordionitem %}} {{% accordionitem title=""17. How can I verify that all my old project storage files are now in /stagedproject?"" id=""faq17"" %}} To verify the list of files in /stagedproject you can run the following on the command line: checkstagedproject MYSHARE Replace MYSHARE with the name of your project's share. This will create ~/stagedprojectfilelist.txt which contains a list of files that have been copied from /oldproject to /stagedproject. This list is compared with /stagedproject/MYSHARE/oldprojectfilelist.txt to ensure that all files have been transferred from /oldproject. ~/stagedprojectfilelist.txt will change if your data transfer is still in progress. The share's owner will be notified once all the data is transferred. {{% /accordionitem %}} {{% accordionitem title=""18. How can I consolidate my files in /stagedproject and /project?"" id=""faq18"" %}} To organize your files efficiently: If you relocated files to new folders to avoid issues with file access performance issues, or if you have duplicates in project storage, please put them back in their original folders using the ""mv"" command. This keeps things neat and prevents duplicates. Be cautious not to overwrite newer files with older ones while moving them. Use the ""rsync"" command to copy files from the /stagedproject folder to the main project folder (/project). This ensures that all essential files are consolidated in one location (/project). If you require assistance with these steps, please contact us via our support webform or during office hours for help."
rc-website-fork/content/service/status.md,"Submit the following script to copy large directories in bulk: The script will also be available through the Open OnDemand Job Composer: Go to Open OnDemand Job Composer Click: New Job From Template Select democopystagedproject In the right panel, click “Create New Job” This will take you to the “Jobs” page. In the “Submit Script” panel at the bottom right, click “Open Editor” Enter your own allocation. Edit the MYSHARE placeholder in the script as needed. Click “Save” when done. Going back to the “Jobs” page, select democopystagedproject and click the green “Submit” button. As we expect a high volume of data migration, please refrain from doing so directly on the login nodes but instead submit it as a job via the provided Slurm script as described above. {{% callout %}} If you have problems with errors in the above rsync command you can try adding the following flags: noowner nogroup noperms notimes. E.g. rsync uav noowner nogroup noperms notimes ${STAGEDPROJECTFOLDER} ${PROJECTFOLDER} 1 ~/rsync.log 2 ~/rsyncerror.log {{% /callout %}} {{% /accordionitem %}} {{% accordionitem title=""19. How can I get help with the data migration process?"" id=""faq19"" %}} We have placed a list of your old Project storage files in the toplevel folder of your new share on /stagedproject (i.e. /stagedproject/myshare/oldprojectfilelist.txt). You may use this list to prioritize folders and files for your data migration (see “Can I pick which of my files are transferred first?”). Researchers with an urgent need to access files that have not been migrated to the new Project storage system yet may submit a support request through our webform. Please indicate as precisely as possible which folders or files should be transferred first so we can prioritize these items. All files will be transferred eventually. {{% highlight %}} Do you have additional questions? Please contact our"
rc-website-fork/content/service/status.md,"user services team, or join us for our virtual office hours every Tuesday, 35 p.m. and Thursday, 1012 p.m. starting March 6. {{% /highlight %}} {{% /accordionitem %}} {{% /accordiongroup %}} Technical Details On February 26, RC engineers will disconnect the Active File Management (AFM) tether, remount the old Project storage system (GPFS) separately, and purge all ""stub files"" (files that are staged on new storage system and appear in a directory listing, but have not yet been transferred from old Project). This will allow a clear differentiation between transferred and untransferred data. Data already transferred to the new Project storage system are expected to perform optimally without any issues, while untransferred data on the old system will need to be manually transferred by RC staff. Staff will continue to respond to data transfer requests on a first come, first served basis. Once the transfer process has been stabilized, engineers will begin transferring any remaining files that users did not explicitly request to be moved. Incident Status Log {{% scrollable height=""500px"" %}} 20240227, 2:51 PM The /stagedproject folder is now available readonly on Rivanna login nodes. 20240227, 7:03 AM About 33% of all stub files have been deleted on the new Project storage system. A subset of the stub files are still visible on the new system. Access of these stub files will produce File Not Found Errors since they don’t physically exist on the new storage system and are uncoupled from the old system now. All stub files will be deleted through an automated process eventually over the next days and weeks. 20240226, 05:32 PM: The new Project storage was remounted on all Rivanna nodes and the UVA Standard Security Storage data transfer node (DTN). Deletion of stub files was initiated. 20240226, 04:15 PM: The old and new Project"
rc-website-fork/content/service/status.md,"storage systems were unmounted on all Rivanna nodes and the UVA Standard Security Storage data transfer node (DTN) to complete the disconnection process. 20240226, 09:00 AM: Engineers began to disconnect the old and new Project storage systems. 20240219, 02:00 PM Project storage is currently unavailable on Open OnDemand. On 26 February at 9:00 am EST, RC engineers will switch to an alternate data transfer mechanism between the legacy Research Project Storage filesystem and the new Project Storage system. As a result, users will no longer have direct access to the legacy system. Files will be staged to an intermediate location for users to copy. To facilitate the migration process, please indicate which directories or files should be prioritized for transfer using our data transfer request form. Additional information about the file transfer efforts and Project Storage system status is available on our Data Migration status page. 20240216, 01:22 PM On 26 February at 9:00 am EST, RC engineers will switch to an alternate data transfer mechanism between the legacy Research Project Storage filesystem and the new Project Storage system. As a result, users will no longer have direct access to the legacy system. Files will be staged to an intermediate location for users to copy. To facilitate the migration process, please indicate which directories or files should be prioritized for transfer using our data transfer request form. Additional information about the file transfer efforts and Project Storage system status is available on our Data Migration status page. 20240209, 06:22 PM SMB/NFS exports have been enabled for new Project storage. Data migration from old Project storage to new Project storage is ongoing. Firsttime access of old files and old directory listings is significantly slower than normal. Users may encounter on occasion “OSError: [Errno 5] Input/output errors” which should be reported through"
rc-website-fork/content/service/status.md,"our support webform https://www.rc.virginia.edu/form/supportrequest/. For their ongoing work users should create new Project storage directories that are as close to the top level directory of their storage share as possible. Directory listings and traversals in these new top level directories is expected to show better performance. 20240208, 08:05 AM Rivanna is back in service following maintenance. Data migration from old Project storage to new Project storage is ongoing. Firsttime access of old files and old directory listings is significantly slower than normal. Users may encounter on occasion “OSError: [Errno 5] Input/output errors” which should be reported through our support webform https://www.rc.virginia.edu/form/supportrequest/. For their ongoing work users should create new Project storage directories that are as close to the top level directory of their storage share as possible. Directory listings and traversals in these new top level directories is expected to show better performance. 20240207, 06:00 AM Rivanna, Research Project storage, and Research Standard storage will be down for maintenance on Tuesday, February 6 beginning at 6 a.m. All systems are expected to return to service by 6 a.m. on Wednesday, February 7. 20240205, 07:55 AM Data migration from old Project storage to new Project storage is ongoing. Firsttime access of old files and old directory listings is currently significantly slower than normal. For their ongoing work users should create new Project storage directories that are as close to the top level directory of their storage share as feasible. Directory listings and traversals in these new top level directories is expected to show better performance. NFS and SMB mounts for new Project storage will be enabled on February 6. new Project storage will be made available through the Open OnDemand file browser at the same time. {{% /scrollable %}}"
rc-website-fork/content/service/tiers.md,"We offer a variety of services, most of which are provided at no charge to the researcher. Training and Technical Support {tier1trainingtechnicalsupport} Research Computing provides training for the use of our systems. The training includes onboarding, introductory sessions, and computational workshops. We also provide customized group trainings which you can request here. We provide technical support to researchers by assisting with system access issues and troubleshooting computational problems on our systems. To receive online technical support, researchers can submit a ticket. We also have weekly virtual office hours where researchers can speak to Research Computing staff. Training and Technical Support are provided at no charge. {{< button buttonurl=""/form/supportrequest/"" buttonclass=""primary"" buttontext=""Submit a Ticket"" }} Consultations: Advising We provide guidance and recommend best practices on our systems in service areas such as data storage and code improvement, and can also discuss project feasibility. While a consultation does not include any handson work by Research Computing personnel, the outcome of a consultation may include a recommendation for a handson collaboration. Consultations are provided at no charge. {{< button buttonurl=""/form/supportrequest/"" buttonclass=""primary"" buttontext=""Request a Consultation"" }} Collaborations: Expertise and Custom Solutions Research Computing has a team of computational experts specializing in a variety of fields, such as HighPerformance Computing (HPC), bioinformatics, artificial intelligence (AI), image processing, and more. Whether you need assistance with project analysis or grantrelated tasks, our experts are ready to collaborate and develop tailormade solutions for your research needs. Basic Collaborations Our Basic Collaborations are designed for tasks that do not require significant time commitment from our team. For example, we may leverage our expertise to efficiently debug codes or rewrite scripts that parallelize your job. Typically, these collaborations involve less than 20 hours of work, with no more than 8 hours of effort required in any given week. We are pleased"
rc-website-fork/content/service/tiers.md,"to offer this service at no charge to the researchers. InDepth Collaborations We also offer comprehensive collaboration services, where our experts thoroughly examine your data, suggest appropriate analyses, streamline workflows, or create custom programs for data analysis. In special cases, we may design architectures to facilitate your workflows. These tailored solutions typically involve over 20 hours of effort from our team members. Because of the extensive time and specialized expertise required, we charge a fee depending on the skills and efforts required. {{< button buttonurl=""/form/supportrequest/"" buttonclass=""primary"" buttontext=""Request a Collaboration"" }}"
rc-website-fork/content/service/bioinformatics.md,"UVA Research Computing (RC) can help with your bioinformatics project. Nextgeneration sequence data analysis RC staff can help you start to use popular bioinformatics software for functions such as Genome assembly, referencebased and/or denovo WholeGenome/Exome sequence analysis for variant calling/annotation RNASeq data analysis to quantify, discover and profile RNAs Mircobiome data analysis, including 16S rRNA surveys, OTU clustering, microbial profiling, taxonomic and functional analysis from whole shotgun metagenomic/metatranscriptomic datasets Epigenetic analysis from BSAS/ChIPSeq/ATACSeq Computing Platforms UVA has three computing facilities available to researchers: Rivanna and Afton, for nonsensitive data, and Ivy, for sensitive data. In addition, cloudbased services offer a computing environment for running flexible, scalable ondemand applications. RC can work with your team to determine the computing platform best suited for your research project. Rivanna and Afton Highperformance Computing Clusters Rivanna and Afton are the university's highperformance computing systems for highthroughput multithreaded jobs, parallel jobs, and memory intensive largescale data analyses. The architecture is specifically suited for large scale distributed genomic data analysis, with many 100+ bioinformatics software packages installed and ready to use. Learn more Ivy HighSecurity / HIPAA Computing Ivy is a HIPAA and CUI compliant cluster at UVA for working with sensitive data. Researchers have access to a group of bioinformatics software on Ivy Linux VMs. Learn more Cloud Computing We can explore the possibility of using cloud infrastructure (AWS/GCP) for your bioinformatics analysis and data storage. For certain applications, the 'elasticity' of cloud computing may prove beneficial for saving time and reducing costs of data analysis and sharing. The RC team is available for consultation on your project needs. Learn more Consulting We can assist with choosing the right package, developing a workflow, porting it to one of our computing platforms, and running the jobs. Request a Consultation {{% topofpage %}}"
rc-website-fork/content/service/cloud.md,"Run your Cloud computing is ideal for running flexible, scalable applications on demand, in periodic bursts, or for fixed periods of time. UVA Research Computing works alongside researchers to design and run research applications and datasets into Amazon Web Services, the leader among public cloud vendors. This means that server, storage, and database needs do not have to be estimated or purchased beforehand – they can be scaled larger and smaller with your needs, or programmed to scale dynamically with your application. Service Oriented Architecture A key advantage of the cloud is that for many services you do not need to build or maintain the servers that support the service you simply use it. Here are some of the building blocks available using cloud infrastructure: Compute Storage Databases Containers / Docker Analytics / Data Management Continuous Integration Sensor / IoT Data Streaming Message Queues / Brokers SMS / Push Integration Alexa Skills / Speech Integration Serverless Computing Code Build / Validation Researchers Using the Cloud Serverless Web UVA faculty and researchers can share data, findings, tools and other resources from static HTML content published to object storage. This simple method for publishing can cost only a few dollars a month and requires no server management. Data Lakes A new paradigm in data storage and processing, data lakes help researchers by providing a central repository for both structured and unstructured data, of any type or size. These data can then be siphoned off for processing, either in realtime streams or in queues for later analysis. Services in Support of HPC Users of HPC usually have more than enough computing power to run their jobs. But what if you need a relational or NoSQL database, a messaging service, or offsite storage? Researchers have begun integrating the cloud into their HPC jobs to"
rc-website-fork/content/service/cloud.md,"create, use, and manage external services like these. Workflows & Pipeline Management Researchers need flexibility for where they run their data pipelines it might be on a personal computer, a lab server, an HPC cluster, or a cloud instance. We are working with faculty to extend some commonlyused pipeline tools so that they can create and push jobs to cloudbased resources, regardless of the cloud vendor. Longterm Cold Storage AWS Glacier and Google Nearline/Coldline offer researchers ""cold"" offsite storage for longterm backups of infrequentlyaccessed data. Many researchers use Glacier to store terabytes of source data to fulfill grants and federal research project compliance. Other Common Use Cases Proofs of concept To verify a system or design works, to benchmark processing speeds, we may use shortlived instances to learn from before building a production system. Test / Development environments For installing test packages, trying new ideas, and testing design patterns. Dynamic / flexible / scaling application stacks When future traffic or load cannot be determined beforehand, deploying into a dynamic environment means the infrastructure is not locked into any set type of CPU/RAM or scale. Shortterm or fast deployment projects For almost immediate computing needs, existing users can create new instances as needed. Container deployments Run microservices (such as Docker containers) in an environment that can loadbalance their traffic and maintain container health. Reference Architecture To get an idea of how public or private cloud resources are used in realworld and research scenarios, visit one of these Solution Architecture References: AWS Architecture Center. Google Cloud Solutions Architecture Reference | GCP Builder Tutorials. Azure Solution Architecture | Azure Reference Architectures. Some examples from AWS: Batch Processing Build autoscalable batch processing systems like video/image/datastream processing pipelines. Large Scale Processing and Huge Data sets Build highperformance computing systems that involve Big Data. Time Series"
rc-website-fork/content/service/cloud.md,"and Streaming Data Processing Build elastic systems that process chronological data. Cloud Services at UVA Request an Account Researchers or labs who would like to use AWS for their computing infrastructure should request an account through UVA Information Technology Services (ITS). They currently support deployments in Amazon Web Services and Microsoft Azure, and offer both managed and selfservice options. ITS Cloud Solutions Training & Implementation With an AWS account in hand, you will need some training. ITS can provide you with links to selfpaced training for both AWS and Azure. Research Computing If you need help with how to design your project in a cloud environment, or thinking through how to migrate your existing projects, contact us for a consultation. Sensitive Data in the Cloud If your cloudbased project involves any sensitive data (HIPAA, PHI, etc.) you must request approval from the Information Security office at UVA. You will be required to verify that your application, infrastructure, and staff can meet all minimum requirements for the secure transfer and handling of sensitive data. Please note that while sensitive data projects are possible in the cloud, their approval is not automatic nor guaranteed. Solution Architecture / Consulting We have experience designing and delivering solutions to the public cloud using industry best practices. If you have a project and would like to discuss options, pricing, design, or implementation, we are available for consultation. Our staff includes an AWS certified solution architect, and the RC team uses AWS for our own internal systems and development. Request a Consultation Training We also offer inperson, handson workshops and sessions on working with the cloud. Workshops cover a number of topics, from creating object storage buckets and simple compute instances to more complex datadriven workflows and Docker containers, If you have an idea for a workshop"
rc-website-fork/content/service/cloud.md,"or would like to schedule training for your lab or group, please contact us. {{% topofpage %}}"
rc-website-fork/content/service/dac.md,"Assisting Researchers across Grounds with Big Data Management and Analysis The DAC offers comprehensive support for projects on Research Computing platforms at every stage, from initial grant proposals and data ingestion to the development of finalized models and manuscript preparation, ensuring effective and efficient use of highperformance computing resources. Our consultants provide tailored guidance and technical support to researchers to ensure that their projects benefit from the latest methodologies and best practices in big data analytics. Our team consists of experts with diverse backgrounds, offering a wealth of knowledge and experience in various domains, including bioinformatics, AI and machine learning, drug discovery, statistical analysis, image processing, highperformance computing, and data management and compliance, among others. Our services and resources range from embedding one of our team members in your project and creating custom workflows to leasing storage and purchasing time on our highperformance cluster. These services are designed to be flexible and adaptable, allowing us to support researchers across a wide range of disciplines and research questions, and can vary in scope, rate, and timecommitment from DAC team members. PIs may budget for DAC support in their grants and DAC members can be listed as key personnel. We are committed to enhancing the quality and impact of research across Grounds by providing exceptional data analytics support using UVA HPC platforms. If you would like to learn more about how the DAC can assist with your project, or if you have specific needs or questions, please contact us at RCDAC@virginia.edu. Service Areas Project Preparation The computational needs of each project are unique. To help you prepare, we provide: Project feasibility and best practices advising Software and compute platform selection and configuration RC Systems Training HPC Support We assist researchers with effective and efficient use of HPC resources, including: Technical Support Best"
rc-website-fork/content/service/dac.md,"practices advising Code parallelization, optimization, and debugging Data Storage and Compliance RC provides a variety of Research Data Storage options. We can help you decide which option best fits your needs and compliance requirements, and assist in transferring your data onto our storage. Data Wrangling and Analysis We assist researchers with big data wrangling, analysis, and visualization, including creation of customized data pipelines and data dashboards. Code Improvement To assist you with code improvements such as enhancing efficiency, optimizing performance, and ensuring scalability we offer services in code debugging, optimization, parallelization, and containerization. Computational Domain Expertise We have a variety of expertise including conducting academic research. We can provide specialized assistance in topics such as: AI and Machine Learning Image Processing Bioinformatics Text Analysis Grant Proposal Support We can help you prepare documents such as: Data analysis plans Budget justifications for compute resources Letters of Support Services Provided at No Charge Training in Using RC Systems and Technical Support Consultation (does not include handson work by DAC personnel) Basic Collaboration (typically less than 20 hours of handson work by DAC personnel) Services Provided for a Fee If you are interested in a paid service, please contact us at RCDAC@virginia.edu for rates and other information. InDepth Collaboration (typically 20 or more hours of handson work by DAC personnel) How to Get Started with DAC Support If you would like DAC assistance with a project, please contact us at RCDAC@virginia.edu. Generally, we will start with a consultation to discuss project feasibility. If your project is approved and meets the basic or indepth collaboration requirements, a DAC team member will schedule a meeting to discuss staff effort and deliverables. For an indepth collaboration, we will create a detailed Statement of Work (SOW) that outlines the support researchers can expect from the DAC, detailing"
rc-website-fork/content/service/dac.md,"the project scope, budget, technical tasks to be completed, timeline, team member percent effort, and the deliverables to be provided at the end of the project. The SOW ensures all parties have a mutual understanding of the project's objectives and the level of support that will be offered. DAC Awards The DAC has the following awards currently available. Faculty can apply for these awards by submitting a proposal. More information and proposal instructions are provided at the corresponding links. For a list of previous DAC Award winners and corresponding projects, click here. Small Awards (up to $10,000 in services) Large Awards (up to $100,000 in services) Featured Collaborations We have years of experience with collaborations. To see more examples, visit our Projects page. {{< dacfeatured }} Team Jacalyn Huband, PhD: Assistant Director of the Data Analytics Center Madeline Erickson: Administrative Support Consultants Angela Boakye Danquah Ed Hall, PhD Kathryn Linehan, PhD Priyanka Prakash, PhD Andrew Strumpf Deb Triant, PhD"
rc-website-fork/content/service/consult.md,"Research Computing consulting services can help you take your research to the next level through the use of advanced computational techniques. Our experts are accustomed to working with researchers from across Grounds who are ready to take advantage of the University’s highperformance resources, but need help conceptualizing and executing their computationintensive projects. RC consulting team members provide professional advice, handson training, and constructive feedback at every stage of the process. Since our founding in 2008, we have assisted scores of academic researchers looking for better ways to solve complex optimization, parallelization, workflow, and data analysis issues. From concept to code, we’re here to help. Our team members forge creative partnerships and tackle complex computing problems which may take days or even months to solve. A majority of RC consulting services are free, but some longterm collaborations require contributions from outside funding. Name Email User ID Phone Describe your project Send Cancel"
rc-website-fork/content/service/dtc.md,"Support for research using modern wearables, smartwatches, smartphones, and IoT devices. The DTC is a one stop shop for researchers conducting experiments with modern wearables, smartwatches, smartphones, and IoT devices. The DTC exists to make it faster, easier, and more affordable to use modern consumer devices in research. Whether you're just getting started with devices/apps in your research or have a long track record, the DTC can meet you where you're at and provide the services you need. What can the Digital Technology Core do for me? The DTC understands different teams have different needs. We are open to working with you in the way that is best for you. Below is a list of some of the ways we've worked with teams in the past. If one of these sound like what you're looking for please contact us. Create smartphone apps to collect data, share content, and intervene to change behavior Collect and act on data from multiple devices (e.g., Oura ring, Whoop band, and Smartphone) Utilize a secure cloud where you can safely send data from IoT devices and wearables in real time Brainstorm ideas within the digital technology space (we love to help you formulate your ideas) Isn't building an app time consuming and expensive? Traditionally, yes, but the DTC offers a model that we call ""hosted"" apps, which can be built faster and more cheaply than traditional apps. A ""hosted"" app is in between building an app from scratch and using someone else's app. If your goal is to run a research study, a ""hosted"" app will do everything you want. If your goal is to eventually have your own standalone app, think of a ""hosted"" app as an affordable prototype. The hosted approach has many benefits for you: Maintenance costs are minimized due to a"
rc-website-fork/content/service/dtc.md,"shared infrastructure New features are available for free due to a shared infrastructure App store review and approval processes are avoided Updates to your app can be released in a matter of minutes Your app will be available on both iOS and Android without any extra work Can I collect data from device ? Probably, most modern wearable device support integration via web APIs. If a device you want to work with provides API integration (e.g., Apple Watch, Android Wear, Whoop, Oura, BioStrap, FitBit) then we can easily add it to our ecosystem (if we haven't already). If you aren't sure if we support your device contact us and ask. What happens to data you collect for me? All data collected by the Digital Technology Core ends up in the DTC cloud. To access your data you simply need to create an account with the DTC and then you can download your data at any time. The data we collect for you is owned by you and governed by your IRB. What does it cost to work with the Digital Technology Core? The exact cost for a project is determined on a perproject basis, though in general costs come from two sources: 1. Paying to add new functionality (this is charged as hours of developer time) 2. Paying to use existing functionality (this is charged monthly to defer maintenance costs) Do you offer funding grants? Yes, if you have a research idea but don't yet have grant money, you can apply for DTC funding. We call thse awards ""seed ""grants"". The goal of DTC seed grants is to support you enroute to a larger and more competitive research grant. As such, awards are typically around $10,000, which covers adding one new feature to the DTC ecosystem and completing a study"
rc-website-fork/content/service/dtc.md,"with participants. If you want to apply for a seed grant we encourage you to talk with us first. In our initial conversation we can let you know if your proposal is something the DTC could fund. For eligibility criteria and application template see DTC seed grants. How do I include the DTC in my grant budget? PIs may budget for DTC support in their grants. To ensure that DTC support is included in your upcoming grant proposal, we recommend that you schedule a consultation to explore the ways in which we can assist you. Our services and resources range from embedding one of our team members in your project to providing access to our existing software infrastructure for mobile data collection. Ready to get started? Contact us today using any of the methods on our contact page. If you still aren't sure if the DTC can help you, please reach out anyways. Tech jargon can be hard to follow, and sometimes all you need is to talk it through."
rc-website-fork/content/form/retired/storage-ptao.md,"{{< formcookies }} {{% formuserinfo %}} Classification Select FacultyStaffPostdoctoral AssociateOther Affiliation Select College of Arts & Sciences School of Data Science School of Engineering and Applied Sciences School of Medicine Darden School of Business UVA Health System Other Type of Request Create new storage share Increase size of existing share Decrease size of existing share Retire existing share Space (TB) The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments. Storage Platform Research Project Storage ({{% storagepricing project %}}/TB/year) Research Standard Storage ({{% storagepricing standard %}}/TB/year) Ivy Central Storage ({{% storagepricing ivy %}}/TB/year) Internal Use / Public DataThis storage platform is appropriate for public or internal use data. Sensitive / Highly Sensitive DataThis storage platform is appropriate for highly sensitive data such as HIPAA, FERPA, CUI, etc. MyGroup Ownership MyGroups name under your Eservices user ID. If you don’t have one, we can create one for you. You will have access to the MyGroups management and will be able to add/remove users for your project. Shared Space Name This is the name to be applied to your shared storage space. By default, the space will be named according to the MyGroups associated with the storage request. If you would prefer a different identifier, indicate the name for the space. Project Title Project Summary Grant Summary Grant Agency Grant Number {{% billing %}} Data Agreement The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a webbased tool that automatically tracks when data can be safely transferred or"
rc-website-fork/content/form/retired/storage-ptao.md,"destroyed. I understand Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/form/retired/fdm.md,Alternate forms with FDM implemented: SU Allocation Purchase Storage Purchase Ivy FDM needs to be implemented in the SERVICES application: https://services.rc.virginia.edu/
rc-website-fork/content/form/retired/allocation-purchase-ptao.md,"{{% formuserinfo %}} Allocation Pricing {{< allocationpricing }} Name of PI {{% billing %}} Is the PI of your account a UVA faculty member? Yes No (NonUVA personnel are charged $0.07/SU) I agree that this allocation will be used for research purposes only Agree Disagree Title of Award (if applicable) Total number of SUs requested Total amount to be charged to FDM $ SU expiration date (if applicable) Apply this purchase to which allocation Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/form/retired/omero-ptao.md,"From the microscope to publication, OMERO handles all your images in a secure central repository. You can view, organize, analyze and share your data from anywhere you have internet access. Work with your images from a desktop app (Windows, Mac or Linux), from the web or from 3rd party software. Over 140 image file formats supported, including all major microscope formats. Use the form below to request access for your group or lab to manage and analyze data in our OMERO database service. {{% formuserinfo %}} Classification Select FacultyStaffPostdoctoral AssociateOther Affiliation Select College of Arts & Sciences School of Data Science School of Engineering and Applied Sciences School of Medicine Darden School of Business UVA Health System Other Type of Request Create new storage share Increase size of existing share Decrease size of existing share Retire existing share Data Sensitivity Moderately sensitive / public Space (TB) The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments. MyGroup Ownership MyGroups name under your Eservices user ID. If you don’t have one, we can create one for you. You will have access to the MyGroups management and will be able to add/remove users for your project. Project Title Project Summary Grant Summary Grant Agency Grant Number PTAO Financial Contact Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/form/retired/containers-ptao.md,"{{% formuserinfo %}} Classification Select FacultyStaffPostdoctoral AssociateOther Affiliation Select College of Arts & Sciences School of Data Science School of Engineering and Applied Sciences School of Medicine Darden School of Business UVA Health System Other Project Summary Please describe your project and the container images you want to run. Tier of Service <= 5 containers ($5/month total) 6 15 containers ($10/month total) 15 containers ($48/month total) Billing Tiers are selected and paid for by the PI. Submit this form again if you wish to change your tier. Stopped containers do not incur charges, nor does local cluster storage or remote NFS mounts to /project storage. Project storage pricing can be found here. Storage No storage required Persistent cluster storage required NFS mount of project storage is required Storage Capacity (GB) The size of storage if required. Specify in 1GB increments. SSL/HTTPS Required No Yes Netbadge Authentication No Yes PTAO Financial Contact Please enter the name and email address of your financial contact. Data Agreement The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a webbased tool that automatically tracks when data can be safely transferred or destroyed. I understand Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/form/retired/omero.md,"From the microscope to publication, OMERO handles all your images in a secure central repository. You can view, organize, analyze and share your data from anywhere you have internet access. Work with your images from a desktop app (Windows, Mac or Linux), from the web or from 3rd party software. Over 140 image file formats supported, including all major microscope formats. Use the form below to request access for your group or lab to manage and analyze data in our OMERO database service. {{% formuserinfo %}} Classification Select FacultyStaffPostdoctoral AssociateOther Affiliation Select College of Arts & Sciences School of Data Science School of Engineering and Applied Sciences School of Medicine Darden School of Business UVA Health System Other Type of Request Create new storage share Increase size of existing share Decrease size of existing share Retire existing share Data Sensitivity Moderately sensitive / public Space (TB) The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments. MyGroup Ownership MyGroups name under your Eservices user ID. If you don’t have one, we can create one for you. You will have access to the MyGroups management and will be able to add/remove users for your project. Project Title Project Summary Grant Summary Grant Agency Grant Number {{% billingfdm %}} Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/form/retired/database.md,"A Relational Database Service is available for researchers who need such resources in Rivanna or microservice applications. Currently database services are limited to the MySQL RDBMS, and can only be accessed from within the HPC network. Note that you cannot connect directly to this database service from elsewhere on campus, VPN, etc. but only from Rivanna or other Research Computing systems. Upon approval of this request you will be given the following: The database host endpoint address A database username A database password Connections can then be made over port 3306 to the endpoint using normal MySQL tools and libraries. A GUI user interface is available at https://phpmyadmin.uvadcos.io/ for convenience of management. Databases are automatically backed up nightly, and 7 days are retained. Learn more: Database Basics Rivanna Software {{% formuserinfo %}} Classification Select FacultyStaffPostdoctoral AssociateOther Affiliation Select College of Arts & Sciences School of Data Science School of Engineering and Applied Sciences School of Medicine Darden School of Business UVA Health System Other MySQL Database Use Case Please describe your database requirements and the project they are associated with. Anticipated Storage Capacity (GB) The size of storage expected over time. Specify in 1GB increments. This should not exceed 20GB. SDS Capstone Group The name of your SDS capstone group, if applicable. Billing Database Billing is paid for by the PI or SDS Capstone Faculty Advisor. Database services currently cost $5/database/month in addition to any charges for other deployed microservices containers. (See DCOS billing tiers for more information.) Seven days of backups are automatically stored for each database. PTAO Financial Contact Please enter the name and email address of your financial contact. Data Agreement The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely"
rc-website-fork/content/form/retired/database.md,"stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a webbased tool that automatically tracks when data can be safely transferred or destroyed. I agree Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/post/.ipynb_checkpoints/2023-july-scratch-transfer-checkpoint.md,"During the July 18th maintenance, we installed a new /scratch file storage system on Rivanna. We have created sample scripts and instructions to help you transfer your files from the previous file system to the new one. These instructions are available here"
rc-website-fork/content/about/people/xu.md,"Xinyue is currently a secondyear UVA master student in computer science. With a diverse background in computer science, she is proficient in Python and C++, and has experience with data science and cloud deployments. Xinyue is familiar with machine learning algorithms, applying them across various fields such as autonomous navigation for ROSbot cars and bioinformatics for cancer survival analysis. Her diverse expertise and passion for technology keep her moving forward. Education B.S. Computer Science M.S. Computer Science University of Virginia (Expected Graduation: 2025)"
rc-website-fork/content/about/people/kureishi.md,"Namai is a rising thirdyear UVA student majoring in computer science. She is proficient in Python, Java, and JavaScript, and is passionate about software development, cybersecurity,and statistics. At Research Computing, she hopes to apply her skills and learn new things. Education B.A. Computer Science University of Virginia (Expected Graduation: 2027)"
rc-website-fork/content/about/people/periyasamy.md,"Hari is currently a fourthyear UVA student pursuing computer science and applied statistics with a concentration in data science. At Research computing, he has helped out with the maintenance of the website, which is based in Hugo. He has also helped out on the software development side of research computing to create some Flaskbased APIs in python. He is currently a software development engineer at Amazon (AWS) focusing on security tools, and is planning to return for the Fall and Spring semester. Education B.A. Computer Science B.A. Applied Statistics University of Virginia (Expected Graduation: 2026)"
rc-website-fork/content/about/people/lin.md,"Diana is currently a firstyear UVA student pursuing electrical and computer engineering. She has experience with machine learning and software development, with her favorite languages being Python, JavaScript, and C++. At Research Computing, Diana is excited to support research and gain more experience with highperformance computing. In the future, she is hoping to further explore software and embedded development. Education B.S. Computer Engineering University of Virginia (Expected Graduation: 2027)"
rc-website-fork/content/about/people/mistele.md,"Ivey is a secondyear student in UVA’s inaugural undergraduate data science class, intending to concentrate in systems. She primarily works with Java, R, and Python. She has experience in data cleaning and visualization using multiple programming languages, including Python, R, and SQL. Outside of UVA HPC, she plays baritone saxophone in UVA’s jazz ensemble. Education B.S. Data Science; University of Virginia (Expected May, 2027)"
rc-website-fork/content/about/people/khan.md,"Muneer is currently a secondyear undergraduate student studying computer science. He has experience with fullstack software development, primarily using TypeScript and the Next.js framework with PostgreSQL for database. At Research Computing, Muneer is hoping to learn more about parallel computing and software containers. Education B.S. Computer Science; University of Virginia (Expected May, 2027)"
rc-website-fork/content/about/people/duy.md,Camden is experienced in working in an HPC environment and coding with Python. He works to provide user support for the users of Rivanna. His background includes materials physics and mathematics.
rc-website-fork/content/about/people/quist.md,"Wright is a 4thyear computer science student at UVA studying computer science and studio art, with a focus on web applications and software engineering. Outside the classroom, Wright's interests include textile art, hiking, and woodworking. Education B.S. Computer Science B.A. Studio Art University of Virginia SEAS (Expected Graduation: 2025)"
rc-website-fork/content/about/people/siller.md,"Karsten's background is in developmental cell biology, genetics and light microscopy. He leads Research Computing's User Services team and provides technical support, training and project consultation for UVA's centralized compute platforms. His focus areas are HPC workflows, software containers, Python programming and image processing. Selected Publications Zhang J, Wallrabe H, Siller K, Mbogo B, Cassidy T, Alam SR, Periasamy A. (2025) Measuring Metabolic Changes in Cancer Cells Using Two‐Photon Fluorescence Lifetime Imaging Microscopy and Machine‐Learning Analysis. J Biophotonics 18 (1), e202400426. doi:10.1002/jbio.202400426 Sun R, Siller K. (2024) HPC Container Management at the University of Virginia. Practice and Experience in Advanced Research Computing 2024, 73, 14. doi:10.1145/3626203.3670568 Zimyanin V, Magaj M, Yu CH, Gibney T, Mustafa B, Horton X , Siller K, Cueff L, Bouvrais H, Pécréaux J, Needleman D, Redemann S. (2024) Using 3D Large Scale Tomography to Study Force Generation in the Mitotic Spindle. Microscopy and Microanalysis 30 (Supplement1), ozae044. 344. doi:10.1093/mam/ozae044.344 Best MN, Lim Y, Ferenc NN, Kim N, Min L, Wang DB, Sharifi K, Wasserman AE, McTavish SA, Siller KH, Jones MK, Jenkins PM, Mandell JW, Bloom GS. (2023) Extracellular Tau Oligomers Damage the Axon Initial Segment. J Alzheimers Dis. 93 (4). doi:10.3233/JAD221284. Alam SR, Wallrabe H, Christopher KG, Siller KH, Periasamy A. (2022) Characterization of mitochondrial dysfunction due to laser damage by 2photon FLIM microscopy. Sci Rep. 12:11938. doi:10.1038/s4159802215639z Cao R, Wallrabe H, Siller K, Periasamy A. (2020) Optimization of FLIM imaging, fitting and analysis for autofluorescent NAD(P)H and FAD in cells and tissues. Methods Appl. Fluoresc. 8(2):024001. doi:10.1088/20506120/ab6f25. Lauren K Rudenko LK, Wallrabe H, Periasamy A, Siller K, Svindrych Z, Seward ME, Best MN, Bloom GS. (2019) Intraneuronal Tau Misfolding Induced by Extracellular Amyloidβ Oligomers. J Alzheimers Dis. 71(4):11251138. doi:10.3233/JAD190226. Cao R, Wallrabe H, Siller K, Rehman Alam S, Periasamy A. (2019) Singlecell redox states"
rc-website-fork/content/about/people/siller.md,"analyzed by fluorescence lifetime metrics and tryptophan FRET interaction with NAD(P)H. Cytometry A. 95(1):110121. doi:10.1002/cyto.a.23711. Wallrabe H, Svindrych Z, Alam SR, Siller KH, Wang T, Kashatus D, Hu S, Periasamy A. (2018) Segmented cell analyses to measure redox states of autofluorescent NAD(P)H, FAD & Trp in cancer cells by FLIM. Sci Rep. 8(1):79. doi:10.1038/s4159801718634x. Janssens DH, Hamm DC, Anhezini L, Xiao Q, Siller KH, Siegrist SE, Harrison MM, Lee CY. (2017) An Hdac1/Rpd3Poised Circuit Balances Continual SelfRenewal and Rapid Restriction of Developmental Potential during Asymmetric Stem Cell Division. Dev Cell 40(4):367380.e7. doi:10.1016/j.devcel.2017.01.014. Doyle SE, Pahl MC, Siller KH, Ardiff L, Siegrist SE. (2017) Neuroblast niche position is controlled by Phosphoinositide 3kinasedependent DECadherin adhesion. Development 144(5):820829. doi:10.1242/dev.136713. Siller KH, Doe CQ. (2009) Spindle orientation during asymmetric cell division. Nat Cell Biol. 11(4):36574. doi:10.1038/ncb0409365. Siller KH, Cabernard C, Doe CQ. (2006) The NuMArelated Mud protein binds Pins and regulates spindle orientation in Drosophila neuroblasts. Nat Cell Biol. 8(6):594600. doi:10.1038/ncb1412. Siller KH, Serr M, Steward R, Hays TS, Doe CQ. (2005) Live imaging of Drosophila brain neuroblasts reveals a role for Lis1/dynactin in spindle assembly and mitotic checkpoint control. Mol Biol Cell. 16(11):512740. doi:10.1091/mbc.e05040338. See Google Scholar for a complete list of publications."
rc-website-fork/content/about/people/vo.md,"Jennifer is a fourthyear student at UVA pursuing a B.S in Computer Science and a minor in Data Science. She has a strong foundation in programming languages like Python and C, and particularly favors Java. She has experience in making data visualizations and contributing to web and game development projects. Enthused about learning and exploration, she enjoys trying new foods and playing board games, with Catan being her first choice. Jennifer is driven by a desire to help others and making a positive impact by using her skills to innovate and support others in achieving common goals."
rc-website-fork/content/about/people/addakula.md,More about this user goes here.
rc-website-fork/content/about/people/chamakuri.md,More about this user goes here.
rc-website-fork/content/about/people/triant.md,Deb's background is in bioinformatics with an emphasis in animal genetics. Her research has primarily involved the analysis of genomic and transcriptomic sequence datasets. She is also passionate about teaching and training scientists and has taught workshops on computational genomics and programming for biologists.
rc-website-fork/content/about/people/shankar.md,"Mohan is currently a firstyear graduate student pursuing chemical engineering with a focus on computational catalysis. Primarily, he uses Python, density functional theory (DFT) software like VASP and Gaussian, and machine learning models like FLARE and ALLEGRO for his work. Education B.S. Chemistry, Highest Distinction; University of Virginia M.S. Chemical Engineering; University of Virginia (Expected May, 2026)"
rc-website-fork/content/about/people/hongyan.md,"Hongyan Wu is a graduate student at the University of Virginia studying Computer Science. He specializes in Machine Learning, Reinforcement Learning and Robotics."
rc-website-fork/content/about/people/strumpf.md,Andrew is a Data Manager with a background in public health and clinical research. He helps users comply with University policies for research data and develop customized data pipelines.
rc-website-fork/content/about/people/galitz.md,"Matthew Galitz is an undergraduate student at University of Virginia studying Mathematics and Computer Science. He specializes in Python scripting and software development. His interests include artificial intelligence, brazilian jiu jitsu, and powerlifting."
rc-website-fork/content/about/people/jpark.md,"Juwon is a third year student at the University of Virginia pursuing a major in Applied Statistics and a minor in Data Science. She contributes to the development of a program that rebuilds R libraries for users. She has experience performing data analysis and creating data visualizations in R, Python, and SAS, and is passionate about utilizing data to make meaningful interpretations in the world. Juwon currently works as a data volunteer for a local nonprofit organization that provides essentials for underresourced families with children. One of her favorite hobbies include collecting and organizing her sock collection."
rc-website-fork/content/about/people/sun.md,"Ruoshi is experienced in electronic structure codes for computational materials science. He takes part in managing the HPC software stacks, building Apptainer/Docker containers, and providing user support/research consultation. He is a volunteer pianist at the University Hospital through the ""Music for Healing"" program. Selected Publications K Esfarjani, H Stokes, SN Sadeghi, Y Liang, B Timalsina, H Meng, J Shiomi, B Liao, and R Sun, ""ALATDYN: A set of Anharmonic LATtice DYNamics codes to compute thermodynamic and thermal transport properties of crystalline solids,"" Computer Physics Communications 312, 109575 (2025). doi:10.1016/j.cpc.2025.109575 R Sun, M Asta, and A van de Walle, ""Firstprinciples thermal compatibility between Rubased Resubstitute alloys and Ir coatings,"" Computational Materials Science 170, 109199 (2019). doi:10.1016/j.commatsci.2019.109199 R Sun, C Woodward, and A van de Walle, ""Firstprinciples study on Ni3Al (111) antiphase boundary with Ti and Hf impurities,"" Physical Review B 95, 21412 (2017). doi:10.1103/PhysRevB.95.214121 R Sun and DD Johnson, ""Stability maps to predict anomalous ductility in B2 materials,"" Physical Review B 87, 104107 (2013). doi:10.1103/PhysRevB.87.104107 R Sun, MKY Chan, and G Ceder, ""Firstprinciples electronic structure and relative stability of pyrite and marcasite: Implications for photovoltaic performance,"" Physical Review B 83, 235311 (2011). doi:10.1103/PhysRevB.83.235311 See Google Scholar for a complete list. Education Ph.D. Materials Science and Engineering Massachusetts Institute of Technology (2013) B.S. Materials Science and Engineering B.S. Mathematics University of Illinois at UrbanaChampaign (2008)"
rc-website-fork/content/about/people/park.md,More about this user goes here.
rc-website-fork/content/about/people/huband.md,"Jacalyn (Jackie) Huband, Ph.D. is the Assistant Director of the Data Analytics Center (DAC) within Research Computing at the University of Virginia (UVA). In addition to managing the DAC resources, she is a facilitator to researchers who are transitioning their codes onto a highperformance computing (HPC) platform. She also has been instrumental in creating learning opportunities within Research Computing, including Workshops, Brown Bag Lunches, and Journal Clubs. Prior to her work at UVA, Jackie has been an Associate Professor in Computer Science and a programmer in industry. She has experience programming in R, Python, Matlab, C/C++, and Fortran."
rc-website-fork/content/about/people/baller.md,More about this user goes here.
rc-website-fork/content/about/people/prakash.md,"Priyanka (Pri) Prakash joined the Research Computing (RC) Data Analytics Center as a Research Computing Scientist in Jan 2024. She joins us from MD Anderson Cancer Center, Houston, TX where she worked as an Institute Research Investigator. Prior to that she was a computational scientist at the Frederick National Laboratory for Cancer Research (FNLCR, Frederick, MD). She has MS in organic chemistry, and she completed her PhD in Biological Sciences and Bioengineering from the Indian Institute of Technology Kanpur, India (IITK). She has extensively worked in the field of computational biophysics, computeraided drug design (CADD) and in integrating AI/ML and molecular simulation approaches in CADD. She has published 30+ original peerreviewed research articles in journals including Cell, PNAS, Biophys J, Scientific Rep, in the field of her domain expertise and served as an inventor on investigational new drug applications and patents. Her domain expertise is drug discovery, cancer biology, computational biophysics and computational chemistry and she has been a user of HPC environment (TACC, XSEDE, Biowulf) for more than a decade. She has served as coPI on two awarded grants for compute time on NSFfunded ACCESS systems. She has designed and developed innovative computational strategies including workflows/computational pipelines in CADD and computational biophysics/chemistry with tremendous positive impact in academia as well as pharmaoriented industry setups. She is highly experienced in scientific communication and grantwriting and has served as a coI on an awarded NIH R01."
rc-website-fork/content/about/people/siadaty.md,"Steven is a secondyear in the College of Arts and Sciences studying computer science and statistics. He has experience working with Python, Java, and C. His interests focus on deep learning and AI integration, particularly involving bioinformatics and genetic sequencing. At Research computing Steven hopes to expand his understanding of the software development workflow, and to make high performance computing resources more available to UVA students and staff."
rc-website-fork/content/about/people/linehan.md,"Kathryn is an applied mathematician with the Data Analytics Center. Her current research interests include numerical linear algebra, lowrank matrix approximation and applications, and scientific computing. Prior to joining RC, she was a Research Scientist with the Social and Decision Analytics Division of UVA's Biocomplexity Institute where she specialized in natural language processing and machine learning for social science applications. She has experience with HPC, general purpose GPU computing, and academic programming with University of Maryland faculty from the Math, Computer Science, and Chemistry/Biochemistry departments. Education Ph.D. Applied Mathematics & Statistics, and Scientific Computation University of Maryland, College Park (May 2024) Certificate in Data Science Georgetown University, School of Continuing Studies (2018) M.S. Applied Mathematics & Statistics, and Scientific Computation University of Maryland, College Park (2009) B.A. Mathematics Hood College (2006)"
rc-website-fork/content/about/people/kim.md,"Jinwoo is currently a secondyear UVA student pursuing a major in computer science. He has experience with Python, Java, and C, ranging from software development and web development. At Research computing, he hopes to be able to help out with the RC website using Hugo, and learning more about HPC. In the future, he is hoping to pursue software development as his main focus. Education B.S. Computer Science University of Virginia (Expected Graduation: 2026)"
rc-website-fork/content/about/people/parece.md,Hana is a Computer Scientist with a strong background in a variety of programming languages. She is most experienced with Python and C/C++ and is passionate about teaching others about those languages. She helps to provide user support for the users of Rivanna and is seeking to learn more about GPU computing to better help those in an HPC environment.
rc-website-fork/content/about/people/losen.md,More about this user goes here.
rc-website-fork/content/about/people/garrevenkata.md,"Bhargav is a Computer Science student in the School of Engineering with experience in web development, Python, Java, and Linux, as well as video and tutorial production. He is passionate about creating websites and writing scripts to enhance efficiency. At Research Computing, Bhargav aims to deepen his knowledge of highperformance computing while making the University’s extensive computing resources more accessible to UVA students and staff. Outside of technology, Bhargav’s hobbies include playing quizbowl and watching standup."
rc-website-fork/content/about/people/eubanks.md,"Adam Eubanks is an undergraduate student at Brigham Young University studying Applied Math. He specializes in Open OnDemand app development and machine learning with HPC, as well as containerization."
rc-website-fork/content/about/people/alabi.md,More about this user goes here.
rc-website-fork/content/about/people/boakyedanquah.md,"Angela is a computational scientist in the Data Analytics Center at RC. She supports faculty and community members' machine and deep learning research. Angela is experienced in working in HPC and Cloud computing environments. Education M.S. Data Science University of Virginia (2023) B.A. Statistics, Economics University of Virginia (2021)"
rc-website-fork/content/about/people/vu.md,"Megan is a current 2ndyear student double majoring in Computer Science and Cognitive Science. She has experience coding in Python, Java, and SQL. Megan is an active member of Women in Computing Sciences, Student Game Developers, and the Cavalier Symphony Orchestra. In her free time, Megan likes to draw, play video games, and doom scroll on Wikipedia."
rc-website-fork/content/about/people/hussein.md,"Fadumo Hussein is a graduate student at the University of Virginia studying Data Science. Equipped with experience in Python, R, SQL, and SAS, Fadumo excels in conducting data analysis, model development, and visualization, all aimed at supporting organizational objectives and fostering positive outcomes. Outside of Research Computing, she enjoys spending time outdoors in nature and indulging in reading."
rc-website-fork/content/about/people/holcomb.md,More about this user goes here.
rc-website-fork/content/about/people/hall.md,More about this user goes here.
rc-website-fork/content/about/people/tran.md,Dominic is a 3rd year in the School of Engineering studying computer science. He's interested in systems programming and uses C and Python. Education B.S. Computer Science University of Virginia (Expected Graduation: 2027)
rc-website-fork/content/about/people/andino.md,"Gladys is the Strategic Services and Education Manager for ITSResearch Computing at UVA. She leads strategic planning for researcher training and outreach initiatives coordinating workshops and tutorials to support UVA’s computational research community. She adeptly manages a team of student workers, optimizing ITSResearch Computing’s efficiency by delegating routine tasks, thereby enabling staff to concentrate on domainspecific research challenges, all while grooming the next generation of HPC and scientific computing professionals. With a strong background in biological sciences and expertise in bioinformatics, Gladys supports computational biology research, particularly in genomic data analysis and highperformance computing applications. She is also part of the Data Analytics Center (DAC) consulting team at UVA, advising on bioinformatics workflows and computational methodologies. Since 2021, Gladys has served as the Founder and Chair of Virginia Women in HPC, a program she has successfully expanded across seven Virginia institutions. She organizes virtual events featuring distinguished technical speakers to champion women and allies in HPC and technology. Selected Publications Gladys Andino, Scott L. Delinger, Jacob Fosso Tande, Timothy Middelkoop, Claire Mizumoto, David P. Reddy, and Michael D. Weiner. Onboarding Research Computing and Data Professionals. in Practice and Experience in Advanced Research Computing (PEARC ’24), July 21–25, 2024, Providence, RI, USA. ACM, New York, NY, USA, 2024. https://doi.org/10.1145/3626203.3670582 Gladys Andino, Scott L. Delinger, Jacob Fosso Tande, Timothy Middelkoop, Claire Mizumoto, David P. Reddy, and Michael D. Weiner. An Onboarding Checklist for Research Computing and Data Professionals, Zenodo, 2024. https://doi.org/10.5281/zenodo.11074226 Harrell SL, Brazil M, Younts A, Dietz DT, Smith P, Gough E, Zhu X, Andino GK. Mentoring Undergraduates into CyberFacilitator Roles. PEARC '18: Proceedings of the Practice and Experience on Advanced Research Computing. 2018 July 22; 70:17. DOI: 10.1145/3219104.3219138 Andino GK, Marisa B, Gribskov M, Smith P. Advancing the Representation of Women in HPC at Purdue University. PEARC '17: Proceedings of"
rc-website-fork/content/about/people/andino.md,"the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact. 2017 July; (56):13. DOI: 10.1145/3093338.3104175 Andino GK, Gribskov M, Anderson DL, Evans JD, Hunt GJ. Differential gene expression in Varroa jacobsoni mites following a host shift to European honey bees (Apis mellifera). BMC Genomics. 2016 Nov 16;17(1):926. PubMed Central PMCID: PMC5112721 Hunt GJ, Given K, Tsuruda JT, Andino GK. Breeding mitebiting bees to control Varroa. Bee Culture: The Magazine of American Beekeeping. 2016 March 23; :4147. Available from: https://www.beeculture.com/breedingmitebitingbeestocontrolvarroa/ Krupke CH, Hunt GJ, Eitzer BD, Andino G, Given K. Multiple routes of pesticide exposure for honey bees living near agricultural fields. PLoS One. 2012;7(1):e29268. PubMed Central PMCID: PMC3250423 Andino GK, Hunt GJ. A scientific note on a new assay to measure honeybee mitegrooming behavior. Apidologie. 2011; (42):481484. DOI: 10.1007/s1359201100041 Education Ph.D. Entomology Purdue University, (2014) B.S. Science in Agriculture Zamorano PanAmerican Agricultural School (2002)"
rc-website-fork/content/about/people/tomar.md,"Diya is a 2nd year student at the University of Virginia and is pursuing a degree in Computer Science with a minor in Data Science. She has been programing since 7th grade and has been involved in robotics during middle and high school. She has experience in languages like Python and C++, and has worked on web development projects. As a Research Computing Student Worker, Diya has mainly focused on converting workshop materials into easily digestible markdowns. She has also been involved in providing feedback to improve the RC websites and has helped document workshop attendance through Jira ticket submissions. Diya is looking forward to working on machine learning projects in the future. Education B.A. Computer Science, Minor in Data Science University of Virginia (Expected 2026)"
rc-website-fork/content/userinfo/ivy/_index.md,"The UVA secure environment consists of Ivy virtual machines (Linux and Windows) and Rio HPC. Researchers can use Ivy and Rio to process and store sensitive data with the confidence that the environment is secure and meets requirements for HIPAA, FERPA, and certain controlledaccess data (e.g. dbGaP, NIMH NDA, etc.). However, projects involving CUI or ITAR data cannot access Rio at this time. To access the High security Rio HPC, researchers need to request an Ivy Linux VM which serves as a login node. {{< systemsboilerplate }} Overview Ivy provides virtual computing environments (virtual machines) specifically designed for interactive and smallscale analysis of highly sensitive data. Ivy Linux VMs can also act as a frontend for accessing the Rio HPC environment, which is optimized for largescale analysis of sensitive data. Projectspecific storage volumes are seamlessly mounted to the VMs and made accessible on the HPC system, facilitating smooth transitions between tasks performed on the VM and the HPC environment. In order to obtain access to either system, users must Submit an account request, Complete the Information Security Awareness Training, and Ensure their personal computer meets all High Security VPN requirements. Requesting Access Security Training High Security VPN Storage Virtual Machines Using the Rio HPC System Data Transfer In/Out of Ivy Requesting Access {{% highlight %}} {{% pieligibility %}} {{% /highlight %}} Access to Ivy resources is projectbased, limited to PIs and their designees, and requires approval. Once a project is approved a PI and her/his researchers must sign a RUDA (one for every researcher on each project). Request an Ivy Account Security Training {training} In order to use Ivy, researchers must complete the High Security Awareness Training (HSAT). This training takes approximately 10 minutes to complete. Please complete the training at the following link: https://in.virginia.edu/hsattraining. High Security VPN The High"
rc-website-fork/content/userinfo/ivy/_index.md,"Security VPN (HSVPN) allows researchers to connect to Ivy securely both on and off grounds. In order to use the HSVPN, users must ensure that their personal machines meet the following requirements. More information on HSVPN compliance can be found on the ITS website: https://in.virginia.edu/vpncheck Install the Cisco AnyConnect Secure Mobility Client. This can be found at the UVA ITS Software Gateway. Be sure to install the version of VPN Client HS 4.6 that is compatible with your personal computer's operating system. More detailed instructions for installing the VPN client can be found on the ITS website. Install Opswat. Opswat checks if your computer is compliant with HSVPN requirements. Opswat can be downloaded from the UVA ITS Software Gateway. {{% callout %}} If your personal machine's operating system is no longer supported and does not allow for disk encryption, having OPSWAT installed will not resolve the issue. The recommended solution is to upgrade the operating system or acquire a device with an updated OS that meets these security requirements. {{% /callout %}} Install Antimalware software (Windows Defender recommended). Antimalware software must be installed on your machine. Windows Defender is behavioralbased antimalware software and meets UVA's HSVPN requirements. Windows Defender can be downloaded from the UVA ITS Software Gateway. Connecting and Signing In 1 Authentication You will sign in to all Ivy resources using your UVA computing ID and Eservices password. Because of Ivy's high security requirements, your Eservices password must be changed every 60 days. Need help resetting your Eservices password? Reset Your Password If you are working from a secure Health Systems workstation you are ready to connect. If you are working from elsewhere on or off Grounds you will need Duo MFA and a High Security VPN connection. 2 Duo MFA To connect to the Ivy environment with"
rc-website-fork/content/userinfo/ivy/_index.md,"VPN you will need to install the Duo Mobile multifactor authentication (MFA) app on your smartphone. Get Duo for iPhone in the App Store Get Duo for Android on Google Play In the context of Ivy, Duo allows you two ways to provide a second factor of authentication beyond your password: via a random 6digit key, or via a push message direct to your phone. Set Up Duo 3 High Security VPN With your UVA computing ID, Eservices password, and Duo Mobile in hand, you must run the Cisco AnyConnect software to start a UVA High Security VPN connection every time you use any Ivy resource. AnyConnect will authenticate to the UVA network using a digital certificate installed on your workstation. More information on VPN from ITS: High Security VPN installation and connection instructions. How to create, install, and use digital certificates. Learn More about UVA VPN Once you have completed these three steps, you will be connected to the secure Ivy network. From there you can connect to a Virtual Machine, or use a web browser to access JupyterLab. Storage Ivy VM and Rio HPC have a pool of over 2 petabytes of Network Attached Storage shared amongst users. A PI specifies the storage space s/he would like to have when requesting access to either of these environments. Virtual machines do not come with any significant disk storage of their own. Virtual Machines A virtual machine (VM) is a computing instance dedicated to your project. Multiple users can sign in to a single VM. Virtual machines come in two platforms, Rocky 8 Linux and Windows Server 2019. Each platform is available in numerous instance types. Refer to the grid below for specifics. {{< pricing ivy }} Once created, your instance will be assigned a private IP address that you"
rc-website-fork/content/userinfo/ivy/_index.md,"will use to connect to it (in the format 10.xx.xx.xx). VMs exist in a private, secure network and cannot reach outside resources on the Internet. Most inbound and outbound data transfer is managed through the Data Transfer Node (see below). Connecting to your VM Before connecting to your VM, you must run the High Security VPN. Make sure that you have the VPN client installed on your laptop/desktop. Next, you will need to know two pieces of information: The type of VM that you have, either Windows or Linux; The IP address of your VM (e.g., 10.xxx.xxx.xxx). The steps for connecting to the VM will depend on the type of VM and, to a lesser extent, the operating system of your laptop/desktop (i.e., MacOS or Windows). To connect to a Windows VM from a Mac, you will need the Microsoft Remote Destop application which you can download here . Windows laptops/desktops already have the Remote Desktop Connection application installed. STEPS TO CONNECT TO YOUR VM Follow the steps below for the type of VM that you have: Connecting to a Windows VM Start the High Security VPN Run the Remote Desktop application (see comment above for installing this application on Macs) Enter the IP address for your VM Sign in with your Eservices password and your computing ID prefixed by ESERVICES as the username (i.e. ESERVICES\mst3k) Connecting to a Linux VM Start the High Security VPN Open a web browser and enter the IP address for your VM (e.g., https://10.xxx.xxx.xxx) If you get a warning message, you may need to click on Advanced Settings and/or a Connent Anyway option, depending on your web browser Use your Netbadge credentials to log in In addition to connecting to a Linux VM through a web browser, you have the option of connecting with"
rc-website-fork/content/userinfo/ivy/_index.md,"an ssh client. To do this, follow these steps: Start the High Security VPN Open the ssh client on your laptop/desktop (Terminal application on a Mac or Command Prompt on a Windows PC) and type: ssh mst3k@10.xxx.xxx.xxx, where mst3k is replaced with your user ID. When prompted for a password, use your Eservices password. Software Every virtual machine (Linux or Windows) comes with a base installation of software by default. These help researchers by providing the basic tools for data processing and manipulation. Additional software packages are preapproved and available for installation upon request. See the lists below for options. Preinstalled Software {{< rawhtml }} PREINSTALLED Linux Software Click on each for details: {{% ivyapprovedsoftware os=""Linux"" installation=""preinstalled"" category=""all"" %}} PREINSTALLED Windows Software Click on each for details: {{% ivyapprovedsoftware os=""Windows"" installation=""preinstalled"" category=""all"" %}} {{< /rawhtml }} Python/R Packages Anaconda Python and R packages are available to users through the normal pip, conda, and CRAN and library installation methods. ADDITIONAL APPROVED SOFTWARE (Available by Request) If you require additional software not listed, you must submit a request. Requests are reviewed by the UVA ISPRO office for security and regulatory compliance and, if approved, will be installed for you. ADDITIONAL Linux Groups Click on each for more information: All Packages Bioinformatics Data Analysis Database Software Image Processing Software Details for Linux ADDITIONAL Windows Groups Click on each for more information: All Packages Bioinformatics Data Analysis Database Software Image Processing Software Details for Windows To request installation of optional software packages, please use the web request form provided through this link: Request Ivy Software Installing Python Packages on Your VM CREATING CONDA ENVIRONEMENT Researchers often require Python packages that are not included in the base installation of Anaconda. Users can install additional Python packages on their VMs using conda environments. Conda environments allows"
rc-website-fork/content/userinfo/ivy/_index.md,"users to install packages in isolated environments to avoid version conflicts with other users on the VM. Windows Launch ""Anaconda Prompt"" from the Start Menu. From the prompt, issue the command: conda create n myenv package1 package2 where myenv is the name you wish to give your new conda environment, and package1 and package2 are the names of the Python packages you want to install. To activate and use your new environment, issue the command: conda activate myenv Linux Log into your VM via SSH or log in through your web browser and launch the Terminal. From the prompt, issue the command: conda create n myenv package1 package2 where myenv is the name you wish to give your new conda environment, and package1 and package2 are the names of the Python packages you want to install. To activate and use your new environment, issue the command: conda activate myenv Creating a Conda Environment with a Specific Python Version If you require a specific version of Python, you can create a new conda environment with: conda create n myenv python=2.7 INSTALLING PACKAGES After creating your conda environment, you can install additional libraries with pip and conda. Installing Packages with pip Use pip from the command line to install individual packages: pip install numpy You can search for a package: pip search panda To see which packages you have installed already: pip list You can install packages listed in a requirements.txt file (one package per line): pip r requirements.txt To save a list of your currently installed packages in a requirements.txt file: pip freeze requirements.txt Installing packages with conda conda works similarly to pip. To install a package: conda install scipy To search for a package: conda search scipy And to list all packages in your environment: conda list Once installed on"
rc-website-fork/content/userinfo/ivy/_index.md,"your VM, packages will persist and you will not need to install them again. You will only need to import them again in your code. Scheduled Maintenance Beginning Sunday, April 14, your Ivy virtual machine (VM) will be rebooted on the 2nd Sunday of each month between 5:00 a.m. and 6:00 a.m. EST while RC engineers install security updates. Any sessions running during this period will be terminated. Windows and Linux VMs will be rebooted at the same time. If you have any questions or problems with your software applications after the security updates have been installed, you may contact our user services team. JupyterLab Notebooks {{% callout %}} JupyterLab is a webbased interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. JupyterLab is extensible and modular: write plugins that add new components and integrate with existing ones. Learn more about Jupyter {{% /callout %}} Using the Rio HPC System Access Access to the Rio HPC requires an Ivy Linux VM to serve as a login node. Similar to other Ivy VMs, access to the Rio HPC is projectbased. For details on requesting an Ivy Linux VM and accessing it, please refer to the instructions provided above. Please note that PIs must specifically request for their associated Linux VM to be provisioned as a frontend to Rio. Access to Rio from the VM is not granted by default. As outlined above, VMs are available in various sizes. Please request a VM that is appropriately sized for your specific workflow. For larger groups or projects involving computationally intensive tasks, we recommend selecting a larger VM, with a preference for Small or above. Request Ivy VM for Rio"
rc-website-fork/content/userinfo/ivy/_index.md,"System Details HARDWARE CONFIGURATION Currently, Rio comprises 39 compute nodes, providing a total of 1,560 x86 64bit compute cores. Each HPC node is equipped with 375 GB of RAM to accommodate memoryintensive applications. Rio also includes an NVIDIA HGX H200 GPU, and additional GPU nodes designed to support AI and machine learning workloads will be integrated in the near future. JOB QUEUES Similar to our clusters Rivanna and Afton in standard security zone, Rio is a managed resource. Users must submit jobs to queues controlled by a resource manager, also known as a queueing system. The manager in use on Rio is Slurm. Slurm refers to queues as partitions because they divide the machine into sets of resources. There is no default partition and each job must request a specific partition. Partitions and access policies are subject to change, but the following table shows the current structure. Detailed information on Slurm and instructions for submitting jobs to the HPC can be found here. For an introduction to the Rio HPC system, please see our tutorial. Data Transfer In/Out of Ivy/Rio {datatransferinoutofivy} Moving sensitive data into the Ivy VM platform (and Rio) is possible through a secure Globus DTN (data transfer node). The Ivy DTN is connected to a pool of secure storage called “HighSecurity Research Standard Storage”, which in turn is connected to Ivy VMs. Only active research projects using Ivy virtual machines can use this service. How to Connect to the DTN and Transfer Files Before transferring files to Ivy, you will need Globus installed on the computer you are transferring data from. Globus can be downloaded from https://www.globus.org/globusconnectpersonal. Ensure that you are NOT connected to the HSVPN. Data transfer will not work if you are connected to the HSVPN. Open Globus in your web browser: https://app.globus.org/filemanager. When logging"
rc-website-fork/content/userinfo/ivy/_index.md,"in, select University of Virginia and log in with Netbadge. Once you are in the Globus File Manager, select the twopanel view by clicking the twopanel button beside the Panels button in the topright corner of the page. This should open a second panel on the page, so that you have two side by side. In one panel, click on the Collections field and select your computer. You can then click to the directory that contains the data you want to move, or type the path to the directory in the Path field. Click the files or folders you want to transfer to select them. In the remaining panel, click on the Collections field and search for and select the UVA IVYDTN. Select the storage share to which you want to transfer data. (Unless you are part of multiple Ivy projects, you should only see one storage folder.) Click the Start button beneath the first panel (should be highlighted) to begin the data transfer. Once the data transfer is complete, you will be able to access the data in your VM by clicking the HighSecurity Research Standard Storage shortcut on your VM's desktop."
rc-website-fork/content/userinfo/faq/storage-faq.md,"Accidental File Deletions Why Lease Storage Leased Storage Options File Transfer with Globus Permission Denied Error with Globus Help! I deleted my files accidentally! What can I do? For your home storage, the directory is /home/.snapshots . Snapshots are created once per day. Find the date you wish to find the snapshot for and navigate to your computing id. For GPFS Research Project (leased) storage, the directory is /gpfs/gpfs0/project/.snapshots. Neither Research Standard (leased) nor scratch storage is backed up in any way. Why should I lease storage? Leasing storage from Research Computing means that you do not have to run your own data server or backup system. You can lease storage for lab data without using any of the Research Computing computational resources, or you can lease storage for use with our computing facilities. What are my options for leased storage? Research Computing offers two tiers of leased storage, Research Standard and Research Project. Please see our storage page for details. Where can I learn more about the Globus file transfer tools? Globus maintains a well documented FAQ webpage that answers common questions related to security, file transfer and sharing, Globus endpoints and the command line interface (CLI) tools. I keep getting a 'Permission Denied' error when trying to transfer my files through Globus. What can I do? If you are certain that you have write permissions in the target directory and read permissions in the origin directory, you may be experiencing a common error with hidden files such as .AppleDouble or Thumb.db. You can resolve this issue by opening the Transfer and Timer Options menu at the center of the Globus screen (between the two blue ""Start"" buttons). Then check the box for Skip files on source with errors to tell Globus to ignore files that trigger a 'file"
rc-website-fork/content/userinfo/faq/storage-faq.md,not found' or 'permission denied' error.
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"General Usage Allocations Research Software Job Management Storage Management Data Transfer Downloading Files Other Questions General Usage How do I gain access to Rivanna/Afton? A faculty member must first request an allocation on the HPC system. Full details can be found here. How do I log on to Rivanna/Afton? Use an SSH client from a campusconnected machine and connect to login.hpc.virginia.edu. Instructions for using ssh and other login tools, as well as recommended clients for different operating systems, are here. You can also access the HPC system through our Webbased interface Open OnDemand or FastX. Please note that the old Domain Name System (DNS) entries for logging into Rivanna/Afton HPC have been removed. Please refer to the table below for the updated login names. |Old|New| ||| |rivanna.hpc.virginia.edu | login.hpc.virginia.edu| |rivannadesktop.hpc.virginia.edu | fastx.hpc.virginia.edu| |rivannaportal.hpc.virginia.edu | ood.hpc.virginia.edu| {{% offcampus %}} How do I reset my current password / obtain a new password? {howdoiresetmycurrentpasswordobtainanewpassword} Access to the HPC cluster requires a valid ITS (Netbadge) password. If you are unable to log in, you should first try resetting your ITS password here. If the problem persists, contact ITS through their online Helpdesk. Keep in mind that ITS requires annual resetting of your password. If you see a ""password expired"" message, you will need to change it through ITS. What happens to my account when I leave UVA? ITS controls access to the University’s computing resources, so when you or your students leave, you/they may lose access to many of these resources. Sponsored accounts allow people who work or volunteer at UVA, but who are not paid by UVA, to access the University’s computing resources. Researchers with sponsored accounts cannot request RC services but they are allowed to use the systems we manage as members of a Grouper (requires VPN connection) group controlled by a"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"UVA Principal Investigator (PI). Details on sponsored accounts are posted on the ITS sponsored accounts page. Why am I seeing WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED when I log in? Some users logging in through ssh may encounter this error message. If you receive this message, please see our instructions on how to clear this error. When I try to log in with ssh, nothing happens when I type my password! When you type your password, the ssh program does not echo your typing or move your cursor. This is normal behavior. When running Firefox on the HPC system, I get : ""Firefox is already running, but is not responding. To open a new window, you must first close the existing Firefox process, or restart your system."" What can I do? From a terminal in your home directory on the HPC system, run the commands: rm rf ~/.mozilla/firefox/.default/.parentlock rm rf ~/.mozilla/firefox/.default/lock When should I use FastX Web, when should I use an Open OnDemand Desktop session? Both allow you to run applications with graphical user interfaces in a Linux Desktop environment. Open OnDemand Desktop: Runs your session on allocated resources on a compute node. Ideal for running computeintensive singlenode applications with graphical user interface. Does not require a VPN connection from offGrounds locations. Recommended practice for running interactive jobs (particularly for coursework with a reservation). FastX Web: Runs all users' sessions on a single frontend node. Good for lightweight file management, script editing. Requires a VPN connection from offGrounds locations. How can I view .pdf or .csv files on Rivanna/Afton? For .pdf files, run the command: atril filename.pdf You can also open Atril from a FastX or Open OnDemand desktop environment from the Applications→ffice menu. The atril command can also be used to display image files, e.g. .png and .jpg"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"files. Or you may use eom FILE (Eye of MATE) from a terminal. Alternatively, you can open Eye of MATE from the MATE Desktop menu Applications→Graphics. For .csv files, run the command: oocalc filename.csv where filename is a placeholder for the specific filename. The oocalc command invokes the LibreOffice spreadsheet program ""Calc."" If logged on to a FastX or Open OnDemand Desktop, use the menu Applications→Office to access it. Why does it hang on log in? Why do OpenOnDemand interactive apps give conflicting package errors? It could be that your .bashrc file is loading too many or conflicting modules respectively. See our Modules page on how to load modules within best practices. If your .bashrc file is getting too crowded, you should replace it with the default here: Dedicated Computing Service Unit Allocation or Dedicated ComputingWhat is the right HPC service for me? The following table might help you decide which model suits you the best. SU Allocations Dedicated Computing Description Access to variety number of cores and node types Access to a [set of] dedicated node[s] of prespecified hardware with fixed number of cores Lifetime Standard alloc: 1 year; paid alloc: unlimited 5 year (expected hardware EOL) Queue times System load dependent; standard alloc: default priority; paid alloc: shorter than standard allocation; no preemption None(assuming no contention for the dedicated resources by other group members) Max walltime 37d Typically 1 to 5 years Ideal workload Even or bursty Even GPU or largemem Yes If that node type was purchased (can be expensive!) To learn about RC's service models and how to request and access each, plese refer to here. Can I still get access to HPC allocations without having to pay? Yes, standard and instructional allocations remain available free of charge. What will happen to my unused SUs that"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"I purchased before Jan 7, 2025? The service unit balance of your paid allocation will carry forward as is. Please be aware of the new service unit consumption rates which are more directly tied to the hardware type number of cpu cores, memory, and specialty hardware (e.g. GPUs) requested. What hardware options are available under the Dedicated Computing services? See here. Can the node I purchased under the Dedicated Computing model be configured as my personal login node to the HPC system? No. Dedicated computing hardware is configured as compute nodes only. Allocations What is an allocation? Time on the HPC system is allocated as Service Units (SUs). One SU corresponds to one corehour. Multiple SUs make up what is called an allocation (e.g., a new allocation = 100K SUs). Allocations are managed through Grouper (requires VPN connection) groups. These groups must be created by the Principal Investigators (PIs) prior to submitting an allocation request. Full details can be found here. How can I request an allocation? The different Service Unit (SU) allocation types are explained in this article. It includes links to our allocation request webforms. How do I check my allocation status on Rivanna/Afton? {howdoicheckmyallocationstatusonrivanna} Run the allocations command. The output may look like this: The Balance column shows the total of unused service units (SUs); the Reserved column shows the number of SUs held for current active jobs (pending or running). The Effective and Available columns show the difference of Balance and Reserved, i.e. the amount of SUs available for future jobs. After a job completes, the SUs actually consumed will be deducted from the allocation Balance and any SUs unused by that job will be released from the Reserved pool. In all cases you can only submit additional jobs if the available SU amount is sufficient"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"to cover the full SU request for the jobs. You do not need any allocation service units to access the frontend or files in your directories as long as your account is active. If you don't see your allocation, it may mean that you've been removed from the allocation group or that your allocation has expired. How do I check an allocation's expiration date? To check an allocation's expiration date run allocations a <allocation group command. Alternatively, run mamlistallocations. Only Standard Allocations and Instructional Allocations have an expiration date. PIs may request renewal of their expired allocation. Purchased Allocations never expire. How are Service Units Reserved? When a job is submitted the account manager calculates the required maximum amount of Service Units (SUs) using the assumption that the job will run the full amount of time requested. These SUs are held in reserve as a ""lien"" against the allocation charged for the job. When the job completes the lien is released and the actual SUs consumed are deducted from the allocation balance. See How do I check my allocation status on Rivanna/Afton? for specifics. How are Service Units charged for specialty hardware, e.g. GPU and large memory nodes? Service Units (SUs) serve as a general single currency on the HPC system. SUs in a given allocation account can be used freely to run jobs on nodes in the standard, parallel, gpu and interactive queues. Please note that the SU charge rate is different for some of the specialty hardware, e.g. the GPU nodes, as listed here. How is my ""Fairshare"" impacted by the changed SU charge rates? Your Fairshare value is driven by your SU consumption and affects the priority of jobs that you submit. This is true for both the standard and purchased allocations. If the changes to the"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"SU consumption rates increases your SU consumption you will see a proportional impact on your Fairshare value. How do I create a group or manage members in my allocations? You must use the Grouper (requires VPN connection) interface to create the group, and you must have administrative access to the group. New groups will require two owners who hold active roles at UVA, as well as a third departmental owner. Group owners will be required to perform an annual attestation of group membership. If group owners do not complete attesting to the validity of their group, the members will be automatically removed from the group. Note that If you need to set up a new group or modify a group that was created after November 28th, 2023, go to Grouper. Legacy MyGroups groups created before November 28th, 2023, can be accessed through the ""Legacy MyGroups"" folder on Grouper. How do I check allocation usage of individual group members? Please visit here to see how to generate an allocation usage report. How can I estimate the expected SU consumption for a new job? We have developed a utility in Open OnDemand (OOD) called the Slurm Script Generator. This tool generates a Slurm script based on the parameters specified by the user. Additionally, it estimates the number of Service Units (SUs) that will be billed based on the time requested in the script. I submitted a job and received an error “Invalid account or account/partition combination specified”. What should I do? All resource requests through the Open OnDemand interactive apps or through slurm batch jobs require you to specify an allocation for your job. If you do not input an allocation name, you will get this error. If you are experiencing this error and you have input an allocation, verify what allocations"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"you are a part of as described here. Verify that you are inputting the allocation name exactly as you see it all in lowercase. I submitted a job and receive an error ""Insufficient balance. Applying funds failure for JobId="". What should I do? The error indicates that your allocation group does not have enough service units to execute the job. Check your allocation status as described here. Also verify that your allocation has not expired, see here. Only Standard Allocations, and Instructional Allocations have an expiration date. PIs may request renewal of their expired allocation. Purchased Allocations never expire. Research Software How do I use research software that's already installed? We use the lmod system for managing software environments. Learn more about how to use lmod. Does RC install research software? Our staff will install software onto the HPC system if it is of wide applicability to the user community. Software used by one group should be installed by the group members, ideally onto leased storage for the group. We can provide assistance for individual installations. For help installing research software on your PC, please contact Research Software Support at resconsult@virginia.edu. Is there any other way to install research software that I need? Some groups and departments have installed a bundle of software they need into shared space. Please see your departmental IT support personnel if your department has its own bundle. Can I run this Docker container on Rivanna/Afton? We do not run Docker on the HPC system. Instead we use Apptainer. Apptainer can run Docker images directly, or you can convert a Docker image to an Apptainer image. To import existing Docker images, use the apptainer pull command. module load apptainer apptainer pull docker://account/image Software images built by Research Computing are hosted on Docker Hub. For example, to"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"pull our PyTorch 1.5.1 image, run: apptainer pull docker://uvarc/pytorch:1.5.1 Please visit this page for more details. Can I run application/container X on a GPU? Please check the user manual for your application/container before running on a GPU. For instance, scikitlearn does not have GPU support; hence using GPUs for scikitlearn will not help with your job performance but will only cost you more service units (see SU charge rate here) and prevent other users from using the GPUs. https://scikitlearn.org/stable/faq.htmlwillyouaddgpusupport How can I make my Jupyter notebook from JupyterLab to run as a batch job on Rivanna/Afton? Capture the information that you use to start up a JupyterLab session. It helps to take a screenshot of the web form where you enter the partition, number of cores, amount of memory, etc. You will need that information for requesting resources on a compute node. Note which kernel is used to run your notebook. This information will be needed later. Convert the notebook to a regular script. To do this, go into the notebook that you want to convert. In the upper left corner, click on File Export Notebook As Export Notebook to Executable Script . This will download the script onto your laptop. On my computer, this leaves a blank window on my screen. But, if I close that tab on my browser, the tab with the notebook returns. I’m now down with the notebook and can terminate the session. Upload the “executable script” to the HPC system. In Open onDemand dashboard view, on the black ribbon across the top, click on Files Home Directory. This will open a page that shows the files that you have in your home directory on Rivanna. At the top of the page, toward the right, is a button labelled “Upload”. Click on that button. In"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"the dialog box that appears, click on “Choose File”. This will allow you to go to the downloaded file and select it. Create a Slurm script to run your code. The Slurm script list the resources and instructions that are needed to run your “executable script”. See the following link: https://www.rc.virginia.edu/userinfo/hpc/slurm/ Open a terminal window on the HPC system, and move to the location where your scripts are. We recommend using the webbased FastX application (see below). Once in a terminal window, type sbatch followed my the name of your Slurm script. https://www.rc.virginia.edu/userinfo/hpc/login/remotedesktopaccess Job Management How do I submit jobs? You submit jobs by writing a Slurm script and submitting it with the sbatch command. Please see our Slurm documentation. If you would like assistance in generating Slurm scripts, please check out our Slurm Script Generator. Simply input the parameters of your job to get a fullyworking Slurm script. How do I submit an interactive job? If you wish to run a program that requires a graphical user interface or generates other graphics for display, such as a plot or chemical model, use one of the Open OnDemand interactive apps. Several are available, but if you one you wish to use isn't in the list, submit an interactive Desktop request. If you will be using the command line for your interactive job you may use the locallywritten ijob command. The minimum required options are A and c for allocation and number of cores. Run ijob h for a list of all options. For more information see the documentation. What queues can I use? After logging in, run the command qlist to see a list of queues and their availability. Run qlimits for the restrictions on submitting to each queue. How do I choose which queue to use? Queues are set"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"up to emphasize onecore (serial or threaded), multinode parallel, and specialty hardware including largememory nodes and GPUs. Serial jobs requiring only 1 compute node: standard Parallel jobs requiring up to 50 compute notes: parallel Jobs requiring the use of GPUs: gpu Jobs for interactive sessions or quick tests of code: interactive More information about queue policy is at the HPC homepage. How do I use the interactive queue? The interactive queue is ideal for code development or other short interactive jobs that require active monitoring. Examples include Slurm ijobs and OOD interactive apps like JupyterLab, RStudio Server, MATLAB, etc. The interactive queue has a time limit of 12 hours per job, and users can request up to a maximum of 24 CPU cores or 2 GPUs and up to 216G of CPU memory across all jobs. For example, you can run 24 serial jobs or one 24core job. To request a GPU on with OOD apps, you'll be asked if you want to use a GPU. If yes is selected you can choose one or two GPUs. In slurm, the user must specify the gres=gpu flag for GPU access. If two GPUs are desired in Slurm, you can specify gres=gpu:2. How do I check the status of my jobs? From a terminal, run the command squeue u computingID. Replace computingID with your specific UVA computing ID. From Open OnDemand, use the Job Viewer and select ""Your Jobs"" as the filter. If reporting a problem to us about a particular job, please let us know the JobID for the job that you are having a problem with. Why is my job not starting? Several things can cause jobs to wait in the queue. Paid allocations have priority over standard allocations. If members of your group under the same PI using the"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"same quality of service level (i.e. paid or standard) have consumed a large amount of compute time in the recent past, the “fair share” algorithm will give other users outside of your group higher priority access ahead of you. Finally, the queue you requested may simply be very busy. If your job is pending there will be another field with the reason; if it is “Resources” that means that the resource you requested isn’t available. If the reason is “Priority” it means that a job with higher priority than yours is running. Your job will rise in priority as it waits so it will start eventually. How can I check when my job will start? To request an estimate from the queueing system of your start time, run squeue u $USER start for all your jobs, or squeue j <jobid start for a specific job. Slurm will provide an estimate of the day and time your job will start. Why was my job killed? Usually this is because you inadvertently submitted the job to run in a location that the compute nodes can't access or is temporarily unavailable. If your jobs exit immediately this is usually why. Other common reasons include using too much memory, too many cores, or running past a job's time limit. You can run sacct: If it's still not clear why your job was killed, please contact us and send us the output from sacct. Why can't I submit jobs anymore? In order to be allowed to submit jobs, you must not be overallocated with your /scratch usage and you must have some remaining service units. There is a limit of 10 TB of space used per user in each /scratch directory and if you exceed either of those limits, you will not be able to"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"run jobs until you clean up. To check whether this is the case, run hdquota s If you have not exceeded the limits on /scratch, check whether your account has allocation units remaining by running allocations Why do I get sbatch error: Batch script contains DOS line breaks If you use a Windows editor to create Slurm batch scripts, when you try to run them you may encounter an error sbatch: error: Batch script contains DOS line breaks (\r\n) sbatch: error: instead of expected UNIX line breaks (\n). Windows and Linux use different conventions to mark the end of each line. Many applications on the HPC system, such as compilers, Matlab, etc., understand Windows endofline markers, but the shell does not. This is easy to fix by running the dos2unix command dos2unix myscript.slurm It will not hurt to run dos2unix on a file that doesn't need it. Sometimes you get {^M} character at the end of every line when the file was imported from Windows environment. dos2unix usually takes care of the problem, but not 100% all the time. How do I check how much SU's my job has burnt? To find out how many Service Units (SUs) a specific job has consumed, users can run the following command. Here the value under the Amount column shows the amount of SUs consumed. The timeframe can be controlled using the s(starting time) and e(end time) flags. $ mamlisttransactions a <allocationname s 20241101 e 20241203 s:starting date e: end date How do I check the efficiency of my completed jobs? Run the command seff on the Slurm job ID: udcba3436deepLearning$seff 40330441 Job ID: 40330441 Cluster: shen User/Group: teh1m/users State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 2 CPU Utilized: 00:15:14 CPU Efficiency: 89.08% of 00:17:06 corewalltime Job Wallclock time: 00:08:33"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"Memory Utilized: 6.89 GB Memory Efficiency: 58.76% of 11.72 GB udcba3436deepLearning$ The output of this command is also contained in the email sent by Slurm once your job completes. My jobs are failing due to an incorrect environment setup, but I am loading my modules and/or conda environments correctly in my job script. What is wrong? When submitting jobs using sbatch, Slurm will remember the environment that you were working in. This means that loaded modules, activated conda environments, and generally all the environment variables set in the terminal prior to job submission will follow through into your job. A way to avoid this issue from happening is to include the following line into your Slurm script: This makes it so that Slurm does not carry over any environment variables into your running job. Be sure to include the necessary module load or conda activate commands in your script to run your code. If you are using srun in your Slurm script, see an example script here Storage Management What storage options are available to me to use on Rivanna/Afton? All users are provided a 200GB home directory for longerterm storage. This directory provides ""snapshots"" though it is not backed up. Each user also is provided 10TB of temporary ""scratch"" storage accessible as /scratch/$USER where $USER will stand for your ID. Scratch storage is fast but is not backed up in any way. If the free storage is not sufficient, you need snapshots of your files, or you wish to share space among a research group, the group should lease storage. Why should I use /scratch storage? Scratch storage is fast and provides a large quantity of free space. However, there are limits on the number of files and the amount of space you may use. This is to maintain"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"the stability and performance of the system. Please review our scratch filesystem policy for details. If you use or expect to use a large number of files please contact us. How do I obtain leased storage? Research Computing offers two tiers of leased storage, Research Standard and Research Project. Please see our storage page for details. How do I check my disk usage? Run hdquota on a HPC frontend. How do I check my /scratch usage on Rivanna/Afton? Run the command hdquota s: hdquota s If you have used up too much space, created too many files, or have ""old"" files you may be regarded as ""overallocated"". Please note that if you are overallocated, you won't be able to submit any new jobs until you clean up your /scratch folder. If I'm over my disk quota in either in my /home directory or my /scratch directory, how can I determine my disk usage? You can run the following command from your /home or /scratch directory to see how your disk usage is distributed across subdirectories, and where you need to remove files. You can increase maxdepth to go further down in the directory structure. du . h maxdepth=1|sort h r If I'm over my file limit in /scratch, how can I determine where all the files are located? From your /scratch directory, run the following command to determine where you need to remove files. find . type f | cut d/ f2 | sort | uniq c How long can I store files in /scratch? /scratch is designed to serve as fast, temporary storage for running jobs, and is not longterm storage. For this reason, files are periodically marked for deletion from all /scratch directories. Please review the /scratch filesystem policy for more details. Store longerterm files in your home"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"directory or purchased storage. How do I share data in my /scratch or leased storage with a colleague? To share data from your /scratch directly with any other user, use Globus sharing. If your colleague also has an account on UVA HPC, he or she does not need to set up a personal endpoint but can simply log into the uvamainDTN endpoint and navigate to his or her /scratch directory to transfer the files. If you wish to share data in leased space with a member of your group, be sure that permissions are set so that the group member can access your subdirectory. The college can then simply use the data directly, or copy it elsewhere. If you wish to share data from your leased storage to a colleague who is not a member of the group, use Globus sharing in the same manner as sharing /scratch. Data Transfer How do I transfer data from UVA Box to my /scratch directory on Rivanna/Afton? Log into UVA HPC using the webbased FastX and launch the MATE Desktop interface. Then from the top menu bar, open firefox through the FastX desktop, in the upper right hand corner of the browser window you should see 3 horizontal bars. Click on that and then select Preferences from the dropdown window. In the new window scroll down until you see Downloads and select ‘Always ask you where to save files’. Then when you go to Box to download, a new window will pop up and if you click on ‘Other locations’, you can navigate to your scratch directory. How do I transfer data from my /scratch directory on Rivanna/Afton to my UVA Box account? Log into UVA HPC using the webbased FastX and launch the MATE Desktop interface. Then from the top menu bar,"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"open firefox through the FastX desktop and log into your UVA Box account. Once logged in to box, click on the New + button (upper right) to upload a file/folder. In the left sidebar of the new window, select Other Locations/Computer/scratch/ to navigate to your scratch directory and select the files/folders you want to upload to your box account. What Linux commands can I use to transfer files to/from Rivanna/Afton? Smaller files can be transferred to/from Rivanna/Afton using scp, sftp, and rsync as well as standard FTP tools. Larger files should be moved using Globus. Read more about data transfer. I need to push and commit code changes from Rivanna/Afton to my GitHub account. How do I set that up? You must first generate an ssh key and then copy it to your git repository. Here are the instructions for generating the ssh key and what to do on your git page: To generate an ssh key, see the following link: ssh key generation Click on the dropdown menu next to my Git profile picture in the upper right corner; Select Settings; Click on SSH and GPG keys in the left column; Click on the New SSH Key button and followed the directions to upload your ssh key. Make sure that the ssh key is in your authorizedkeys file in your .ssh directory on Rivanna/Afton. The next step is to clone the repository using the ssh link. If you have already cloned the repository using the http link and made a number of changes to your files, you won’t want to redo them. Rename the directory that was created when you first cloned the repository. Then, reclone the repository using the ssh link and copy all of the files you had changed to the new directory. Finally, push those changes"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"back to the repository. How do I add external or mapped network drives onto the Globus Path When you first set up Globus, it only has access to certain folders of your local drive. You can add additional locations such as mapped network drives or external hard drives in the Globus Options/Preferences menu. Windows: Right click the Globus icon Options + to add a new folder Mac: Globus icon Preferences Access + to add a new folder Click the Up button in the Globus File Manager to navigate to higher level folders. Downloading Files What commandline tools are available on Rivanna/Afton for downloading files from web? wget wget can be used to download files over HTTP,HTTPS and FTP protocols. You can use wget to download files from a single URL or multiple URLs. For example to download a file from a website you can use the following command: bash wget https://example.com/file.zip curl In addition to what mentioned for wget, curl can be used to upload files to a server as well. To download a file from a website, you can use the following command: bash curl O https://example.com/file.zip axel axel not only downloads files over different protocols, but accelerates the process by using multiple connections to retrieve files from the destination. Axel is available on Rivanna/Afton through module load axel. The syntax for using axel over 10 connections is as follows: bash axel n 10 http://example.com/file.zip wget, curl or axel? For rather small files of size <1GB, it might be easier to use wget or curl since module loading is not necessary. For large files it is recommended to use axel on a compute node. Below is a simple comparison between the download rate of these tools on a single core compute node on Rivanna/Afton: | tool | 100MB |"
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"1GB | |||| | wget | ~5s | 36s | | curl | ~5s | 35s | | axel | ~2s | 8s | Other Questions What if my question doesn't appear here? Take a look at our User Guide. If your answer isn't there, contact us."
rc-website-fork/content/userinfo/faq/_index.md,General Do you have a general computing question? Read our FAQ› Rivanna and Afton High Performance Computing Platforms Read our FAQ › Ivy Secure Data Computing Platform Read our FAQ › Storage Research Data Storage & Transfer Read our FAQ ›
rc-website-fork/content/userinfo/k8s/deployments.md,"Kubernetes is a container orchestrator for both shortrunning (such as workflow/pipeline stages) jobs and longrunning (such as web and database servers) services. Containerized applications running in the UVARC Kubernetes cluster are visible to UVA Research networks (and therefore from Rivanna, Afton, Skyline, etc.). Web applications can be made visible to the UVA campus or the public Internet. Kubernetes Research Computing runs microservices in a Kubernetes cluster that automates the deployment of many containers, making their management easy and scalable. This cluster will eventually consist of several dozen instances, 2000 cores and 2TB of memory allocated to running containerized services. It will also have over 300TB of cluster storage and can attach to both project and standard storage. {{% highlightdanger %}} The research Kubernetes cluster is hosted in the standard security zone. It is suitable for processing standard sensitivity or internal use data. Highly sensitive data (PHI, FERPA, etc.) are not permitted on this platform. {{% /highlightdanger %}} Design Principles Deployments within the UVARC Kubernetes cluster are configured with a ""Desired State"" architecture. This means that deployments are described in code (k8s YAML, Helm charts, Jsonnet & Kustomize files) and the cluster maintains the described state all the time. This is often called the ""GitOps"" model, since the deployment files are code that can be tracked and versioned in a Git repository. As researchers iterate on their application and build+test their containers, deployments themselves can be managed separately with new container versions. This model has some distinct advantages for both research users and engineers: Deployments should be defined in code. Handbuilt deployments are as brittle and unreproducible as handmade Docker containers. This helps maintain the state of applications as well as for disaster recovery. We do not grant users commandline access to the K8S API. kubectl and helm require the overhead"
rc-website-fork/content/userinfo/k8s/deployments.md,"of user authentication, roles, permissions, and network connectivity to the control plane that are unnecessary. Permissions in the GitOps model are granted via the deployment's Git repository, not at the cluster level. Deployment Lifecycle The lifecycle of applications themselves is different from, and should be independent from, various deployments of that application. Running your containerized application in Kubernetes requires you to think of two separate activities: (1) developing, testing, and building your application and its dependencies; and (2) deploying your application stack in the cluster. Development The first activity is generally well understood by researchers who may write their apps in Python, R, or other languages. As the app evolves, it is containerized using a Dockerfile and tested. Finally, more advanced projects will use automation tools such as GitHub Actions or Jenkins to build, test, and publish the application container(s) image to a container registry such as Docker Hub or GitHub Container Registry (GHCR). Delivery The second activity is less understood by researchers, since running docker locally for testing is different from cluster deployments. It is our belief that researchers should not be required to learn kubectl or other cluster management commands, instead simply defining their deployment in code and letting automation tools take it from there. We suggest you use a separate repository for all your deployment files, so that these two activities remain entirely separate. This lifecycle is known to engineers and developers as CI/CD, or Continuous Integration / Continuous Delivery, as it describes how modern applications are built, packaged, delivered, and deployed each of which may take more than one form. Continuous Integration is the process of developers continually iterating on the features, logic, and innerworkings of their application stack. This may be as small as bug fixes and as large as a complete redesign. The final"
rc-website-fork/content/userinfo/k8s/deployments.md,"product of the CI stage is a deliverable that can be run in test, user acceptance, or production modes. CI tools help automate the packaging and publication of that deliverable. In the case of microservices this is most often a container image that is ready to use. Continuous Delivery is the process of taking that deliverable and running it in an environment such as a public cloud, a server, etc. However, the CD process is normally more elegant than stopping the existing version of an application and replacing it. CD tools attempt to gently roll new versions into operation without any service disruption. And, using normal performance health checks, if the new version does not gain a healthy state, the CD orchestrator will roll back the container version. ArgoCD is UVARC's choice for a Kubernetesbased CD tool as it offers the ""desired state"" model described above, accepts a number of deployment formats, and is robust enough for distributed production clusters. Here's a brief explanation of ArgoCD and the entire CI/CD lifecycle: {{< youtube ""MeU5k9ssrs"" }} Launching Your Application The following ingredients come together to run your application Namespace Service launches generally require the creation of a namespace if users do not already have one. Namespaces serve as logical and organizational dividers, keeping the management of the services of User A isolated from those of User B. Namespaces also allow administrators to monitor or limit the maximum consumable resources (i.e. CPU and memory) for all deployments within the namespace. Deployment The basic unit of your running application. This defines what container image to use, what it should be named, how many replicas should run, what storage should be mounted and where, as well as resource limits. Service A running deployment that expects incoming requests must be exposed as a service, over"
rc-website-fork/content/userinfo/k8s/deployments.md,"a port. This allows Kubernetes to route traffic to the container(s). Some deployments, such as a recurring task or cron job, may not need a service or ingress definition. Ingress For service deployments such as web servers that expect incoming requests, the ingress definition maps a hostname (example.pods.uvarc.io) with a service. The UVARC cluster runs multiple ingress controllers to handle incoming traffic. Secrets / env Variables The best practice for passing sensitive information (usernames, passwords, keys, tokens, credentials) into a running container is to pass them in as encrypted environment variables called secrets. We use kubeseal for encrypting secrets into plaintext. This text can be added and committed to public repositories since their decryption key is stored privately in the cluster. Secrets can be consumed as env variables or as files mapped within the file hierarchy of the container. Normal env variables can also be passed to the container by defining them within the deployment spec file or as a config map that stores several keyvalue env vars. Storage We offer the ability to mount from three pools of persistent storage: Research Value Storage Research Project Storage Local Cluster Storage priced by TB increments like Project Storage. Observability Observability is the process of debugging, monitoring, or determining the state of your applications behind the scenes. We provide three levels of access into your microservices: ArgoCD A GUI to check the state of your deployments within Kubernetes. Lens A GUI to view pods, logs, shell into your pods, view storage and secrets, etc. Download Lens here kubectl Programmatic CLI access to the same resources as Lens. Connecting Services Kubernetes offers two simple ways to connect your microservices: Interpod communication When launching more than one container in your deployment specification, the containers can communicate by name, without a service definition. Your containers"
rc-website-fork/content/userinfo/k8s/deployments.md,"would launch in the same pod, which means they run on the same physical server and have immediate access to each other. Servicebased communication Running pods are assigned an arbitrary internal IP address within the cluster, and exposed internally within a namespace through service definitions (see above). Fortunately, containers within a namespace are provided all other service addresses within that namespace as env variables. This means that one of your pods, (e.g. MYAPI) will be exposed via the name defined in its specification (in all caps, e.g. MYAPISERVICE), that can be consumed by other pods running within that namespace, simply by referring to that variable. Division of Responsibilities What we take care of: Underlying physical infrastructure: Servers, networks, cabling, power, cooling. Underlying hosts: Operating system, patching, mounts to cluster/remote storage. Kubernetes API: Core k8s services across the cluster, high availability. Kubernetes Ingress: The ability to map traffic to pods. With SSL as necessary. Observability Tools: K8S Dashboard, ArgoCD Dashboard, Lens GUI to monitor and inspect your deployments. Deployment templates: To help you get started. What you take care of Creation, versioning and maintenance of container image(s). Maintenance of your deployment's scale, version, env variables and secrets. Debugging your application as necessary. Cluster Status {{% k8sgrafanaembed %}} Next Steps Have a containerized application ready for launch? Or want a consultation to discuss your microservice implementation? Request Access {{< consultbutton }}"
rc-website-fork/content/userinfo/accord/environments.md,"Back to Overview After creating a project and logging into the ACCORD platform, you will next choose an environment. The environments currently available on ACCORD are listed below. We welcome your suggestions for additional environments to be included in the future. RStudio RStudio is the standard IDE for research using the R programming language. JupyterLab Jupyter Lab allows for interactive, notebookbased analysis of data. A good choice for pulling quick results or refining your code in numerous languages including Python, R, Julia, bash, and others. Theia Python Theia Python is a rich IDE that allows researchers to manage their files and data, write code with an intelligent editor, and execute code within a terminal session. Theia C++ Theia C++ is a rich IDE that allows researchers to compile their own code. Runtimes & Limitations Computing environments can be used for both shortterm and longterm jobs. For extended runs, you may close your browser tab and return later. Outbound access to the Internet is restricted to package and library mirrors such as pip, PyPi, CPAN, CRAN, and others."
rc-website-fork/content/userinfo/accord/userguide.md,"Back to Overview Projects Request Access A PI can request access using an online form. Please include a description of the project data, level of sensitivity, the anticipated scope of computing for the project, and any supplemental information such as IRB approval. Request Access to ACCORD Your Project A new project creates the following: 1TB project storage 50GB home directory Add or Remove Team Members Additional team members can be added or removed by request using the following online form. Please include the researcher's full name, email, and home institution. Add or remove researchers Transfer Data Details on Globus data transfer coming soon Environments Create an environment {{< button buttonclass=""primary"" buttontext=""Open the ACCORD Platform"" buttonurl=""https://accord.uvarc.io/"" }} From the ACCORD console, select the project you want to work with and the desired resource tier. We currently offer the following resource tier options: Small (2 cores & 16 GB RAM) Medium (4 cores & 32 GB RAM) Large (8 cores, 64 GB RAM) XLarge ( 16 cores & 64 GB RAM) Next, select an environment. Your container should be running within a few seconds and will appear under ""Running Environments"" on the same page. To learn more about your choices of environments, see Environments Connect to an environment Once you have created an environment, click on the ""CONNECT"" button for the appropriate environment and a new browser tab will open. Terminate an environment When you are finished with your environment, please terminate it. Using the ""Running Environments"" section of the ACCORD console, find the environment you wish to terminate. On the far right will be a red ""Terminate"" button. Clicking this will terminate your environment. Note that your saved files and storage are never terminated or destroyed in this process. Terminated environments cannot be recovered. However, they can be replicated. Replicate an"
rc-website-fork/content/userinfo/accord/userguide.md,"environment To replicate an environment you used before, scroll down on the ACCORD console to see a list of environments you have run before. Click the orange ""Run"" button next to an environment you want to reuse. Software and data Software requirements A modern web browser such as Chrome, Firefox, Safari, or Edge. Access to your institution's VPN Install and register OPSWAT, a posturechecking client. {{< button buttonclass=""primary"" buttontext=""Learn More About OPSWAT"" buttonurl=""https://www.opswat.com/"" }} Data retention PIs may suspend a project at any time using the console. Project data is stored for 6 months and then automatically removed. Suspended projects can be reinstated by request before this time."
rc-website-fork/content/userinfo/accord/accord-support.md,"Name Email University/Institution Department/Organization Brief description of your request Details of your request Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed. Submit"
rc-website-fork/content/userinfo/accord/rstudio.md,Back to Overview RStudio is the standard IDE for research using the R programming language. Learn more about RStudio
rc-website-fork/content/userinfo/accord/jupyter.md,"Back to Overview Jupyter Lab allows for interactive, notebookbased analysis of data. A good choice for pulling quick results or refining your code in numerous languages including Python, R, Julia, bash, and others. Learn more about Jupyter Lab"
rc-website-fork/content/userinfo/accord/faq.md,"Back to Overview What is the link to the accord portal? {{< button buttonclass=""primary"" buttontext=""Open the ACCORD Platform"" buttonurl=""https://accord.uvarc.io/"" }} Who can use ACCORD? Researchers from public universities across the state of Virginia are invited to request access. ACCORD is also open to researchers from other states who are working on COVID related projects How much does ACCORD cost? ACCORD is free with 1TB of storage How much storage does ACCORD support? ACCORD gives each project a free 1TB of storage. Additional storage can be purchased How is data transferred in/out of ACCORD? ACCORD uses Globus to transfer data How do I learn more about ACCORD? {{< button buttonclass=""primary"" buttontext=""Learn more about ACCORD"" buttonurl=""https://www.rc.virginia.edu/userinfo/accord/about/"" }}"
rc-website-fork/content/userinfo/accord/projects.md,"Back to Overview The fundamental organizing unit for your work in ACCORD is a project. A project consists of: Researchers a group of approved collaborators. Storage import and store 1TB (more upon request) of data for your project. Researchers All Principal Investigators (PI) are carefully vetted and approved by the ACCORD team before they are given access to the ACCORD computing platform. PIs manage personnel for their projects, transfer code and data, and control their storage. Coinvestigators are also vetted before they are given access to the ACCORD platform. Once a coinvestigator has been onboarded, they can be added to any project by that project’s PI. Data Data can be transferred using the Globus federated GridFTP platform. The ACCORD DTN (data transfer node) endpoint address can be found once you sign in to the ACCORD Console. ACCORD user access is nonhierarchical and makes no distinction between various project personnel when granting permissions. All project members have equal access to project data, i.e. there is no privileged user or access for any given project. Storage ACCORD users automatically receive 1TB of free storage with each project. Additional storage can be purchased for a fee."
rc-website-fork/content/userinfo/accord/theia.md,"Back to Overview Theia Python is a rich IDE that allows researchers to manage their files and data, write code with an intelligent editor, and execute code within a terminal session. Learn more about the Theia Python IDE"
rc-website-fork/content/userinfo/accord/about.md,"Back to Overview What is it ACCORD (Assuring Controls Compliance of Research Data) gives researchers from public universities across the state of Virginia access to a multicore computing system capable of processing and storing deidentified sensitive data. ACCORD is appropriate for deidentified PHI, FERPA, business confidential, and other types of deidentified sensitive data. This is especially important for schools that lack the financial, staffing, or technical resources for such systems. ACCORD is designed to encourage collaborative research partnerships across institutions by managing the risks associated with sensitive data sharing. ACCORD is projectbased, which means use of the platform centers on individual projects. A principal investigator requests access to the platform (specifying a project title and level of sensitivity), populates the project with coinvestigators, and imports data. Each of the investigators can create and use computing environments to run analysis on the data. When done with the analysis, the investigators can spin down the pla/tform to free the resources. Who can use it This platform is open for approved academic research on sensitive data. Researchers from public universities across the state of Virginia are invited to request access. ACCORD is also open to researchers from other states who are working on COVID related projects. ACCORD is appropriate for deidentified PHI, FERPA, deidentified HIPAA, business confidential, and other types of sensitive data. More restrictive levels, such as CUI, FISMA, iTar, and PCI cannot be satisfied. How do I use it ACCORD is entirely webbased, which means it can be easily accessed using a modern browser such as Chrome, Firefox, Edge or Safari. Once you have created a project in ACCORD, you can perform computational research using one of the application environments. Currently we offer RStudio, JupyterLab, Theia C++, and Theia Python. More detailed information can be found in our User Guide ACCORD"
rc-website-fork/content/userinfo/accord/about.md,"offers no SSH, FTP, Remote Desktop, or VNC access. Who runs it The ACCORD project was developed by the Research Computing Group at the University of Virginia and funded through an NSF grant awarded to Ron Hutchins who serves as the Principal Investigator."
rc-website-fork/content/userinfo/accord/_index.md,"Welcome to ACCORD (Assuring Controls Compliance of Research Data), a webbased platform which allows researchers from public universities across the state of Virginia to analyze and store their sensitive data in a central location. ACCORD is appropriate for deidentified PII, FERPA, deidentified HIPAA, business confidential, and other types of deidentified sensitive data Thanks to funding provided by the National Science Foundation (Award : 1919667), ACCORD is available at no cost to researchers in the state of Virginia. Partners Listed below are our partner universities for ACCORD: Get Started About Learn about ACCORD. Learn More Support Ticket Open a support ticket for ACCORD Open a Support Ticket Launch Open the ACCORD Portal and get to work. Launch Projects Learn how ACCORD manages projects Learn More Environments Learn what environments ACCORD supports Learn More Security Learn about ACCORD's security Learn More User Guide Learn how to get started and work on ACCORD Learn More FAQs FAQs about ACCORD Learn More"
rc-website-fork/content/userinfo/accord/security.md,"Back to Overview ACCORD is appropriate for deidentified PII, FERPA, deidentified HIPAA, business confidential, and other types of deidentified sensitive data. ACCORD cannot be used to process highlyrestricted data such as CUI, FISMA, iTAR, and PCI data. Authentication ACCORD does not have its own user identity store but instead relies upon authentication via your home institution's single signon tool. Authorization All members of a project have equal access to the data storage for that project, without sudo or root privileges. Closed Environments ACCORD environments have no outbound connectivity to the Internet other than approved library and tool repositories (PyPi, CPAN, CRAN, etc.). Connections to tools such as GitHub and external APIs are not allowed. Encryption All connectivity to ACCORD environments is encrypted using SSL over HTTPS. Data transfers in/out via the Globus DTN meet FIPS 1402 compliance. Isolation ACCORD environments cannot have any access to other environments. Environments run within isolated Kubernetes pods and their network connectivity is isolated and encrypted. Private Environment URLs When you request an ACCORD environment, a unique HTTPS endpoint is created for you and can only be used by you. For example: https://jupyternotebook1a2b3c4d5emst3k.uvarc.io/ These environments cannot be shared. Logging All user interactions with ACCORD are logged including account creation, approval, project creation, changes in group membership, the creation of/changes to environments, and file uploads/downloads using a browser or the Globus DTN. Client PostureChecks Access to ACCORD is restricted to computers that are sufficiently updated and meet minimum security requirements. To verify this, ACCORD uses OPSWAT, a small piece of software that users install on their local computers. Step 1: Install the VPN Assessment Application (Opswat) Opswat will be installed during the onboarding process for ACCORD. Step 2: Resolve Security Requirement Issues Requirement 1: Operating System Update Operating System for Mac (version 10.14.0 or higher) Open"
rc-website-fork/content/userinfo/accord/security.md,"System Preferences Click on Software Update Click Update Now Note: Updating the Operating System may take up to a couple of hours. Do not shut down your computer or allow it to run out of battery during the update process. A restart of your computer may occur after the updates are complete. Update Operating System for Windows 10 Open Windows Update by clicking the Start button in the lower left corner. In the search box, type ""Update"", and then, in the list of results, click either Windows Update or Check for updates. Click the Check for updates button and then wait while Windows looks for the latest updates for your computer. If you see a message telling you that important updates are available, or telling you to review important updates, click the message to view and select the important updates to install. In the list, click the important updates for more information. Select the checkboxes for any updates that you want to install, and then click ""OK"". Click Install updates. Note: Updating the Operating System may take up to a couple of hours. Do not shut down your computer or allow it to run out of battery during the update process. A restart of your computer may occur after the updates are complete. If you encounter issues while trying to update your Windows computer, visit the Fix Windows Update Issues Windows Support webpage Requirement 2: HostBased Firewall Hostbased firewall software must be installed and enabled. Enable Firewall for macOS (All Versions) Open System Preferences Select Security and Privacy Select Firewall Click the lock in the lowerleft corner and enter your credentials. Select Turn On Firewall Close System Preferences Enable Firewall for Windows 10 Select the Start button, then select Settings (the gear icon). Select Windows Security from the menu on"
rc-website-fork/content/userinfo/accord/security.md,"the left. Select Firewall & network protection. You may then see several networks (i.e., Domain network, Private network). Select each network one at a time and set the Windows Defender Firewall to On. Requirement 3: Antivirus / Antimalware Software At least one antimalware software must be installed and enabled. We recommend the following: Antivirus for Mac We recommend using either Gatekeeper or Microsoft Defender for Endpoint for Macs. Antivirus for Windows We recommend using either Microsoft Defender or Microsoft Defender for Endpoint for Windows. Most common antivirus software is acceptable, except those made by Kaspersky Labs. Requirement 4: Device Password The device must be password protected, and it must lock automatically if there is no activity detected for at least 10 minutes. Configure your device to require a password to log in. Also, set your device’s screensaver or security settings to automatically lock after 10 minutes of no activity. Requirement 5: WholeDisk Encryption Wholedisk encryption software must be installed and enabled. Accepted applications include BitLocker, Dell Data Protection, and FileVault."
rc-website-fork/content/userinfo/hpc/storage.md,"There are a variety of options for storing largescale research data at UVa. Public and Internal Use data storage systems can be accessed from the Rivanna and Afton high performance computing systems. Storage Directories Name Quota Price Data Protection Accessible from Best Practices /home 200GB Free {{% backuppolicy rivannahome %}} Rivanna/Afton /home is best used as a working directory when using Rivanna/Afton interactively. Slurm jobs run against /home will be slower than those run against /scratch. The /home directory is a personal storage space that is not shareable with other users. /scratch 10TB Free {{% backuppolicy rivannascratch %}}, Data removed 90 days after last file access time Rivanna/Afton /scratch is a high performance parallel filesystem that is suitable for large scale computational work. Data should be moved from /scratch for longterm storage. The /scratch directory is for personal use and not shared with other users. Scratch Cleanup Policy {{% scratchpolicy %}} Request Additional Storage Researchers can lease additional storage, Research Standard or Research Project storage, for sharing public or internal use data within a research group. Research Standard and Research Project storage volumes are mounted on Rivanna/Afton and can also be accessed from local workstations. Learn more about our storage offerings. Storage requests can be placed through this form: Storage Requests"
rc-website-fork/content/userinfo/hpc/basepod.md,"HGX H200 Nodes May 1, 2025 We’re excited to announce the release of our newest highperformance compute node featuring the NVIDIA HGX H200 platform. This node is built on the Dell PowerEdge XE9680 server model, and we currently have one server available. The node is equipped with dual Intel Xeon Platinum 8468 CPUs, offering a total of 96 cores running at 2.1GHz, and comes with a massive 2TB of memory. This addition features eight NVIDIA HGX H200 GPUs based on the NVIDIA Hopper architecture. Each H200 GPU includes 141 GB of memory and delivers up to 4.8 TB/s of memory bandwidth. Highspeed communication between GPUs is supported via 900 GB/s NVLink, while connectivity between CPU and GPU uses PCIe Gen5, providing up to 128 GB/s bidirectional bandwidth. More information can be found here. This node is now integrated into our SLURM scheduler and can be accessed using the gpu partition. However, to maximize the efficiency of this highdemand resource, it is excluded from access via the Open OnDemand platform and is only available through batch job requests. Information for accounting and scheduling purposes can be found here. We invite users with GPUintensive workflows to take advantage of these powerful nodes. Introducing the NVIDIA DGX BasePOD™ May 30, 2023 As artificial intelligence (AI) and machine learning (ML) continue to change how academic research is conducted, the NVIDIA DGX BasePOD, or BasePOD, brings new AI and ML functionality UVA's HighPerformance Computing (HPC) system. The BasePOD is a cluster of highperformance GPUs that allows large deeplearning models to be created and utilized at UVA. The NVIDIA DGX BasePOD™ on Rivanna and Afton, hereafter referred to as the POD, is comprised of: 18 DGX A100 nodes with 2TB of RAM memory per node 80 GB GPU memory per GPU device Compared to the regular"
rc-website-fork/content/userinfo/hpc/basepod.md,"GPU nodes, the POD contains advanced features such as: NVLink for fast multiGPU communication GPUDirect RDMA Peer Memory for fast multinode multiGPU communication GPUDirect Storage with 200 TB IBM ESS3200 (NVMe) SpectrumScale storage array which makes it ideal for the following types of jobs: The job needs multiple GPUs on a single node or even multiple nodes. The job (can be single or multiGPU) is I/O intensive. The job (can be single or multiGPU) requires more than 40 GB GPU memory. (The nonPOD nodes with the highest GPU memory are the regular A100 nodes with 40 GB GPU memory.) Detailed specs can be found in the official document (Chapter 3.1). Accessing the POD The POD nodes are contained in the gpu partition with a specific Slurm constraint, requested with C or constraint=. Slurm script Open OnDemand Select NVIDIA A100 in the GPU type dropdown. Select the number requested in the appropriate textbox. Select Yes for Show Additional Options. Into the h “Optional: Slurm Option” textbox type: Cgpupod Remarks Before running on multiple nodes, please make sure the job can scale well to 8 GPUs on a single node. Multinode jobs on the POD should request all GPUs on the nodes, i.e. gres=gpu:a100:8. You may have already used the POD by simply requesting an A100 node without the constraint, since 18 out of the total 20 A100 nodes are POD nodes. As we expand our infrastructure, there could be changes to the Slurm directives and job resource limitations in the future. Please keep an eye out for our announcements and documentation. Usage examples Deep learning As of October 3, 2023 we are migrating toward NVIDIA’s NGC containers for deep learning frameworks such as PyTorch (2.0+) and TensorFlow (2.13+), as they have been heavily optimized to achieve excellent multiGPU performance. {{< alertgreen"
rc-website-fork/content/userinfo/hpc/basepod.md,"}} Warning: Distributed training is not automatic! Your code must be parallelizable. If you are not familiar with this concept, please visit: TensorFlow distributed , PyTorch DDP. {{< /alertgreen }} MPI codes Please check the manual of your code regarding the relationship between the number of MPI ranks and the number of GPUs. For computational chemistry codes (e.g. VASP, QuantumEspresso, LAMMPS) the two are oftentimes equal, e.g. If you are building your own code, please load the modules nvhpc and cuda which provide NVIDIA compilers and CUDA libraries. The compute capability of the POD A100 is 8.0. For documentation and demos, refer to the “Resources” section at the bottom of this page. GPUenabled modules A complete list of GPUenabled modules on the HPC system can be found here. Please refer to their respective pages and/or module load messages (if any) for usage instructions."
rc-website-fork/content/userinfo/hpc/login.md,"The UVA HPC systems (Rivanna and Afton) are accessible through a web portal, secure shell terminals, or a remote desktop environment. For of all of these access points, your login is your UVA computing ID and your password is your Eservices password. If you do not know your Eservices password you must change it through ITS. {{< offcampus }} Webbased Access Open OnDemand is a graphical user interface that allows access to HPC via a web browser. The Open OnDemand access point is ood.hpc.virginia.edu. Within the Open OnDemand environment users have access to a file explorer; interactive applications like JupyterLab, RStudio Server & FastX Web; a command line interface; and a job composer and job monitor to submit jobs to the Rivanna and Afton clusters. Detailed instructions can be found on our Open OnDemand documentation page. Launch Open OnDemand Learn more about Open OnDemand Secure Shell Access (SSH) UVA HPC is accessible through ssh (Secure Shell) connections using the hostname login.hpc.virginia.edu. Windows Windows users must install an ssh client application. We recommend MobaXterm, but you may also use other clients such as PuTTY. Install MobaXterm Mac OSX and Linux OSX and Linux users may connect through a terminal using the command ssh Y mst3k@login.hpc.virginia.edu SSH key authentication is also permissible. Using X11 Applications with ssh X11 applications can be run via an ssh connection as long as it is configured correctly. The Y option specifies this for the commandline application run in a terminal. Windows users who install MobaXterm do not need to add Y in an ssh session since this is the default for MobaXterm. Other clients such as PuTTY must be configured to allow X11 packets to be transferred. Mac users must install XQuartz in order to be able to run graphical (X11) applications locally. Graphical X11 applications"
rc-website-fork/content/userinfo/hpc/login.md,"may be slow through a standard ssh login. For extensive use of graphical applications we recommend FastX. For more details and for troubleshooting information, please see our ssh page. Remote Desktop Access Users who wish to run X11 graphical applications may prefer the FastX remote desktop web interface. The FastX web client is accessible at fastx.hpc.virginia.edu. Your login credentials are your UVA computing ID and your Eservices password. Connect to FastX via Web Learn more about FastX Web"
rc-website-fork/content/userinfo/hpc/slurm.md,"SLURM Would you like to take an interactive SLURM quiz? y/N | Overview UVA HPC is a multiuser, managed environment. It is divided into login nodes (also called frontends), which are directly accessible by users, and compute nodes, which must be accessed through the resource manager. Users prepare their computational workloads, called jobs, on the login nodes and submit them to the job controller, a component of the resource manager that runs on login nodes and is responsible for scheduling jobs and monitoring the status of the compute nodes. We use Slurm, an opensource tool that manages jobs for Linux clusters. Jobs are submitted to the Slurm controller, which queues them until the system is ready to run them. The controller selects which jobs to run, when to run them, and how to place them on the compute node or nodes, according to a predetermined site policy meant to balance competing user needs and to maximize efficient use of cluster resources. Slurm divides a cluster into logical units called partitions (generally known as queues in other systems). Different partitions may contain different nodes, or they may overlap; they may also impose different resource limitations. The UVA HPC environment provides several partitions and there is no default; each job must request a partition. To determine which queues are available, log in to the HPC System and type qlist at a Linux commandline prompt. For the memory and core limits on each queue, use the command qlimits Local Queue Configuration Several queues (partitions) are available for different types of jobs. One queue is restricted to singlenode (serial or threaded) jobs; another for multinode parallel programs, and others are for access to specialty hardware such as largememory nodes or nodes offering GPUs. For the current queue configuration and policies on UVA HPC please"
rc-website-fork/content/userinfo/hpc/slurm.md,"see its homepage. Slurm Architecture Slurm has a controller process (called a daemon) on a head node and a worker daemon on each of the compute nodes. The controller is responsible for queueing jobs, monitoring the state of each node, and allocating resources. The worker daemon gathers information about its node and returns that information to the controller. When assigned a user job by the controller, the worker daemon initiates and manages the job. Slurm provides the interface between the user and the cluster. Slurm performs three primary tasks: Manage the queue(s) of jobs and settles contentions for resources; Allocate a subset of nodes or cores for a set amount of time to a submitted job; Provide a framework for starting and monitoring jobs on the subset of nodes/cores. To submit a job to the cluster, you must request the appropriate resources and specify what you want to run with a Slurm Job Command File. In most cases, this batch job file is simply a bash or other shell script containing directives that specify the resource requirements (e.g. the number of cores, the maximum runtime, partition specification, etc.) that your job is requesting along with the set of commands required to execute your workflow on a subset of cluster compute nodes. Batch job scripts are submitted to the Slurm Controller to be run on the cluster. When the script is submitted to the resource manager, the controller reads the directives, ignoring the rest of the script, and uses them to determine the overall resource request. It then assigns a priority to the job and places it into the queue. Once the job is assigned to a worker, the job script is run as an ordinary shell script on the ""master"" node, in which case the directives are treated as comments."
rc-website-fork/content/userinfo/hpc/slurm.md,"For this reason it is important to follow the format for directives exactly. The remainder of this tutorial will focus on the Slurm command line interface. More detailed information about using Slurm can be found in the official Slurm documentation. Job Scripts Jobs are submitted through a job script, which is a shell script (usually written in bash). Since it is a shell script it must begin with a ""shebang"" This is followed by a preamble describing the resource requests the job is making. Each request begins with SBATCH followed by an option. After all resource requests have been established through SBATCH, the script must describe exactly how to run the job. module purge module load gcc ./mycode In our installation of Slurm, the default starting directory is the directory from which the batch job was submitted, so it may not be necessary to change directories. Configurable Options in Slurm Slurm refers to processes as ""tasks."" A task may be envisioned as an independent, running process. Slurm also refers to cores as ""cpus"" even though modern cpus contain several to many cores. If your program uses only one core it is a single, sequential task. If it can use multiple cores on the same node, it is generally regarded as a single task, with multiple cores assigned to that task. If it is a distributed code that can run on multiple nodes, each process would be a task. Options Note that most Slurm options have two forms, a short (singleletter) form that is preceded by a single hyphen and followed by a space, and a longer form preceded by a double hyphen and followed by an equals sign. In a job script these options are preceded by a pseudocomment SBATCH. They may also be used as commandline options on their"
rc-website-fork/content/userinfo/hpc/slurm.md,"own. | Option | Usage | ||| | Number of nodes | N <n or nodes=<n | | Number of tasks | n <n or ntasks=<n | | Number of processes (tasks) per node | ntaskspernode=<n | | Total memory per node in megabytes | mem=<M | | Memory per core in megabytes | mempercpu=<M | | Wallclock time | t dhh:mm:ss or time=dhh:mm:ss | | Partition (queue) requested | p <part or partition <part | | Account to be charged | A <account or account=<allocation | {{< highlight }} The mem and mempercpu options are mutually exclusive. Job scripts should specify one or the other but not both. {{< /highlight }} Environment variables These are the most basic; there are many more. By default Slurm changes to the directory from which the job was submitted, so the SLURMSUBMITDIR environment variable is usually not needed. SLURMJOBID SLURMSUBMITDIR SLURMJOBPARTITION SLURMJOBNODELIST Slurm passes all environment variables from the shell in which the sbatch or salloc (or ijob) command was run. To override this behavior in an sbatch script, add To export specific variables use Optionally export and set with Standard Output and Standard Error By default, Slurm combines standard output and standard error into a single file, which will be named slurm\<jobid.out. You may rename standard output with To separate standard error from standard output, you must rename both. Slurm Script Generator If you would like assistance in generating Slurm scripts, please check out our Slurm Script Generator. Simply input the parameters of your job to get a fullyworking Slurm script. Submitting a Job Job scripts are submitted with the sbatch command, e.g.: $ sbatch hello.slurm The job identification number is returned when you submit the job, e.g.: $ sbatch hello.slurm Submitted batch job 18341 Submitting an Interactive Job If you wish"
rc-website-fork/content/userinfo/hpc/slurm.md,"to run a job directly from the shell, you can run an interactive job. If you are using any kind of graphical user interface (GUI) you should use one of the Open OnDemand interactive apps. This offers direct access to Jupyterlab, VSCode Server, RStudio Server, the MATLAB desktop, and others. For graphical applications not available through one of the dedicated apps, such as the Totalview debugger or some bioinformatics packages, use the Open OnDemand Desktop app. From the Desktop you can open a terminal window, load modules, and start any application you wish. Please note that a few GUI applications require a GPU so you must request that partition in the online form. If you wish to run an interactive job from the command line, you can use our local command ijob to obtain a login shell on a compute node. $ ijob <options ijob is a wrapper around the Slurm commands salloc and srun, set up to start a bash shell on the remote node. The options are the same as the options to salloc, so most commands that can be used with SBATCH can be used with ijob. The request will be placed into the queue specified: $ ijob c 1 A mygroup p standard time=100:00:00 salloc: Pending job allocation 25394 salloc: job 25394 queued and waiting for resources There may be some delay for the resource to become available. salloc: job 25394 has been allocated resources salloc: Granted job allocation 25394 For all interactive jobs, the allocated node(s) will remain reserved as long as the terminal session is open, up to the walltime limit, so it is extremely important that users exit their interactive sessions as soon as their work is done sothat the user is not charged for unused time. If you are using an interactive"
rc-website-fork/content/userinfo/hpc/slurm.md,"app, be sure to delete session if you exit the session before time runs out. In the case of a commandline ijob, the job will be terminated if you exit your shell for any reason, including shutting down the local computer from which your login originates. $ exit salloc: Relinquishing job allocation 25394 A full list of options is available from SchedMD's salloc documentation, or you may run ijob help at the command line. Displaying Job Status The squeue command is used to obtain status information about all jobs submitted to all queues. Without any specified options, the squeue command provides a display which is similar to the following: The fields of the display are clearly labeled, and most are selfexplanatory. The TIME field indicates the elapsed walltime (hrs:min:secs) that the job has been running. Note that JOBID 12346 has the name bash, which indicates it is an interactive job. In that case, the TIME field provides the amount of walltime during which the interactive session has to be open (and resources have been allocated). The ST field lists a code which indicates the state of the job. Commonly listed states include: PD PENDING: Job is waiting for resources; R RUNNING: Job has the allocated resources and is running; S SUSPENDED: Job has the allocated resources, but execution has been suspended. A complete list of job state codes is available here. To check on your job's status $ sstat <jobid For detailed information $ scontrol show job <jobid To request an estimate of when your pending job will run $ squeue start j <jobid Canceling a Job Slurm provides the scancel command for deleting jobs from the system using the job identification number: $ scancel 18341 If you did not note the job identification number (JOBID) when it was submitted,"
rc-website-fork/content/userinfo/hpc/slurm.md,"you can use squeue to retrieve it. To cancel all of your jobs $ scancel u mst3k To cancel all of your pending jobs $ scancel t PENDING u mst3k To cancel all jobs with a specified name $ scancel name myjob To restart (cancel and rerun) $ scontrol requeue <jobid For further information about the squeue command, type man squeue on the cluster frontend machine or see the Slurm Documentation. Job Arrays A large number of jobs can be submitted through one request if all the files used follow a strict pattern. For example, if input files are named input1.dat, ... , input1000.dat, we could write a job script requesting the appropriate resources for a single one of these jobs with {{< pullcode file=""/static/scripts/jobarray.slurm"" lang=""nohighlight"" }} In the output file name, %a is the placeholder for the array ID. We submit with $ sbatch array=11000 myjob.sh The system automatically submits 1000 jobs, which will all appear under a single job ID with separate array IDs. The SLURMARRAYTASKID environment variable can be used in your command lines to label individual subjobs. The placeholder %A stands for the overall job ID number in the SBATCH preamble lines, while %a represents the individual task number. These variables can be used with the output option. In the body of the script you can use the regular environment variable SLURMTASKID if you wish to differentiate different job IDs and SLURMARRAYTASKID for the jobs within the array. To submit a range of task IDs with an interval $ sbatch array=11000:2 To submit a list of task IDs $ sbatch array=1,3,9,11,22 Using Files with Job Arrays For more complex commands, you can prepare a file containing the text you wish to use. Your job script can read the file line by line. In the following example,"
rc-website-fork/content/userinfo/hpc/slurm.md,"you must number your subtasks starting from 1 sequentially. You must prepare the optionsfile.txt in advance and each line must be the options you wish to pass to your program. {{< pullcode file=""/static/scripts/jobarrayfiles.slurm"" lang=""nohightlight"" }} The double quotes and curly braces are required. Canceling Individual Tasks in an Array One task $ scancel <jobid<taskid A range of tasks $ scancel <jobid[<taskid1<taskid2] A list of tasks $ scancel <jobid[<taskid1,<taskid2,<taskid3] Specifying Job Dependencies With the sbatch command, you can invoke options that prevent a job from starting until a previous job has finished. This constraint is especially useful when a job requires an output file from another job in order to perform its tasks. The dependency option allows for the specification of additional job attributes. For example, suppose that we have two jobs where job2 must run after job1 has completed. Using the corresponding Slurm command files, we can submit the jobs as follows: sbatch job1.slurm Submitted batch job 18375 sbatch dependency=afterok:18375 job2.slurm Notice that the dependency has its own condition, in this case afterok. We want job2 to start only after the job with id 18375 has completed successfully. The afterok condition specifies that dependency. Other commonlyused conditions include the following: after: The dependent job is started after the specified jobid starts running; afterany: The dependent job is started after the specified jobid terminates either successfully or with a failure; afternotok: The dependent job is started only if the specified jobid terminates with a failure. More options for arguments of the dependency condition are detailed in the manual pages for sbatch found here or by typing man sbatch at the Linux command prompt. We also are able to see that a job dependency exists when we view the job status listing, although the explicit dependency is not stated, e.g.: Job Accounting"
rc-website-fork/content/userinfo/hpc/slurm.md,"Data When submitting a job to the cluster for the first time, the walltime requirement should be overestimated to ensure that Slurm does not terminate the job prematurely. After the job completes, you can use sacct to get the total time that the job took. Without any specified options, the sacct command provides a display which is similar to the following: To include the total time, you will need to customize the output by using the format options. For example, the command % sacct format=jobID format=jobname format=Elapsed format=state yields the following display: The Elapsed time is given in hours, minutes, and seconds, with the default format of hh:mm:ss. The Elapsed time can be used as an estimate for the amount of time that you request in future runs; however, there can be differences in timing for a job that is run several times. In the above example, the job called python took 21 minutes, 27 seconds to run the first time (JobID 18352.0) and 16 minutes, 8 seconds the last time (JobID 18353.2). Because the same job can take varying amounts of time to run, it would be prudent to increase Elapsed time by 10% to 25% for future walltime requests. Requesting a little extra time will help to ensure that the time does not expire before a job completes Sample Slurm Command Scripts In this section are a selection of sample Slurm command files for different types of jobs. For more details on running specific software packages, please see the software pages. Basic Serial Program This example is for running your own serial (singlecore) program. It assumes your program is in the same folder from which your job script was submitted. {{< pullcode file=""/static/scripts/simpleserialjob.slurm"" lang=""nohighlight"" }} MATLAB This example is for a serial (one core) Matlab job. {{< pullcode"
rc-website-fork/content/userinfo/hpc/slurm.md,"file=""/static/scripts/simplematlabjob.slurm"" lang=""nohighlight"" }} Python This script runs a Python program. {{< pullcode file=""/static/scripts/simplepythonjob.slurm"" lang=""nohighlight"" }} R This is a Slurm job command file to run a serial R batch job. {{< pullcode file=""/static/scripts/simpleRjob.slurm"" lang=""nohighlight"" }} Job Scripts for Parallel Programs {jobsusingagpu} Distributed Memory Jobs If the executable is a parallel program using the Message Passing Interface (MPI), then it will require multiple processors of the cluster to run. This information is specified in the Slurm nodes resource requirement. The script mpiexec is used to invoke the parallel executable. This example is a Slurm job command file to run a parallel (MPI) job using the OpenMPI implementation: {{< pullcode file=""/static/scripts/mpijob.slurm"" lang=""nohighlight"" }} In this example, the Slurm job file is requesting two nodes with sixteen tasks per node (for a total of thirtytwo processors). Both OpenMPI and IntelMPI are able to obtain the number of processes and the host list from Slurm, so these are not specified. In general, MPI jobs should use all of a node so we'd recommend ntaskspernode=40 on the parallel partition, but some codes cannot be distributed in that manner so we are showing a more general example here. Slurm can also place the job freely if the directives specify only the number of tasks. In this case do not specify a node count. This is not generally recommended, however, as it can have a significant negative impact on performance. {{< pullcode file=""/static/scripts/mpijobfreeplacement.slurm"" lang=""nohighlight"" }} MPI over an odd number of tasks {{< pullcode file=""/static/scripts/mpijoboddnumber.slurm"" lang=""nohighlight"" }} Threaded Jobs (OpenMP or pthreads) Slurm considers a task to correspond to a process. Specifying a number of cpus (cores) per node ensures that they are on the same node. Slurm does not set standard environment variables such as OMPNUMTHREADS or NTHREADS, so the script must transfer that information explicitly."
rc-website-fork/content/userinfo/hpc/slurm.md,"This example is for OpenMP: {{< pullcode file=""/static/scripts/openmpjob.slurm"" lang=""nohighlight"" }} Hybrid The following example runs a total of 32 MPI processes, 8 on each node, with each task using 5 cores for threading. The total number of cores utilized is thus 160. {{< pullcode file=""/static/scripts/hybridjob.slurm"" lang=""nohighlight"" }} GPU Computations {gpuintensivecomputation} The gpu queue provides access to compute nodes equipped with RTX2080Ti, RTX3090, A6000, V100, A100, and H200 NVIDIA GPU devices. {{< highlight }} In order to use GPU devices, the jobs must to be submitted to the gpu partition and must include the gres=gpu option. {{< /highlight }} {{< pullcode file=""/static/scripts/gpujob.slurm"" lang=""nohighlight"" }} The second argument to gres can be rtx2080, rtx3090, v100, a100, or h200 for the different GPU architectures. The third argument to gres specifies the number of devices to be requested. If unspecified, the job will run on the first available GPU node with a single GPU device regardless of architecture. Two models of NVIDIA A100 GPUs are available; 2 nodes with 40GB of GPU memory per GPU, and 18 nodes with 80GB of memory per GPU. To make a specific request for an 80GB A100 node, please add a constraint to the Slurm script: $ sstat j format=AveCPU,MaxRSS AveCPU MaxRSS 7607:19:+ 95534696K Includes fund 000 (mst3klab) Generated on Mon Aug 31 00:00:00 2020. Reporting fund activity from 20200801 to Now. Beginning Balance: 100000.000 Ending Balance: 99000.000 Debit Summary User Jobs SUs mst1k 500 500.000 mst2k 200 300.000 mst3k 100 200.000 End of Report ` NonPI Regular users can run the previous command, but it will only show your own usage. You may use the sreport command for the total CPU time of your group members: $ sreport cluster UserUtilizationByAccount Start=20200801 Accounts=<yourallocation t Hours Note that this may be different from the actual allocation usage, since sreport"
rc-website-fork/content/userinfo/hpc/slurm.md,"is unaware of our SU charge policy, but can serve as an estimate. To find out how many Service Units (SUs) a specific job has consumed, users can run the following command. Here the value under the Amount column shows the amount of SUs consumed. The timeframe can be controlled using the s(starting time) and e(end time) flags. $ mamlisttransactions a <allocationname s 20241101 e 20241203 s:starting date e: end date Documentation"
rc-website-fork/content/userinfo/hpc/access.md,"{{< formcookies }} Compute time on Rivanna/Afton is available through two service models. Service Unit (SU) Allocations. One SU corresponds to one corehour. Multiple SUs make up what is called an SU allocation (e.g., a new allocation = 1M SUs). Dedicated Computing. This model allows researchers to lease hardware managed by Research Computing (RC) as an alternative to purchasing their own equipment. It provides dedicated access to HPC resources with no wait times. Below, you’ll find information on eligibility for access, account creation, and the various types of SU allocations along with their pricing. PI Eligibility {{% pieligibility %}} Account Creation Each PI should create his/her own Grouper group using the ITS Group Management Service. New groups will require two owners who hold active roles at UVA, as well as a third departmental owner. The PI may designate one or more group administrators but must remain a member of the group. Collaborators with UVA Eservices accounts, regardless of status, can be added to the Grouper group once it has been created. (Collaborators outside of UVA must request a temporary, sponsored Eservices account.) Grouper group names should consist of lowercase letters, digits, or underscores only and must begin with a letter. Please do not use spaces in the group name. {{% highlight %}} Whether you need to set up a new group, modify a group or access the legacy MyGroups groups, go to Grouper which requires VPN connection. For new groups, specify ""This group will be used for Rivanna/Afton access"" in the description section of the Service Now request form to expedite group creation. Please add yourself as a member to the group in order for us to fulfill any allocation request related to this group. {{% /highlight %}} Each PI is ultimately responsible for managing the roster of users in"
rc-website-fork/content/userinfo/hpc/access.md,"his/her group although PIs may delegate daytoday management to one or more other members. When users are added or deleted, accounts are automatically created. Group owners will be required to perform an annual attestation of group membership. If group owners do not complete attesting to the validity of their group, the members will be automatically removed from the group. Manage Grouper SU Allocations Pricing {{< pricing allocations}} Types {allocationtypes} Standard Allocations Standard allocations require a brief summary of the research project along with an explanation of the computations to be performed. Standard allocations must be renewed annually along with a synopsis of results from the original allocation. There cannot be more than 1 PI per Grouper group. Standard allocations expire 12 months after they are disbursed. Available to: Eligible PIs Request New / Renew Standard Allocation Allocation Purchases Time on Rivanna and Afton can also be purchased using an FDM. Purchasers are given a higher priority in the queue and their SUs never expire. iAs an alternative to purchasing SU's, RC offers dedicated computing which allows researchers to request exclusive access to a subset of HPC nodes for extended periods. See below for more information. Available to: Eligible PIs who need priority access and premium service. Purchase an Allocation Instructional Allocations Instructional allocations provide limited access to Rivanna and Afton and are available to UVA instructors who are teaching a class or leading a training session. Faculty who wish to request an instructional allocation should choose a Grouper account name using the class rubric, e.g. cs5014. Service units will be automatically purged 2 weeks after the class ends unless the instructor requests an extension. Instructors are required to submit a fresh instructional allocation request—either a new request or a renewal request—at the start of each semester. Available to: Faculty who"
rc-website-fork/content/userinfo/hpc/access.md,"intend to use HPC resources in their class. Read the full policy and guide for instructors. Request an Instructional Allocation Dedicated Computing {dedicatedcomputing} Dedicated computing is an alternative to selfmanaged lab systems and condominium nodes. This option provides researchers with exclusive access to HPC resources without wait times, eliminating the need for RC to manage the lifecycle of hardware purchased by researchers. Dedicated Computing involves nodes that RC has procured as part of its largescale HPC acquisitions being “leased” to researchers for a term of one year or longer. These leased nodes are configured with the same system image as the primary HPC environment, ensuring consistency and minimizing support overhead. Once the lease term ends, dedicated nodes are returned to the public queues, making them available for general HPC use. Available to: Eligible PIs who need exclusive access to a subset of HPC nodes for extended periods. Request Dedicated Computing Pricing {{< pricing dedicatedcomputing }}"
rc-website-fork/content/userinfo/hpc/_index.md,"{{% callout %}} {{< getallocationblurb name=""Afton"" }} {{% /callout %}} {{% callout %}} {{< getallocationblurb name=""Rivanna"" }} {{% /callout %}} {{< systemsboilerplate }} {{< lead }} The sections below contain important information for new and existing Rivanna and Afton users. Please read each carefully. {{< /lead }} {{< lead }} New users are invited to attend one of our free orientation sessions (""Introduction to HPC"") held throughout the year. {{< /lead }} Sign up for an ""Intro to HPC"" session Get Started Access / Allocations Learn how to request an allocation and add collaborators. Request an Allocation Logging In Log in through a Web browser or a commandline tool. Learn More File Transfer Moving files between Rivanna/Afton and other systems. Learn More Software See a listing of available software. Learn More Storage Options for free shortterm and leased longterm storage Learn More Running Jobs in Slurm Submitting jobs to Rivanna/Afton through the Slurm resource manager Learn More Job Queues Determine the best queue (or “partition”) for running your jobs. Learn More Usage Policies Understand the terms and conditions for using Rivanna/Afton. Learn More FAQs Frequently Asked Questions. Rivanna and Afton FAQ Overview A high performance computing cluster is typically made up of at least four service layers: Login nodes Where you log in, interact with data and code, and submit jobs. Compute nodes Where production jobs are run. On Rivanna and Afton these nodes are heterogeneous; some have more memory, some have GPU devices, and so forth. Partitions are homogeneous so you can select specialty hardware by your partition request, sometimes along with a resource request (gres). Storage Where files are stored, accessible by all nodes in the cluster. Resource Manager A software system that accepts job requests, schedules the jobs on a node or set of nodes, then manages"
rc-website-fork/content/userinfo/hpc/_index.md,"their execution. Click on elements of the image to learn more: System Details Hardware Configuration {hardwareconfiguration} {{< newrivannaspecs }} {{< systemsboilerplate }} Job Queues Rivanna and Afton are managed resources; users must submit jobs to queues controlled by a resource manager, also known as a queueing system. The manager in use on Rivanna and Afton is Slurm. Slurm refers to queues as partitions because they divide the machine into sets of resources. There is no default partition and each job must request a specific partition. Partitions and access policies are subject to change, but the following table shows the current structure. Note that memory may be requested per core or for the overall job. If the total memory required for the job is greater than the number of cores requested multiplied by the maximum memory per core, the job will be charged for the additional cores whether they are used or not. In addition, jobs running on more than one core may still require a request of total memory rather than memory per core, since memory per core is enforced by the system but some multicore software packages (ANSYS, for example) may exceed that for a short time even though they never exceed cores x memory/core. {{< newrivannaqueue }} Remarks standard maximum aggregate CPU cores allowed for a single user’s running jobs is 1000. parallel requires at least 2 nodes and 4 CPU cores. Slurm's default memory unit is in MB. Different units may be specified, e.g. mem=100G, where 1G = 1024M. The gpu partition is dedicated to jobs that can utilize a general purpose graphics processing unit (GPGPU). In Slurm scripts you must request at least one GPU device through gres=gpu. Jobs that do not utilize any GPUs are not allowed in this partition. interactive maximum aggregate CPU cores"
rc-website-fork/content/userinfo/hpc/_index.md,"(GPUs) is 24 (2) for a single user. The NVIDIA DGX BasePOD and HGX H200 GPU nodes offer highperformance GPUs that bring new AI and ML functionality to support parallel GPU computing and large deeplearning models. Currently, H200 nodes are not accessible through Open OnDemand and can only be utilized via batch job submissions. Learn More Usage Policies Research computing resources at the University of Virginia are for use by faculty, staff, and students of the University and their collaborators in academic research projects. Personal use is not permitted. Users must comply with all University policies for access and security to University resources. The HPC system has additional usage policies to ensure that this shared environment is managed fairly to all users. UVA's Research Computing (RC) group reserves the right to enact policy changes at any time without prior notice. Login Nodes Exceeding the limits on the login nodes (frontend) will result in the user’s process(es) being killed. Repeated violations will result in a warning; users who ignore warnings risk losing access privileges. Scratch Directory {{% scratchpolicy %}} Software Licenses Excessive consumption of licenses for commercial software, either in time or number, if determined by system and/or RC staff to be interfering with other users' fair use of the software, will subject the violator's processes or jobs to termination without warning. Staff will attempt to issue a warning before terminating processes or jobs but inadequate response from the violator will not be grounds for permitting the processes/jobs to continue. Inappropriate Usage Any violation of the University’s security policies, or any behavior that is considered criminal in nature or a legal threat to the University, will result in the immediate termination of access privileges without warning. Acceptable Use of the University’s Information Technology Resources According to UVA policy, users are prohibited"
rc-website-fork/content/userinfo/hpc/_index.md,"from downloading or using applications such as TikTok, WeChat, DeepSeek, and similar software on any RC resources. For more details, please see here."
rc-website-fork/content/userinfo/storage/personal-computing.md,"Box® Nonsensitive cloud storage UVA Box is a cloudbased storage and collaboration service that gives eligible members of the University community the ability to access, store, and share up to 1 TB of public / internal use University files securely—anywhere, anytime, on any device. Read more DropBox®/Sookasa® Highly Sensitive Data (PHI/PII) storage If you plan on storing highly sensitive data such as PHI or PII, UVA Health System offers a secure encrypted storage for Health System affiliated researchers, students, and staff. ""DropBox Sookasa"" is a free cloudbased service hosted on Dropbox that can be accessed over the internet on any device. Highly sensitive data such as a HIPAAcompliant dataset or PHI/PII must be stored in a Sookasa folder. It can be used to share files between Health System users. Requesting your DropBox account You have to make a request to Health System IT in order to gain access to the UVA HS DropBox Sookasa service. Click here to access the request form on the HIT website Login using your Health System username and password Click on the green colored ""Account Request"" icon Underneath the System menu select ""Dropbox"" Underneath the Role menu select the ""Standard Account"" Fill in your computing ID, and HS email address. You may check ""smart select"" to verify if your ID is properly entered Click ""Add to Cart"" Click ""Checkout"" after entering any comments about your request Read more CrashPlan® Nonsensitive cloud desktop backup CrashPlan is a cloudbased desktop backup service. It securely backs up your endpoint devices to the cloud. CrashPlan provides: Cloud storage for backup of up to 4 endpoint devices per user Protection against cryptoransomware and other malicious software that destroys/encrypts content on enduser’s devices Protection of University data on endpoint devices from loss due to hard drive failure, computer failure, etc. CrashPlan"
rc-website-fork/content/userinfo/storage/personal-computing.md,"is currently offered at no cost to the University community until June 30, 2018. After that, the cost model/fee structure will be determined for continued use of the service. During this initial phase, the system has a peruser quota of 250GB. If you need more space and have a valid use case, please contact ITS via the link below. Read more"
rc-website-fork/content/userinfo/storage/research-standard.md,"Overview The Research Standard Storage file system provides users with a solution for research data storage and sharing. Public, internal use, and sensitive research data can be stored in Research Standard storage, and UVA Information Security provides details about data sensitivity classifications. Members in the same group have access to a shared directory created by the team lead or PI. Group membership can be defined and managed through Grouper (requires VPN connection). Research Standard storage is mounted on the HPC cluster and can also be accessed on a personal computer with an SMB mount, allowing for pointandclick file manipulation. As of July, 2024, Each PI with a Research Computing account will have up to 10 TB of Research Standard Storage at no charge. If you are not a researcher, UVA ITS offers Value storage for longterm storage of large scale data. More information about ITS's various storage options can be found on their website. Request Research Standard Storage Research Standard storage can be requested for $45/TB/YR through our Storage Request Form. Users can specify the size and name of the Research Standard storage directory and the name of an existing Grouper group that can access the space. If the Grouper group does not yet exist, please create one through the ITS Group Management System before filling out the Research Standard storage request form. When your Research Standard Storage share is created, you will receive an email detailing your NFS mount standard.hpc.virginia.edu:vol, where refers to the specific volume number, and the SMB map \\standard.hpc.virginia.edu\Groupergroupname. Once the space is available, the PI can grant access to lab member by adding them to the Grouper group. Users in the Grouper group will see the directory (/standard/Sharedspacename)` after logging into UVA HPC. {{% groupcreationtip %}} Drive Mapping with Research Standard Storage Research Standard storage"
rc-website-fork/content/userinfo/storage/research-standard.md,"can be drive mapped on a personal computer to enable draganddrop file manipulation and transfer between your PC and your value storage share. Detailed instructions for mapping network drives on Windows and Mac machines can be found on the UVa Research Computing HowTo pages. File Manipulation and Navigation with Research Standard Storage Research Standard storage is based on a Linux file system similar to storage spaces on the cluster, including /home and /scratch. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd). If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below: A Gentle Introduction 10 Essential Linux Commands How To Manage Files From The Linux Terminal Shell Novice For more help, please feel free to contact RC to set up a consultation or visit us during office hours."
rc-website-fork/content/userinfo/storage/data-transfer.md,"Data transfer Public & Moderately Sensitive Data Transfer Secure Copy (scp) scp uses secure shell (SSH) protocol to transfer files between your local machine and a remote host. scp can be used with the following syntax: scp [source] [destination] scp SourceFile mst3k@login.hpc.virginia.edu:/scratch/mst3k scp SourceFile mst3k@login.hpc.virginia.edu:/project/Groupergroupname Detailed instructions and examples for using scp are listed here. Secure File Transfer Protocol (sftp) sftp is a network protocol for secure file management. Instructions and examples for using sftp are located here. Graphical FileTransfer Applications Filezilla and Cyberduck, and MobaXterm are examples of open source SFTP client software for file management through an interactive graphical user interface. Instructions for using these SFTP clients can be found here. Globus Connect (Large Data Transfer) Globus provides access to data on local machines and HPC file systems, as well as external institutions and facilities. Globus is well suited for transferring both small files and large amounts of data. More information on Globus data transfer can be found here. Public & Moderately Sensitive Data Storage Systems /home, /scratch, and /project storage are based on a Linux file system. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd). If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below: A Gentle Introduction 10 Essential Linux Commands How To Manage Files From The Linux Terminal Shell Novice"
rc-website-fork/content/userinfo/storage/research-project.md,"Overview The Research Project Storage file system provides users with a collaborative space for data storage and sharing. Public, internal use, and sensitive research data can be stored in Research Project storage, and UVA Information Security provides details about data sensitivity classifications. Members in the same group have access to a shared directory created by the team lead or PI. Group membership can be defined and managed through Grouper (requires VPN connection). /project storage is mounted on the HPC cluster and runs on a new scaleout NAS file system. If you are not a researcher, UVA ITS offers Value storage for longterm storage of large scale data. More information about ITS's various storage options can be found on their website. Request Research Project Storage Research Project storage can be purchased for 70$/TB/YR through our Storage Request Form. When filling out the form, the PI can specify the size and name of the Research Project storage directory and the name of an existing or new Grouper group that can access this space. We recommend choosing a Grouper group name specific to your group or collaboration for the Research Project storage directory. This will reduce confusion in the future if you manage multiple Grouper groups and directories on other storage systems. Once the request has been submitted, the PI will receive a notification that the /project space has been provisioned within 24 hours. Once the space becomes available, the PI can grant access to lab members by adding them to the Grouper group. Users in the Grouper group will see the directory (/project/Sharedspacename) after logging into UVA HPC. {{% groupcreationtip %}} Drive Mapping with Research Project Storage Research Project storage can be drive mapped on a personal computer to enable draganddrop file manipulation and transfer between your PC and your value storage"
rc-website-fork/content/userinfo/storage/research-project.md,"share. Detailed instructions for mapping network drives on Windows and Mac machines can be found on the UVa Research Computing HowTo pages. File Manipulation and Navigation with Research Project Storage Research Project storage is based on a Linux file system similar to storage spaces on the cluster, including /home and /scratch. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd). If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below: A Gentle Introduction 10 Essential Linux Commands How To Manage Files From The Linux Terminal Shell Novice"
rc-website-fork/content/userinfo/storage/storage-options.md,"UVA Research Computing provides multitiered storage solutions for your data. From smallscale personal computing options to highperformance parallel file systems for serious computational runs, various systems are available to researchers. LargeScale Research Data Storage UVa offers a number of institutional solutions for storing and managing largescale research data. Each of these can serve different usecases depending on budget and archival needs. All of these systems are mounted and visible to local highperformance computing resources. Learn more Cloud Storage Solutions Longterm and highcapacity storage solutions are also available in Amazon Web Services and Google Cloud Platform. Both provide object storage for files of any type, and lowcost archival storage (comparable to tape backups). Learn more NonResearch (Enterprise) Data Storage UVa ITS provides a tiered catalog of data storage options for data not intended to be analyzed in research projects. Examples of appropriate data could include administrative records or departmental information. Learn more Commercial Data Sharing and Archiving Solutions There are a number of commercially licensed tools available to UVa researchers for free. These products, including UVa Box, Dropbox (Health System) and CrashPlan, are most suitable for smallscale storage needs. Box Dropbox (HS Only) CrashPlan"
rc-website-fork/content/userinfo/storage/data-sensitivity.md,"StandardSecurity Zone (SSZ) HighSecurity Zone (HSZ) Storage Computing Environments Storage Computing Environments Data Classification Research Project (/project) Research Standard (/standard) Rivanna/Afton (/home & /scratch) HighSecurity Research Standard Ivy VM (/home) Rio (/home & /scratch) ACCORD Public ✅ ✅ ✅ ✅ ✅ ✅ ✅ InternalUse ✅ ✅ ✅ ✅ ✅ ✅ ✅ Sensitive ✅ ✅ ✅ ✅ ✅ ✅ ✅ HighlySensitive ❌ ❌ ❌ ✅ ✅ ✅ ❌ Limited Dataset 1 ❌ ❌ ❌ ✅ ✅ ✅ ❌ DeIdentified Dataset 2 ✅ ✅ ✅ ✅ ✅ ✅ ✅ HIPAA 3 ❌ ❌ ❌ ✅ ✅ ✅ ❌ CUI 4 ❌ ❌ ❌ ✅ ✅ ❌ ❌ ControlledAccess Data 5 ❌ ❌ ❌ ✅ ✅ ✅ ❌ FERPA 6 ❌ ❌ ❌ ✅ ✅ ✅ ✅ ITAR 7 ❌ ❌ ❌ ✅ ✅ ❌ ❌ PCIDSS 8 ❌ ❌ ❌ ❌ ❌ ❌ ❌ 1 Limited datasets have direct identifiers removed, but may contain indirect identifiers including, complete dates, age, city, state, and complete ZIP code. 2 Deidentified datasets contain no identifiers. Note: identifiers can be recoded such that the source information is anonymized (e.g. date shifting, urban/rural determinations vs. ZIP codes, randomly generated subject identifier, etc.) 3 Health Insurance Portability and Accountability Act (HIPAA). Information protected under HIPAA includes any protected health information (PHI) in the medical record that can identify an individual. More information can be found here. 4 Controlled Unclassified Information (CUI). CUI data is information the government creates or possesses that requires safeguarding or dissemination controls when handling. More information can be found here. 5 Controlledaccess data are protected NIH data whose access is controlled by implementing security measures to verify the identity of requesters and their inteded data use, even if it is deidentified or lacks explicit limitations on subsequent use. This includes controlledaccess data downloaded from"
rc-website-fork/content/userinfo/storage/data-sensitivity.md,"the following controlledaccess data repositories: Database of genotypes and phenotypes (dbGaP), BioData Catalyst, NCI Genomic Data Commons, ‌‌The NHGRI Genomic Data Science Analysis, Visualization, and Informatics LabSpace (AnVIL), National Institute of Mental Health Data Archive (NDA), NIA Genetics of Alzheimer's Disease Data Storage Site (NIAGADS). The full list of controlledaccess repositories can be found here. Projects with a data use agreements approved after 1/25/25 are required to protect controlledaccess data acquired from a controlledaccess repository in compliance with NIST 800171 security controls. More information can be found here. 6 Family Educational Rights & Privacy Act (FERPA). FERPA is a federal law that governs access to student education records. This includes personally identifiable information (PII) like name, SSN, date of birth, grades, and course schedules. More information can be found here. 7 International Traffic in Arms Regulations (ITAR). This includes military technology and software, technical data, and services. More information can be found here. 8 Payment Card Industry Data Security Standards (PCIDSS). PCIDSS is a set of security standards that define how payment card data should be protected. This includes data containing debit card or credit card information. More details can be found here."
rc-website-fork/content/userinfo/storage/cloud.md,"Amazon Web Services Tiered object storage Amazon S3 and Glacier offer cloudbased, affordable, unlimited capacity for storage from anywhere. Advanced features include scalability, lifecycle management, encryption, and sharing. S3 is ideal for static files that need to be retrieved from any location (PDFs, images, video, etc.). Glacier is archival storage, perfect for grant compliance that requires data retention. How RC can help: Lower pricing UVA has an Internet2 discount available for educational use. Contact us to create an account for you or your research project. Cost estimates Cloud storage is not free. Consideration should be made to the size of your files and how often they will be retrieved. We can help estimate storage costs in AWS. Security We can assist you in keeping your files private, or sharing with the appropriate parties. We can also help you understand encryption models available in S3/Glacier. Management Automated lifecycle management is available in either service. We can help you set up policies to move data from S3 to Glacier after N days or years, or to delete after the appropriate length of time. Access S3 and Glacier can be accessed via a web UI, commandline, or other thirdparty tools. RC can discuss the appropriate tools for your users. Training Attend one of our free cloud workshops, or receive inperson training from our staff to understand additional features available. Learn more Google Cloud Storage Tiered data storage Google Cloud Storage is a tiered storage service that allows for worldwide storage and retrieval of any amount of data at any time. You can use Google Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download. How RC can help: Lower pricing UVA has an Internet2 discount"
rc-website-fork/content/userinfo/storage/cloud.md,"available for educational GCP accounts. Contact us to create an account for you or your research project. Cost estimates Cloud storage is not free. Consideration should be made to the size of your files and how often they will be retrieved. We can help estimate storage costs in AWS. Security We can assist you in keeping your files private, or sharing with the appropriate parties. We can also help you understand encryption models available in Google Cloud Storage. Management Automated lifecycle management is available in Cloud Storage. This allows for the automated movement of files to another tier after N days or years, or to delete after the appropriate length of time. Access Google Cloud Storage can be accessed via a web UI, commandline, or other thirdparty tools. RC can discuss the appropriate tools for your users. Training Attend one of our free cloud workshops, or receive inperson training from our staff to understand additional features available. Learn more"
rc-website-fork/content/userinfo/storage/non-sensitive-data.md,"/home /home is a free 50GB space provided to users of the HPC system and is visible from the login and compute nodes. /home is the default working directory when logging on to UVA HPC. Users can also access their home directory at /home/$USER, where $USER is an individual's UVa computing ID. /scratch /scratch is a Lustre high performance parallel filesystem accessible via the login and compute nodes. {{% callout %}} {{% scratchpolicy %}} {{% /callout %}} How to request /home and /scratch space /home and /scratch space can be obtained by requesting an allocation on UVA HPC. The process of getting access to Rivanna is described here. Research Project Storage The Research Project Storage file system provides users with a collaborative space for data storage and sharing. Members in the same group have access to a shared directory created by the team lead or PI. Group membership can be defined and managed through the Grouper (requires VPN connection). /project storage is mounted on the HPC cluster and runs on a new scaleout NAS file system. How to request /project storage space /project storage can be purchased for {{% storagepricing project %}}/TB/YR by using this form. When filling out the form, the PI can specify the size of the /project directory and the name of an existing or new Grouper group that can access this space. We recommend choosing a Grouper group name specific to your group or collaboration for the /project directory. This will reduce confusion in the future if you manage multiple Grouper groups and directories on other storage systems. Once the request has been submitted, the PI will receive a notification that the /project space has been provisioned within 24 hours. Once the space becomes available, the PI can grant access to lab members by adding them"
rc-website-fork/content/userinfo/storage/non-sensitive-data.md,"to the Grouper group. Users in the Grouper group will see the directory (/project/Groupergroupname) after logging into UVA HPC. Addition and removal of users is managed by the PI of the group. {{% groupcreationtip %}} Public & Moderately Sensitive Data Storage Systems {publicmoderatelysensitivedatastorage} /home, /scratch, and /project storage are based on a Linux file system. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd). If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below: A Gentle Introduction 10 Essential Linux Commands How To Manage Files From The Linux Terminal Shell Novice"
rc-website-fork/content/userinfo/storage/sensitive-data.md,"Overview Residing within the High Security Zone (HSZ), the Ivy secure computing environment is designed to fit your highly sensitive research computing needs and meets HIPAA, FERPA, CUI and ITAR compliance standards. Within the HSZ, researchers can store their highly sensitive research data in HighSecurity Research Standard Storage. Ivy Central Storage {ivycentralstorage} Ivy Central Storage (ICS) was an HSD parking zone and central storage pool with a capacity greater than 1PB. This storage space was available for researchers with highly sensitive data and could be mounted on an Ivy Virtual Machine. As of 10/15/24, ICS will be upgraded to HighSecurity Research Standard Storage. HighSecurity Research Standard Storage {hsstandardstorage} HighSecurity Research Standard Storage is an HSD storage area within the HSZ with a capacity greater than 6PB. HighSecurity Research Standard Storage is similar to Research Standard Storage, however it is integrated with the HighSecurity Data Transfer Node and mounted on an Ivy virtual machine (VM) to create a highly secure environment. For added security, files stored on HighSecurity Research Standard Storage are read & write only. Note: snapshots, backup, and replication are not provided. Researchers can request space on HighSecurity Research Standard Storage by first requesting an Ivy account using the Ivy request form. Researchers are granted 1TB of space at nocost, and additional space can be requested in 1TB increments using our Storage Request form. Data Transfer to Ivy To ensure that files are always secure, data can only be transferred to Ivy through the HighSecurity Data Transfer Node using Globus Connect. Globus provides access to data on local machines and HSZ storage. Data can then be moved between HSZ storage as needed. Globus is well suited for transferring both small files and large amounts of data. More information on Globus data transfer can be found here. Sensitive Storage Data"
rc-website-fork/content/userinfo/storage/sensitive-data.md,Transfer High level Overview
rc-website-fork/content/userinfo/omero/_index.md,"{{% callout %}} OMERO is a database for management of imaging data. UVA is hosting a centralized OMERO database instance backed by centralized storage that facilitates sharing, processing and annotating images for your research group and invited collaborators. {{% /callout %}} Overview With the advent of highthroughput screening, the need for efficient image management tools is greater than ever. From the microscope to publication, OMERO is a database solution that handles all your images in a secure central repository. You can view, organize, analyze and share your data from anywhere you have internet access. Work with your images from a desktop app (Windows, Mac or Linux), on UVA's high performance computing platform (Rivanna and Afton), from the web, or through 3rd party software like Fiji and ImageJ, Python, and MATLAB. OMERO is able to read over 140 proprietary file formats, including all major microscope formats. Getting Access OMERO accounts can be requested by submitting a OMERO request form. Only faculty members may submit a request. By default, all group members will be able to view their own and others' data. Group members can make annotations on each other's data, but cannot modify or delete another member's data. For details on obtaining more restrictive or flexible permissions for your group members, please read the Group/User Permissions section. Pricing: {{% storagepricing omero %}} / TB per year. Request Access to OMERO Connecting to the OMERO database Once you have an OMERO account you can log in and begin importing images. You can log into your OMERO account on your computer using the OMERO.insight desktop client, or you can use the OMERO web interface through your web browser. When off Grounds, you have to connect through the UVA VPN in order to access the OMERO database. We recommend to connect to the UVA"
rc-website-fork/content/userinfo/omero/_index.md,"More Secure Network if available. The UVA Anywhere VPN should only be used if the UVA More Secure Network is not available. Logging in with OMERO.insight Set up the Software on your Computer The UVA server is running OMERO 5.4.10. In order to connect to that server you need to install the compatible Windows, Mac, or Linux client. Windows: OMERO.insight5.4.10ice36b105win.zip Mac: OMERO.insight5.4.10ice36b105mac.zip Linux: OMERO.insight5.4.10ice36b105linux.zip Please download the .zip file appropriate for your computer and install the client following these installation instructions. Open OMERO.insight and follow the configuration instructions to set up the connection to the UVA OMERO server. Under step 3 of the instructions, enter omero.hpc.virginia.edu as the server address. The port address needs to be set to 4064. To log in, enter your computing ID (e.g. mst3k) in the Username field. For Password, enter the password emailed to you after your initial account request (you will be able to change this after logging in for the first time). The OMERO database password is not the same as your Eservices or Netbadge password. Click Login. Logging in with OMERO.web Go to http://omero.hpc.virginia.edu. Log in with your computing ID (e.g. mst3k) and the password that was emailed to you upon your initial account request. The OMERO database password is not the same as your Eservices or Netbadge password. Important things to note when using the OMERO web client interface: OMERO.web cannot be used to import images Tags cannot be created in OMERO.web Changing your OMERO Database Password The OMERO database password is not the same as your Eservices or Netbadge password. After logging into OMERO for the first time, it is highly recommended that you change your password. You can manage your account settings using either OMERO.insight or OMERO.web. Password Change in the OMERO.insight Desktop Client After logging into OMERO using"
rc-website-fork/content/userinfo/omero/_index.md,"the desktop app, click the Administration tab in the side menu. Click the arrow next to your lab/group account name, then click your own name to open the user account menu. Enter your new password in the New Password field and click the Change Password button. Password Change in the OMERO Web Client After logging in to OMERO in your web browser, click your name in the top right corner of the screen and then click User settings. Click the Change Password button. Enter your current password and your desired new password. Click OK when complete. If you cannot remember your password, please contact us through our support request webform. Our system administrators can reset the password for you. Group/User Permissions OMERO users can have one of two user roles, Group owner or Group member. As a Group owner of a lab/group account, you can edit the permissions of other users in your group. This can be done from the desktop app OMERO.insight. In the Administration tab of the sidebar menu, click the name of your group. This will open the Group settings menu. Select your desired permissions and click Save. There are several permissions settings, which are described below. By default, all group permissions are set to ReadAnnotate. Permission Type Description Private The group owner can view data of all group members but cannot add annotations. Regular group members can only view and annotate their own data. This permissions setting does not allow for much collaboration. ReadOnly The group owner can view data of all group members but cannot add annotations. Regular group members can only view and annotate their own data. This permissions setting does not allow for much collaboration. ReadAnnotate The group owner and group members can view and annotate each other's data. Regular members cannot modify"
rc-website-fork/content/userinfo/omero/_index.md,"or remove other members' images. [Default] ReadWrite The Group owner and Group members can view, annotate, modify, and delete each other's data. Image Analysis with OMERO OMERO is compatible with a variety of thirdparty image processing software packages. Using these OMEROsoftware bindings, you can import images from OMERO to your software such as Fiji or Python,and then process and analyze them as usual. You can then export any results or preprocessed images back to OMERO. Using OMERO to serve images to your analysis software has many benefits over more traditional methods of reading imaging data. With OMERO, there is no need to download the images directly to your local machine. ImageJ/Fiji Images managed by OMERO can be imported using the ImageJ/Fiji plugin for OMERO. Detailed instructions for installing and using the plugin can be found in OMERO's online documentation: https://help.openmicroscopy.org/imagej.html. An introduction with example scripts that demonstrate the basic concepts of writing Fiji scripts to interact with the OMERO database are described in our tutorial . MATLAB and Python You can install packages to connect to OMERO with MATLAB or Python. These packages include functions for connecting to the OMERO server and reading and exporting data. OMERO's online documentation for the OMERO MATLAB language bindings can be found here: https://docs.openmicroscopy.org/omero/5.5.0/developers/Matlab.html More information on the OMERO Python language bindings can be found here: https://docs.openmicroscopy.org/omero/5.5.0/developers/Python.html. More indepth tutorials and sample scripts will be available on our workshop site soon!"
rc-website-fork/content/userinfo/linux/uva-anywhere-vpn-linux.md,"ITS does not support the UVA Anywhere VPN client on Linux. These instructions may work but they are provided for user information only. UVA RC does not support usage of the VPN on any platform. Setting up the VPN Install Software Prerequisites You must install some software using yum,dnf, or aptget. Note the slight difference in naming convention between distributions. Rocy/Alma/RedHat/Fedora These distributions need the following packages: openssl openconnect NetworkManageropenconnect NetworkManageropenconnectgnome Ubuntu The packages are the same but the names are different. Ubuntu 18.04 and up requires an additional package. openssl openconnect networkmanageropenconnect networkmanagergnome networkmanageropenconnectgnome It will be necessary for Network Manager to be able to manage the connection. Obtain a Certificate Go to this unpublicized Web location to obtain a certificate for n nonspecific OS. You will be required to sign in with Netbadge. Once authenticated, fill out the form. Your passphrase need not be related to your Netbadge password, and it must be 15 characters or fewer. The MAC address of your system is optional for UVA Anywhere. Click the link to download the certificate. You will receive a file ending in .p12. In this example we will assume it is named mst3k.p12. Do not click the Next button. Once the download is completed, you may close the tab for the certificate site. {{< figure src=""/images/linux/downloadcert.png"" alt=""downloadcert"" width=30% }} Configure with Network Manager Click the network app in your tray, or go to SettingsNetwork. Choose VPN and click the + to add a VPN. Select the Cisco Anyconnect compatible VPN option. Fill in the blanks for a new VPN. Please use the More Secure VPN if you have access to it. The gateway is moresecurevpn1.itc.virginia.edu. Otherwise, use the UVA Anywhere VPN whose gateway is uvaanywhere1.itc.virginia.edu as shown in the figure below. NetWork Manager may not recognize the format."
rc-website-fork/content/userinfo/linux/uva-anywhere-vpn-linux.md,"You can use the file manager of your desktop system to drag and drop the file into both the ""User Certificate"" and the ""Private Key"" boxes. Click ""Add."" In the Details tab, make sure that ""Make available to all users"" is not checked. This should be the default. Connecting to the VPN Start the VPN through the Network Manager, either through the applet in the tray (Ubuntu) or in the Notifications section of the taskbar (Rocky/Alma/Fedora). The state can be controlled through the right arrow. For the first connection, you may need to go through the Settings application to connect. After that, log out. When you log back in, your VPN should appear in the taskbar or tray (the illustration was taken from a Rocky Linux installation). More Secure VPN UPDATE: Users of the More Secure VPN will now be required to authenticate through Duo before connecting. When prompted for a password, enter the word push or PUSH (it is not casesensitive); you will then receive an approval notification on your mobile device. After approving the request, the client will connect to the VPN. Alternatively, you may enter a passcode generated by Duo as the password."
rc-website-fork/content/userinfo/howtos/_index.md,General General tips and tricks for computational research. General HowTos › Rivanna and Afton High Performance Computing platforms HPC HowTos › Ivy Secure Data Computing Platform Ivy HowTos › Storage Research Data Storage & Transfer Storage HowTos ›
rc-website-fork/content/userinfo/reference/bowtie.md,"{{< define Bowtie ""Bowtie 2 is an ultrafast and memoryefficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and pairedend alignment modes."" }} Overview Bowtie is a part of a suite of tools for pipeline processing: Bowtie: Ultrafast short read alignment CloudBurst: Sensitive MapReduce alignment Contrail: Cloudbased de novo assembly Crossbow: Genotyping, cloud computing Cufflinks: Isoform assembly, quantitation Lighter: Fast error correction Myrna: Cloud, differential gene expression Tophat: RNASeq splice junction mapper"
rc-website-fork/content/userinfo/reference/jq.md,"jq jq is like sed for JSON data you can use it to slice and filter and map and transform structured data with the same ease that sed, awk, grep and friends let you play with text. Installation Follow the instructions available on https://stedolan.github.io/jq/ for installing the latest version for your platform. Basic Usage jq is used to parse JSON, which helps with programmatic interaction with many APIs. For example, you can retrieve data from an open API like GitHub: curl 'https://api.github.com/repos/stedolan/jq/commits?perpage=5' And then pipe that output to jq to begin to parse the results. You can filter down to just the first record [0]: curl 'https://api.github.com/repos/stedolan/jq/commits?perpage=5' \ | jq '.[0]' And then begin to drill down to specific elements of the response hierarchy, building into a new structure: curl 'https://api.github.com/repos/stedolan/jq/commits?perpage=5' \ | jq '.[0] | {message: .commit.message, name: .commit.committer.name}' which results in this response: { ""message"": ""Merge pull request 162 from stedolan. Closes 161"", ""name"": ""Stephen Dolan"" } jq is extremely useful alongside the AWS commandline tools. Or, if you wanted to grab a series of values from all entries you retrieved, you could filter into an array. Here are the sha values for 5 recent commits by the user stedolan: curl 'https://api.github.com/repos/stedolan/jq/commits?perpage=5' | jq r .[].sha which results in this output: dc679081fa770c260ca9a569a8a4fdbb10bcdc20 597c1f6667746058e88a9f6fb0415f80fe114b18 125071cf005e687d4beba9d5822b1c6a72d7d14c 2fb099e4cfe5a9fedd55a1ace44ae2c5ee02cb12 6f9646a44ff0046126f5a2c3010e92a974da7c48 Realworld examples Here are some bash snippets for various tasks that combine the AWS CLI with jq: {{< gist nmagee d13a67b82859fcef53acff568ecb114d }} Online Testing The authors of jq also provide https://jqplay.org/ as a space where you can interactively build and test your jq parsing."
rc-website-fork/content/userinfo/reference/rstudio-docker.md,"{{< define ""RStudio Server"" ""RStudio Server enables you to provide a browser based interface to a version of R running on a remote Linux server, bringing the power and productivity of the RStudio IDE to serverbased deployments of R. The instructions below will launch your RStudio environment locally within a Docker container."" }} What is Docker? {{< youtube aLipr7tTuA4 }} Install Docker Click the button below and download the appropriate Docker Edition for your platform. We suggest the CE ""Community Edition."" Download Docker Run RStudio Locally Run these two commands for a webbased deployment of RStudio Server on your local workstation: docker pull rocker/rstudio docker run d p 8787:8787 rocker/rstudio More Information Using the RStudio container Other related images Sharing Files between your Computer and the Container"
rc-website-fork/content/userinfo/reference/aws-bioinformatics.md,"Setting up computational infrastructure on AWS is a welldefined, though somewhat timeconsuming process. This introduction is designed to explain some of the terminology, define the steps required to set up AWS, and point the user to some excellent tutorials/resources created for bioinformatics and genomics. Setting up a server on AWS involves making decisions on the following broad categories. Set Up a Computational Server An excellent tutorial that covers the steps for creating an EC2 (Elastic Compute Cloud) instance (up to logging into your EC2) along with a myriad of questions such as pricing and what kind of computing and storage resources to choose from geared towards bioinformatics and genomics is available here. Broadly speaking, setting up a server (EC2 instance) requires the following steps. AWS user account: The user account can be your own, or you could be part of a group user account (e.g., your lab’s account on AWS). You (or the administrator of the account) can set up permissions for you, such as whether you are allowed to stop instances started by other members of the lab. AWS calls it an IAM Role. See here for AWS policies and signup procedures for user accounts. Amazon Machine Image (AMI): AMI is an operating system with or without predefined computational power, memory, hard disk, or software tools. If you start from scratch, you will choose all the elements stepbystep. We recommend using Ubuntu as your operating system as quite a few community bioinformatics tools are built for Ubuntu. At this time, NCBI BLAST is available as a community AMI on Amazon, and we expect more to be available over time. EC2 Instance: If you are building an AMI from scratch, you’ll have to choose the right processing power, memory, and network capabilities for the analyses required. Each operating system available"
rc-website-fork/content/userinfo/reference/aws-bioinformatics.md,"on AWS can come with slightly different processing and memory options, but for the most part the capabilities are comparable. As a start, an instance with 4 cores and 32 GB of memory should be reasonable for a lot of genomics analyses, though for large scale analyses you could need more computing power and storage space. Depending on your needs, you can buy more computational power from AWS. Elastic Block Storage (EBS): This is the equivalent of choosing how much hard disk (SSD) to add to your EC2. You can do that while specifying the EC2 or later on as your storage requirements increase or decrease over time. EBS is one of multiple storage options available with AWS, but is a good first choice (along with S3 for long term storage capability) for a researcher starting off on AWS. 500GB of EBS is probably a good start to try out a few analyses. Please see here for more information on storage options with AWS. Setting up security protocols to connect with your AMI. This can include which IP addresses are recognized for inbound and outbound communication, methods of connecting with the AMI, etc. Logging in using ssh and installing the software you’d like to use, e.g., Bowtie2, Samtools, or Bedtools. Installing Software Once you’re logged into an EC2 instance, you can install most software using the aptget command on Ubuntu and yum on Amazon Linux, e.g., aptget y install samtools bedtools pythonmysqldb To install MACS, first install pip and then use pip to install MACS: aptget y install pythonpip pip install macs2 To install SRA Toolkit, scroll down to the section SRA Toolkit here, and follow the stepbystep instructions. Considerations when using AWS If you are trying to ssh into your instance and you get “operation timed out” error, check"
rc-website-fork/content/userinfo/reference/aws-bioinformatics.md,"in the EC2 Dashboard on AWS that the instance is running. Another reason for getting the error could be that the EC2 instance only accepts inbound network connections from a specific IP, so if you change your WiFi networks your laptop’s IP may not be recognized. Go to EC2 Dashboard, click on Running Instances, click on the instance you are trying to connect to, under Description click on Security groups, click on Inbound, and Edit to add a connection with Source “MyIP.” Some of the bioinformatics software is available on aptget but not yum. Hence, we recommend using Ubuntu as the operating system over Amazon Linux. Your EC2 instance you are setting up with all the tools you will need for your analysis can be saved as an AMI. If you stop and start the instance it will boot up again with all the software you installed, but if you terminate the instance you can use a saved AMI to create the instance again without much work."
rc-website-fork/content/userinfo/reference/bowtie-docker.md,"{{< define ""Bowtie"" ""Bowtie is an ultrafast, memoryefficient short read aligner. It aligns short DNA sequences (reads) to the human genome at a rate of over 25 million 35bp reads per hour. Bowtie indexes the genome with a BurrowsWheeler index to keep its memory footprint small: typically about 2.2 GB for the human genome (2.9 GB for pairedend). The instructions below will launch your Bowtie environment locally within a Docker container."" }} What is Docker? {{< youtube aLipr7tTuA4 }} Install Docker Click the button below and download the appropriate Docker Edition for your platform. We suggest the CE ""Community Edition."" Download Docker Run Bowtie Locally Run these two commands for a webbased deployment of Bowtie on your local workstation: docker pull biocontainers/bowtie docker run d biocontainers/bowtie More Information About the BioContainers Project BioContainers on GitHub BioContainers Registry"
rc-website-fork/content/userinfo/reference/boto3.md,"boto3 the AWS SDK for Python Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2. You can find the latest, most up to date, documentation at Read the Docs, including a list of services that are supported. boto3 is unavailable on the Ivy Secure Platform. Install the boto3 library: Install using pip on Linux/Mac: pip install boto3 or for Windows (or the latest release) simply clone from GitHub: $ git clone git://github.com/boto/boto3.git $ cd boto3 $ python setup.py install Authentication boto can run under AWS authentication granted in a few ways: Inherited from user environment variables Using hardcoded AWS credentials in your code (Never in production / Never committed to git) Using hardcoded AWS credentials in a local config.json file (Never committed to git) Using AWS IAM roles assigned to the EC2 instance when you created it (Best practice) Create the client The client is generally referenced directly by name as a resource: Make your request Here is an example of sending a message to an SQS queue: {{< gist nmagee f55e6d1c03a673a44333e70d1fa6872c }} Errors Note that error handling and parsing error messages is not built into most boto3 requests. You should use try and catch, and botocore can help you display, log, or act on errors."
rc-website-fork/content/userinfo/reference/google-cloud-sdk.md,"Google Cloud SDK The Cloud SDK is a set of tools for Cloud Platform. It contains gcloud, gsutil, and bq, which you can use to access Google Compute Engine, Google Cloud Storage, Google BigQuery, and other products and services from the commandline. You can run these tools interactively or in your automated scripts. Download, Install, and Setup Python 2.7 is required to install and use the Google Cloud SDK. Visit https://cloud.google.com/sdk/ and download the installer for your OS platform. For Mac/Linux users, move the decompressed googlecloudsdk folder to an appropriate place, then run the ./install.sh script. Windows users have an .exe wizard that will complete the installation process. To set up after installation, run gcloud init and you will authenticate (using a web browser) into your Google account. Follow the prompts to create a project, etc. Services that incur charges will have to be associated with billing information (storage, compute, etc.) Basic Usage To get help: gcloud help To see information about your installation: gcloud info Included with the package is the gsutil tool for Google cloud storage gsutil mb gs://mybucket Make a bucket gsutil ls gs://mybucket/folder/ List contents of a bucket subfolder gsutil cp .txt gs://mybucket Copies all text files up into bucket Realworld Examples A backup script to run nightly and keep two weeks of archives: {{< gist nmagee fe999280428f15ebed98ca88942fc29f }} A snippet to create an expiring, signed URL of an object within a bucket: gsutil signurl d 10m <privatekeyfile gs://<bucket/<object A snippet to monitor a bucket for changes and send alerts to a web endpoint. This would trigger a notification every time a new object is added or deleted, or if metadata is updated: gsutil notification watchbucket https://example.com/notify \ gs://examplebucket"
rc-website-fork/content/userinfo/reference/index.md,By language and platform Docker / Containers Bioinformatics / Genomics AWS for Bioinformatics Bowtie 2 Genomics and Bioinformatics Pipelines
rc-website-fork/content/userinfo/reference/anaconda.md,"{{< define ""Anaconda"" ""Anaconda is a distribution of Python geared toward data science. It includes a package manager, environment manager, and over 700 supplementary packages. Within the Ivy Secure Environment, Anaconda is available on both Linux and Windows VMs."" }} Basic Usage For both Linux and Windows VMs, both Anaconda 2 and 3 are installed. For Linux users in Ivy, run Python in one of two ways: /opt/anaconda2/bin/python2.7 Ver 2.7 /opt/anaconda3/bin/python3.5 Ver 3.5 For Windows users in Ivy, Installing packages A full mirror of the Anaconda package repository is available to Ivy users. To browse packages, see the Anaconda package list. Packages can be installed using the conda utility that ships with Anaconda: General format /opt/anaconda2/bin/conda install <packagename Real examples /opt/anaconda2/bin/conda install pyyaml /opt/anaconda2/bin/conda install simplejson To make this command simpler, depending upon the version of Anaconda you prefer, add an alias to your .bashrc file: alias conda='/opt/anaconda2/bin/conda' For Anaconda2 alias conda='/opt/anaconda3/bin/conda' For Anaconda3 See below for more information about .bashrc. Confusion about Python versions The following versions of Python are available in Ivy's Linux VMs: /usr/bin/python2.6 /usr/bin/python /usr/lib/python2.6 /usr/lib64/python2.6 /opt/anaconda2/bin/python2.7 /opt/anaconda2/bin/python /opt/anaconda3/bin/python3.5 /opt/anaconda3/bin/python /opt/anaconda3/bin/python3.5m This means at least three versions of Python are available to you: 2.6, 2.7, and 3.5. If your code has specific requirements for a Python version (usually there are differences in how you write against 2.x versus 3.x) find the version that suits your needs. In order to run a Python script against a specific version, simply declare the path at the head of your script with a ""shebang"" like this: !/opt/anaconda2/bin/python2.7 Then be sure to chmod 755 myscript.py to make it executable. Run your script with ./myscript.py and your it will execute against the specified version of Python. In order to make a specific choice of Python more convenient when you are writing and"
rc-website-fork/content/userinfo/reference/anaconda.md,"executing code in the console, you may want to edit your ~/.bashrc file and add an alias like this. While you're there, set an alias for conda too: alias python='/opt/anaconda2/bin/python2.7' set a new default ver of Python alias conda='/opt/anaconda2/bin/conda' conda for Anaconda2 Finally, issue a source ~/.bashrc command to reread your file to the session, and test with python V. Subsequent logins will use the new value as well."
rc-website-fork/content/userinfo/reference/bioinformatics-pipelines.md,"The fast pace of innovation, data generation, and collaboration in genomics and bioinformatics has necessitated in new data processing frameworks. This guide is aimed at introducing bioinformatics researchers to some of the latest innovations in pipeline development on desktops, AWS, and HPC systems. The number of tools available to researchers and the pros and cons of each can be somewhat daunting. This post is a minimalistic introduction that serves as a quick reference to the stateoftheart. There are three major categories of data processing innovations happening in the bioinformatics community. One is to string together known tools and best practices in packages that can be run with little programming from the researcher. The other is to come up with scripting languages that can be used to develop a fully custom pipeline. Scripting languages provide advantages that range from determining failure points in the pipeline to automating resource management. The third is in a way a hybrid of the two, where the pipelines can be customized to an extent, but also don't need significant programming from the researcher. As an important aside, a number of pipeline packages are being built around the StarCluster architecture. StarCluster is an opensource library that makes setting up computing clusters on AWS quite painless and efficient. The documentation is quite excellent with quick start guides and tutorials to get familiar with the architecture. Prebuilt Pipelines Most of the NextGen analyses revolve around conducting analyses using the best practices codified over time. For RNASeq, ChIPSeq, cancer variant calling, etc. bcbionextgen and Omics Pipe are two solutions that provide easy setup, automated analysis, and easy maintenance of a pipeline. The pipeline can be set to take advantage of cluster resources by running on the StarCluster. Scripting Languages Packaged pipelines such as bcbionextgen and Omic Pipe are not fully"
rc-website-fork/content/userinfo/reference/bioinformatics-pipelines.md,"customizable without significant coding. The standard solution has been to write scripts (such as using Shell) stringing programs together to develop a custom pipeline. Producing production quality pipelines through Shell scripting, again, requires significant coding, and hence, the need for new scripting frameworks. One such framework is bpipe which solves a few problems involved in traditional scripting, including pinpointing failures, cleaning up failed runs, and the difficulties associated with modifying a pipeline. One other development framework that attempts to simplify the process of writing pipelines for different computing architectures is BigDataScript. Scripts written in BigDataScripts can be used on a desktop, HPC cluster, or AWS without material modifications. This simplifies the task of running pipelines on multiple computing resources. ""Hybrid"" Pipelines The poster child for hybrid pipelines is the Galaxy toolkit. Galaxy is a webbased interface that lets researchers combine genomic tools in a flexible graphical way without programming the dependencies and failure conditions themselves. The learning curve for Galaxy is short, and the toolkit covers a wide range of cases where no more programming would be required, but is not geared for researchers looking for a fullycustomizable solution. Further Resources For an overview of pipeline development and scripting innovations, see here. An exhaustive list of data processing pipelines being developed is available here."
rc-website-fork/content/userinfo/reference/ebs-storage.md,"How to add additional EBS storage to an existing EC2 instance. The following instructions can be performed in either the AWS web interface or the AWS commandline tools, Create the EBS volume Create the EBS volume of the appropriate size and type. Make sure you create it in the same availability zone as the instance you want to attach it to. For instance, if you have a Linux instance running in USEast1c, then make sure to create your EBS volume in that zone. Attach the EBS volume Attach your new EBS volume to your EC2 instance. You will be asked for a mount point. Format the EBS volume"
rc-website-fork/content/userinfo/reference/domino-data-lab.md,"{{< define ""Domino Data Lab"" ""DDL is blah blah blah blah blahblah blah blah blah blahblah blah blah blah blahblah blah blah blah blahblah blah blah blah blahblah blah blah blah blah"" }} Overview The DDL Environment Packages Jupyter / NNN Notebooks"
rc-website-fork/content/userinfo/reference/docker.md,"{{< define ""Docker"" ""Docker is a popular version of LXC (Linux Containers). Docker containers wrap up a piece of software in a complete filesystem that contains everything it needs to run: code, runtime, system tools, system libraries – anything you can install on a server. This guarantees that it will always run the same, regardless of the environment it is running in."" }} What is Docker? {{< youtube PfTKwblbkpE }} Install Docker Docker is available for Windows, Mac, and Linux. Download the appropriate Docker Edition for your platform directly from Docker. We suggest the CE ""Community Edition."" Download Docker Finding Containers There are thousands of prebuilt containers already available for common use cases. If you need a web server, a database instance, or portions of a genomics pipeline, there is probably a container ready for you to use. Here are some good places to search for containers: Docker Hub BioContainer GitHub Running Containers If you have found a container you would like to try, download it (using the nginx web server as an example): docker pull nginx View a list of all container images you have pulled: docker images REPOSITORY TAG IMAGE ID CREATED SIZE whalesay latest 188e03692c84 25 hours ago 277 MB rocker/rstudio latest 919e13c956b8 2 weeks ago 990 MB nginx latest 6b914bbcb89e 3 weeks ago 182 MB helloworld latest 48b5124b2768 2 months ago 1.84 kB docker/whalesay latest 6b362a9f73eb 22 months ago 247 MB Run a container image: docker run d nginx This runs the container as a daemon (service). But you may want to expose the container to a specific port locally, so that you can interact with it. For example, if you wanted to expose nginx locally over port 80, enter this: docker run d p 8080:80 nginx The p 8080:80 flag publishes your local computer's port"
rc-website-fork/content/userinfo/reference/docker.md,"8080 with the container's port 80. Another useful flag for runtime is a volume mapping, so that your running container can read or write to portions of your local computer's filesystem. So, extending the earlier command: docker run d p 8080:80 v /User/local/dir:/var/www/html nginx View all running containers: docker ps a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d17f542be53 rocker/rstudio ""/init"" 18 hours ago Up 18 hours 0.0.0.0:87878787/tcp elegantbanach You can also run containers interactively (i.e. logging in) instead of running as a service. This allows you to explore the structure, features, or configuration of a container, or modify how it works: docker run it nginx /bin/bash This runs the container interactively (i) in a pseudoTTY (t), and instantiates a shell for your session to use. Once you are done, simply exit the shell and you will leave the container and return to your local computer's shell. If you have made any changes to the container, be sure to save it. Building Containers If you cannot find just the right container, you can always build your own. There are two ways to do this: Download a container image, run it and log into it, and customize as if it were your own custom virtual machine. Then, save the container for later deployment. Instructions for interactively logging into a container can be found above. Alternatively, you can write a custom Dockerfile and build the container from scratch, using docker build. More on Docker files and builds can be found at https://docs.docker.com/engine/getstarted/stepfour/ Tutorials Docker Training Docker documents this process in great detail, and provides a stepbystep overview of their container system. Launch Katacoda Interactive Labs Katacoda offers a free series of interactive trainings that build sequentially. The tutorials require you to engage with the Docker commandline as you progress. Launch {{<"
rc-website-fork/content/userinfo/reference/docker.md,space }} {{< space }}
rc-website-fork/content/userinfo/reference/aws-cli.md,"AWS CommandLine Interface The AWS CLI is a commandline interface to the AWS service APIs. It allows users to interact programmatically with services such as EC2, S3, and others via bash and PowerShell scripts. This can be useful for local scripts or automated cron tasks. The AWS CLI also serves as a good introduction for programmatic API interaction using languagespecific SKDs (Python, C, PHP, Go, etc.) Learn more at the AWS Command Line Interface page. Install the AWSCLI Mac/Linux Using pip you can install the current release: pip install awscli Windows Download and run the 64bit or 32bit Windows installer. Configure With Credentials Using the access key and secret access key generated for your account, enter those into the AWS CLI configuration. Use this command: aws configure When prompted, enter the appropriate region you are working in, such as useast1 and your preferred output format text | table | json. Profiles If you access AWS through numerous accounts, you can create multiple profiles. To do this, use the profile myprofile parameter when configuring the AWS CLI, with a name you like (replacing myprofile. Then to use a profile: aws profile mycoolprofile ec2 describeinstances Basic Usage The aws command is used, followed by the service name, and then the specific operation you want to call: {{< gist nmagee 2f8426406a99c6cfd11e11d8e2aee11b }} Contextual Help General help with listing services: aws help Find available commands specific to one service: aws ec2 help Specific parameters for a call within a service: aws ec2 describeinstances help Realworld example Use a bash script to turn off your EC2 instance at night, and send you a notification. Use a similar script for a morning startup: {{< gist nmagee 64bbe2b80fd90514b463032d01ba8d9f }} Combine with other CLI tools See the jq page for more examples of how to make AWS CLI"
rc-website-fork/content/userinfo/reference/aws-cli.md,calls and parse the response JSON in meaningful ways.
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/image-processing.md,"Preapproved packages The following software packages are preapproved for image processing on an Ivy Linux VM KNIME KNIME is open source analytics platform for data mining and pipelining. KNIME's Image Processing Plugin allows users to perform common image processing techniques such as registration, segmentation, and feature extraction. KNIME is compatible with over 120 image file types and can be used alongside ImageJ. ImageJ ImageJ is a Javabased image processing program developed at the NIH. ImageJ can be used interactively through a graphical user interface or automatically with Java. OpenCV OpenCV is an open source library for computer vision applications. OpenCV includes modules for image processing, video analysis, machine learning, and much more."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/complete-list.md,"{{< hszsoftware moduleclasses=""all"" }}"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/_index.md,"{{% callout %}} Each Linux Virtual Machine (VM) comes with a set of preinstalled software applications. Each VM can further be customized via installation of optional software packages. {{% /callout %}} An overview of available software packages for Windows VMs is provided here. Preinstalled Software {{< ivyapprovedswdetailed os=""Linux"" installation=""preinstalled"" category=""all"" }} Optional Software In addition to the preinstalled software, researchers may request installation of the following approved software packages for their Virtual Machine. Request Ivy Software Bioinformatics {{< ivyapprovedswdetailed os=""Linux"" installation=""optional"" category=""Bioinformatics"" }} Data Analysis {{< ivyapprovedswdetailed os=""Linux"" installation=""optional"" category=""Data Analysis"" }} Database Software {{< ivyapprovedswdetailed os=""Linux"" installation=""optional"" category=""Database Software"" }} Engineering {{< ivyapprovedswdetailed os=""Linux"" installation=""optional"" category=""Engineering"" }} Image Processing {{< ivyapprovedswdetailed os=""Linux"" installation=""optional"" category=""Image Processing"" }}"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/image-processing.md,"Preapproved packages The following software packages are preapproved for image processing on an Ivy Windows VM Axiovision Axiovision is software for microscopy image processing and analysis. Axiovision is highly configurableto meet the needs of your individual workflows. KNIME KNIME is open source analytics platform for data mining and pipelining. KNIME's Image Processing Plugin allows users to perform common image processing techniques such as registration, segmentation, and feature extraction. KNIME is compatible with over 120 image file types and can be used alongside ImageJ. ImageJ ImageJ is a Javabased image processing program developed at the NIH. ImageJ can be used interactively through a graphical user interface or automatically with Java. OpenCV OpenCV is an open source library for computer vision applications. OpenCV includes modules for image processing, video analysis, machine learning, and much more."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/_index.md,"{{% callout %}} Each Windows Virtual Machine (VM) comes with a set of preinstalled software applications. Each VM can further be customized via installation of optional software packages. {{% /callout %}} An overview of available software packages for Linux VMs is provided here. Preinstalled Software {{< ivyapprovedswdetailed os=""Windows"" installation=""preinstalled"" category=""all"" }} Optional Software In addition to the preinstalled software, researchers may request installation of the following approved software packages for their Virtual Machine. Request Ivy Software Bioinformatics {{< ivyapprovedswdetailed os=""Windows"" installation=""optional"" category=""Bioinformatics"" }} Data Analysis {{< ivyapprovedswdetailed os=""Windows"" installation=""optional"" category=""Data Analysis"" }} Database Software {{< ivyapprovedswdetailed os=""Windows"" installation=""optional"" category=""Database Software"" }} Engineering {{< ivyapprovedswdetailed os=""Windows"" installation=""optional"" category=""Engineering"" }} Image Processing {{< ivyapprovedswdetailed os=""Windows"" installation=""optional"" category=""Image Processing"" }}"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/database/sw-list.md,"The following database software are available on the Ivy Linux Virtual Machines MySQL Is the most popular opensource relational database, used in academia and industry worldwide. It has been in use for over 20 years and is backed by a large developer community. It is available in both free and proprietary versions. MariaDB MariaDB is a community developed version of MySQL, and is highly compatible with MySQL and other relational databases. Existing databases can be easily migrated between MySQL and MariaDB, and vice versa. PostgreSQL Unlike MariaDB and MySQL, PostgreSQL is an object relational database, and can be used in a manner similar to other relational databases. It is available for free, and is opensource under its own special license type, called PostgreSQL License. MongoDB MongoDB is a NoSQL, Document store database. It treats all data as keyvalue pair documents, stored within collections, instead of tables. MongoDB is great for large scale data processing needs."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/programming/openjdk-8.md,"Java SDK Overview Ivy Linux VMs are installed with Java SDK 1.8. Java is a popular ObjectOriented programming language and is used in a multitude of scenarios. It is available under the GNU General Public License for all users. The SDK consists of a large number of tools such as javac that help in application development. Running Java commands from the Command Line Open a Command Line Terminal and enter java followed by the desired command. E.g. to find the version of the SDK java version Running your code To compile java code, first cd to the location of your .java file and then do javac <yourclassname.java After the java compiler runs and gives no errors, a .class file would be created. Run the following command to see the output: java <yourclassname Important Note While Ivy VMs have full support for the Java SDK, certain aspects of programming fullscale Java on Ivy may not work without running into issues. In order to execute Java programs correctly, please load the entire package into the VM's storage instead of compiling it on the VM. More Information Please visit the official Oracle documentation to learn more about Java at this web address."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/programming/perl.md,"Perl Our VMs have Perl 5.16.3 available as part of the base linux VM. Licensed as open source under the GPL, it is most often used to develop mission critical software, and has excellent integration with markup languages such as HTML, XML, amongst others. Since it is both ObjectOriented and procedural, it could be used within a multitude of programming projects. It includes built in database integration via its DBI module. Other than DBI, it has thousands of modules, making it one of the most extensible languages. Due to its interpreted nature, Perl is similar to Python and would be easy to understand for those familiar with Python. Running Perl code Perl has an interactive interpreter, which could be run by simply typing perl e <perlcodegoes here. E.g. to print a number: perl e 'print 10' the e flag is simply to denote that the code is not a file, but code itself. To run a Perl script, do the following: perl Installing modules Since Ivy VM's do not allow outward connections to CPAN's website, you would have to install perl modules using the procedure below: Check if CPAN is installed and configured on your VM by typing cpan into a terminal window: cpan If it asks you if CPAN needs to be configured, type yes Once it is configured, type cpan to enter the CPAN shell In a browser from outside the Ivy VM, search for the proper name of the Perl module you wish to download search.cpan.org E.g. if you want to install the MySQL driver for Perl, type install DBD::MySQL This would start the installation of the module. Ivy is able to download modules from CPAN using this method. You could manually install a module from its compressed file, once you have transferred the file into Ivy."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/programming/perl.md,"However, using the process above downloads the modules' dependencies as well. Verifying if a module is installed Run the following command after installing your module : perldoc l DBD::mysql (e.g. if you installed DBD::mysql). It should output the path to the installed module. More Information Please visit the official Perl website for more details."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/rodeo.md,"Rodeo Overview Our Linux VMs are installed with Rodeo version 2.5, as of the last update. Rodeo is a lightweight, Python based, IDE for data science. It has a very streamlined codetoplot workflow, with easily extensible packages that make it simple to analyze difficult patterns in data. It includes many data analysis features under one roof, and adopts features from iPython Notebook (it actually runs atop the iPython kernel). Like most Python projects, it is open source and available for free. Launching Rodeo You can launch Rodeo from the Applications menu. It is a selfcontained IDE that would not require any knowledge of the command line. Rodeo can be used in the same manner as any other Python IDE such as iPython Notebook or Jupyter Notebook. Basic Rodeo Usage It is important to understand that all Python code, such as lists, Dataframes, etc. are saved to the Environment. We then use the Environment tab to view our data. E.g. if you create a Dataframe df = pd.DataFrame(np.random.rand(50,3),columns=['col1','col2','col3']) You can then open the Environment tab to view this in tabular form. Command History in Rodeo All commands can be viewed under the History tab More Information For more information, please visit the official [Rodeo website] (https://github.com/yhat/rodeo)"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/stata.md,"Stata Overview Stata is a graphical data analysis tool developed by StataCorp, and is short for Statistics and Data. It is used in various disciplines, including biomedicine, economics, epidemiology, among others. It is capable of performing statistical analysis, simulations, regression, and data management. Besides the standard version Stata also ships with the MP version (multi=processing), and SE for large databases. {{% callout %}} Users requesting an installation of Stata are required to provide their own license. Please consult with us before requesting an installation. {{% /callout %}} You may also request a Stata license from the UVa Software Gateway Installing programs from SSC Please first run the following commands to use the proxy: set httpproxy on set httpproxyhost ""figgiss.hpc.virginia.edu"" set httpproxyport 8080 You can now install new packages with the ssc install command."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/sw-list.md,"Available Packages The following Data Analysis packages are available on the Ivy Linux Virtual Machines MATLAB MATrix LABoratory (MATLAB for short) is a software designed for quick scientific calculations, such as matrix manipulation, plotting, and others. It has hundreds of builtin functions for a wide variety of computations and several tools designed for specific research disciplines, including statistics and partial differential equations. Limited licenses available, for more information on MATLAB and licensing, please click [here] (/userinfo/ivy/ivylinuxsw/dataanalysis/matlab) SAS SAS is large platform independent software with multiple components, and is used for statistical analysis, data ETL operations, as well as several other reporting problems. Limited licenses available, for more information on SAS and licensing, please click here Stata Stata is a large GUI based data analysis software package. It is great for statistical analysis in a broad spectrum of research disciplines. Requires user to bring their own license(s). For more information on Stata, please click here IDL IDL is a programming language used widely in the areas of medicine, physics, and astronomy. Image processing can be performed easily on it. Users can use it on the VM using its command line based interface. Limited licenses available, for more information IDL and licensing, please click here Apache cTAKES The clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open source system to extract information from clinical health records. It uses natural language processing to extract information about the patient, drugs, symptoms, procedures, etc. It is an efficient clinical analysis tool, and was specifically written for the clinical domain. cTAKES requires preinstallation approval. Please get in touch with us regarding your requirements before making a VM request. For information about cTAKES, please click here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/idl.md,"IDL Overview IDL, short for Interactive Data Language, is an interactive shell based data analysis programming language. Used vastly in medical imaging, it can quickly create visualizations and graphs of large data sets in a few easy steps due to its vector nature. FORTRAN users would be familiar with the IDL syntax. IDL is not to be confused with Java IDL or Microsoft IDL. Basic IDL Usage To start IDL, open a terminal window and type idl. This will start the interactive shell. Variables in IDL To initialize variables in IDL, type: <variablename = <variablevalue e.g. x = 3 and hit Enter/Return To print the variable, type print, x Arrays in IDL To make arrays in IDL, follow the format below: <arrayname = [val1, val2, val3,...,valx] e.g. myarray = [1,2,3,4] Licensing We have a limited number of IDL Licenses available, which are provided on a firstcomefirstserve basis. As a consequence, availability of IDL on your VM is not always guaranteed once a VM request is submitted. Please consult with us before requesting IDL. More Information For more Information on IDL, please consult the IDL documentation on its [official website] (https://www.nv5geospatialsoftware.com/Products/IDLlanguage)"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/r.md,"R Overview R is an open source programming language, used by Data Miners, Scientists, Data Analysts, and Statisticians. It is available under the GNU GPL V2 license from the Comprehensive R Archive Network R can be used for many statistical, modeling, and graphical solutions. It is very ObjectOriented in nature and is easily extensible. Running the command line R console Type R at the terminal to launch the R console. Installing packages Our Linux VMs come equipped with R preinstalled. Most major R packages are also installed and further could be installed from CRAN using (from within the R console) install.packages(""yourpackagename"") Once the package is loaded, you could start it using library(yourpackagename) Running R Scripts from the command line Simply type in the following Rscript /path/to/script/yourscriptfile.R More Information For more information, please visit the official R website"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/anaconda.md,Anaconda Our VMs have Python 2 and 3 available as part of the Anaconda distribution. Please refer to this page for more information.
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/sas.md,"SAS Overview SAS is a commanddriven software package used for statistical analysis and data visualization. It is available in . It is one of the most widely used statistical software packages in both industry and academia. You may use it if you have a large number of statistical algorithms. It is not limited to an industry, and could be used in both scientific and nonscientific contexts. We only offer the Teaching & Research version at the moment. Common Usage For this example we will use a common scenario from SAS Clinical Standards Toolkit, which is used for supporting clinical research activities. The SAS Clinical Standards Toolkit initially focuses on standards defined by the Clinical Data Interchange Standards Consortium (CDISC). CDISC is a global, open, multidisciplinary, nonprofit organization that has established standards to support the acquisition, exchange, submission, and archival of clinical research data and metadata. Starting SAS Open a terminal window and type sas. Getting a list of all installed standards %cstgetregisteredstandards( cstOutputDS=work.regStds ); Creating Data Sets Used by the Framework %cstcreatedsfromtemplate( cstStandard=CSTFRAMEWORK, cstType=control, cstSubType=reference, cstOutputDS=work.sasrefs ); Licensing We have a limited number of SAS Licenses available, which are provided on a firstcomefirstserve basis. As a consequence, availability of SAS on your VM is not always guaranteed once a VM request is submitted. Please consult with us before requesting SAS. More information For more information on SAS, please consult its official documentation here You may obtain the Administration version of SAS from the UVA Software Gateway portal here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/ctakes.md,"cTAKES Overview cTAKES or The clinical Text Analysis and Knowledge Extraction System, is a Mayo Clinic developed Natural Language Processing (NLP) tool used to extract information out of clinical records. It is opensource, and built on the Apache Unstructured Information Management Architecture. cTAKES is modular, expandable, for a number of generic use cases, and contains excellent best practice notes. cTAKES Usage cTAKES components Some of cTAKES components are listed below: Sentence boundary detection (OpenNLP technology) Tokenization (rulebased) Morphologic normalization (NLM’s LVG) POS tagging (OpenNLP technology) Shallow parsing (OpenNLP technology) Named Entity Recognition Negation and context identification (both based on NegEx) cTAKES Named Entities cTAKES contains the following Named Entities: Drug mentions Disease/disorder mentions Sign/symptom mentions Anatomical site mentions Smoking status classifier Detailed drug mention annotator Peripheral Artery Disease (PAD) annotator Dependency parser cTAKES Installation Please note that cTAKES requires preinstallation approval by us. Please consult us prior to requesting a new VM regarding your cTAKES requirements. More Information For more information on cTAKES, and how to use it, please visit the following links: Official Website: http://ctakes.apache.org/index.html Documentation: https://cwiki.apache.org/confluence/display/CTAKES/cTAKES cTAKES scientific paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2995668/"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/matlab.md,"MATLAB Overview MATLAB is a highperformance language for technical computing. It integrates computation, visualization, and programming environment. MATLAB stands for MATrix LABoratory. MATLAB was made to provide easy access to matrix software developed by the LINPACK (linear system package) and EISPACK (Eigen system package) projects. MATLAB includes a programming language environment with builtin editing and debugging tools, and supports objectoriented programming. Programming in MATLAB MATLAB has many advantages compared to conventional computer languages (e.g., C, FORTRAN) for solving technical problems. MATLAB is an interactive system whose basic data element is an array, and almost all problems can be solved in MATLAB using that one data element. Starting MATLAB To start MATLAB, open a terminal window and type matlab Since MATLAB is a large software, it may take time to load up. When it starts, the first screen to appear would be the MATLAB Desktop, with several windows within it. These might include the ribbons (Home, Plot, and App), the Command Window, the Current Folder window, and the Workspace window. Basic Arithmetic The following are basic arithmetic operators in MARLAB (Multiplication), + (Addition), (Subtraction), / (Division) In order to perform a simple calculation click anywhere on the Command Window and type 1+2+3 and hit Enter/Return. the result shows up on the Command Window as Ans = 6 Variables To create a variable, type the name of the variable, followed by the equals sign (assignment operator) and the value. Hit Enter/Return to store the value. x = 12 If you type x again on the Command Window, you would see the value of the variable displayed. Variables could be used in any number of arithmetic calculations. Arrays A very basic plot can be drawn using MATLAB arrays. To create an array in MATLAB, use the square brackets: x = [1 2 3"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/matlab.md,"4 5 6] y = [4 4 7 3 7 1] Notice how the array elements do not have commas between them. These arrays could be used like any other variable. Licensing University of Virginia has recently upgraded the Matlab license so that Matlab is available to everyone at UVa. There is one version of Matlab for students, faculty, and staff. MATLAB is available on the Windows, Mac OSX, and Linux platforms. To get started go to the UVa Software Gateway Matlab can be found under the Data Analysis & Research grouping. The Campus Wide License configuration now includes all 100+ MathWorks products. To better reflect the new configuration they have renamed the license option to Campus Wide Suite. For further information see the URL https://data.library.virginia.edu/researchsoftware/matlab/ More Information For help, type help in the Command Window or click on the Help button on the HOME ribbon. For more Information on MATLAB itself, please visit the official website here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/sw-list.md,"Available Packages The following bioinformatics packages are available on the Ivy Linux Virtual Machines Bowtie2 Bowtie2 is a memoryefficient tool for aligning short sequences to long reference genomes. For bowtie2 usage information, please click [here] (/userinfo/ivy/ivylinuxsw/bioinformatics/bowtie2) HISAT2 HISAT2 is a fast and sensitive tool for aligning short reads against the general human population (as well as single reference genome) Requires approval before installation For HISAT2 usage information, please click here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/cufflinks.md,"Cufflinks Overview The Cufflinks suite of tools can be used to perform a number of different types of analyses for RNASeq experiments. The Cufflinks suite includes a number of different programs that work together to perform these analyses. The complete workflow, performing all the types of analyses Cufflinks can execute, are summarized below."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/bowtie2.md,"Bowtie2 is a memoryefficient tool for aligning short sequences to long reference genomes. It indexes the genome using FM Index, which is based on BurrowsWheeler Transform algorithm, to keep its memory footprint small. Bowtie2 supports gapped, local and pairedend alignment modes. Alignment to a known reference using Bowtie2 is often an essential first step in a myriad of NGS analyses workflows. Bowtie2 Usage Alignment using bowtie2 is a 2step process indexing the reference genome, followed by aligning the sequence data. Create indexes of your reference genome of interest stored in reference.fasta file: bowtie2build [option(s)] <reference.fasta <bt2indexbasename This will create new files with the provided basename and extensions .1.bt2, .2.bt2, .3.bt2 and .4.bt2, .rev.1.bt2 and .rev.2.bt2. These files constitute the index. Align pairedend reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores: bowtie2 x <bt2indexbasename 1 <sampleR1.fq 2 \ <sampleR2.fq p <N S <output.sam The alignment results in SAM format are written to the file output.sam More Information For more information, please refer to the Bowtie2 manual. Citation: If you use bowtie2 for your work, please cite: Langmead B, Salzberg S. Fast gappedread alignment with Bowtie 2. Nature Methods. 2012, 9:357359"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/hisat2.md,"Please note that HISAT2 requires approval prior to installation on the VM HISAT2 is a fast and sensitive tool for aligning short reads against the general human population (as well as single reference genome). It indexes the genome using a Hierarchical Graph FM Index (HGFM) strategy, i.e. a large set of small indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp). HISAT2 Usage: Alignment using HISAT2 is a 2step process indexing the reference genome, followed by aligning the sequence data. Create indexes of your reference genome of interest stored in reference.fasta file: hisat2build [option(s)] This will create new files with the provided basename and extensions .ht2. These files constitute the index. Align pairedend reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores: hisat2 x 1 2 p S The alignment results in SAM format are written to the file output.sam Note on using the sraacc option Since Ivy VM’s do not allow outside connections, sraacc option will not work with HISAT2. If users plan to use SRA data, they will have to download it and move into Ivy prior to alignment. Please refer to our Globus user guide for help on how to do that. More information For detailed information, please refer to HISAT2 manual. Citation: If you use HISAT2 for your work, please cite: Kim D, Langmead B and Salzberg SL. HISAT: a fast spliced aligner with low memory requirements. Nature Methods 2015"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/productivity/libreoffice.md,"LibreOffice Overview Our Linux VMs come prepackaged with the open source alternative to Microsoft Office(R), called LibreOffice. As of last writing, version 5 is installed, including the specific software suites mentioned below. LibreOffice is compatible with all Microsoft Office formats, and can be connected to services like Google Drive or DropBox. It is available under the Mozilla Public License. LibreOffice is full GUI software and would require you to RDP into your VM or use a graphical tool such as FastX in order to render it. LibreOffice Writer LibreOffice Writer is the word processor component of LibreOffice. It can save documents in .Doc, .Docx, .PDF, etc. LibreOffice Calc LibreOffice Calc is the spreadsheet component of LibreOffice. It can save spreadsheets in CSV, XLS, and PDF formats. LibreOffice Impress LibreOffice Impress is the presentation maker component of LibreOffice. It can save presentations in PPT, PPTX, and SWF formats. LibreOffice Draw LibreOffice Draw is the vector graphics component of LibreOffice. It can save files in various graphical formats such as SVG and others. LibreOffice Base LibreOffice Base is the database component of LibreOffice. It uses the HSQLDB as its storage engine, and can be used as a front end for larger databases systems such as MySQL and MariaDB. More Details For more details, please visit LibreOffice's [official website] (https://www.libreoffice.org/)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/strawberry-perl.md,"Perl Our VMs have Strawberry Perl 5.24 available as part of the Windows VM, as of the last writing. Licensed as open source under the GPL, it is most often used to develop mission critical software, and has excellent integration with markup languages such as HTML, XML, amongst others. Since it is both ObjectOriented and procedural, it could be used within a multitude of programming projects. It includes built in database integration via its DBI module. Other than DBI, it has thousands of modules, making it one of the most extensible languages. Due to its interpreted nature, Perl is similar to Python and would be easy to understand for those familiar with Python. Running Perl code Strawberry Perl has an interactive interpreter, available under Start All Programs Strawberry Perl. Perl commands could be executed by simply typing perl e <perlcodegoes here. E.g. to print a number: perl e 'print 10' the e flag is simply to denote that the code is not a file, but code itself. To run a Perl script, do the following: perl Installing modules Since Ivy VM's do not allow outward connections to CPAN's website, you would have to install perl modules using the procedure below: Check if CPAN is installed and configured on your VM by typing cpan into a terminal window: cpan If it asks you if CPAN needs to be configured, type yes Once it is configured, type cpan to enter the CPAN shell In a browser from outside the Ivy VM, search for the proper name of the Perl module you wish to download search.cpan.org E.g. if you want to install the MySQL driver for Perl, type install DBD::MySQL This would start the installation of the module. Ivy is able to download modules from CPAN using this method. You could manually install"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/strawberry-perl.md,"a module from its compressed file, once you have transferred the file into Ivy. However, using the process above downloads the module's dependencies as well. Verifying if a module is installed Run the following command after installing your module : perldoc l DBD::mysql (e.g. if you installed DBD::mysql) It should output a path to the installed module. More Information For more information on Strawberry Perl, please visit the official website."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/openjdk-8.md,"Java SDK Overview Ivy Windows VMs are installed with Java SDK 1.8. Java is a popular ObjectOriented programming language and is used in a multitude of scenarios. It is available under the GNU General Public License for all users. The SDK consists of a large number of tools such as javac that help in application development. Running Java commands from the Command Prompt Open a Windows Command Prompt and enter java followed by the desired command. E.g. to find the version of the SDK java version Running your code To compile java code, first cd to the location of your .java file and then do javac <yourclassname.java After the java compiler runs and gives no errors, a .class file would be created. Run the following command to see the output: java <yourclassname Important Note While Ivy VMs have full support for the Java SDK, certain aspects of programming fullscale Java on Ivy may not work without running into issues. In order to execute Java programs correctly, please load the entire package into the VM's storage instead of compiling it on the VM. More Information Please visit the official Oracle documentation to learn more about Java at this web address."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/perl.md,"Perl Our VMs have Perl 5.16.3 available as part of the base linux VM. Licensed as open source under the GPL, it is most often used to develop mission critical software, and has excellent integration with markup languages such as HTML, XML, amongst others. Since it is both ObjectOriented and procedural, it could be used within a multitude of programming projects. It includes built in database integration via its DBI module. Other than DBI, it has thousands of modules, making it one of the most extensible languages. Due to its interpreted nature, Perl is similar to Python and would be easy to understand for those familiar with Python. Running Perl code Perl has an interactive interpreter, which could be run by simply typing perl e <perlcodegoes here. E.g. to print a number: perl e 'print 10' the e flag is simply to denote that the code is not a file, but code itself. To run a Perl script, do the following: perl Installing modules Since Ivy VM's do not allow outward connections to CPAN's website, you would have to install perl modules using the procedure below: Check if CPAN is installed and configured on your VM by typing cpan into a terminal window: cpan If it asks you if CPAN needs to be configured, type yes Once it is configured, type cpan to enter the CPAN shell In a browser from outside the Ivy VM, search for the proper name of the Perl module you wish to download search.cpan.org E.g. if you want to install the MySQL driver for Perl, type install DBD::MySQL This would start the installation of the module. Ivy is able to download modules from CPAN using this method. NB: You could manually install a module from its compressed file, once you have transferred the file into"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/perl.md,"Ivy. However, using the process above downloads the modules' dependencies as well."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/rodeo.md,"Rodeo Overview Our Windows VMs are installed with Rodeo version 1.3, as of the last update. Rodeo is a lightweight, Python based, IDE for data science. It has a very streamlined codetoplot workflow, with easily extensible packages that make it simple to analyze difficult patterns in data. It includes many data analysis features under one roof, and adopts features from iPython Notebook (it actually runs atop the iPython kernel). Like most Python projects, it is open source and available for free. Launching Rodeo You can launch Rodeo from the Start menu. It is a selfcontained IDE that would not require any knowledge of the command line. Rodeo can be used in the same manner as any other Python IDE such as iPython Notebook or Jupyter Notebook. Basic Rodeo Usage It is important to understand that all Python code, such as lists, Dataframes, etc. are saved to the Environment. We then use the Environment tab to view our data. E.g. if you create a Dataframe df = pd.DataFrame(np.random.rand(50,3),columns=['col1','col2','col3']) You can then open the Environment tab to view this in tabular form. Command History in Rodeo All commands can be viewed under the History tab More Information For more information, please visit the official [Rodeo website] (https://github.com/yhat/rodeo)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/stata.md,"Stata Overview Stata is a graphical data analysis tool developed by StataCorp, and is short for Statistics and Data. It is used in various disciplines, including biomedicine, economics, epidemiology, among others. It is capable of performing statistical analysis, simulations, regression, and data management. Besides the standard version Stata also ships with the MP version (multi=processing), and SE for large databases. {{% callout %}} Users requesting an installation of Stata are required to provide their own license. Please consult with us before requesting an installation. {{% /callout %}} You may also request a Stata license from the UVa Software Gateway Installing programs from SSC Please first run the following commands to use the proxy: set httpproxy on set httpproxyhost ""figgiss.hpc.virginia.edu"" set httpproxyport 8080 You can now install new packages with the ssc install command."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/spss.md,"SPSS Overview SPSS (or Statistical Package for Social Sciences), was initially developed as a social survey project but later on has grown to encompass statistical applications in almost all disciplines. Different industries use SPSS for their data analysis work. Its features include database management, reporting, graphing, among many others. SPSS Usage SPSS is available only on the Windows VM at the moment. To run SPSS go to: Start Menu All Programs IBM SPSS Statistics Licensing We have a limited number of SPSS licenses available, which are provided on a firstcomefirstserve basis. As a consequence, availability of SPSS on your VM is not always guaranteed once a VM request is submitted. Please consult with us before requesting SPSS. More Information For detailed documentation on SPSS, please visit the official website here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/sw-list.md,"Available Packages The following Data Analysis packages are available on the Ivy Windows Virtual Machines MATLAB MATrix LABoratory (MATLAB for short) is a software designed for quick scientific calculations, such as matrix manipulation, plotting, and others. It has hundreds of builtin functions for a wide variety of computations and several tools designed for specific research disciplines, including statistics and partial differential equations. Limited licenses available, for more information on MATLAB and licensing, please click [here] (/userinfo/ivy/ivywindowssw/dataanalysis/matlab) SAS SAS is large platform independent software with multiple components, and is used for statistical analysis, data ETL operations, as well as several other reporting problems. Limited licenses available, for more information on SAS and licensing, please click here Stata Stata is a large GUI based data analysis software package. It is great for statistical analysis in a broad spectrum of research disciplines. Requires user to bring their own license(s). For more information on Stata, please click here IDL IDL is a programming language used widely in the areas of medicine, physics, and astronomy. Users can use it on the VM using its command line based interface. x Limited licenses available, for more information on IDL and licensing, please click here Apache cTAKES The clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open source system to extract information from clinical health records. It uses natural language processing to extract information about the patient, drugs, symptoms, procedures, etc. It is an efficient clinical analysis tool, and was specifically written for the clinical domain. cTAKES requires preinstallation approval. Please get in touch with us regarding your requirements before making a VM request. For information about cTAKES, please click here SPSS SPSS is a large scale, yet easy to use data analysis packages. It has been widely used since the last few decades and is"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/sw-list.md,"considered one of the world's de facto data analysis and reporting tools. Limited licenses available, for more information on SPSS and licensing, please click here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/idl.md,"IDL Overview IDL, short for Interactive Data Language, is an interactive shell based data analysis programming language. Used vastly in medical imaging, it can quickly create visualizations and graphs of large data sets in a few easy steps due to its vector nature. FORTRAN users would be familiar with the IDL syntax. IDL is not to be confused with Java IDL or Microsoft IDL. Licensing We have a limited number of IDL Licenses available, which are provided on a firstcomefirstserve basis. As a consequence, availability of IDL on your VM is not always guaranteed once a VM request is submitted. Please consult with us before requesting IDL. More Information For more Information on IDL, please consult the IDL documentation on its [official website] (https://www.nv5geospatialsoftware.com/Products/IDLlanguage)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/r.md,"R Overview R is an open source programming language, used by Data Miners, Scientists, Data Analysts, and Statisticians. It is available under the GNU GPL V2 license from the Comprehensive R Archive Network R can be used for many statistical, modeling, and graphical solutions. It is very ObjectOriented in nature and is easily extensible. Running Rstudio from the desktop You can start R in a Graphical interface using the RStudio application from the desktop Running the command line R console Type R at the command prompt to launch the R console. Installing packages Our Windows VMs come equipped with R preinstalled. Most major R packages are also installed and further could be installed from CRAN using (from within the R console) install.packages(""yourpackagename"") Once the package is loaded, you could start it using library(yourpackagename) Running R Scripts from the command line Simply type the following on Command Prompt Rscript path\\to\\script\\yourscriptfile.R More Information For more information, please visit the official R Studio website."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/anaconda.md,"Anaconda Our VMs have python 2 and 3 available as part of the Anaconda distribution. Anaconda comes installed with many packages best suited for scientific computing, data processing, and data analysis, while making deployment very simple. Its package manager conda installs and updates python packages and dependencies, keeping different package versions isolated on a projectbyproject basis. Anaconda is available as open source under the New BSD license. It also ships with pip, the common python package manager. Installing packages Packages could be installed via pip or conda package managers Installing packages on a Windows VM A) Using conda From the Start menu, open a new Command Prompt (or Anaconda prompt) window, and type: conda search packagename (search for a package by name) conda install packagename (install a package) conda update packagename upgrade (upgrade the package to the latest stable version) conda list (list all installed packages) B) Using pip From the Start menu, open a new Command Prompt (or Anaconda Prompt) window and type: pip search packagename (search for a package by name) pip install packagename (install a package) pip update packagename upgrade (upgrade the package to the latest stable version) pip list (list all installed packages) Running Python2 and Python3 using Virtual Environments {runningpython2andpython3usingvirtualenvironments} You can specify which version of Python you want to run using conda. This can be done on a projectbyproject basis, and is part of what is called a ""Virtual Environment"". A Virtual Environment is simply your isolated copy of Python in which you maintain your own version of files and directories. It enables you to keep other projects unaffected. With projects that have similar dependencies, you can freely install different versions of the same package without worry on two different Virtual Environments. In order to jump between two VE's, you simply activate or deactivate"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/anaconda.md,"your environment. Follow the steps below: Update Conda: conda update conda Set up your Virtual Environment: conda create n yourenvnamegoeshere (default Python version: use conda info to find out) OR conda create n yourenvnamegoeshere python=versiongoeshere (to find specific Python versions, use conda search ""^python$"") If it asks you for y/n, hit y to proceed. It will start the installation Activate your newly created environment activate yourenvnamegoeshere Install a package in your activated environment conda install n yourenvnamegoeshere yourpackagenamegoeshere OR conda install n yourenvnamegoeshere \ yourpackagenamegoeshere=versiongoeshere OR (even better) In your home directory or Conda installation folder, create a file called .condarc (if not already there) Inside the file write the following: createdefaultpackages yourpackagenamegoeshere yourpackagenamegoeshere yourpackagenamegoeshere ... Now everytime you create a new environment, all those packages listed in .condarc will be installed. 6. To end the current environment session: deactivate 7. Remove an environment: conda remove n yourenvnamegoeshere all More Information For more information, please visit the official [Anaconda website] (https://anaconda.org/anaconda/python)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/sas.md,"SAS Overview SAS is a commanddriven software package used for statistical analysis and data visualization. It is available in . It is one of the most widely used statistical software packages in both industry and academia. You may use it if you have a large number of statistical algorithms. It is not limited to an industry, and could be used in both scientific and nonscientific contexts. We only offer the Teaching & Research version at the moment. Common Usage For this example we will use a common scenario from SAS Clinical Standards Toolkit, which is used for supporting clinical research activities. The SAS Clinical Standards Toolkit initially focuses on standards defined by the Clinical Data Interchange Standards Consortium (CDISC). CDISC is a global, open, multidisciplinary, nonprofit organization that has established standards to support the acquisition, exchange, submission, and archival of clinical research data and metadata. Starting SAS Open the Start Menu and from All Programs locate the SAS folder. Within that click on SAS 9.4. You may also run SAS via the SAS Studio, which uses a browser. Getting a list of all installed standards %cstgetregisteredstandards( cstOutputDS=work.regStds ); Creating Data Sets Used by the Framework %cstcreatedsfromtemplate( cstStandard=CSTFRAMEWORK, cstType=control, cstSubType=reference, cstOutputDS=work.sasrefs ); Licensing We have a limited number of SAS Licenses available, which are provided on a firstcomefirstserve basis. As a consequence, availability of SAS on your VM is not always guaranteed once a VM request is submitted. Please consult with us before requesting SAS. More information For more information on SAS, please consult its official documentation here You may obtain the Administration version of SAS from the UVA Software Gateway portal here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/ctakes.md,"cTAKES Overview cTAKES or The clinical Text Analysis and Knowledge Extraction System, is a Mayo Clinic developed Natural Language Processing (NLP) tool used to extract information out of clinical records. It is opensource, and built on the Apache Unstructured Information Management Architecture. cTAKES is modular, expandable, for a number of generic use cases, and contains excellent best practice notes. cTAKES Usage cTAKES components Some of cTAKES components are listed below: Sentence boundary detection (OpenNLP technology) Tokenization (rulebased) Morphologic normalization (NLM’s LVG) POS tagging (OpenNLP technology) Shallow parsing (OpenNLP technology) Named Entity Recognition Negation and context identification (both based on NegEx) cTAKES Named Entities cTAKES contains the following Named Entities: Drug mentions Disease/disorder mentions Sign/symptom mentions Anatomical site mentions Smoking status classifier Detailed drug mention annotator Peripheral Artery Disease (PAD) annotator Dependency parser cTAKES Installation Please note that cTAKES requires preinstallation approval by us. Please consult us prior to requesting a new VM regarding your cTAKES requirements. More Information For more information on cTAKES, and how to use it, please visit the following links: Official Website: http://ctakes.apache.org/index.html Documentation: https://cwiki.apache.org/confluence/display/CTAKES/cTAKES cTAKES scientific paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2995668/"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/matlab.md,"MATLAB Overview MATLAB is a highperformance language for technical computing. It integrates computation, visualization, and programming environment. MATLAB stands for MATrix LABoratory. MATLAB was made to provide easy access to matrix software developed by the LINPACK (linear system package) and EISPACK (Eigen system package) projects. MATLAB includes a programming language environment with builtin editing and debugging tools, and supports objectoriented programming. Programming in MATLAB MATLAB has many advantages compared to conventional computer languages (e.g., C, FORTRAN) for solving technical problems. MATLAB is an interactive system whose basic data element is an array, and almost all problems can be solved in MATLAB using that one data element. Starting MATLAB To start MATLAB, open the Start menu window and find MATLAB. Since MATLAB is a large software, it may take time to load up. When it starts, the first screen to appear would be the MATLAB Desktop, with several windows within it. These might include the ribbons (Home, Plot, and App), the Command Window, the Current Folder window, and the Workspace window. Basic Arithmetic The following are basic arithmetic operators in MARLAB (Multiplication), + (Addition), (Subtraction), / (Division) In order to perform a simple calculation click anywhere on the Command Window and type 1+2+3 and hit Enter/Return. the result shows up on the Command Window as Ans = 6 Variables To create a variable, type the name of the variable, followed by the equals sign (assignment operator) and the value. Hit Enter/Return to store the value. x = 12 If you type x again on the Command Window, you would see the value of the variable displayed. Variables could be used in any number of arithmetic calculations. Arrays A very basic plot can be drawn using MATLAB arrays. To create an array in MATLAB, use the square brackets: x = [1 2"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/matlab.md,"3 4 5 6] y = [4 4 7 3 7 1] Notice how the array elements do not have commas between them. These arrays could be used like any other variable. Licensing University of Virginia has recently upgraded the Matlab license so that Matlab is available to everyone at UVa. There is one version of Matlab for students, faculty, and staff. MATLAB is available on the Windows, Mac OSX, and Linux platforms. To get started go to the UVa Software Gateway Matlab can be found under the Data Analysis & Research grouping. The Campus Wide License configuration now includes all 100+ MathWorks products. To better reflect the new configuration they have renamed the license option to Campus Wide Suite. For further information see the URL https://data.library.virginia.edu/researchsoftware/matlab/ More Information For help, type help in the Command Window or click on the Help button on the HOME ribbon. For more Information on MATLAB itself, please visit the official website here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/sw-list.md,"Available Packages The following bioinformatics packages are available on the Windows Virtual Machines Bowtie2 For more information on bowtie2, please click [here] (/userinfo/ivy/ivywindowssw/bioinformatics/bowtie2) HISAT2 Requires approval before installation. For more information on HISAT2, please click here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/bowtie2.md,"Bowtie2 is a memoryefficient tool for aligning short sequences to long reference genomes. It indexes the genome using FM Index, which is based on BurrowsWheeler Transform algorithm, to keep its memory footprint small. Bowtie2 supports gapped, local and pairedend alignment modes. Alignment to a known reference using Bowtie2 is often an essential first step in a myriad of NGS analyses workflows. Bowtie2 Usage Alignment using bowtie2 is a 2step process indexing the reference genome, followed by aligning the sequence data. Create indexes of your reference genome of interest stored in reference.fasta file: bowtie2build [option(s)] <reference.fasta <bt2indexbasename This will create new files with the provided basename and extensions .1.bt2, .2.bt2, .3.bt2 and .4.bt2, .rev.1.bt2 and .rev.2.bt2. These files constitute the index. Align pairedend reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores: bowtie2 x <bt2indexbasename 1 <sampleR1.fq 2 \ <sampleR2.fq p <N S <output.sam The alignment results in SAM format are written to the file output.sam More Information For more information, please refer to the Bowtie2 manual. Citation If you use bowtie2 for your work, please cite: Langmead B, Salzberg S. Fast gappedread alignment with Bowtie 2. Nature Methods. 2012, 9:357359"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/hisat2.md,"Please note that HISAT2 requires approval prior to installation on the VM HISAT2 is a fast and sensitive tool for aligning short reads against the general human population (as well as single reference genome). It indexes the genome using a Hierarchical Graph FM Index (HGFM) strategy, i.e. a large set of small indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp). HISAT2 Usage: Alignment using HISAT2 is a 2step process indexing the reference genome, followed by aligning the sequence data. Create indexes of your reference genome of interest stored in reference.fasta file: hisat2build [option(s)] This will create new files with the provided basename and extensions .ht2. These files constitute the index. Align pairedend reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores: hisat2 x 1 2 p S The alignment results in SAM format are written to the file output.sam Note on using the sraacc option Since Ivy VM’s do not allow outside connections, sraacc option will not work with HISAT2. If users plan to use SRA data, they will have to download it and move into Ivy prior to alignment. Please refer to our Globus user guide for help on how to do that. More information For detailed information, please refer to HISAT2 manual. Citation If you use HISAT2 for your work, please cite: Kim D, Langmead B and Salzberg SL. HISAT: a fast spliced aligner with low memory requirements. Nature Methods 2015"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/productivity/sumatrapdf.md,"Sumatra PDF Overview Sumatra PDF is an open source software to view PDF files in Windows. It could be used to view PDF documents stored within the Ivy VM. As of the latest version, Sumatra supports multiple formats including PDF, EPUB, MOBI, and XPS. Running Sumatra PDF From the Start menu, go to All Programs and search for Sumatra PDF. Click on the icon to run it. More Information For more information, visit the Sumatra PDF official website."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/productivity/microsoft-office-professional-plus.md,"Microsoft Office Overview The Ivy Windows VMs are installed with Microsoft Office 2016. Features such as OneDrive are not available since Ivy is not connected to the public internet. Therefore in order to load documents in and out of the VM, you would have to use the Globus DTN. Available Software The following software packages are available for use on the Ivy Windows VM: Word 2016 Excel 2016 PowerPoint 2016 Access 2016 OneNote 2016 Outlook 2016 Publisher 2016 Running Office All Office software could be accessed from the Start menu using Start All Programs More Information For more Information about Microsoft Office, please visit its official website."
rc-website-fork/content/userinfo/hpc/software/spark.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Using Spark interactively There are three ways to use Spark interactively: 1. Shell prompt 1. Open OnDemand PySpark 1. Open OnDemand Desktop If you need the Web UI you must use the third method (OOD Desktop). Shell prompt First submit an ijob. Scala/PySpark To start up a Scala or PySpark shell prompt, run sparkshell or pyspark. For example: $ sparkshell ... Spark context Web UI available at http://udcxxxxxx:4040 Spark context available as 'sc' (master = local[], app id = local1633023285536). Spark session available as 'spark'. Welcome to / / / / \ \/ \/ / / '/ // ./,// //\ version 3.1.2 // Using Scala version 2.12.10 (OpenJDK 64Bit Server VM, Java 11.0.2) Type in expressions to have them evaluated. Type :help for more information. scala ` R To start an R prompt, you must load R first. Then run sparkR. If the R version is different from 4.1.0, you will see a warning message: package ‘SparkR’ was built under R version 4.1.0 We recommend loading the closest available version. Open OnDemand PySpark Python users can run Spark in a JupyterLab interface via the PySpark Interactive App on Open OnDemand. Open OnDemand Desktop Spark provides a user interface (UI) for you to monitor your Spark job. If you intend to use the Web"
rc-website-fork/content/userinfo/hpc/software/spark.md,"UI, you must request a Desktop session through Open OnDemand. The URL is displayed upon launching Spark and is of the form http://udcxxxxxx:4040 where udcxxxxxx is the hostname of the compute node. You can either rightclick on the link and select ""Open Link,"" or enter localhost:4040 in the browser. Jupyter notebook/lab You can redirect pyspark to open a Jupyter notebook/lab as follows. The jupyter command is provided by the jupyterlab module. Set two environment variables: export PYSPARKDRIVERPYTHON=jupyter export PYSPARKDRIVERPYTHONOPTS=lab If you'd prefer a notebook session, replace lab with notebook. Navigate to your working directory and run: pyspark This will start up Jupyter inside a browser automatically. Use the ""Python 3"" kernel. The example below estimates the value of pi in a PySpark session running on 16 cores, with the JupyterLab window on the left and the Spark Web UI event timeline on the right. Note that the SparkContext object sc is initialized automatically. Slurm Script Templates for Batch Jobs Local mode on a single node {{< pullcode file=""/static/scripts/sparksinglenode.slurm"" lang=""nohighlight"" }} You must initialize SparkContext explicitly in your script, e.g.: python from pyspark import SparkContext sc = SparkContext(""local[]"") The Spark log is written to slurm<JOBID.out. After the job is finished, use the seff <JOBID command to verify that the cores are used effectively: $ seff 1232109 ... Cores per node: 10 CPU Utilized: 01:17:16 CPU Efficiency: 82.20% of 01:34:00 corewalltime ... If the CPU efficiency is much lower, please consider using fewer cores for your future jobs. Standalone cluster mode using multiple nodes As of 5/29/2024 this is no longer working. We will update as soon as we have a solution. For the time being please use the local mode up to 96 cores in the afton partition. We gratefully acknowledge the Pittsburg Supercomputing Center for giving us permission to use"
rc-website-fork/content/userinfo/hpc/software/spark.md,"their Spark configuration and launch scripts. Before using multiple nodes, please make sure that your job can use a full standard node effectively. When you request N nodes in the standalone cluster mode, one node is set aside as the master node and the remaining N1 nodes are worker nodes. Thus, running on 2 nodes will have the same effect as running on 1 node. {{< pullcode file=""/static/scripts/sparkmultinode.slurm"" lang=""nohighlight"" }} In the above Slurm script template, note that: Request parallel nodes with exclusive access. You may reduce the number of cores if the job needs more memory per core. Your code should begin with: python from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() sc = SparkContext(conf=conf) spark = SparkSession(sc) You may need to set the number of partitions explicitly, e.g. in the second argument of sc.parallelize(): python sc.parallelize(..., os.environ['PARTITIONS']) where the PARTITIONS environment variable is defined as the total number of cores on worker nodes in the Slurm script for your convenience. Without doing so only one partition will be created on each node. Benchmark We used a code that estimates the value of pi as a benchmark. The following table illustrates good scaling performance across multiple nodes (40 cores per node) on the HPC system. | Nodes | Worker nodes | Time (s) | |:|:|:| |1|1|134.4| |3|2|71.3| |5|4|39.6| |9|8|23.6| Cleanup Temporary files are created inside your scratch directory during a multinode Spark job. They have the form: sparkmst3korg.apache.spark.deploy.master.Master1udcaw332c1.out spark8147c5b8eb704b98809e19fdbcf3eafb app202110121138170000 blockmgrb41a7c79cbf449f0b373f6c6467e9d01 You may safely remove these files when your job is done by running: {{< codesnippet }} find /scratch/$USER maxdepth 1 regextype sed ( name ""spark$USER"" o regex './spark[09az]{8}.' o regex './app[09]{14}.' o regex './blockmgr[09az]{8}.' ) exec rm rf {} \; {{< /codesnippet }} Make sure that you do not use this pattern to name other"
rc-website-fork/content/userinfo/hpc/software/spark.md,files!
rc-website-fork/content/userinfo/hpc/software/wdltool.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/software/engineering.md,"Overview Several software packages for computeraided engineering are available on the UVA HPC system. General considerations Some engineering software packages utilize single node, multicore or multinode MPI for parallel execution. Accordingly, the Slurm job scripts should contain either of the following two SBATCH directives: Single Node MultiCore SBATCH N request M nodes (replace with a number) SBATCH ntaskspernode= request L MPI processes per node You should launch your program with as the MPI executor, for example for Quantum Espresso srun pw.x in mymol.in ` Please see the page of the particular package you wish to use for more details. VASP Users The Vienna AbInitio Simulation Package, is licensed by individual groups and we do not have a common installation. We have basic instructions for building VASP on the HPC system at its page. Available Engineering Software To get an uptodate list of the installed engineering applications, log on to UVA HPC and run the following command in a terminal window: module keyword cae To get more information about a specific module version, run the module spider command, for example: module spider ansys List of Engineering Software Modules {{< rivannasoftware moduleclasses=""cae"" }} Using a Specific Software Module To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module. After loading a module, you are ready to run the application(s) provided by the module. For example: module load ansys"
rc-website-fork/content/userinfo/hpc/software/chemistry.md,"Overview Many popular software packages for computational chemistry are available on Rivanna and Afton. General considerations Most computational chemistry packages utilize MPI for parallel execution. Accordingly, the Slurm job scripts should contain the following two SBATCH directives: Please see the page of the particular package you wish to use for more details. VASP Users The Vienna AbInitio Simulation Package, is licensed by individual groups and we do not have a common installation. We have basic instructions for building VASP on the HPC system at its page. Available Chemistry Software To get an uptodate list of the installed chemistry applications, log on to UVA HPC and run the following command in a terminal window: module keyword chem To get more information about a specific module version, run the module spider command, for example: module spider quantumespresso List of Chemistry Software Modules {{< rivannasoftware moduleclasses=""chem"" }} Using a Specific Software Module To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module. After loading a module, you are ready to run the application(s) provided by the module. For example: module load intel quantumespresso"
rc-website-fork/content/userinfo/hpc/software/quantumespresso.md,"Description {{% moduledescription %}} Local support is not available. For detailed documentation and tutorials, visit the {{% softwarename %}} website. QuantumEspresso (QE) has a large and active community of users; to search or join the mailing list see the instructions here. Software Category: {{% modulecategory %}} Available Versions We built versions of QE incorporating the most popular optional packages. To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Users may build their own versions of QE if they wish to use a different compiler+MPI combination, or to choose individual optional packages. Instructions are available at the installation FAQ. Example Slurm script To run the system version of QE, a script similar to the following can be used. QE has many options so only the most basic is shown. Please run the CPU version on nongpu partitions and the GPU version only on the gpu partition. In both cases, we highly recommend running a benchmark to decide how many CPU cores and/or GPU devices you should use. CPU {{< pullcode file=""/static/scripts/qecpu.slurm"" lang=""nohighlight"" }} GPU {{< pullcode file=""/static/scripts/qegpu.slurm"" lang=""nohighlight"" }}"
rc-website-fork/content/userinfo/hpc/software/compilers.md,"UVA HPC offers multiple compiler bundles for C, C++, and Fortran. Different compilers have different strengths and weaknesses and different error messaging and debugging features, so users should be willing to try another one when appropriate. The modules system manages the compiler environment and ensures that only compatible libraries are available for loading. Many users of compiled languages are working with codes that can employ MPI for multinode parallel runs. MPI users should first understand how their chosen compiler works, then see the MPI instructions at our parallel programming page. Compiled languages can be more difficult to debug, and the assistance of a good debugger can be essential. Descriptions of debuggers available on the HPC system can be found at our debuggers and utilities page. Available Compilers on The HPC System {{< rivannasoftware moduleclasses=""compiler"" exclude=""mpi"" }} GNU Compiler The GNU Compiler Collection compilers are free, opensource tools. Additional tools included are the gdb debugger and the gprof performance profiler. For detailed documentation, visit the GNU website. The compilers are: Fixedformat Fortran: gfortran [options] filename.f Freeformat Fortran: gfortran [options] filename.f90 C: gcc [options] filename.c C++: g++ [options] filename.cpp or g++ [options] filename.cxx A list of compiler options can be obtained by invoking the compiler with the help option. For example: gfortran help More information is available from the manpage, e.g.: man g++ The default GNU compilers on the HPC system are typically fairly old. Newer versions can be invoked through an appropriate module. For available versions, please run module spider gcc It is important to load the correct module if you want to use more advanced features available in newer standards, particularly for C++ and Fortran. module load gcc It may be necessary to use an older compiler for programs using GPGPUs. Available GNU Compilers {{< moduleversions module=""gcc"" }} Intel Compiler"
rc-website-fork/content/userinfo/hpc/software/compilers.md,"The Intel Linux Fortran and C/C++ compilers are for x8664 processorbased systems running Linux. These compilers have specific optimizations for Intel architectures. The University has floating network licenses for the Intel compiler suite as well as for the Math Kernel Libraries. For detailed information, review the documentation on the Intel C/C++ and Fortran compiler website. The Intel compilers are accessed on the cluster by using the modules software to dynamically set the appropriate environmental variables (e.g. PATH and LDLIBRARYPATH). To initialize your environment to use the Intel compilers, use the command: module load intel Fortran fixed format: ifort [options] filename.f Fortran free format: ifort [options] filename.f90 C: icc [options] filename.c C++: icpc [options] filename.cpp or: icpc [options] filename.cxx A list of compiler options can be obtained by invoking the compiler with the help option, e.g.: ifort help or by accessing the manpage, e.g. man ifort To see the available versions, run module spider intel Then load the appropriate module, in this case the default version module load intel Available Intel Compilers {{< moduleversions module=""intel"" }} Important note for Fortran programmers: Nearly all Fortran code must be compiled with the flag heaparrays added or it will encounter a segmentation violation. If you still experience segmentation violations, recompile with g CB (for debugging and bounds checking respectively) and run the program under the control of a debugger. Once the program is debugged, be sure to remove the g and certainly the CB flags and recompile with O or O ipo. If that works, try O3 or O3 ipo for a higher level of optimization. If mathematical libraries are needed, we strongly recommend the Intel Math Kernel Libraries (MKL). They provide LAPACK, BLAS, and a number of other libraries. They are highly optimized, especially for Intel architecture, and they automatically work with the"
rc-website-fork/content/userinfo/hpc/software/compilers.md,"compiler. The Intel module for any version loads the MKL that is compatible with that compiler. The module script sets an environment variable MKLDIR (with MKLROOT as a synonym). This variable can be used in scripts and makefiles. For example, in a [mM]akefile: LIBS=L$(MKLROOT). The MKL consists of a number of libraries, and which ones to link is not always obvious. In newer Intel compilers a flag mkl can be added for both compiler and linker. However, that does not always suffice, so Intel provides a link line advisor at their site. Remember that default integers are 32 bits even on 64bit systems. The MKL bundled with our compiler suite includes ScaLAPACK. NVIDIA CUDA Compiler The NVIDIA HPC SDK C, C++, and Fortran compilers support GPU acceleration of HPC modeling and simulation applications with standard C++ and Fortran, OpenACC directives, and CUDA. NVIDIA CUDA compilers are accessed on the HPC cluster by using modules to dynamically set the appropriate environmental variables (e.g. PATH and LDLIBRARYPATH). To initialize your environment to use the CUDA compilers, use the command module load cuda or module load nvhpc Available NVIDIA CUDA Compilers {{< moduleversions module=""cuda"" }} {{< moduleversions module=""nvhpc"" }} Please see here for details. PGI Compiler Please use the nvhpc module instead (see previous section). Building on the HPC System For more information about building your code on UVA HPC, please see our howto."
rc-website-fork/content/userinfo/hpc/software/python.md,"Overview Python is an integrated technical computing environment that combines sophisticated computation, advanced graphics and visualization, and a highlevel programming language. Learning Python Research Computing offers an online ""Introduction to Programming in Python"" course. Click here to start learning Python. Python on the HPC System The default Python is required for system purposes and is generally too old for applications. We offer Python through the Miniforge distribution. View our Miniforge on the HPC System documentation for details."
rc-website-fork/content/userinfo/hpc/software/openfoam.md,"Description {{< moduledescription }} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Overview OpenFOAM is an opensource, free software package for computational fluid dynamics (CFD). It is powerful, and is an alternative to ANSYS Fluent, but it may have a steeper learning curve than Fluent's. OpenFOAM does not provide a graphical user interface for setting up problems. Tutorials Tutorials are available online. OpenFOAM itself is distributed with a number of tutorials. To find them, run ls $FOAMTUTORIALS Generally there are several tutorials for each topic. Once you have chosen one, you will need to copy all its files to one of your directories, since you cannot write into the general OpenFOAM directories; for example mkdir foamtest cd foamtest cp r $FOAMTUTORIALS/compressible/rhoSimpleFoam/aerofoilNACA0012 Documentation for the tutorials is available at the OpenFOAM site. Parallel Processing OpenFOAM has been compiled on the HPC system to run with MPI. Please see our Slurm documentation for information about running MPI programs. PostProcessing OpenFOAM uses ParaView for visualization of results. You must use the version of Paraview that has been compiled to be compatible with OpenFOAM. It will be loaded automatically when you load the openfoam module. It is invoked through the paraFoam command. You must use our FastX Web access on UVA HPC in order to run Paraview. To invoke paraFoam, start a terminal on the MATE desktop and run"
rc-website-fork/content/userinfo/hpc/software/openfoam.md,vglrun c proxy paraFoam
rc-website-fork/content/userinfo/hpc/software/apptainer.md,"Introduction Apptainer is a continuation of the Singularity project (see here). On December 18, 2023 we migrated from Singularity to Apptainer. Containers created by Singularity and Apptainer are mutually compatible as of this writing, although divergence is to be expected. One advantage of Apptainer is that users can now build container images natively on the UVA HPC system. Apptainer and UVA HPC (after 12/18/2023) Apptainer is available as a module. The RC staff has also curated a library of preprepared Apptainer container images for popular applications as part of the shared software stack. Descriptions for these shared containers can be found via the module avail and module spider commands. module load apptainer module avail Loading any of these container modules produces an onscreen message with instructions on how to copy the container image file to your directory and how to run the container. Building Apptainer Containers To build your own image from scratch, create an Apptainer definition file (say myimage.def) and run: module load apptainer apptainer build myimage.sif myimage.def For containers larger than several GBs we recommend that you perform the build on a compute node in the largemem partition, either through a batch job or an interactive job. Building such containers on the frontend will likely fail silently due to insufficient memory. Details on how to write a definition file will be provided in this forthcoming workshop. What is Inside the Container? Use the shell command to start up a shell prompt and navigate (more later). For containers built with Apptainer, you can use the runhelp command to learn more about the applications and libraries: apptainer runhelp /path/to/sif You can also use the inspect runscript command to find the default execution command. Using the TensorFlow module as an example: This shows that python will be executed when you run"
rc-website-fork/content/userinfo/hpc/software/apptainer.md,"(more later) the container. Running nonGPU Images If your container does not require a GPU, all that is necessary is to load the apptainer module and provide it with a path to the image. module load apptainer apptainer <CMD <OPTIONS <IMAGEFILE <ARGS CMD defines how the container is used. There are three main commands: run: Executes a default command inside the container. The default command is defined at container build time. exec: Executes a specific application/command inside the container as specified with ARGS. This provides more flexibility than the run command. shell: Starts a new interactive command line shell inside the container. OPTIONS define how the apptainer command is executed. These can be omitted in most cases. IMAGEFILE refers to the single Apptainer container image file, typically with a .sif or .simg extension. ARGS define additional arguments passed inside the container. In combination with the exec command they define what command to execute inside the container. apptainer run apptainer run myimage.sif This executes a default application or set of commands inside the container. The default application or set of commands to execute is defined in the image build script and cannot be changed after the container is built. After execution of the default command, the container is closed. apptainer exec apptainer exec myimage.sif python myscript.py This is similar to apptainer run but more versatile by allowing the specification of the particular application or command to execute inside the container. In this example it launches the python interpreter and executes the myscript.py script, assuming that Python was installed into the container image. After execution of the command, the container is closed. apptainer shell apptainer shell myimage.sif This opens a new shell inside the container, notice the change of the prompt: Apptainer Now you can execute any command or application defined in"
rc-website-fork/content/userinfo/hpc/software/apptainer.md,"the container, for example ls to list all files in the current directory: Apptainer ls You can navigate the container file system, including /scratch and /nv, and run any application that is installed inside the container. To leave the interactive container shell, type exit: Apptainer exit Running GPU Images Apptainer can make use of the local NVIDIA drivers installed on the host. To use a GPU image, load the apptainer module and add the nv flag when executing the apptainer shell, apptainer exec, or apptainer run commands. module load apptainer apptainer <CMD nv <IMAGEFILE <ARGS Example: module load tensorflow/2.10.0.sif apptainer run nv $CONTAINERDIR/tensorflow2.10.0.sif myscript.py In the container build script, python was defined as the default command to be executed and apptainer passes the argument(s) after the image name, i.e. myscript.py, to the Python interpreter. So the above apptainer command is equivalent to apptainer exec nv $CONTAINERDIR/tensorflow2.10.0.sif python myscript.py This image was built to include CUDA and cuDNN libraries that are required by TensorFlow. Since these libraries are provided by the container, we do not need to load the CUDA/cuDNN libraries available on the host. Running Images Interactively Start an ijob: ijob A mygroup p gpu gres=gpu c 1 salloc: Pending job allocation 12345 salloc: job 12345 queued and waiting for resources salloc: job 12345 has been allocated resources salloc: Granted job allocation 12345 If your image starts a graphical user interface or otherwise needs a display, you should use the Open OnDemand Desktop rather than a commandline ijob. Once the Desktop is launched, start a terminal window and type the commands as in any other shell. module purge module load apptainer apptainer shell nv /path/to/sif Running Image NonInteractively as Slurm jobs Example script: Interaction with the Host File System Each container provides its own file system. In addition, directories of"
rc-website-fork/content/userinfo/hpc/software/apptainer.md,"the host file system can be overlayed onto the container file system so that host files can be accessed from within the container. These overlayed directories are referred to as bind paths or bind points. The following system directories of the host are exposed inside a container: /tmp /proc /sys /dev In addition, the following user directories are overlayed onto each container by default on the HPC system: /home /scratch /nv /project Due to the overlay these directories are by default the same inside and outside the container with the same read, write, and execute permissions. This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits. The /nv and /project directories refer to leased storage locations that may not be available to all users. Disabling the Default Bind Paths Under some circumstances this default overlay of the host file systems is undesirable. Users can disable the overlay of /home, /scratch, /nv, /project by adding the c flag when executing the apptainer shell, apptainer exec, or apptainer run commands. For example, apptainer run c myimage.sif Adding Custom Bind Paths Users can define custom bind paths for host directories via the B/bind option, which can be used in combination with the c flag. For example, the following command adds the /scratch/$USER directory as an overlay without overlaying any other user directories provided by the host: apptainer run c B /scratch/$USER myimage.sif To add the /home directory on the host as /rivanna/home inside the container: apptainer run c B /home:/rivanna/home myimage.sif"
rc-website-fork/content/userinfo/hpc/software/snakemake.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Snakemake workflow: Snakemake is a workflow management system used to create reproducible and scalable data analyses Workflows are written in Python and can be deployed in parallel on the HPC system Workflows can be executed in containerized environments: Conda or Apptainer Snakemake rules: Snakemake follows the GNU Make paradigm Workflows are defined in rules, starting with the target rule Dependencies between the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be parallelized Config.yaml file: Config files are for users to input filenames and paths for the workflow In the case below, the user inputs 3 samples for a simple RNAseq pipeline Threads can be passed as an argument for multithreading Snakefile: The Snakefile contains the rules of your workflow (the steps) The target rule is your final output, Snakemake will determine the order of the rules in order to create that output Each rule consists of 3 required parts: the input files, the output files, and the shell (command) Below is an example of a rule to align sequences using hisat. The log and threads options are optional, but included for reference The target output is a gene count matrix in a csv format After"
rc-website-fork/content/userinfo/hpc/software/snakemake.md,"the rule alignhisat is completed, the workflow can move to the next rule stringtieassemble Notice that the output of alignhisat is a .bam file, this is now the input to the rule stringtieassemble rule stringtieassemble: input: genomegtf=config['GENOMEGTF'], bam=""alignhisat2/{sample}.bam"" output: ""stringtie/assembled/{sample}.gtf"" threads: config['THREADS'] shell: ""stringtie p {threads} G {input.genomegtf} "" ""o {output} l {wildcards.sample} {input.bam}"" You can add as many rules as you like as long as they are sequential with inputs and outputs Slurm for Snakemake: The Snakemake pipeline can be executed using a SLURM script on the HPC system Below is an example script to submit to the standard partition with 8 threads This script is using a conda environment called rnaseq {{< pullcode file=""/static/scripts/snakemake.slurm"" lang=""nohighlight"" }} Dry Runs: Dryruns are a great way to check your commands before running them The code is printed, but not actually run For a dry run, use snakemake n"
rc-website-fork/content/userinfo/hpc/software/mpi.md,"Overview MPI stands for Message Passing Interface. The MPI standard is defined by the Message Passing Interface Forum. The standard defines the interface for a set of functions that can be used to pass messages between processes on the same computer or on different computers. MPI can be used to program shared memory or distributed memory computers. There is a large number of implementations of MPI from various computer vendors and academic groups. MPI is supported on the HPC clusters. MPI On the HPC System MPI is a standard that describes the behavior of a library. It is intended to be used with compiled languages (C/C++/Fortran). Several implementations of this standard exist. UVA HPC supports OpenMPI for all our compilers and IntelMPI for the Intel compiler. MPI can also be used with the interpreted languages R and Python through packages that link to an implementation; on the HPC system these languages use OpenMPI. Selecting Compiler and Implementation An MPI implementation must be built with a specific compiler. Consequently, only compilers for which MPI has been prepared can be used with it. All versions of the Intel compiler will have a corresponding IntelMPI. For OpenMPI run module spider openmpi This will respond with the versions of OpenMPI available. To see which version goes with which compiler, run module spider openmpi/<version For example: module spider {{< modulefirstversion modulename=""openmpi"" }} Example output: You will need to load all module(s) on any one of the lines below before the ""{{< modulefirstversion modulename=""openmpi"" }}"" module is available to load. gcc/11.4.0 This shows that OpenMPI version {{< modulefirstversion modulename=""openmpi"" }} is available for gcc 11.4.0. Once a choice of compiler and MPI implementation have been made, the modules must be loaded. First load the compiler, then the MPI. For instance, to use OpenMPI with gcc 11.4.0,"
rc-website-fork/content/userinfo/hpc/software/mpi.md,"run module load gcc/11.4.0 module load openmpi To load the Intel compiler version and its IntelMPI version, run module load intel However, for Intel 18.0, run: module load intel/18.0 module load intelmpi/18.0 It is also possible to combine these into one line, as long as the compiler is specified first (this can result in errors if you are not using the default compiler, however) module load gcc openmpi For a detailed description of building and running MPI codes on the HPC system, please see our HowTo. Available MPI library modules {{< rivannasoftware moduleclasses=""mpi"" }} Example Slurm Scripts This example is a Slurm job command file to run a parallel (MPI) job using the OpenMPI implementation: {{< pullcode file=""/static/scripts/mpijob.slurm"" lang=""nohighlight"" }} In this example, the Slurm job file is requesting two nodes with sixteen tasks per node for a total of 32 processes. Both OpenMPI and IntelMPI are able to obtain the number of processes and the host list from Slurm, so these are not specified. In general, MPI jobs should use all of a node, but some codes cannot be distributed in that manner so we are showing a more general example here. Slurm can also place the job freely if the directives specify only the number of tasks. In this case do not specify a node count. This is not generally recommended, however, as it can have a significant negative impact on performance. {{< pullcode file=""/static/scripts/mpijobfreeplacement.slurm"" lang=""nohighlight"" }} Example: MPI over an odd number of tasks {{< pullcode file=""/static/scripts/mpijoboddnumber.slurm"" lang=""nohighlight"" }} MPI with OpenMP The following example runs a total of 32 MPI processes, 8 on each node, with each task using 5 cores for threading. The total number of cores utilized is thus 160. {{< pullcode file=""/static/scripts/hybridjob.slurm"" lang=""nohighlight"" }}"
rc-website-fork/content/userinfo/hpc/software/gurobi.md,"Gurobi The Gurobi Optimizer is a stateoftheart solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multicore processors, using the most advanced implementations of the latest algorithms. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. To load the most recent version of {{% softwarename %}}, at the terminal window prompt run: module load gurobi For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} License and Permission We have an academic site license that allows UVA faculty, staff, and students to use Gurobi on the HPC system. The license is restricted to academic use and all use for commercial purposes is forbidden. Please submit a ticket if you are UVA faculty/staff/student and need access to the software. Using Gurobi There are several ways to use Gurobi. First load the module. Gurobi command prompt Run: gurobi.sh Python To import gurobipy as a Python module, you can use either Gurobi's own python3.7 executable or a different python. gurobi/10.0.1 uses python3.7 and gurobi/11.0.0 uses python3.11. Gurobi Python Please replace python with python3.7 or python3.11 in your Slurm scripts. However, note that Gurobi does not provide pip. If you need additional Python packages please use a nonGurobi Python (e.g. via module load miniforge). See next section. NonGurobi Python Gurobi/10.0.1 supports Python versions 2.7, 3.6 3.9. Please follow the instructions in the module load message. To check the version of your python, run python V. If you are using the base python"
rc-website-fork/content/userinfo/hpc/software/gurobi.md,"from the miniforge module and have trouble installing additional packages, run: bash export PYTHONPATH=$EBROOTMINIFORGE/lib/python3.11:$PYTHONPATH You will still be able to import gurobipy from the gurobi module. Do not install your own gurobipy. If you followed these instructions and still have trouble importing gurobipy in your Python script, please use the Gurobi Python. Julia The GUROBIHOME environment variable is already defined. Load the julia module and run: julia import Pkg Pkg.add(""Gurobi"") Pkg.build(""Gurobi"")"
rc-website-fork/content/userinfo/hpc/software/pytorch.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} PyTorch Jupyter Notebooks Jupyter Notebooks can be used for interactive code development and execution of Python scripts and several other codes. PyTorch Jupyter kernels are backed by containers in the corresponding modules. Accessing the JupyterLab Portal Open a web browser and go to: https://ood.hpc.virginia.edu. Use your “Netbadge” credentials to log in. On the top right of the menu bar of the Open OnDemand dashboard, click on Interactive Apps. In the dropdown box, click on JupyterLab. Requesting access to a GPU node To start a JupyterLab session, fill out the resource request webform. To request access to a GPU, verify the correct selection for the following parameters: Under Rivanna Partition, choose ""GPU"". Under Optional GPU Type, choose a GPU type or leave it as ""default"" (first available). Click Launch to start the session. Editing and Running the Notebook Once the JupyterLab instance has started, you can edit and run your notebook as described here. PyTorch Slurm jobs The following is a Slurm script template. The commented numbers correspond to the items in the ensuing notes. Notes: The Slurm script needs to include the SBATCH p gpuand SBATCH gres=gpu directives in order to request access to a GPU node and its GPU device."
rc-website-fork/content/userinfo/hpc/software/pytorch.md,"Please visit the Jobs Using a GPU section for details. To use the pytorch container, load the apptainer and pytorch modules. You may choose a different version (see module spider above). Do not load the cuda or cudnn modules since these libraries are included with pytorch. The nv flag sets up the container's environment to use a GPU when running a GPUenabled application. The run command executes the default command defined in the container, which in this case is python. What follows after the .sif is passed as arguments. In summary, the apptainer command can be translated as: ""Use the python interpreter inside the pytorch container to execute pytorchexample.py with GPU enabled."" PyTorch Interactive Jobs (ijob) Start an ijob. Note the addition of p gpu and gres=gpu to request access to a GPU node and its GPU device. ijob A mygroup p gpu gres=gpu c 1 module purge module load apptainer pytorch/2.0.1 apptainer run nv $CONTAINERDIR/pytorch2.0.1.sif pytorchexample.py Interaction with the Host File System The following user directories are overlayed onto each container by default on the HPC system: /home /scratch /nv /standard /project Due to the overlay, these directories are by default the same inside and outside the container with the same read, write, and execute permissions. This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits. The /nv and /project directories refer to leased storage locations that may not be available to all users."
rc-website-fork/content/userinfo/hpc/software/ide.md,"Editors Several text editors are available on the HPC system. Most provide features such as syntax coloring. Vim (Vi iMproved) Vim is an updated version of the early Unix text editor vi (for ""visual""). It provides many extensions over plain vi. On the HPC system, the vi command is equivalent to the vim command. Vim is primarily utilized through keyboard commands. Once learned, it is extremely efficient to use. Many tutorials can be found online such as https://vim.fandom.com/wiki/Tutorial. Emacs Emacs is another wellknown Unix text editor. Like vim, it is largely operated through the keyboard. It can run a compiler and debugger so has some of the capabilities of an IDE. An introduction can be found here. Nano Nano is a simplified version of Emacs. It is easy to use and mostly selfexplanatory. An introduction is available at its homepage. Pluma Pluma is a simple WYSIWYG text editor provided by the MATE desktop. It is a variant of gedit and we provide an alias to it, so either name should work. It is very similar to Notepad++ on Windows and can do syntax coloring. IDEs An Integrated Development Environment (IDE) provides more features than a text editor. They are nearly all graphical in nature and so must be used through a graphicscapable frontend. On the HPC system we recommend using them via FastX. Geany Geany is a lightweight IDE. In some respects it is intermediate between a text editor such as pluma and a fullfeatured IDE. It is capable of managing building C/C++/Fortran programs, including through make. It provides syntax coloring for many languages other than the three compiled languages. It is accessed through a module: module load geany {{% moduleversions module=""geany"" %}} Code Server See here {{% moduleversions module=""codeserver"" %}}"
rc-website-fork/content/userinfo/hpc/software/gaussian.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} License and Permission We have a site license. Please contact us if you need access to the software. (NonPIs should ask their PI to send in the request.) For research, please load gaussian. For course work, please load gaussian/grads16. GaussView The GaussView graphical interface is available on the UVA HPC system. Users must log in through a client capable of displaying X11 graphics. We recommend the Desktop session through Open OnDemand. GaussView can be used to create input files for Gaussian jobs which should then be submitted to the compute nodes. To start GaussView, in an X11enabled terminal first load the gaussian module as above, then run gview & The ampersand (&) returns the terminal to input mode while GaussView is running. Note: If the above command launches a text editor instead of GaussView, either you have not been granted access or your PATH is incorrectly configured. In either case, please run the following commands and send us the output: SingleCore Slurm Script This is a Slurm job command file to run a Gaussian 16 batch job. In this example, the Gaussian 16 input is in the file h2o.com. If no output file is specified, it will go to h2o.log. The"
rc-website-fork/content/userinfo/hpc/software/gaussian.md,"script assumes it will be submitted from the user's /scratch directory and the input file is in the same directory. Gaussian also tends to use a lot of memory, so we make sure to request the amount per core that is available. We pass that to g16 with the m flag. Be sure the value is less than or equal to what you request from Slurm. {{< pullcode file=""/static/scripts/gaussianserial.slurm"" lang=""nohighlight"" }} Multicore Gaussian Job By default, Gaussian runs on a single core. However, many jobs can efficiently utilize more than one core on a node. See the Gaussian documentation for their recommendations. Not all jobs will scale at all, and some will scale only to a limited number of cores, so it's important to run tests and track the speedup for multicore jobs, so as not to waste resources or service units. The Gaussian documentation on multicore jobs contains instructions to specify core numbers and they are moving to this system, away from users specifying the number of cores. However, on a resourcemanaged system the user must not specify core numbers, since these are assigned by Slurm. Gaussian 16 still provides an option to request a particular number of cores. The safest way in a resourcemanaged environment is to use the commandline option with a Slurm environment variable. {{< pullcode file=""/static/scripts/gaussianmulticore.slurm"" lang=""nohighlight"" }} Multinode Computations with Linda A few types of computation can make effective use of more than one node through Gaussian's Linda parallel execution system. The Gaussian documentation states that ""HF, CIS=Direct, and DFT calculations are Linda parallel, including energies, optimizations, and frequencies. TDDFT energies and gradients and MP2 energies and gradients are also Linda parallel. Portions of MP2 frequency and CCSD calculations are Linda parallel."" Only a few very large scale computations should need to use Linda."
rc-website-fork/content/userinfo/hpc/software/gaussian.md,"If your code does not scale well past a small number of threads, you may be able to use multiple nodes to increase the effective number of processes. Some jobs may also scale acceptably beyond 20 cores and so will benefit from Linda. Linda requires that your processes be able to ssh between nodes and you must specify ssh in the Link 0 section of your description file with To request permission for internode ssh, please contact us. Linda does not utilize the highspeed Infiniband network, so it is best to use one Linda worker per node. You specify the node list using information from Slurm, then use a cpuspertask directive as for the multicore case above. {{< pullcode file=""/static/scripts/gaussianmultinode.slurm"" lang=""nohighlight"" }} galloc: could not allocate memory According to here: Explanation of error This is a memory allocation error due to lack of memory. Gaussian handles memory in such a way that it actually uses about 1GB more than %mem. Fixing the error The value for %mem should be at least 1GB less than the value specified in the job submission script. Conversely, the value specified for mem in your job script should be at least 1GB greater than the amount specified in the %mem directive in your Gaussian input file. The exact increment needed seems to depend on the job type and input details; 1GB is a conservative value determined empirically. If you encounter this error, please keep the m value passed to g16 constant and increase the mem value passed to SBATCH. For example: See here for a list of common Gaussian error messages."
rc-website-fork/content/userinfo/hpc/software/cromwell.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} The Backend Configuration In order to allow Cromwell to interact with the HPC system (via SLURM), we need to define a backend to dispatch jobs. A Cromwell configuration file, written in HOCON syntax, can be used to define the execution behavior of the pipeline and its integration with a job scheduler, in this case SLURM. The following configuration can be used as a base, you can save it as cromwellrivanna.conf in your home directory on the system. The include statement: The default Cromwell configuration values are set via Cromwell’s application.conf file that is part of the Cromwell installation. To ensure that you always have the defaults from the application.conf, you must include it at the top of your new configuration file. include required(classpath(""application"")) ... Slurm backend In our customized cromwellrivanna.conffile, the Slurm backend is specified via the actorfactory field and should be set to ConfigBackendLifecycleActorFactory. backend { default = ""SLURM"" providers { SLURM { actorfactory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" config { ... } } } } The config field contains custom configurations defined in the following subfields: root: defines the working directory filesystems: defines file copying and duplication strategies runtimeattributes: defines job scheduler parameters submit, jobidregex, checkalive, kill: define key job scheduler commands"
rc-website-fork/content/userinfo/hpc/software/cromwell.md,"The root field This backend assumes that the Cromwell process and the jobs both have read/write access to the current working directory of the job. ... config { root = ""workdir"" ... } ... When Cromwell runs a workflow, it will create a directory ./workdir/<workflowuuid. This is called the workflowroot and it is the root directory for all activity in this workflow. The filesystems field This block defines the filesystem to store the directory structure and results of an executed workflow. ... config { ... filesystems { local { localization : [""copy"", ""hardlink"", ""softlink""] caching { duplicationstrategy: [""copy"", ""hardlink"", ""softlink""] hashingstrategy: ""file"" } } } } ... Each call has its own subdirectory located at <workflowroot/call<callname. This is the <calldir. Any input files to a call need to be localized into the <calldir/inputs directory. The above stanza defines the localization strategy copy / hardlink / softlink, in that order, until one works. The caching block defines Cromwell’s behavior if call caching is enabled. The runtimeattributes field The next codeblock defines default runtime attributes for each call. ... config { ... runtimeattributes = """""" Int runtimeminutes = 600 Int cpu = 1 Int requestedmemorymb = 8000 String queue = ""standard"" String allocation = ""MYALLOCATION"" """""" } ... Here, we are initializing various runtime variables: runtimeminutes, cpu, requestedmemorymb, queue, allocation with default values, i.e. we are defining our workflow environment. You can keep the defaults but must update the allocation field with the name of your specific allocation. {{% callout %}} Note: The runtime attributes defined in your WDL task will override these defaults. This is useful to customize the environment for each call! {{% /callout %}} The submit field The <workflowroot/call<callname/execution/ directory for each call will contain a script file, which will have the Slurm job submission command formed by the"
rc-website-fork/content/userinfo/hpc/software/cromwell.md,"submit codeblock, using the runtime attributes defined earlier. The jobidregex, checkalive, and kill configuration values define how to capture the job identifier from the stdout of the submit command, how to check if the job is still running, and how to kill the job. Running Cromwell The path to your custom Cromwell backend configuration file is passed as Dconfig.file command line option. To submit and run the pipeline as defined in the myWorkflow.inputs.json and myWorkflow.wdl files, execute these command: module load {{% modulefirstversion %}} java Dconfig.file=~/cromwellrivanna.conf \ jar /path/to/cromwell[VERSION].jar \ run myWorkflow.wdl \ i myWorkflow.inputs.json An example of how to run a bioinformatics pipeline is documented here. Additional Documentation Please refer the Cromwell Backends documentation for additional details."
rc-website-fork/content/userinfo/hpc/software/rstudio.md,"Overview RStudio Server is a webbased interface to RStudio a development environment for R programming. Research Computing provides a web portal where RStudio Server can be accessed on the HPC system. However, to use RStudio Server, you must have an account on UVA HPC. Instructions for setting up an account can be found here. Accessing RStudio Server To access RStudio Server, you will begin by connecting to our Open OnDemand portal: Open a web browser and go to URL: https://ood.hpc.virginia.edu. Use your “Netbadge” credentials to log in. On the top right of the menu bar of the Open OnDemand dashboard, click on “Interactive Apps”. In the dropdown box, click on “RStudio Server”. Note that if you want to use your local R packages installed through the R module, you need to run echo ""RLIBSUSER=~/R/goolf/x.y"" ~/.Renviron before launching the instance. Here x.y is the majorminor version of R, e.g. 4.3. Alternatively, you may run inside RStudio: .libPaths('~/R/goolf/x.y') Requesting an Instance Your instance (or copy) of RStudio will run on a compute node. So, it will need a list of resources, such as partition, time, and allocation. If you are new to UVA HPC, you may want to read the Getting Started Guide to learn more about the partitions. After connecting to RStudio through Open OnDemand, a form will appear where you can fill in the resources for RStudio. When done filling in the resources, click on the blue “Launch” button at the bottom of the form. It will take a few minutes for the system to gather the resources for your instance of RStudio. When the resources are ready a “Connect to RStudio Server” button will appear. Click on the button. Using RStudio When RStudio opens in your web browser, it will appear just like the RStudio that you have on"
rc-website-fork/content/userinfo/hpc/software/rstudio.md,"your laptop or desktop. You can use it just as you always have, including installing packages. (If you have not used RStudio in the past, you may wish to review this tutorial.) Closing the Interactive Session When you are done, quit the RStudio Server application and terminate the session. The interactive session will be closed and the allocated resources will be released. If you leave the session open, your allocation will continue to be charged until the specified time limit is reached."
rc-website-fork/content/userinfo/hpc/software/workflow_managers.md,Workflow managers are used to create reproducible and scalable analysis pipelines. These managers are useful when you have a series of scripts that you want to tie together in the form of a pipeline. The most popular workflow managers on the HPC system are listed below: Snakemake Snakemake is a workflow management system written in Python. It integrates with both conda environments and singularities. Cromwell Cromwell is a Workflow Management System geared towards scientific workflows Nextflow Nextflow enables scalable and reproducible scientific workflows using software containers
rc-website-fork/content/userinfo/hpc/software/namd.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} NAMD with MPI The NAMD module was built on the HPC system with MPI support. Below is a Slurm script template. {{< pullcode file=""/static/scripts/namd.slurm"" lang=""nohighlight"" }} You may want to benchmark it to see how well it scales for the type of job that you are running. Please refer to our tutorial on this topic."
rc-website-fork/content/userinfo/hpc/software/vasp.md,"Description {{% moduledescription %}} Local support is not available. The package is supported by its developers through their documentation site. VASP is licensed software and licenses are issued to individual research groups. Each group must build and maintain its own copy of the code. Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Building VASP VASP is typically built with the Intel compiler and relies on Intel's Math Kernel Libraries (MKL). VASP users should read our documentation for this compiler before beginning. VASP version 5.4.1 and up provides a sample makefile.include.linuxintel that can be modified for local requirements and for different distributions of MPI. We recommend that users copy makefile.include.linuxintel from the arch subdirectory to makefile.include in the toplevel VASP directory, i.e. cp arch/makefile.include.linuxintel ./makefile.include This makefile.include is preconfigured to use the Intel compiler, IntelMPI, and the Intel MKL libraries. We recommend a few local modifications: VASP is written primarily in Fortran and on the HPC system the compiler option heaparrays should be added to the makefile.include. This can be added to the FFLAGS variable, e.g. FFLAGS = heaparrays assume byterecl w It is advisable to change the SCALAPACK library name to lmklscalapacklp64.so. To use OpenMPI, the user must also change the Fortran compiler to FC=mpif90 and the BLACS library to lmklblacsopenmpilp64 while leaving SCALAPACK = lmklscalapacklp64.a. Installation details can be found on the VASP wiki:"
rc-website-fork/content/userinfo/hpc/software/vasp.md,"5.x, 6.x. Example Slurm script To run VASP, the user prepares a group of input files with predetermined names. The path to the vasp binary must be provided to the Slurm process manager srun; in the example below we assume it is in a directory bin under the user's home directory. All input and potential files must be located in the same directory as the Slurm job script in this example. {{< pullcode file=""/static/scripts/vasp.slurm"" lang=""nohighlight"" }} Known issues Slow CHGCAR file write We have received a few reports that a VASP job may occasionally appear to hang at the end during the ""writing wavefunction"" step. The slowness actually happens to CHGCAR instead of WAVECAR (the cause of which is unclear). You can disable the file write in INCAR: LCHARG = .FALSE. Alternatively, if you set up VASP jobs using ASE's Python package, you can disbale CHGCAR writing using: lcharg = False vaspgam on AMD node When running vaspgam on AMD nodes (i.e. all nodes in parallel, Afton nodes in standard), ScaLAPACK must be disabled or else your job may hang at the first electronic step. In INCAR: LSCALAPACK = .FALSE. The ASE Python pacakge disables ScaLAPACK through the line: lscalapack = False Alternatively, if your job fits on 40 cores or fewer, you can choose not to disable ScaLAPACK and run it in standard with the rivanna constraint so that it will not land on an AMD node: All ASE tags for the INCAR can be found in the GitHub repository for ASE's VASP calculator."
rc-website-fork/content/userinfo/hpc/software/gatk.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. For a GitHub reference, visit: https://github.com/broadinstitute/gatk Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Note: Make sure to invoke GATK using the gatk wrapper script rather than calling the jar directly, because the wrapper will select the appropriate jar file (there are two!) and will set some parameters for you. For help on using gatk itself, run gatk help To print a list of available tools, run gatk list To print help for a particular tool, run gatk ToolName help General Syntax To run a GATK tool locally, the syntax is: gatk ToolName toolArguments Basic Usage Examples Below are few trivial examples of using GATK4 tools in singlecore mode. 1. PrintReads PrintReads is a generic utility tool for manipulating sequencing data in SAM/BAM format. In order to print all reads that have a mapping quality above zero in 2 input BAMs (say input1.bam and input2.bam) and write the output to output.bam. gatk PrintReads \ I input1.bam \ I input2.bam \ O output.bam \ readfilter MappingQualityZero 2. HaplotypeCaller The HaplotypeCaller is capable of calling SNPs and indels simultaneously via local denovo assembly of haplotypes in an active region. Basic syntax for variantonly calling on DNAseq. gatk javaoptions ""Xmx4g"" HaplotypeCaller \ R reference.fasta \ I sample1.bam [I sample2.bam ...] \ [dbsnp"
rc-website-fork/content/userinfo/hpc/software/gatk.md,"dbSNP.vcf] \ [strandcallconf 30] \ [L targets.intervallist] \ o output.raw.snps.indels.vcf Note: Here, we are setting the maximum Java heap size to 4GB. This argument varies based on the volume of data athand. Note: If you are working with human reference genome, please refer the local genome repository on Rivanna at /project/genomes/Homosapiens/ for the reference.fasta, as well as the corresponding GATK data bundle at /project/genomes/Homosapiens/GATKbundle/, for resource files like the dbSNP, hapmap, 1000G. No need to download them to your working directory. For example: if you were to run HaplotypeCaller on referencealigned BAMs for 3 samples (say sample1hg38.bam, sample2hg38.bam and sample3hg38.bam), accessing files from the Rivanna genomes repository. gatk javaoption ""Xmx4g"" HaplotypeCaller \ R /project/genomes/Homosapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.fa \ I sample1hg38.bam \ I sample2hg38.bam \ I sample3hg38.bam \ dbsnp /project/genomes/Homosapiens/GATKbundle/hg38/dbsnp146.hg38.vcf.gz \ strandcallconf 30 \ o output.raw.snps.indels.vcf The output will be written to the file output.raw.snps.indels.vcf, in the Variant Call Format. Parallelism in GATK4 The concepts involved and their application within GATK are well explained in this article. In GATK3, there were two options for tools that supported multithreading, controlled by the arguments nt/numthreads and nct/numcputhreadsperdatathread. In GATK4, tools take advantage of an opensource industrystandard Apache Spark software library. Sparkenabled GATK tools Not all GATK tools use Spark. Check the respective Tool Doc to make sure of Sparkcapabilities. Briefly; Spark is a piece of software that GATK4 uses to do multithreading, which is a form of parallelization that allows a computer (or cluster of computers) to finish executing a task sooner. You can read more about multithreading and parallelism in GATK here. The ""sparkified"" versions have the suffix ""Spark"" at the end of their names. Many of these are still experimental; please carefully check for expected output, and validate against nonspark tools. You DO NOT need a Spark cluster to run Sparkenabled GATK tools! While"
rc-website-fork/content/userinfo/hpc/software/gatk.md,"working on a compute node (with multiple CPU cores), the GATK engine can use Spark to create a virtual standalone cluster in place, for its multithreaded processing. ""local""Spark Usage Example: The PrintReads tool we explored above has a Spark version called: PrintReadsSpark. In order to set up a local Spark environment to run the same job using 8 threads, we can use the sparkmaster argument. gatk PrintReadsSpark \ sparkmaster local[8] \ I input1.bam \ I input2.bam \ O output.bam \ readfilter MappingQualityZero Note: Make sure to request for 8 CPU cores before executing the above command, either by starting an interactive session using ijob or by submitting the job via a Slurm batch submission script. Below is an example gatkprintReadsSpark.slurm.sh batch submission script for the above job. {{< pullcode file=""/static/scripts/gatk.slurm"" lang=""nohighlight"" }} Note: replace <YOURALLOCATION with your allocation group. To submit the job. sbatch gatkprintReadsSpark.slurm.sh To monitor the progress of the job. jobq OR squeue u <mst3k replace <mst3k with your computing ID."
rc-website-fork/content/userinfo/hpc/software/deeplabcut.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Dockerfile We cannot use the official Docker image on the UVA HPC system because: the CUDA version is incompatible with our NVIDIA driver version (as of August 2021); at runtime it tries to download pretrained models inside the container, which is not possible via Apptainer. For further details please visit here. Usage Python script Please submit jobs to the GPU partition. A Slurm script template is provided below. {{< pullcode file=""/static/scripts/deeplabcut.slurm"" lang=""nohighlight"" }} GUI Please request a Desktop session on the GPU partition via our Open OnDemand portal. Open a terminal and load the module. Then execute: {{< codesnippet }} module load apptainer deeplabcut apptainer run nv $CONTAINERDIR/deeplabcut2.2.1.1anipose.sif m deeplabcut {{< /codesnippet }}"
rc-website-fork/content/userinfo/hpc/software/imageprocessing.md,"Available Software To get an uptodate list of the installed image processing and visualization tools, log on to UVA HPC and run the following command in a terminal window: module keyword vis To get more information about a specific module version, run the module spider command, for example: module spider blender/2.78c List of Image Processing and Visualization Software Modules {{< rivannasoftware moduleclasses=""vis"" }} Running Interactive Visualizations Many of the provided image processing and visualization applications provide a graphical user interface (GUI). In order to use a GUI on the HPC system, users must log in through a client capable of displaying X11 graphics. We recommend FastX Web which provides a GPU to accelerate rendering. To start an applications GUI in an X11enabled terminal, first load the software module and then run the GUI application executable, e.g. module load blender When connected to UVA HPC via FastX Web, rendering of the graphical user interface can be accelerated by executing this command: module load blender vglrun c proxy blender & The ampersand & returns the terminal to input mode while the application is running."
rc-website-fork/content/userinfo/hpc/software/cellprofiler.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} The latest version of CellProfiler is available as an Apptainer container. Containers encapsulate applications, in this case CellProfiler, and all their required libraries isolated from the application and libraries provided by the system. The basic concepts of software containers, and Apptainer container in particular, are explained here. We recommend using the latest CellProfiler container version whenever possible. Please contact us for help with this package. CellProfiler can be run interactively with a graphical user interface (GUI) or noninteractively without any user interface. The interactive GUI mode is used to define image analysis pipelines; the noninteractive mode is used for image batch processing based on previously configured image analysis pipelines. Preparation The CellProfiler container image file is provided in a shared user space. For best performance it is recommended that users copy this container to their individual /scratch storage location. This has to be done only once and the following steps describe this process. In a terminal window on the HPC system execute these commands: module load apptainer cellprofiler cp $CONTAINERDIR/cellprofiler4.2.5.sif /scratch/$USER Image Pipeline Configuration Option A: ssh terminal In a terminal window on your local workstation execute the following command: ssh Y YOURID@login1.hpc.virginia.edu Continue with instructions under Starting an interactive CellProfiler"
rc-website-fork/content/userinfo/hpc/software/cellprofiler.md,"job. Please note that this option may be very slow. Option B: Starting an interactive CellProfiler job To start an interactive job and launch the CellProfiler graphical user interface from within the container, obtain desktop through the Open OnDemand Desktop app, start a terminal window, then run the following commands module load apptainer cellprofiler apptainer run $CONTAINERDIR/cellprofiler4.2.5.sif Noninteractive Slurm jobs for batch image processing If you have a large number of images that all need to be processed in the same manner, you can use compute nodes for efficient noninteractive batch image processing. The details of CellProfiler's batch processing strategy are explained here. Setup Move image files to be processed to a directory accessible on the HPC system (ideally /scratch). Use an interactive CellProfiler session to define a CellProfiler image analysis pipeline file (.cppipe) that defines how those particular images are to be processed, see Interactive Jobs with Graphical User Interface for Image Pipeline Configuration. In the interactive CellProfiler session, add the CreateBatchFiles module to the end of your pipeline and click Analyze Images. This will create the file Batchdata.h5 which defines the entire image processing step including paths to the images. Note: The pipeline batch file created in step 3 contains hardcoded paths to the tobeprocessed image files. So steps 2 and 3 need to be repeated when you want to process images in a different directory. Create and submit the Slurm job script A general premise in the batch processing workflow is that processing of images can occur independently from each other in a parallel fashion. The easiest way to implement parallel image processing with CellProfiler is to create a job array where each job in the array (referred to as job array task) processes a unique subset of the total image set. Let us assume that we"
rc-website-fork/content/userinfo/hpc/software/cellprofiler.md,"have a directory with 100 image files to process in /scratch/$USER/images and that we completed steps 13 as described above. The following two steps create the Slurm job script and submit it to the cluster: Create/edit Slurm job script /scratch/mstk/cpjobs/cellprofiler.slurm (see below). This script defines a job array with 100 tasks, each task processing a single image, loads the cellprofiler container module and runs the CellProfiler.py script inside the container, and passes the /scratch/$USER/pipelines/Batchdata.h5 file with the image processing definition to the CellProfiler instance Run these commands to submit the job and execute the preconfigured image analysis pipeline.: cd /scratch/$USER/cpjobs sbatch cellprofiler.slurm The Slurm job script cellprofiler.slurm: {{< pullcode file=""/static/scripts/cellprofiler.slurm"" lang=""nohighlight"" }} The directive SBATCH array=100 defines the size of the job array, i.e. the creation of 100 job tasks, each running a single CellProfiler instance. The directive SBATCH cpuspertask=1 specifies that each job task, i.e. each CellProfiler instance, runs on a single cpu core since CellProfiler does not support multithreading. SLURMARRAYTASKID is a variable set by Slurm when the job is running. For each job array task this variable is set to a unique value between 1 and 100 (job array size). We use it to define for each job array task which image needs to be processed."
rc-website-fork/content/userinfo/hpc/software/lammps.md,"Description LAMMPS (Largescale Atomic/Molecular Massively Parallel Simulator) is a moleculardynamics code. The code is designed to be easy to modify or extend with new functionality. LAMMPS can run on a single core but is designed to be highly efficient running on a large number of cores in parallel using messagepassing techniques and a spatial decomposition of the simulation domain. It solves systems ranging from single atoms through polymers and proteins to rigid collections of particles. A variety of force fields is supported. Local support is not available. LAMMPS has documentation and tutorials at its Website. It has a large and active community of users; to search or join the mailing list see the instructions here. Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Users may build their own versions of LAMMPS if they wish to use a different compiler and MPI combination, or to choose individual optional packages. Instructions are available at the LAMMPS Getting Started"" page. Example Slurm script To run the system version of LAMMPS, a script similar to the following can be used. LAMMPS has many options so only the most basic is shown. CPU {{< pullcode file=""/static/scripts/lammpscpu.slurm"" lang=""nohighlight"" }} GPU {{< pullcode file=""/static/scripts/lammpsgpu.slurm"" lang=""nohighlight"" }}"
rc-website-fork/content/userinfo/hpc/software/paraview.md,"Description {{< moduledescription }} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Interactive Sessions through Open Ondemand Interactive sessions of {{% softwarename %}} can be launched through the HPC web portal, Open OnDemand. To launch an instance of {{% softwarename %}}, you will begin by connecting to our Open OnDemand portal. Your {{% softwarename %}} session will run on a GPU node. In addition, you need to specify required resources, e.g. time, your UVA HPC allocation, etc.. If you are new to UVA HPC, you may want to read the Getting Started Guide to learn more about the partitions. Starting an Interactive Session Open a web browser and go to URL: https://ood.hpc.virginia.edu. Use your Netbadge credentials to log in. This will open the Open OnDemand web portal. On the top banner of the Open OnDemand dashboard, click on Interactive Apps. In the dropdown box, click on {{% softwarename %}}. After connecting to {{% softwarename %}} through Open OnDemand, a form will appear where you can fill in the resources for {{% softwarename %}}. {{% softwarename %}} supports GPUs and should be run in the GPU partition. When done filling in the resources, click on the blue Launch button at the bottom of the form. Do not click the button multiple times. It may take a few minutes for the system to gather the resources for"
rc-website-fork/content/userinfo/hpc/software/paraview.md,"your instance of {{% softwarename %}}. When the resources are ready a Launch {{% softwarename %}} button will appear. Click on the button to start {{% softwarename %}}. Using {{% softwarename %}} When {{% softwarename %}} opens in your web browser, it will appear just like the {{% softwarename %}} that you have on your laptop or desktop. Closing the Interactive Session When you are done, quit the {{% softwarename %}} application. The interactive session will be closed and the allocated resources will be released."
rc-website-fork/content/userinfo/hpc/software/cellranger.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/software/gpu.md,"{{% callout %}} Please note that certain modules can only run on specific GPU types. This will be displayed in a message upon loading the module. Certain software applications may also able to take advantage of the advanced capabilities provided by the NVIDIA DGX BasePOD™. Learn More {{% /callout %}} {{< rivannasoftware moduleclasses=""all"" arch=""gpu"" }}"
rc-website-fork/content/userinfo/hpc/software/rapidsai.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/software/modules.md,"The lmod modules system on the HPC system enables users to easily set their environments for selected software and to choose versions if appropriate. The lmod system is hierarchical; not every module is available in every environment. We provide a core environment which contains most of the software installed by Research Computing staff, but software that requires a compiler or MPI is not in that environment and a compiler must first be loaded. {{< button buttonclass=""primary"" buttontext=""View All Modules"" buttonurl=""/userinfo/hpc/software/completelist/"" }} Basic Commands List all available software {{< codesnippet }} module avail {{< /codesnippet }} Use key to list all modules in a particular category. The current choices are {{< codesnippet nobutton }} base, bio, cae, chem, compiler, data, debugger, devel, geo, ide, lang, lib, math, mpi, numlib, perf, phys, system, toolchain, tools, vis, licensed {{< /codesnippet }} Example: {{< codesnippet }} module key bio {{< /codesnippet }} Load the environment for a particular package {{< codesnippet }} module load thepackage {{< /codesnippet }} If you do not specify a version, the system default is loaded. For example, to load the default version of our Python distribution, run {{< codesnippet }} module load miniforge {{< /codesnippet }} If you do not wish to use the default version chosen by the module's environment, you must specify the version explicitly. For example, to select a version of the gcc compiler suite that is different from the default: {{< codesnippet }} module load gcc/13.3.0 {{< /codesnippet }} Note that the 'default' version of a module may change, so if you are developing applications yourself we highly recommend that you load explicit versions of modules; that is, do not invoke the default package, but specify a version. If the version is eventually dropped for newer versions, loading the module will fail, which will make"
rc-website-fork/content/userinfo/hpc/software/modules.md,"you aware that you will need to update your application appropriately. Remove a module {{< codesnippet }} module unload thepackage {{< /codesnippet }} List all modules you have loaded in the current shell {{< codesnippet }} module list {{< /codesnippet }} Change from one version to another {{< codesnippet }} module swap {{< /codesnippet }} For example, if you have loaded gcc/11.4.0 and you wish to switch to intel/2023.1 {{< codesnippet }} module swap gcc/11.4.0 intel/2023.1 {{< /codesnippet }} This will unload the gcc/11.4.0 environment entirely, and load the intel/2023.1 environment. Clear all modules you have loaded module purge Finding prerequisites Use ""module spider"" to find all possible modules. Once you determine the name of the software you want to use, spider will also show you all available versions. Note that spider will sometimes show modules whose version numbers begin with a period, e.g. icc/.17.0. Any such module is “hidden” and users should not be required to load them explicitly. For many applications, you will need to load one or more prerequisites. For example, to find out how to load a specific R version, run module spider R/version where you must replace ""version"" with the value. At one particular time, module spider shows us that the following R versions are available: This shows that two versions of R 3.6.3 are available, one built with the gcc compiler and one built with the Intel compiler suite. We must choose one of those. We'll select the gcc version. module load gcc/7.1.0 module load openmpi/3.1.4 module load R/3.6.3 More about these commands can be found in the documentation. Modules Best Practices Whenever several modules are loaded at the same time, there is the potential for modules to conflict with one another. Conflicting modules can cause code dependent on these modules to not work."
rc-website-fork/content/userinfo/hpc/software/modules.md,"Here are some ways to commit to best practices when using modules: Start with a clean slate module purge before beginning your workflow. If you need to switch what you are doing within the current session, like changing from working in Python to R, purge and load your new modules so there is no chance for conflicts. Know what you are loading When loading modules, it is best to specify what version you are using instead of using the default. If you commit to using the default option each time, you may miss when our default changes and load modules that are no longer compatible with your workflow. Use module spider to see what versions of each module we offer. Advanced Usage If you are consistently loading the same modules on startup, you might find it convenient to load your modules using your .bashrc file. This is NOT within best practices. Interactive apps like Jupyter Labs and RStudio automatically load some modules that they are dependent on. Your .bashrc file still executes on startup within those settings, thus leading to potential conflicts. A better way to load modules efficiently is to use a bash script. Writing a bash script that will load all your necessary modules with an aliased command falls more within best practices than filling your .bashrc file. Whenever you need to switch to a new workflow, module purge then execute your other script. Remember to change your scripts whenever we offer different versions of the modules that you use so your scripts are not out of date. Modules in Job Scripts After the definition of job variables, and before the command line to run the program, add module load lines for every application that you want included in your run environment. Although it is not required, we"
rc-website-fork/content/userinfo/hpc/software/modules.md,"also recommend that you clear your module environment before your job starts executing. For example, to run R version 3.6.3 in the module environment described above, your job script should resemble the following: Please contact us if you encounter any problems using these applications in your job scripts. Creating Your Own Modules If you install your own software, you can create your own modules stored in your home directories. This is not necessary; you can also add the path to your .bashrc file or write a script or just provide the path. But it may be convenient to have a custom module. The first step is to create a directory to store the modulefiles used by lmod. {{< codesnippet }} mkdir $HOME/modulefiles {{< /codesnippet }} Next, download the software package for which you would like to create a module. Carefully follow the instructions to install your software. Remember that most packages assume you can install as an administrator, and that is not permitted on the HPC system, so you must change your installation directory. If you are compiling software you can find a tutorial at our training site. In the following example we are installing a version of git according to its instructions: $ wget https://www.kernel.org/pub/software/scm/git/git2.6.2.tar.gz $ tar xf git2.6.2.tar.gz $ cd git2.6.2 $ ./configure prefix=$HOME/git/2.6.2 $ make $ make install Once the software has been installed, a modulefile (written in the Lua programming language) must be created. The modulefile should be placed in a subdirectory named after the software and the file should be named using the version number of the software. Any text editor of your choice may be used. The modulefile in this example adds ~/git/2.6.2/bin to the user's path so that the personal version of git can be found. $ cd ~/modulefiles $ mkdir git $"
rc-website-fork/content/userinfo/hpc/software/modules.md,"cd git Use an editor to create the following file and name it 2.6.2.lua local home = os.getenv(""HOME"") local version = myModuleVersion() local pkgName = myModuleName() local pkg = pathJoin(home,pkgName,version,""bin"") prependpath(""PATH"", pkg) The first line reads the user’s HOME directory from the environment. The second line uses the “pathJoin” function provided from Lmod. It joins strings together with the appropriate number of “/”. The last line calls the “prependpath” function to add the path to this version of git at the beginning of the user’s path. Finally, to use the module: $ module use $HOME/modulefiles $ module load git $ which git ~/git/2.6.2/bin/git Finding Modules with the Same Name If the user has created a module and at a later date the system provides a newer version, the default behavior if no version is specified to the module load command is to use the highest version available in all module paths searched. The module load command will load git/3.5.4 because it is the highest version. More about user created modules can be found in the documentation."
rc-website-fork/content/userinfo/hpc/software/bwa.md,"Description {{% moduledescription %}} BWA provides three alignment algorithms: BWAbacktrack BWASW BWAMEM The BWAbacktrack algorithm is exclusively used for short sequence reads up to 100bp, the latter two can be used for sequence reads of up to 1MB. The BWAMEM algorithm can also be used for highquality short Illumina sequence reads (< 100bp) in many cases with better performance compared to the original BWAbacktrack algorithm. Therefore, the more universal BWAMEM algorithm is recommended as a starting point for most alignment scenarios. Before any of the alignment algorithms can be used, a FMindex needs to be constructed for the reference genome (see below). Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. For a GitHub reference, visit: https://github.com/lh3/bwa Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Slurm Script Examples Creating a BWA Index for a Reference Genome Index files are created with the bwa index command. A reference genome sequence in FASTA format needs to be provided, e.g. /scratch/$USER/bwaanalysis/refgenome.fa {{< pullcode file=""/static/scripts/bwa.slurm"" lang=""nohighlight"" }} Alignment of Sequence Reads to a Reference Genome BWA provides three basic alignment algorithms to align sequence reads to a reference genome, BWAbacktrack, BWASW, and BWAMEM. Below we show an example for using the BWAMEM algorithm (command bwa mem), which can process short Illumina reads (70bp) as well as longer reads up to 1 MB. The alignment output is"
rc-website-fork/content/userinfo/hpc/software/bwa.md,"saved in SAM file format. The use of SAMtools on the HPC system is documented here. Specification of files Reference genome file: /scratch/$USER/bwaanalysis/refgenome.fa Sequence read file 1: /scratch/$USER/bwaanalysis/read1.fq Sequence read file 2: /scratch/$USER/bwaanalysis/read2.fq Output Alignment (SAM file): /scratch/$USER/bwaanalysis/alnpe.sam Notes: The use of t $SLURMCPUSPERTASK to define the numbe of processing threads based on the numbe of requested cpu core (1 thread / cpu core). Follow the online {{% softwarename %}} documentation to adjust parameters for aligning singleend reads. The use of mem and mempercpu options are mutually exclusive. Job scripts should specify one or the other but not both. Using an Interactive Session to run BWA You should NOT do your computational processing on the head node. In order to obtain a login shell on a compute node, use the ijob command. ijob N 1 n 1 A <YOURALLOCATION p standard c 20 mem=20000 Replace <YOURALLOCATION with your account name to charge SUs. The arguments for c and mem options depend on the resources you will use for the alignment step. For more details about submitting interactive jobs please see here. Load module First, let us load the bwa module: module load bwa In order to check all available bwa commands run: bwa If you wish to check various options for each command run: bwa index bwa mem"
rc-website-fork/content/userinfo/hpc/software/blast.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/software/julia.md,"Description Julia is a highlevel programming language designed for highperformance numerical analysis and computational science. Distinctive aspects of Julia's design include a type system with parametric polymorphism and types in a fully dynamic programming language and multiple dispatch as its core programming paradigm. It allows concurrent, parallel and distributed computing, and direct calling of C and Fortran libraries without glue code. A justintime compiler that is referred to as ""justaheadoftime"" in the Julia community is used. Ref: Wikipedia | Documentation: https://docs.julialang.org () | () () | | | | Type ""?"" for help, ""]?"" for Pkg help. | | | | | | |/ | | | | || | | | (| | | Version 1.6.0 (20210324) / |'|||'| | Official https://julialang.org/ release |/ | julia using Pkg (v1.6) pkg status Status /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Project.toml [91a5bcdd] Plots v1.19.4 [8ba89e20] Distributed [de0858da] Printf julia help? sdata search: isdirpath isdispatchtuple StridedMatrix StridedVecOrMat searchsortedlast Couldn't find sdata Perhaps you meant stat, sort, sort!, sqrt, ispath, lstat, edit, Meta or atan No documentation found. Binding sdata does not exist. (v1.6) pkg add SharedArrays Updating registry at ~/.julia/registries/General Resolving package versions... Updating /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Project.toml [1a1011a3] + SharedArrays No Changes to /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Manifest.toml (v1.6) pkg status Status /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Project.toml [91a5bcdd] Plots v1.19.4 [8ba89e20] Distributed [de0858da] Printf [1a1011a3] SharedArrays julia using SharedArrays help? sdata search: sdata isdirpath isdispatchtuple SharedMatrix StridedMatrix sdata(S::SharedArray) Returns the actual Array object backing S. julia using Distributed launch worker processes numcores = parse(Int, ENV[""SLURMCPUSPERTASK""]) addprocs(numcores) println(""Number of cores: "", nprocs()) println(""Number of workers: "", nworkers()) each worker gets its id, process id and hostname for i in workers() id, pid, host = fetch(@spawnat i (myid(), getpid(), gethostname())) println(id, "" "" , pid, "" "", host) end remove the workers for i in workers() rmprocs(i) end and the output is, Number of cores: 9 Number of workers: 8"
rc-website-fork/content/userinfo/hpc/software/julia.md,"2 11643 udcba2619 3 11644 udcba2619 4 11645 udcba2619 5 11646 udcba2619 6 11649 udcba2619 7 11650 udcba2619 8 11651 udcba2619 9 11652 udcba2619 import MPI MPI.Init() comm = MPI.COMMWORLD myrank = MPI.Commrank(comm) commsize = MPI.Commsize(comm) println(""Hello! I am "", myrank, "" of "", commsize, "" on "",gethostname()) MPI.Finalize() using Flux, CuArrays z = CuArrays.cu([1, 2, 3]) println(2 z) m = Dense(10,5) | gpu x = rand(10) | gpu println(m(x)) and the output is slurm allocates gpus 4 [2, 4, 6] Float32[0.6239201, 0.36249122, 1.1242702, 0.9278027, 0.004131808] `"
rc-website-fork/content/userinfo/hpc/software/jupyterlab.md,"Overview Jupyter Notebooks are documents which can combine executable code, formatted text, and interactive graphics into a single file. Because Notebooks can be shared, they provide developers with a tool for capturing and explaining their computational results. To use a Jupyter Notebook, a web application, such as JupyterLab, is needed. We now provide a web portal where JupyterLab can be accessed on Rivanna and Afton. However, to use JupyterLab, you must have an account on UVA HPC. Accessing JupyterLab To access JupyterLab, you will begin by connecting to our Open OnDemand portal: Open a web browser and go to https://ood.hpc.virginia.edu. Use your Netbadge credentials to log in. On the top right of the menu bar of the Open OnDemand dashboard, click on Interactive Apps. In the dropdown box, click on JupyterLab. Requesting an Instance Your instance (or copy) of JupyterLab will run on a compute node. So it will need a list of resources, such as partition, time, and allocation. If you are new to UVA HPC, you may want to read the HPC User Guide to learn more about the partitions. After connecting to JupyterLab through Open OnDemand, a form will appear where you can fill in the resources for JupyterLab. When done filling in the resources, click on the blue “Launch” button at the bottom of the form. It will take a few minutes for the system to gather the resources for your instance of JupyterLab. When the resources are ready a Connect to Jupyter button will appear. Click on the button and the Notebook session will open in a new tab. The following screenshots illustrate the sequeunce of steps decribed above. Note that under the Work Directory field, the dropdown menu allows you to choose either your HOME or SCRATCH directories. Running Notebooks The JupyterLab dashboard will"
rc-website-fork/content/userinfo/hpc/software/jupyterlab.md,"have two panes: A file browser pane on the left where you will see the files and folders in your HPC directory; and A Launcher pane on the right with tiles for the available kernels (i.e., underlying software that will run the code in your Notebooks). If you already have a Jupyter Notebook in your account, you can maneuver to the file in the filebrowser pane, and doubleclick on the file name to open the Notebook. However, if you want to create a new Notebook, go to the Launcher pane and click on the tile for the desired kernel (e.g., Python 3). If you are more familiar with the classic Notebook environment, you can change the JupyterLab format by clicking on Help and select Launch Classic Notebook. Extensions We provide the following JupyterLab extensions: plotly is an interactive plotting library jupyterlabdash renders Plotly Dash apps as a separate window in JupyterLab jupyterlabtoc autogenerates a table of contents in the left area voila allows you to convert a Jupyter Notebook into an interactive dashboard FAQ How to create custom JupyterLab kernels? Please refer to our howto. How do I load a module in JupyterLab? That is not possible if you request a JupyterLab session. Please refer to our howto for a workaround."
rc-website-fork/content/userinfo/hpc/software/orca.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} License and Permission We have an ""ACADEMIA"" license. Usage is restricted to academic purposes. Please contact us if you need access to the software. Slurm script template Below is a Slurm script template. Please note that: Loading the orca module will automatically load gcc and openmpi. Do not load the latter manually. For your convenience we have defined the environment variable orcadir in the module. The full path to the executable orca is thus $orcadir/orca. Do not use srun/mpirun. The software will take care of MPI. {{< pullcode file=""/static/scripts/orcaserial.slurm"" lang=""nohighlight"" }} Submit the job in the same directory as my.inp. Multinode For larger calculations, you may run on multiple nodes. The following example will run on a total of 120 cores: {{< pullcode file=""/static/scripts/orcamultinode.slurm"" lang=""nohighlight"" }} Important notes: The nprocs in .inp should be equal to the total number of cores requested in your Slurm script. Do not run multinode ORCA jobs from research standard storage /nv. (See here.) We recommend scratch. References For more information please visit: ORCA Forum. You will need to create an account. ORCA Input Library"
rc-website-fork/content/userinfo/hpc/software/bioinformatics.md,"Overview Many commonly used bioinformatics software packages on the HPC clusters are available as individual modules or as Python packages bundled in the bioconda modules. Please see our HowTo for more information about using this software on the HPC system. Software Availability If a particular package is not available, several options are available. If it is sufficiently widely used, Research Computing staff will install it as a new module. If we determine that it is too specialized, you can install it yourself. Please use permanent storage such as your home directory to install software. If you have difficulty we can assist you to install the package. Please see below for a full listing of available bioinformatics software. If you do not find it there, please check the bioconda package before requesting that we install it. Reference Genomes Research Computing makes some standard reference genomes available. For a listing and information about how to copy them, please see our HowTo. Full List of Bioinformatics Software Modules Below is a list of software installed as separate modules. Other packages that are based on Python are available in the bioconda environment. Please see the bioconda page for a listing of those packages. {{< rivannasoftware moduleclasses=""bio"" }}"
rc-website-fork/content/userinfo/hpc/software/alphafold.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions, run: module spider {{< modulename }} For detailed information about a particular version, including the load command, run module spider <name/version. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} AlphaFold 3 Model Parameters The AlphaFold 3 model parameters are subject to the Terms of Use defined here. Our module does not contain the model parameters; instead, each user must submit their own request to DeepMind. Visit here for further instructions. Upon approval you will receive a download url for the file af3.bin.zst (~1 GB). Place it in a directory that is not shared with others, e.g. ~/af3. bash DIR=~/af3 mkdir $DIR cd $DIR wget <yourdownloadurl unzstd af3.bin.zst The last command will extract the file into af3.bin. Slurm Script {{< pullcode file=""/static/scripts/alphafold3.slurm"" lang=""nohighlight"" }} If you put the model parameters in a different location, change the value of modeldir. To see the complete list of flags run: python $EBROOTALPHAFOLD/app/alphafold/runalphafold.py help Refer to the official documentation for more information. AlphaFold 2 Installation details We prepared a Docker image based on the official Dockerfile with some modifications. AlphaFold does not use TensorFlow on the GPU (instead it uses JAX). See issue. Changed tensorflow to tensorflowcpu. There is no need to have system CUDA libraries since they are already included in the conda environment. Switched to micromamba instead of Miniconda. With a threestage build, our Docker image is only 5.4 GB on disk (2.1 GB compressed on DockerHub), almost half the size using the official Dockerfile (10.1 GB). For further details see here. AlphaFold launch command Please refer to runalphafold.py for all available options. Launch script run For your convenience, we have prepared a launch script run"
rc-website-fork/content/userinfo/hpc/software/alphafold.md,"that takes care of the Apptainer command and the database paths, since these are unlikely to change. If you do need to customize anything please use the full Apptainer command. Explanation of Apptainer flags The database and models are stored in $ALPHAFOLDDATAPATH. A cache file ld.so.cache will be written to /etc, which is not allowed on the HPC system. The workaround is to bindmount e.g. the current working directory to /etc inside the container. [B .:/etc] You must launch AlphaFold from /app/alphafold inside the container due to this issue. [pwd /app/alphafold] The nv flag enables GPU support. Explanation of AlphaFold flags The default command of the container is /app/runalphafold.sh. As a consequence of the Apptainer pwd flag, the fasta and output paths must be full paths (e.g. /scratch/$USER/mydir, not relative paths (e.g. ./mydir). You may use $PWD as demonstrated. The maxtemplatedate is of the form YYYYMMDD. Only the database paths in markflagsasrequired of runalphafold.py are included because the optional paths depend on dbpreset (fulldbs or reduceddbs) and modelpreset. Slurm Script Below are some Slurm script templates for version 2.3. Monomer with fulldbs {{< pullcode file=""/static/scripts/alphafoldmonomer.slurm"" lang=""nohighlight"" }} Multimer with reduceddbs {{< pullcode file=""/static/scripts/alphafoldmultimer.slurm"" lang=""nohighlight"" }} Notes For users running large protein jobs: Version 2.3.2dev is based on commit 020cd6d, about 2 years after the official 2.3.2 release. The reason for using a development version is that the package requirements are updated for compatibility on the H200 GPU. Users who have experienced outofmemory errors for large protein calculations should request an H200 GPU (gres=gpu:h200) and load the 2.3.2dev version. Before upgrading to a newer version, please always check the official repo for details, especially on any changes to the parameters, databases, and flags. You may need to request 8 CPU cores due to this line printed in the output: Launching subprocess"
rc-website-fork/content/userinfo/hpc/software/alphafold.md,"""/usr/bin/jackhmmer o /dev/null A /tmp/tmpys2ocad8/output.sto noali F1 0.0005 F2 5e05 F3 5e07 incE 0.0001 E 0.0001 cpu 8 N 1 ./seq.fasta /share/resources/data/alphafold/mgnify/mgyclusters.fa"" You must provide a value for maxtemplatedate. If you are predicting the structure of a protein that is already in PDB and you wish to avoid using it as a template, then maxtemplatedate must be set to be before the release date of the structure. If you do not need to specify a date, by default you can set today’s date. For example, if you are running the simulation on August 7th 2021, set –maxtemplatedate = 20210807. See here. You are not required to use the run wrapper script. You can always provide the full apptainer command."
rc-website-fork/content/userinfo/hpc/software/bioconda.md,"Bioconda Python packages Many bioinformatics Python packages are now maintained and available for the popular Anaconda Python distribution. Python packages for the Anaconda distribution are distributed through a variety of different bundles, called channels. The bioconda channel is specifically set up for the maintenance and distribution of popular bioinformatics packages. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} To view an uptodate list of the Python packages provided by a particular bioconda module, load the bioconda module and run the conda list command. For example: module load bioconda conda list | grep bioconda The environment contains a large number of packages, most of them to support the software of interest in bioinformatics. The grep command filters the Python package list to only show the Bioconda channel packages. The output may look like this:"
rc-website-fork/content/userinfo/hpc/software/sagemath.md,"Description SageMath (previously Sage or SAGE, ""System for Algebra and Geometry Experimentation""[3]) is a computer algebra system with features covering many aspects of mathematics, including algebra, combinatorics, graph theory, numerical analysis, number theory, calculus and statistics. Ref: wikipedia.org SageMath is a free opensource mathematics software system licensed under the GPL. It builds on top of many existing opensource packages: NumPy, SciPy, matplotlib, Sympy, Maxima, GAP, FLINT, R and many more. Access their combined power through a common, Pythonbased language or directly via interfaces or wrappers. Its mission: Creating a viable free open source alternative to Magma, Maple, Mathematica and Matlab. Ref: sagemath.org Available Versions To find the available versions and learn how to load them, run: module spider sagemath/9.0 The sagemath software provides its own Jupyter notebook. To start sagemath, go to fastx.hpc.virginia.edu and select FastX Web. This will open a desktop environment. Then click the terminal icon in the top toolbar and enter: module load apptainer sagemath Read the onscreen instructions carefully to see how to start the Jupyter session in a browser. Then after executing the command to start the sagemath Jupyter notebook, you should see If you select the URL in the bottom line and rightclick to select 'Open Link', a browser will open up to the following page:"
rc-website-fork/content/userinfo/hpc/software/clara-parabricks.md,"Overview Nvidia Clara Parabricks is a GPUaccelerated software suite for performing secondary analysis of next generation sequencing (NGS) DNA and RNA data. It contains GPUenabled versions of popular bioinformatics tools such as the aligners BWAMem and STAR. Loading the container On the HPC system, Clara Parabricks is available as an Apptainer container. To load the claraparabricks container module, you can type: module load apptainer claraparabricks The load command will load a default version of Clara Parabricks, unless another version is specified. To see the available versions, type: module spider claraparabricks Running Clara Parabricks tools The Clara Parabricks container on the HPC system includes many bioinformatics tools for genomics and transcriptomics. Each tool must be accessed using the Apptainer run command to activate the container, followed by the Clara Parabricks pbrun command to call the designated tool, followed by arguments specific to each tool. See below for an example using the fq2bam pipeline tool, which does a BWAMem alignment, sorts reads by coordinates, marks duplicate reads with GATK MarkDuplicates, and optionally generates a BQSR report. {{< pullcode file=""/static/scripts/parabricksfq2bam.slurm"" lang=""nohighlight"" }} Notes on fq2bam Slurm script: Replace <allocation with your allocation name. The apptainer flag nv enables Nvidia GPU support inside the container. The apptainer flag B binds a directory into the container. In this case, we are binding the present working directory ($PWD) into both /workdir and /outputdir inside the container. The variable $CONTAINERDIR is defined by the container module you do not need to assign it a value. This line in the script points the apptainer run command to the appropriate .sif file to call the desired container. The pbrun command tells Clara Parabricks you want to run the subsequent tool (in this case, fq2bam). The arguments following pbrun fq2bam are specific to the Clara Parabricks tool being used. See"
rc-website-fork/content/userinfo/hpc/software/clara-parabricks.md,"the fq2bam reference for more detailed information on these arguments. In this case, the reference fasta file (Homosapiensassembly38.fasta) and fastq data files (sample1.fq.gz and sample2.fq.gz) were downloaded ahead of time and stored in the referenced subdirectories. You should change these paths and file names as needed to point to your specific reference fasta and data files. This script should be saved in a file, called (for example) job.slurm. To run your job, you would submit the script by typing sbatch job.slurm."
rc-website-fork/content/userinfo/hpc/software/fiji.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Interactive Use of Fiji via FastX We recommend to launch the Graphical User Interface (GUI) of Fiji as an interactive job via the Open OnDemand Desktop interactive app. You may request a core count and amount of memory through the text boxes on the Open OnDemand form. Be sure to supply your allocation account where requested. Once the Desktop is launched, open a terminal window. Load the fiji module and start the application: module load fiji ImageJlinux64 mem=32G & Run a Fiji script as Slurm Job To execute a Fiji script noninteractively on a compute node, you can use the following Slurm job script template. {{< pullcode file=""/static/scripts/fiji.slurm"" lang=""nohighlight"" }} Adjust the cpuspertask, mem and time options as needed. Note that not all builtin Fiji functions or Fiji scripts are designed to utilize multiple cpu cores. Replace <YOURALLOCATION with your allocation account. Replace <FIJISCRIPT and <SCRIPTARGS with your custom Fiji script and add script arguments as required by the particular Fiji script. Custom Plugins Users can install their own plugins in their home directory. First create the directory via bash mkdir ~/.plugins Then follow the instructions here, replacing the destination with your local plugin directory."
rc-website-fork/content/userinfo/hpc/software/r.md,"Overview R is a programming language that often is used for data analytics, statistical programming, and graphical visualization. Loading the R module On the UVA HPC system, R is available through our module system. For example, to load R, you can type: module load goolf R Notice that we included goolf in the load command. There are two reasons why including goolf is important: R was built with a compiler, an interface to OpenMPI, and other utilities. The goolf module will ensure that each of these items is loaded. R has many computationallyintensive packages that are built with C, C++, or Fortran. By including goolf, we ensure that the same environment used for building R is loaded for any package installs. The load command will load a default version of R, unless another version is specified. To see the available versions of R, type: module spider R Loading the RStudio module RStudio is a development environment for R. We recommend launching RStudio through our webbased portal to the HPC system. For instructions on how to access it, see RStudio Server on the HPC system. For users who must launch RStudio from the commandline, start up a FastX or Open OnDemand Desktop session and run rstudiolauncher in the terminal. Then follow the instructions. To use your local R packages in RStudio, run: echo ""RLIBSUSER=~/R/goolf/x.y"" ~/.Renviron where x.y is the majorminor version, e.g. 4.3. Installing packages Due to the amount and variability of packages available for R, Research Computing does not maintain R packages beyond the very basic. If you need a package, you can install it in your account, using a local library. For example, to install BiocManager, you can type: If the R interpreter prompts you about creating a local library, type yes. If it asks you to select a"
rc-website-fork/content/userinfo/hpc/software/r.md,"CRAN mirror, scroll down the list it provides and select one of the US sites. Or, you can launch RStudio and install the packages as you would on your laptop. Submitting a SingleCore Job to the Cluster After you have developed your R program, you can submit it to the compute nodes by using a Slurm job script similar to the following: {{< pullcode file=""/static/scripts/rjob.slurm"" lang=""nohighlight"" }} This script should be saved in a file, called (for example) rjob.slurm. To run your job, you would submit the script by typing: sbatch job.slurm Submitting MultiCore Jobs to the Cluster R programs can be written to use multiple cores on a node. You will need to ensure that both Slurm and your R code know how many cores they will be using. In the Slurm script, we recommend using cpuspertask to specify the number of cores. For example: {{< pullcode file=""/static/scripts/rmulticore.slurm"" lang=""nohighlight"" }} For the R code, the number of cores can be passed in with a commandline argument, as shown in the above example with ${SLURMCPUSPERTASK}. The code will need to be designed to read in the commandline argument and establish the number of available cores. For example: cmdArgs < commandArgs(trailingOnly=TRUE) numCores < as.integer(cmdArgs[1]) 1 options(mc.cores=numCores) Or, you if you do not want to use commandline arguments, you can use the function Sys.getenv() in the R code. For example: Do not use the detectCores() function, which is often shown in tutorial examples. It will detect the number of physical cores not how many core Slurm is allowing the program to use. Submitting MPI Jobs to the Cluster R programs can be distributed across multiple nodes with MPI (message passing interface) and the appropriate MPI packages. To run a parallel R job that uses MPI, the Slurm script would be similar to"
rc-website-fork/content/userinfo/hpc/software/r.md,"the following: {{< pullcode file=""/static/scripts/rmpi.slurm"" lang=""nohighlight"" }} The items to notice in this script are i) the number of nodes; ii) the number of tasks; iii) the parallel partition; and iv) the mpirun before the command to run the R code. Submitting Jobs to Rio When running Slurm jobs on Rio, it is essential to ensure that all R packages and environment variables are configured correctly. Rio compute nodes can only run jobs from highsecurity research standard storage, so it's important to ensure that all necessary files and variables point to this location. Before installing any R packages, create an .Renviron file in a directory under your highsecurity research standard storage. For example: touch /standard/ivyxxxxxxx/path/to/R/.Renviron Next, create the .Rprofile file in the same directory as .Renviron and add the following content to the .Rprofile file: Replace the lines with 'ivyxxxxxxx' to the path in your filesystem where the R directory exists. This will ensure that R packages are installed in the correct directory under highsecurity storage.The .Rprofile file also dynamically adjusts to whatever version of R you're working with (RStudio or different module R versions). {{% callout %}} It's important to ensure that the /standard/ivyxxxxxxx/path/to/R/X.Y (Eg 4.3) directory exists before trying to install any new packages. Otherwise, R will try to create a directory under /home. If this happens, you can just close R then try running again and it should install correctly since the path has then been created from the .Rprofile file. Additionally, if switching between R versions, make sure to load R then close it before actually running it on your code. For some reason, RLIBSUSER gets set properly, but if packages are installed right after closing the previous version, they still install in the other /standard/ivyxxxxxxx/path/to/R/X.Y (Eg If using 4.4, packages will install in 4.3). You'll"
rc-website-fork/content/userinfo/hpc/software/r.md,"want to quit R, purge the module, then reload to try again. {{% /callout %}} To ensure the environment variables persist across sessions, add the following to your ~/.bashrc file: export RPROFILE=""/standard/ivyxxxxxxx/path/to/R/.Rprofile"" export RENVIRON=""/standard/ivyxxxxxxx/path/to/R/.Renviron"" These variables will carry over from your virtual machine (VM) frontend to the compute node. If these variables are not set in ~/.bashrc, they can also be exported directly in your Slurm script or via the command line when using ijob: ` export RPROFILE=""/standard/ivyxxxxxxx/path/to/R/.Rprofile"" export RENVIRON=""/standard/ivyxxxxxxx/path/to/R/.Renviron"" module purge module load R/4.3.1 {{% callout %}} Keep in mind to replace/standard/ivyxxxxxxx/path/to/R` with the path to R in your storage share. {{% /callout %}} By following the above steps, you will ensure that your Slurm jobs are properly configured to run with the required R packages and environment settings under the highsecurity research standard storage system. If you have questions about running your R code on the HPC system or would like a consultation to optimize or parallelize your code, contact hpcsupport@virginia.edu."
rc-website-fork/content/userinfo/hpc/software/debuggers.md,"Debuggers To use a debugger, it is necessary to rebuild your code with the g flag added. All object files must be removed anytime compiler flags change. If you have a Makefile run make clean if it is available. The program must then be run under the control of the debugger. For example, if you are using gdb, you run gdb ./myexec Adding debugging flags generally disables any optimization flags you may have added, and can slow down the code. Please remember to recompile with g removed once you have found your bugs. Gnu Debugger (gdb) and Profiler (gprof) The Gnu Compiler Collection compilers are free, opensource tools. Additional tools included are the gdb debugger and the gprof performance profiler. For detailed documentation, visit the Gnu website. The gdb and gprof tools are included with the gcc compiler suite and are loaded with the gcc module. module load gcc Both gdb and gprof are textbased, commandline tools. {{< moduleversions module=""gcc"" }} Intel To load the Intel compilers and associated tools, run module load intel The Intel debugger for versions 16.0 and beyond is a modified gdb and is used in a similar manner. After the Intel module is loaded, it can be accessed as gdbia. Available Intel Compilers {{< moduleversions module=""intel"" }} PGI Compiler The PGI Server Compilers and tools are licensed for Linux systems. module load pgi Available PGI and NVIDIA Compilers {{< moduleversions module=""pgi"" }} PGI provides a very capable debugger, pgdbg. In its default mode, it is graphical, and it requires that an X server run on the user's local desktop machine. It may be run in commandline mode with the text option; see the manpage for a full list of options. As with all debuggers, the user's program must be compiled with the g flag in"
rc-website-fork/content/userinfo/hpc/software/debuggers.md,"order to enable debugging. If you wish to use the graphical debugger and do not have or want to install an X11 server, you can also use FastX. {{< moduleversions module=""nvhpc"" }} The PGI compiler is transitioning to the NVIDIA HPC SDK tools. The NVHPC debugger is called cudagdb. Totalview The most powerful debugger available on the HPC system for OpenMP and MPI codes is Totalview. It is also an excellent generalpurpose debugger. It has a command line interface but is not easy to use in that mode; it is nearly always used through its graphical user interface, which is highly intuitive. For short runs with few processes it can be used on a frontend through FastX; longer or otherwise more demanding debugging runs can occur by running an Open OnDemand Desktop. Valgrind Valgrind is a framework for dynamic analysis tools. The most widely used tool is probably memcheck for detecting memory leaks. Build your code as usual with g, then run it as valgrind leakcheck=yes ./myprog arg1 arg2 valgrind.out The output from memcheck can be voluminous. Running under memcheck will also be much slower than a normal run and can require more memory. Profilers Profilers are tools for locating areas where the code can be sped up. Most profilers function by interrupting the code frequently and randomly, determining what subprogram is executing, and counting the time required for a subprogram to execute, how many times it is called, and so forth. Gnu Profiler (gprof) The Gnu profiler is a commandline, textoriented tool. The code must be recompiled with g p flags. Run the code and usual. This will produce a binary file called gmon by default. Then run the profiler gprof gprof.out Intel Tools VTune Profiler VTune is a powerful profiler for code built with Intel compilers. A gettingstarted"
rc-website-fork/content/userinfo/hpc/software/debuggers.md,"guide is available from Intel. Intel Trace Analyzer The Intel Trace Analyzer, which is focused on MPI codes, is included with our Intel compiler licenses. Intel provides a tutorial here. Some modifications are needed to run under Slurm. Add a flag bootstrap=slurm to the mpirun command, and use `n $SLURMNTASKS} rather than hardcoding a value. Open|SpeedShop Open|SpeedShop is a profiler capable of operating in several different modes, for different ""experiments."" Unlike many profilers, it requires that only the g flag be enabled. Building on the HPC system For general information about building your code on the HPC system, please see our howto"
rc-website-fork/content/userinfo/hpc/software/ansys.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} Local support is minimal; users should make an account at the student forum through the {{% softwarename %}} website for technical support and for obtaining detailed information. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Licensing The current general UVA license can be used for research but is limited in the size of the models it can use, and some of the more advanced features are not available. Users who have their own research licenses with greater capabilities must specify that license. To use such a research license on the UVA HPC system, before running ANSYS set the following environment variable nohighlight export ANSYSLMDLICENSEFILE=1055@myhost.mydept.virginia.edu You may also need nohighlight export ANSYSLISERVERS=2325@myhost.mydept.virginia.edu You must obtain the full names of the hosts and the port numbers from your group's license administrator. The numbers in the above lines are the standard ANYSYS ports, but it is possible they may differ for some license servers; consult your license administrator for specific values. The ANSYSLISERVERS environment variable is generally not necessary if the default port is used, but ANSYSLMDLICENSEFILE will always be required. These environment variables must be set in each shell and in every Slurm script that invokes ANSYS. Using ANSYS Workbench If you wish to run jobs using the Workbench, you need to edit the ~/.kde/share/config/kwinrc file and add the following"
rc-website-fork/content/userinfo/hpc/software/ansys.md,"line: FocusStealingPreventionLevel=0 The workbench application, runwb2, should be executed in an interactive Open OnDemand Desktop session. When you are assigned a node, launch the desktop, start a terminal, load the desired module and start the workbench with the runwb2 command. module load ansys unset SLURMGTIDS runwb2 Be sure to delete your Open OnDemand session if you finish before your requested time expires. MultiCore Jobs It is possible to run multicore jobs through Open OnDemand. In a terminal, load the ansys module and then run the appropriate package frontend: for general ANSYS applications, including CFX, that is the Workbench; for Fluent run fluent to start its graphical interface. Choose the ""Parallel Options"" tab to set up a run. Be sure to use only the number of cores you requested when you launched the OOD Desktop. For longer jobs, and for all multinode jobs, you should run in batch mode using a Slurm script. Please refer to ANSYS documentation for instructions in running from the command line. These examples use threading to run on multiple cores on a single node. ANSYS Slurm Script: {{< pullcode file=""/static/scripts/ansys.slurm"" lang=""nohighlight"" }} CFX Slurm Script: {{< pullcode file=""/static/scripts/cfx.slurm"" lang=""nohighlight"" }} MultiNode MPI Jobs For Fluent specify mpi=intel along with the flag srun to dispatch the MPI tasks using Slurm's task launcher. If more than the default memory per core is required, it is generally better with ANSYS and related products to request a total memory over all processes rather than using mempercpu, because a process can exceed the allowed memory per core. Please refer to our documentation for current information about default memory per core in each partition. These examples also show the minimum number of commandline options; you may require more for large jobs. Fluent Slurm Script: {{< pullcode file=""/static/scripts/fluent.slurm"" lang=""nohighlight"" }} The syntax for"
rc-website-fork/content/userinfo/hpc/software/ansys.md,"CFX is different and includes a ""startmethod."" We recommend Intel MPI. Please refer to documentation for other options that may be required. CFX Slurm script: {{< pullcode file=""/static/scripts/cfxmpi.slurm"" lang=""nohighlight"" }}"
rc-website-fork/content/userinfo/hpc/software/picard.md,"Description {{< moduledescription }} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/software/totalview.md,"TotalView TotalView is a fullfeatured, sourcelevel, graphical debugger for applications written in C, C++, Fortran (77 and 90/95/2003), assembler, and mixed source/assembler codes. It is a multiprocess, multithread debugger that supports multiple parallel programming paradigms including MP and OpenMP. The University has a nearsite license (256 tokens) for Totalview on all versions of Linux. Visit the TotalView website for detailed documentation. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Getting started with TotalView. Your code must be compiled appropriately to use Totalview. For most Unix compilers, the debug flag g must be added to the compilation options, just as it would be for other debuggers such as gdb. Optimization should also generally be suppressed, since optimization can change the code in ways that make it difficult for the debugger to interpret. Once the code has been recompiled and an executable generated, you are ready to invoke Totalview. To start TotalView, execute the following command: module load totalview totalview Totalview is normally used with its X11based graphical user interface and to use it directly, you must have an X server running on your local system or use FastX. Computers running Linux will automatically have an X server available. On Mac OS X you will need to install XQuartz. Windows users must also install an X server; we recommend XminGW. The recommended way to run X applications remotely is to enable X11 port forwarding in your ssh client (SecureCRT, PuTTY, etc.)"
rc-website-fork/content/userinfo/hpc/software/totalview.md,"and run the X server in the background (passively). Another option is the FastX client which can be installed on the user's local system to open a desktop on the cluster frontend. Using Totalview to Debug MPI Codes One of the most powerful features of Totalview is its ability to debug parallel codes. Using the Debugger Through FastX You can use TotalView via FastX. If your debugging work is sufficiently small to run on the frontend, start TotalView like any other X11 application from the command line: module load totalview totalview & Using the Client on Compute Nodes If you have a long debugging job or you want to debug an MPI application, you should run an interactive job through Slurm using the Open OnDemand Desktop interactive application. When the Desktop is launched, start a terminal window and type the above commands as usual."
rc-website-fork/content/userinfo/hpc/software/amber.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website Local support is not available. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/software/complete-list.md,"{{< rivannasoftware moduleclasses=""all"" }}"
rc-website-fork/content/userinfo/hpc/software/berkeleygw.md,"Description {{% moduledescription %}} Local support is not available. For detailed documentation and tutorials, visit the {{% softwarename %}} website. The user forum can be found here. Software Category: {{% modulecategory %}} Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Users may build their own versions of BerkeleyGW if they wish to use a different compiler+MPI combination, or to choose individual optional packages. Please consult the official documentation. Example Slurm script We built BerkeleyGW with GPU support. It can only run on V100 and A100 GPUs. Please use the following Slurm script as a template. {{< pullcode file=""/static/scripts/berkeleygw.slurm"" lang=""nohighlight"" }} We highly recommend running a benchmark to decide how many CPU cores and/or GPU devices you should use."
rc-website-fork/content/userinfo/hpc/software/sas.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Note: SAS scripts may be run on the HPC system through the Slurm queueing system in batch mode, but production interactive jobs on the frontend are not permitted."
rc-website-fork/content/userinfo/hpc/software/yambo.md,"Description {{% moduledescription %}} Local support is not available. For detailed documentation and tutorials, visit the {{% softwarename %}} website. The user forum can be found here. Software Category: {{% modulecategory %}} Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Users may build their own versions of Yambo if they wish to use a different compiler+MPI combination, or to choose individual optional packages. Please consult the official documentation. Example Slurm script We built Yambo with GPU support. It can only run on V100 and A100 GPUs. Please use the following Slurm script as a template. {{< pullcode file=""/static/scripts/yambo.slurm"" lang=""nohighlight"" }} We highly recommend running a benchmark to decide how many CPU cores and/or GPU devices you should use."
rc-website-fork/content/userinfo/hpc/software/intel.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} {{< highlight }} The 2024.0 version is experimental. Most users should load an older version for the time being. {{< /highlight }} Compiler For general information on building code using compilers, please see our HowTo pages: Building Your Code on UVA HPC Building and Running MPI Code The Intel compilers are: {{< table class=""table tablestriped"" }} ||<2024|=2024| |||| C| icc | icx | C++| icpc | icpx | Fortran| ifort | ifort | {{< /table }} Tools Intel Trace Analyzer and Collector The Intel Trace Collector is a lowoverhead tracing library that performs eventbased tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. To use this tool: module load intel source $EBROOTINTEL/parallelstudioxe20../bin/psxevars.sh}} Fill in the with the actual path. In your Slurm script, replace srun myprog with ` SBATCH n ... mpirun trace bootstrap slurm n ${SLURMNTASKS} myprog The Slurm variable${SLURMNTASKS}will expand to the` that you specify in the SBATCH directive. This will write a trace file (.stf) that you can analyze with traceanalyzer. You will need to run this on FastX Web MATE desktop environment (recommended) or add the Y flag when you ssh into the HPC system."
rc-website-fork/content/userinfo/hpc/software/singularity.md,"{{< highlight }} [Deprecated] On Dec 18, 2023 Singularity has been upgraded to Apptainer, a continuation of the Singularity project. {{< /highlight }} Overview Singularity is a container application targeted to multiuser, highperformance computing systems. It interoperates well with Slurm and with the Lmod modules system. Singularity can be used to create and run its own containers, or it can import Docker containers. Creating Singularity Containers To create your own image from scratch, you must have root privileges on some computer running Linux (any version). Follow the instructions at the Singularity site. If you have only Mac or Windows, you can use the Vagrant environment. Vagrant is a prepacked system that runs under several virtualmachine environments, including the free Virtualbox environment. Singularity provides instructions for installing on Mac or installing on Windows. Once you have installed Vitrualbox, you install Singularity's Vagrant image, which contains the prerequisites to author images. You can then follow the instructions for Linux to author your image. How to use a Docker image on the HPC System? You will need to convert the Docker image into Singularity. Please visit our howto page for instructions. If you do not have the ability to create your own image for the HPC system or to use a Docker image, contact us for assistance. Singularity on the HPC system Singularity is available as a module. The RC staff has also curated a library of preprepared Singularity container images for popular applications as part of the shared software stack. Descriptions for these shared containers can be found via the module avail and module spider commands. module load singularity module avail Loading any of these container modules produces an onscreen message with instructions on how to copy the container image file to your directory and how to run the container. What is"
rc-website-fork/content/userinfo/hpc/software/singularity.md,"Inside the Container? Use the shell command to start up a shell prompt and navigate (more later). For containers built with Singularity, you can use the runhelp command to learn more about the applications and libraries: singularity runhelp /path/to/sif For containers built with Docker, use the inspect runscript command to find the default execution command. Using the TensorFlow module as an example: This shows that python will be executed when you run (more later) the container. Running nonGPU Images If your container does not require a GPU, all that is necessary is to load the singularity module and provide it with a path to the image. module load singularity singularity <CMD <OPTIONS <IMAGEFILE <ARGS CMD defines how the container is used. There are three main commands: run: Executes a default command inside the container. The default command is defined at container build time. exec: Executes a specific application/command inside the container as specified with ARGS. This provides more flexibility than the run command. shell: Starts a new interactive command line shell inside the container. OPTIONS define how the singularity command is executed. These can be omitted in most cases. IMAGEFILE refers to the single Singularity container image file, typically with a .sif or .simg extension. ARGS define additional arguments passed inside the container. In combination with the exec command they define what command to execute inside the container. singularity run containerdir = ~mst3k singularity run $containerdir/myimage.sif This executes a default application or set of commands inside the container. The default application or set of commands to execute is defined in the image build script and cannot be changed after the container is built. After execution of the default command, the container is closed. singularity exec singularity exec $containerdir/myimage.sif python myscript.py This is similar to singularity run but more versatile by"
rc-website-fork/content/userinfo/hpc/software/singularity.md,"allowing the specification of the particular application or command to execute inside the container. In this example it launches the python interpreter and executes the myscript.py script, assuming that Python was installed into the container image. After execution of the command, the container is closed. singularity shell singularity shell $containerdir/myimage.sif This opens a new shell inside the container, notice the change of the prompt: Singularity Now you can execute any command or application defined in the container, for example ls to list all files in the current directory: Singularity ls You can navigate the container file system, including /scratch and /nv, and run any application that is installed inside the container. To leave the interactive container shell, type exit: Singularity exit Running GPU Images Singularity can make use of the local NVIDIA drivers installed on the host. To use a GPU image, load the singularity module and add the nv flag when executing the singularity shell, singularity exec, or singularity run commands. module load singularity singularity <CMD nv <IMAGEFILE <ARGS Example: module load tensorflow/2.10.0.sif singularity run nv $CONTAINERDIR/tensorflow2.10.0.sif myscript.py In the container build script, python was defined as the default command to be executed and singularity passes the argument(s) after the image name, i.e. myscript.py, to the Python interpreter. So the above singularity command is equivalent to singularity exec nv $CONTAINERDIR/tensorflow2.10.0.sif python myscript.py This image was built to include CUDA and cuDNN libraries that are required by TensorFlow. Since these libraries are provided by the container, we do not need to load the CUDA/cuDNN libraries available on the host. Running Images Interactively Start an ijob: ijob A mygroup p gpu gres=gpu c 1 salloc: Pending job allocation 12345 salloc: job 12345 queued and waiting for resources salloc: job 12345 has been allocated resources salloc: Granted job allocation 12345 If your"
rc-website-fork/content/userinfo/hpc/software/singularity.md,"image starts a graphical user interface or otherwise needs a display, you should use the Open OnDemand Desktop rather than a commandline ijob. Once the Desktop is launched, start a terminal window and type the commands as in any other shell. module purge module load singularity singularity shell nv /path/to/sif Running Image NonInteractively as Slurm jobs Example script: Interaction with the Host File System Each container provides its own file system. In addition, directories of the host file system can be overlayed onto the container file system so that host files can be accessed from within the container. These overlayed directories are referred to as bind paths or bind points. The following system directories of the host are exposed inside a container: /tmp /proc /sys /dev In addition, the following user directories are overlayed onto each container by default on the HPC system: /home /scratch /nv /project Due to the overlay these directories are by default the same inside and outside the container with the same read, write, and execute permissions. This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits. The /nv and /project directories refer to leased storage locations that may not be available to all users. Disabling the Default Bind Paths Under some circumstances this default overlay of the host file systems is undesirable. Users can disable the overlay of /home, /scratch, /nv, /project by adding the c flag when executing the singularity shell, singularity exec, or singularity run commands. For example, containerdir=~mst3k singularity run c $containerdir/myimage.sif Adding Custom Bind Paths Users can define custom bind paths for host directories via the B/bind option, which can be used in combination with the c flag. For example, the following command adds the /scratch/$USER"
rc-website-fork/content/userinfo/hpc/software/singularity.md,directory as an overlay without overlaying any other user directories provided by the host: singularity run c B /scratch/$USER $containerdir/myimage.sif To add the /home directory on the host as /rivanna/home inside the container: singularity run c B /home:/rivanna/home $containerdir/myimage.sif
rc-website-fork/content/userinfo/hpc/software/bowtie2.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Bowtie2 Example The following example demonstrates how to run a Bowtie sequence alignment on multiple cpu cores on a single HPC node. More details information about the aligner can be found here. Note that Bowtie cannot be executed across multiple nodes. Create a temporary directory We start by creating a temporary directory and copying the Bowtie2 example files into it. module load gcc bowtie2 mkdir p /scratch/$USER/bowtietemp cp r $EBROOTBOWTIE2/example /scratch/$USER/bowtietemp The $USERvariable will expand to your computing ID so you'll be using your personal scratch directory. The EBROOTBOWTIE2 environment variable is set to the Bowtie2 installation directory after you load the bowtie2 module. Note that you have to preload the gcc module before loading bowtie2. The Slurm Job Script The Slurm script defines the HPC resources needed to run the Bowtie2 indexing and alignment. Bowtie2 can utilize multiple cpu cores on a single compute node. It does not support execution on multiple nodes. Let's create a textfile that serves as our job script, alignment.slurm, with the following content: {{< pullcode file=""/static/scripts/bowtie2.slurm"" lang=""nohighlight"" }} You need to replace <YOURALLOCATION with your own HPC allocation name. The $USER variable will expand to your computing ID so you'll be using your personal scratch"
rc-website-fork/content/userinfo/hpc/software/bowtie2.md,"directory. The $SLURMCPUSPERTASK variable is set by the job scheduler to match the cpuspertask directive, in this case 8 cpus core per task (job run). Submitting the Job To run the above script, type: sbatch alignment.slurm This will return a message like this: Submitted batch job 3184590 Check the Job Status To check the status of the job, run squeue u <username where <username is your UVA computing ID that you used to log in. To see a history of your jobs, run: sacct Output Because of parallel processing, the aligned reads might appear in the output SAM file in a different order than they were in the input FASTQ. You can add the reorder flag to your command so that the order does not change, but it is typically not necessary. Troubleshooting Caution: If you create the Slurm job script on a Windows computer and then upload it to the HPC system, you’ll probably get an error when you run it with sbatch that says: sbatch: error: Batch script contains DOS line breaks (\r\n) sbatch: error: instead of expected UNIX line breaks (\n). To fix this, run dos2unix alignment.slurm This will remove unwanted \r from text files."
rc-website-fork/content/userinfo/hpc/software/matlab.md,"MATLAB is an integrated technical computing environment from the MathWorks that combines arraybased numeric computation, advanced graphics and visualization, and a highlevel programming language. Separately licensed toolboxes provide additional domainspecific functionality. Mathworks provides MATLAB examples and tutorials for all experience levels here. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. To load the most recent version of {{% softwarename %}}, at the terminal window prompt run: module load matlab For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{% moduleversions %}} You can work in the MATLAB desktop on the UVA HPC frontend nodes; we recommend FastX for this application. However, the time and memory that a job can use on the frontends are limited, so for longer jobs you should submit your job to compute nodes through Slurm. If your Matlab job requires user interactions via the Matlab interface, you should use Open OnDemand as described in the next section. If you will be running MATLAB through the command line but still wish to use an interactive job, you can create an ijob. Interactive Sessions through Open OnDemand Starting an Interactive Session To launch an instance of {{% softwarename %}}, you will begin by connecting to our Open OnDemand portal. You need to specify required resources, e.g. node partition, time, your UVA HPC allocation, etc.. If you are new to UVA HPC, you may want to read the Getting Started Guide to learn more about the partitions. Open a web browser and go to URL: https://ood.hpc.virginia.edu. Use your Netbadge credentials"
rc-website-fork/content/userinfo/hpc/software/matlab.md,"to log in. This will open the Open OnDemand web portal. On the top banner of the Open OnDemand dashboard, click on Interactive Apps. In the dropdown box, click on {{% softwarename %}}. After connecting to {{% softwarename %}} through Open OnDemand, a form will appear where you can fill in the resources for {{% softwarename %}}. When done filling in the resources, click on the blue Launch button at the bottom of the form. Do not click the button multiple times. It may take a few minutes for the system to gather the resources for your instance of {{% softwarename %}}. When the resources are ready a Connect to {{% softwarename %}} button will appear. Click on the button to start {{% softwarename %}}. Using {{% softwarename %}} When {{% softwarename %}} opens in your web browser, it will appear just like the {{% softwarename %}} that you have on your laptop or desktop. Closing the Interactive Session When you are done, quit the Matlab application. The interactive session will be closed and the allocated resources will be released. If you leave the session open, your allocation will continue to be charged until the specified time limit is reached. Running a Matlab Batch Jobs on the HPC System The HPC system uses the Slurm resource manager to schedule and run jobs on the cluster compute nodes. The following are example Slurm scripts for submitting different types of Matlab batch jobs to the cluster. Submitting a batch job using a single core of a compute node. Once your program is debugged, we recommend running in batch mode when possible. This runs the job in the background on a compute node. Write a Slurm script similar to the following: {{< pullcode file=""/static/scripts/matlabserial.slurm"" lang=""nohighlight""}} The option nodisplay suppresses the Desktop interface and"
rc-website-fork/content/userinfo/hpc/software/matlab.md,"any attempt to run a graphical display. Some MATLAB functions are capable of running on multiple cores. If your code uses linear algebraic operations, those can be multithreaded across multiple cores, so you would need to request the additional cores in your slurm script. Unless you are sure you can use multiple cores effectively it's generally best to restrict your job to one core. The ;exit is important to ensure that the job terminates when the computation is done. The example code pcalc2.m is shown below. Note that passing the SLURMJOBID variable allows the function to save output to a jobspecific filenames. Submitting a batch job using multiple cores on a compute node If you have a Matlab job that can be structured to run across multiple cores, you can greatly speed up the time to your results. The linear algebraic libraries in Matlab are multithreaded and will make use of multiple cores on a compute node. The Parallel Computing Toolkit allows you to distribute for loops over multiple cores using parfor and other parallel constructs in MATLAB. For more information on using the Parallel Computing Toolbox in MATLAB see the. MathWorks documentation . The example function pcalc2.m above uses a parallel for loop (parfor) in MATLAB. To run your parallel MATLAB code across multiple cores on one compute node, you can use a slurm script similar to the following: {{< pullcode file=""/static/scripts/matlabmulticore.slurm"" lang=""nohighlight"" }} The Matlab script setPool1.m creates a local pool of matlab workers on the cores of the compute node. Matlab Jobs using Slurm Job Arrays The Slurm has a mechanism for launching multiple independent jobs with one job script using the array directive. Array of Multicore Parallel Matlab Jobs The following Slurm script uses job arrays to submit multiple parallel Matlab jobs, each running on a"
rc-website-fork/content/userinfo/hpc/software/matlab.md,"nodes of the standard queue. {{< pullcode file=""/static/scripts/matlabjobarray.slurm"" lang=""nohighlight"" }} Parallel Matlab on Multiple Compute Nodes To run Matlab parallel jobs that require more cores than are available on one compute node (e.g. 40), you can launch the Matlab desktop on one of the HPC login nodes. The following procedure will create the cluster profile for your account on UVA HPC: For version R2023a or newer, use the Discover Clusters function in the dropdown of the Parallel menu to create a cluster profile for Rivanna as described in the following link. Discover Clusters and Use Cluster Profiles parfor example Once this configuration is complete you can submit jobs to the cluster using the following commands: spmd example The previous example distributes the iterations of a parfor loop across multiple compute nodes. The following example shows how to solve a linear system of equations (Ax=b) across multiple compute nodes using distributed arrays. The function solverlarge1 looks like the following: Utilizing GPUs with Matlab General guidelines on requesting GPUs on the HPC system Once your job has been granted its allocated GPUs, you can use the gpuDevice function to initialize a specific GPU for use with Matlab functions that can utilize the architecture of GPUs. For more information see the MathWorks documentation on GPU Computing in Matlab. The following slurm script is for submitting a Matlab job that uses 4 GPUs in a parfor loop. For each GPU requested, the script requests one cpu (ntaskspernode). {{< pullcode file=""/static/scripts/matlabgpu.slurm"" lang=""nohighlight"" }} The function gpuTest1 looks like the following. For further information, see https://www.mathworks.com/help/parallelcomputing/examples/runmatlabfunctionsonmultiplegpus.html Matlab Parallel Computing Resources Parallel Computing Toolbox Documentations Parallel and GPU Computing tutorials Performance and Memory Parallel Computing Toolbox — Examples"
rc-website-fork/content/userinfo/hpc/software/texlive.md,"Description TeX Live is TeX Live is intended to be a straightforward way to get up and running with the TeX document production system. It provides a comprehensive TeX system with binaries for most flavors of Unix, including GNU/Linux, macOS, and also Windows. It includes all the major TeXrelated programs, macro packages, and fonts that are free software, including support for many languages around the world. Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Add Local Package We do not support userrequested packages in our module. If you need a package that is not included, please install it locally by following these instructions. module load texlive mkdir p ~/texmf/tex/latex The directory structure of ~/texmf/tex/latex should be the same as ls $EBROOTTEXLIVE/texmfdist/tex/latex Each package should reside in its own directory. Create a subdirectory for your package under ~/texmf/tex/latex and copy the .sty file from ctan.org into the subdirectory. (If the package does not provide a .sty, please follow the installation instructions. Typically, latex .dtx or latex .ins should produce the .sty file.) Then run: $ texhash ~/texmf texhash: Updating /home/mst3k/texmf/lsR... texhash: Done. You should now be able to use the new package locally. dvipng If you encounter this error: FileNotFoundError: [Errno 2] No such file or directory: 'dvipng' please use our dvipng container. Go through all the steps in https://hub.docker.com/r/uvarc/dvipng."
rc-website-fork/content/userinfo/hpc/software/mathematica.md,"Description Mathematica is an integrated technical computing environment that combines numeric and symbolic computation, advanced graphics and visualization, and a highlevel programming language. There are several website resources with Mathematica tutorials and parallel Mathematica training sessions. Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/software/tensorflow.md,"Overview TensorFlow is an open source software library for high performance numerical computation. It has become a very popular tool for machine learning and in particular for the creation of deep neural networks. The latest TensorFlow versions are now provided as prebuilt Apptainer containers on the HPC system. The basic concept of running Apptainer containers on the HPC system is described here. TensorFlow code is provided in two flavors, either with or without support of general purpose graphics processing units (GPUs). All TensorFlow container images provided on the HPC system require access to a GPU node. Access to GPU nodes is detailed in the sections below. TensorFlow and Keras Keras is a highlevel neural networks application programming interface (API), written in Python and capable of running on top of TensorFlow, CNTK, or Theano. Since version 1.12.0, TensorFlow contains its own Keras API implementation as described on the TensorFlow website. What is inside the TensorFlow containers? The TensorFlow modules on the HPC system include common Python packages such as Matplotlib and OpenCV. See https://hub.docker.com/r/uvarc/tensorflow for details. TensorFlow Jupyter Notebooks Jupyter Notebooks can be used for interactive code development and execution of Python scripts and several other codes. A few TensorFlow kernels are available. Accessing the JupyterLab Portal Open a web browser and go to URL: https://ood.hpc.virginia.edu Use your Netbadge credentials to log in. On the top right of the menu bar of the Open OnDemand dashboard, click on “Interactive Apps”. In the dropdown box, click on JupyterLab. Requesting access to a GPU node To start a JupyterLab session, fill out the resource request webform. To request access to a GPU, verify the correct selection for the following parameters: Under Rivanna Partition, choose ""GPU"". Under Optional GPU Type, choose a GPU type or leave it as ""default"" (first available). Click Launch to"
rc-website-fork/content/userinfo/hpc/software/tensorflow.md,"start the session. Review our Jupyter Lab documentation for more details.. Editing and Running the Notebook Once the JupyterLab instance has started, you can edit and run your notebook as described here. TensorFlow Slurm jobs Apptainer can make use of the local NVIDIA drivers installed on a host equipped with a GPU device. The Slurm script needs to include the SBATCH p gpu and SBATCH gres=gpu directives in order to request access to a GPU node and its GPU device. Please visit the Jobs Using a GPU section for details. To run commands in an GPUenabled container image, load the apptainer module and add the nv flag when executing the apptainer run or apptainer exec commands. For example: module load apptainer tensorflow/2.10.0 apptainer run nv $CONTAINERDIR/tensorflow2.10.0.sif tfexample.py In the container build script, python is defined as the default command to be executed and Apptainer passes the argument(s) after the image name, i.e. tfexample.py, to the Python interpreter. So the above apptainer command is equivalent to apptainer exec nv $CONTAINERDIR/tensorflow2.10.0.sif python tfexample.py The TensorFlow container images were built to include CUDA and cuDNN libraries that are required by TensorFlow. Since these libraries are provided within each container, we do not need to load the CUDA/cuDNN libraries available on the host. Example Slurm Script: {{< pullcode file=""/static/scripts/tensorflow.slurm"" lang=""nohighlight"" }} TensorFlow Interactive Jobs (ijob) Start an ijob. Note the addition of p gpu gres=gpu to request access to a GPU node and its GPU device. ijob A mygroup p gpu gres=gpu c 1 Console output"" salloc: Pending job allocation 12345 salloc: job 12345 queued and waiting for resources salloc: job 12345 has been allocated resources salloc: Granted job allocation 12345 Now you can load the apptainer module and execute commands provided by the container. For example: module purge module load apptainer tensorflow/2.13.0 apptainer"
rc-website-fork/content/userinfo/hpc/software/tensorflow.md,"run nv $CONTAINERDIR/tensorflow2.13.0.sif tfexample.py Interaction with the Host File System The following user directories are overlayed onto each container by default on the HPC system: /home /scratch /nv /project Due to the overlay, these directories are by default the same inside and outside the container with the same read, write, and execute permissions. This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits. The /nv and /project directories refer to leased storage locations that may not be available to all users. TensorBoard Request a Desktop session under Interactive Apps via Open OnDemand. Fill out the form to submit the Slurm job. Launch the session and open a terminal in the desktop. Enter these commands: $ module load apptainer tensorflow/2.13.0 $ apptainer shell nv $CONTAINERDIR/tensorflow2.13.0.sif Apptainer python m tensorboard.main logdir=logdir Open the resulting URL (of the form http://localhost:xxxx/) in Firefox. Can I install my own TensorFlow (that works on a GPU)? Yes, you may either pull the official TensorFlow Docker image or create your own environment. We shall use TensorFlow 1.14 as an example. Docker Go to https://hub.docker.com/r/tensorflow/tensorflow/tags and search for the desired version. Use the gpu variant. Note the provided pull command (docker pull tensorflow/tensorflow:1.14.0gpu) and change it into Apptainer. The differences are underlined by ^: bash apptainer pull docker://tensorflow/tensorflow:1.14.0gpu ^^^^^^^^^ ^^^^^^^^^ You will find the Apptainer image tensorflow1.14.0gpu.sif in your current directory. Consult the instructions in the previous sections. Remember to replace $CONTAINERDIR/tensorflow2.13.0.sif with the actual path to your own Apptainer image. Pip Please read the manual for instructions. Especially note the [andcuda] part of the pip install comand. You may consider creating a conda environment (see miniforge) for your local installation."
rc-website-fork/content/userinfo/hpc/software/code-server.md,"Description {{< moduledescription }} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Interactive Sessions through Open Ondemand Interactive sessions of {{% softwarename %}} can be launched through the HPC web portal, Open OnDemand. If you are new to HPC, you may want to read the Getting Started Guide to learn more about the partitions. Python Setup Python users should install the mspython extension and select an appropriate interpreter: Press F1 Search for “python: select interpreter” Choose the miniforge python or any other python in your custom environments. Closing the Interactive Session When you are done, delete the {{% softwarename %}} session from the ""My Interactive Sessions"" tab and the allocated resources will be released."
rc-website-fork/content/userinfo/hpc/software/libraries.md,"Available Software Libraries To get an uptodate list of the installed software libraries, log on to UVA HPC and run the following command in a terminal window: module keyword lib To get more information about a specific module version, run the module spider command, for example: module spider hdf5 List of Software Library Modules {{< rivannasoftware moduleclasses=""numlib,lib"" }}"
rc-website-fork/content/userinfo/hpc/software/smrtlink.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Using SmrtLink On the HPC system, Smrtlink tools can be executed only in noninteractive mode without any graphical user interface. Several Smrtlink tools support code execution on multiple cpu cores. Some of the common tools include blasr, ngmlr, pbalign, and the pbsv (pbsv align and pbsv call) commands. In the Slurm job scripts the number of requested cpu cores (per task) is stored in the environment variable SLURMCPUSPERTASK. PacBio BAM files The BAM format is a binary compressed format for raw or aligned sequence reads. The associated SAM format is a text representation of the same data (specifications for BAM/SAM). PacBioproduced BAM files are a fully compatible extension of the BAM specification. In addition to the typical BAM headers, the PacBio BAM header includes @RG (read group) entries with the following tags: ID, PL, PM, PU, DS. This means that any of the Smrtlink tools that require a PacBio BAM file will not accept any generic BAM files (i.e. nonPacBio BAM files). A more detailed description of the PacBio BAM format can be found here. Running blasr blasr is a read mapping program that maps reads to positions in a genome by clustering short exact matches between the read and the genome, and scoring clusters using alignment. The matches are generated by searching"
rc-website-fork/content/userinfo/hpc/software/smrtlink.md,"all suffixes of a read against the genome using a suffix array. When suffix array index of a genome is not specified, the suffix array is built before producing alignment. This may be prohibitively slow when the genome is large (e.g. Human). It is best to precompute the suffix array of a genome using the program sawriter, and then specify the suffix array on the command line using sa genome.fa.sa. Global chaining methods are used to score clusters of matches. Command line arguments The only required inputs to blasr are a file of reads (Fasta, PacBio BAM, bax.h5) and a reference genome (Fasta format). Although reads may be input in FASTA format, the recommended input is PacBio BAM files because these contain quality value information that is used in the alignment and produce higher quality variant detection. out : specifies the output file. Although alignments can be output in various formats, the recommended output format is PacBio BAM using the bam option. nproc : sets the number of threads used and is matched to the number of cpu cores requested for the Slurm job. To get a more complete description of all available command line options run this command: blasr help Slurm script example {{< pullcode file=""/static/scripts/smrtlinkblasr.slurm"" lang=""nohighlight"" }} Running pbalign pbalign maps PacBio sequences to references using predefined and selectable alignment algorithms (options are blasr, bowtie, or gmap). The input can be a fasta, pls.h5, bas.h5 or ccs.h5 file or a fofn (file of file names). The output can be in SAM or BAM format. If the output is in BAM format, the aligner has to be blasr and QVs will be loaded automatically. Command line arguments pbalign expects a minimum of three command line arguments: the name of the sequence input file (set of subreads or unaligned sequences"
rc-website-fork/content/userinfo/hpc/software/smrtlink.md,"in PacBio BAM format), the path to the reference files (Fasta format), and a filename for the computed alignment results. The blasr algorithm is used by default. nproc : Is used to specify how many cpu cores are available for the alignment computation. This value is matched to the number of requested cpu core via the SLURMCPUSPERTASK environment variable. To get a more complete description of all available command line options run this command: pbalign help Slurm script example {{< pullcode file=""/static/scripts/smrtlinkpbalign.slurm"" lang=""nohighlight"" }} Running ngmlr ngmlr is a longread mapper designed to align PacBio or Oxford Nanopore reads to a reference genome. It is optimized for structural variation detection. Command line arguments ngmlr requires these command line arguments: r : Specifies the path to the reference genome (FASTA/Q, can be gzipped) q : Specifies the path to the read file (FASTA/Q) [/dev/stdin] o : Specifies the path to output file [stdout] t : Is used to specify how many cpu cores are available for the alignment computation. This value is matched to the number of requested cpu core via the SLURMCPUSPERTASK environment variable. To get a more complete description of all available command line options run this command: ngmlr help Slurm script example {{< pullcode file=""/static/scripts/smrtlinkngmlr.slurm"" lang=""nohighlight"" }} Running sawriter sawriter creates a suffix array from a single or list Fasta input files for a reference genome. The suffix array provides an additional index that increases the performance during the mapping step (e.g. via blasr). This is particularly useful when handling large reference files (i.e. larger than bacterial genomes). Command line arguments sawriter expects at least two command line arguments. The first one specifies the output file, e.g. saoutputfile, the remaining ones specify the input files in Fasta format. At least one Fasta file has to be provided. Multiple"
rc-website-fork/content/userinfo/hpc/software/smrtlink.md,"input files can be specified by providing additional Fasta files separated by a whitespace at the end of the command. To get a more complete description of all available command line options run this command: sawriter help Slurm script example {{< pullcode file=""/static/scripts/smrtlinksawriter.slurm"" lang=""nohighlight"" }}"
rc-website-fork/content/userinfo/hpc/software/blender.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Interactive Sessions through Open Ondemand Interactive sessions of {{% softwarename %}} can be launched through the HPC system's web portal, Open OnDemand. To launch an instance of {{% softwarename %}}, you will begin by connecting to our Open OnDemand portal. Your {{% softwarename %}} session will run on a Rivanna/Afton GPU node. In addition, you need to specify required resources, e.g. time, your HPC allocation, etc. If you are new to HPC, you may want to read the Getting Started Guide to learn more about the partitions. Starting an Interactive Session Open a web browser and go to URL: https://ood.hpc.virginia.edu. Use your Netbadge credentials to log in. This will open the Open OnDemand web portal. On the top banner of the Open OnDemand dashboard, click on Interactive Apps. In the dropdown box, click on {{% softwarename %}}. After connecting to {{% softwarename %}} through Open OnDemand, a form will appear where you can fill in the resources for {{% softwarename %}}. {{% softwarename %}} supports GPUs and should be run in the GPU partition. When done filling in the resources, click on the blue Launch button at the bottom of the form. Do not click the button multiple times. It may"
rc-website-fork/content/userinfo/hpc/software/blender.md,"take a few minutes for the system to gather the resources for your instance of {{% softwarename %}}. When the resources are ready a Launch {{% softwarename %}} button will appear. Click on the button to start {{% softwarename %}}. Using {{% softwarename %}} When {{% softwarename %}} opens in your web browser, it will appear just like the {{% softwarename %}} that you have on your laptop or desktop. Closing the Interactive Session When you are done, quit the {{% softwarename %}} application. The interactive session will be closed and the allocated resources will be released."
rc-website-fork/content/userinfo/hpc/software/gnupg.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Generate a key To generate a key, execute the following command: gpg fullgeneratekey and follow the onscreen instructions. If it ends abruptly with this message: gpg: agentgenkey failed: No pinentry Key generation failed: No pinentry please follow these steps: Kill your current gpgagent gpgconf kill gpgagent Start the agent with pinentry gpgagent daemon pinentryprogram /usr/bin/pinentry Run GnuPG gpg fullgeneratekey It will ask you to “perform some other action” but just wait. After a few seconds you will be asked to create a passphrase in a popup window. Again, wait for a few seconds and retype your passphrase in another popup window. At the end you should see something like this:"
rc-website-fork/content/userinfo/hpc/software/gromacs.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Usage on GPU This module is built with CUDA support. A Slurm script template is provided below. {{< pullcode file=""/static/scripts/gromacsgpu.slurm"" lang=""nohighlight"" }}"
rc-website-fork/content/userinfo/hpc/software/perl.md,"Overview Perl is a generalpurpose interpreted programming language, originally developed for text manipulation and now used for a wide range of tasks including system administration, web development, network programming, GUI development, and bioinformatics. Available Versions The default Perl is required for system purposes and is generally too old for applications. We offer more recent versions of Perl as modules. To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} CPAN Modules Users can install their own Perl modules from CPAN via the cpanm command. For instance: cpanm Test::More Run cpanm help for further help. Example Slurm Script"
rc-website-fork/content/userinfo/hpc/software/containers.md,"Overview Containers bundle an application, the libraries and other executables it may need, and even the data used with the application into portable, selfcontained files called images. Containers simplify installation and management of software with complex dependencies and can also be used to package workflows. Please refer to the following pages for further information. Singularity (before Dec 18, 2023) Apptainer (after Dec 18, 2023) Short course: Software Containers for HPC Container Registries for UVA Research Computing Images built by Research Computing are hosted on Docker Hub (and previously Singularity Library). Singularity Library Due to storage limits we can no longer add Singularity images to Singularity Library. There will be no more updates to this registry. Docker Hub In the summer of 2020, we switched to Docker Hub. A complete list of images along with their Dockerfiles can be found in our rivannadocker GitHub repository. These images may or may not be installed as modules on the HPC system. We do not use the latest tag. Specify the exact version when pulling an image. For example: module load apptainer apptainer pull docker://uvarc/pytorch:1.5.1 Images that contain ipykernel can be added to your list of Jupyter kernels. To verify: apptainer exec <containername.sif python m pip list | grep ipykernel If this returns ipykernel <version, proceed here. You are welcome to use/modify our Dockerfiles. Some form of acknowledgment/reference is appreciated."
rc-website-fork/content/userinfo/hpc/software/samtools.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{% modulename %}} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Build Your Own Version Users may build their own versions of {{% softwarename %}} if they wish to use a different compiler or software version. Instructions are available on the {{% softwarename %}} website. Convert SAM to BAM with Samtools samtools view can convert humanreadable SAM files to binary BAM files. Below is a simple command to convert SAM files to BAM files. The S option specifies that the input is in SAM format, while the b option outputs to a BAM file: samtools view bS example.sam example.bam To preview the first five lines of the new BAM file: samtools view example.bam | head Most downstream analyses require your BAM files to be sorted, which can be achieved by: samtools sort example.bam o examplesorted.bam If you would like to visualize your BAM file using some viewer like IGV, you will need to create an index file samtools index examplesorted.bam Finally, samtools flagstat is a good way to get simple statistics from a BAM file including QC, duplicates, mapped reads, and many others samtools flagstat examplesorted.bam Slurm Script Example To run {{% softwarename %}} on the HPC system, a script similar to the following can be used. {{< pullcode file=""/static/scripts/samtools.slurm"" lang=""nohighlight"" }} To speed up your code, use multiple cpus per task. Here, we"
rc-website-fork/content/userinfo/hpc/software/samtools.md,"ask for 8 with the cpuspertask option, but only specify 7 in our samtools command to leave one for the manager process: {{< pullcode file=""/static/scripts/samtoolsthreaded.slurm"" lang=""nohighlight"" }}"
rc-website-fork/content/userinfo/hpc/software/machine-learning.md,"Overview Many machine learning packages can utilize general purpose graphics processing units (GPGPUs). If supported by the respective machine learning framework or application, code execution can be manyfold, often orders of magnitude, faster on GPU nodes compared to nodes without GPU devices. The HPC system has several nodes that are equipped with GPU devices. These nodes are available in the GPU partition. Access to a GPU node and its GPU device(s) requires specific Slurm directives or command line options as described in the Jobs using a GPU Node section. Applications Several machine learning software packages are installed on the UVA HPC system. The most commonly used ones are: TensorFlow PyTorch Other less frequently used software packages include: Dragonn LightGBM XGBoost"
rc-website-fork/content/userinfo/hpc/software/math-statistics.md,"Overview Many popular math and statistics software packages are available on the HPC system. General considerations Available Math & Statistics Software To get an uptodate list of the installed math applications, log on to UVA HPC and run the following command in a terminal window: module keyword math To get more information about a specific module version, run the module spider command, for example: module spider mathematica List of Math & Statistics Software Modules {{< rivannasoftware moduleclasses=""math"" }} Using a Specific Software Module To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module. After loading a module, you are ready to run the application(s) provided by the module. For example: module load mathematica"
rc-website-fork/content/userinfo/hpc/software/miniforge.md,"Overview Miniforge provides the Conda and Mamba package managers, with the default channel being condaforge. (Mamba is a reimplementation of the Conda package manager in C++ that uses a stateoftheart library ""libsolv"" for much faster dependency solving.) We have transitioned from Anaconda to Miniforge on Oct 15, 2024. See here for details. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Installing packages Packages could be installed via the pip, conda, or mamba package managers Using pip Open the bash terminal, and type: module load miniforge pip search packagename (search for a package by name) pip install user packagename (install a package) pip update packagename upgrade (upgrade the package to the latest stable version) pip list (list all installed packages) {{% callout %}} Do not upgrade pip. If you see the following message asking you to upgrade your pip version, it is usually safe to ignore it. You are using pip version x.x.x, however version y.y.y is available. You should consider upgrading via the 'pip install upgrade pip' command. Doing so may result in broken dependencies. (As of 01/10/2020, this error message is suppressed.) {{% /callout %}} However, if you must upgrade pip, please do so in a virtual environment, such as conda. Using conda {packageinstallationwithconda} You can specify which version of Python you want to run using conda. This can be done on a projectbyproject basis, and"
rc-website-fork/content/userinfo/hpc/software/miniforge.md,"is part of what is called a ""Virtual Environment"". A Virtual Environment is simply your isolated copy of Python in which you maintain your own version of files and directories. It enables you to keep other projects unaffected. With projects that have similar dependencies, you can freely install different versions of the same package without worry on two different Virtual Environments. In order to jump between two VE's, you simply activate or deactivate your environment. Follow the steps below: Set up your Virtual Environment: conda create n yourenvnamegoeshere (default Python version: use conda info to find out) OR conda create n yourenvnamegoeshere python=versiongoeshere (This command will automatically upgrade pip to the latest version in the environment. To find specific Python versions, use conda search ""^python$"".) If it asks you for y/n, hit y to proceed. It will start the installation Activate your newly created environment source activate yourenvnamegoeshere Install a package in your activated environment conda install n yourenvnamegoeshere yourpackagenamegoeshere OR conda install n yourenvnamegoeshere \ yourpackagenamegoeshere=versiongoeshere OR (even better) In your home directory or Conda installation directory, create a file called .condarc (if not already there) Inside the file write the following: createdefaultpackages yourpackagenamegoeshere yourpackagenamegoeshere yourpackagenamegoeshere ... Now everytime you create a new environment, all those packages listed in .condarc will be installed. 1. To end the current environment session: conda deactivate 1. Remove an environment: conda remove n yourenvnamegoeshere all 1. To create a JupyterLab tile for your conda environment: Install ipykernel inside your activated environment: conda install c condaforge ipykernel Then, create a new kernel: python m ipykernel install user name=MyEnvName Your new kernel will show up as a tile when you select File New Launcher in JupyterLab. To see all available environments, run conda env list. {{% callout %}} Tip: use mamba instead of conda. Conda"
rc-website-fork/content/userinfo/hpc/software/miniforge.md,"can take a long time to resolve environment dependencies and install packages. A new tool, mamba, has been developed to speed up this process considerably. Simply replace conda with mamba in any commands used to build an environment or install packages. Then you can still call your environment using source activate <env. {{% /callout %}} Python and MPI The most widely used MPI library for Python is mpi4py. We recommend creating an environment for it. When installing on the cluster, please do not use conda install since this will install prebuilt binaries, including a version of MPICH that does not communicate correctly with our Slurm resource manager. The best practice is to install from the condaforge channel using their external versions of an MPI library. These are simply bindings to an MPI package provided by the underlying system. For our system we will use OpenMPI. First load a compiler and a corresponding version of OpenMPI. module load gcc openmpi For the above example, with the versions of gcc available on our system, this is gcc 11.4.0. Now check the version of OpenMPI. bash module list In this example it is OpenMPI 4.1.4. Be aware that specific version numbers will change with time. Now view the available external versions of OpenMPI from condaforge bash conda search f openmpi c condaforge Install the bindings bash conda install c condaforge ""openmpi=4.1.4=external"" Now you can install mpi4py bash conda install c condaforge mpi4py Example Slurm script NonMPI {{< pullcode file=""/static/scripts/pythonserial.slurm"" lang=""nohighlight"" }} MPI {{< pullcode file=""/static/scripts/pythonmpi.slurm"" lang=""nohighlight"" }} Using Conda Environments on Rio When running Slurm jobs on Rio, it is essential to ensure that all packages and environments installed with miniforge are configured correctly. Rio compute nodes can only run jobs from highsecurity research standard storage, so it’s important to ensure that all"
rc-website-fork/content/userinfo/hpc/software/miniforge.md,"necessary files and variables point to this location. The following environment variables should be exported in your ~/.bashrc file to install conda packages and environment into a specific directory (/standard/ivyxxxxxxx/path/to/.conda/): export HTTPSPROXY=http://figgiss.hpc.virginia.edu:8080 export HTTPPROXY=http://figgiss.hpc.virginia.edu:8080 export CONDAPKGSDIRS=""/standard/ivyxxxxxxx/path/to/.conda/pkgs"" export CONDAENVSPATH=""/standard/ivyxxxxxxx/path/to/.conda/envs"" export CONDACACHEDIR=""/standard/ivyxxxxxxx/path/to/.conda/cache"" export CONDAROOT=""/standard/ivyxxxxxxx/path/to/.conda"" export XDGCACHEHOME=""/standard/ivyxxxxxxx/path/to/.conda/cache"" {{% callout %}} Keep in mind to replace /standard/ivyxxxxxxx/path/to/.conda with the path to your .conda directory in your storage share. Additionally, You'll want to make sure all of the sub directories exist under .conda (Eg pkgs, envs, and cache) {{% /callout %}} Conda environments will need to be created with the prefix flag. Eg To access the environment on the compute nodes you'll want to export all of the previous commands and active the full file path to the environment: source activate /standard/ivyxxxxxxx/path/to/.conda/envs/myenv More Information Please visit the official website."
rc-website-fork/content/userinfo/hpc/software/_index.md,"Overview {{< rawhtml }} Research Computing at UVA offers a variety of standard software packages for all UVA HPC users. We also install requested software based on the needs of the highperformance computing (HPC) community as a whole. Software used by a single group should be installed by that group’s members, ideally on leased storage controlled by the group. Departments with a set of widelyused software packages may install them to the lspapps space. The Research Computing group also provides limited assistance for individual installations. For help installing research software on your PC, please contact Research Software Support at resconsult@virginia.edu. {{< /rawhtml }} Software Modules and Containers Software on the HPC system is accessed via environment modules or containers. {{< rawhtml }} Learn about Modules Learn about Containers {{< /rawhtml }} Software by Category Popular Software Packages and Libraries by Domain Biology & Bioinformatics Chemistry Data Science & Machine Learning Engineering Image Processing & Scientific Visualization Math & Statistics {{< rawhtml }} Full list of all software and library modules {{< /rawhtml }} Programming Languages & Tools Python Perl Julia Matlab Mathematica sageMath R & RStudio Jupyter Lab Compilers Parallel & MPI Libraries Debuggers and Profilers Containers IDEs and Editors Workflow Managers Access & File Transfer Tools Open OnDemand FastX MobaXterm SSH Data Transfer Globus"
rc-website-fork/content/userinfo/hpc/software/nvhpc.md,"Compiling for a GPU Using a GPU can accelerate a code, but requires special programming and compiling. Several options are available for GPUenabled programs. OpenACC OpenACC is a standard Available NVIDIA CUDA Compilers {{< moduleversions module=""cuda"" }} {{< moduleversions module=""nvhpc"" }} GPU architecture According to the CUDA documentation, ""in the CUDA naming scheme, GPUs are named smxy, where x denotes the GPU generation number, and y the version in that generation."" The documentation contains details about the architecture and the corresponding xy value. The compute capability is x.y. Please use the following values when compiling CUDA code on the HPC system. | Type | GPU | Architecture | Compute Capability | CUDA Version | |||||| | Datacenter | V100 | Volta | 7.0 | 9+ | | | A100 | Ampere | 8.0 | 11+ | | | A40 | Ampere | 8.6 | 11+ | | | H200 | Hopper | 9.0 | 11.8+ | | RTX | A6000 | Ampere | 8.6 | 11+ | | GeForce | RTX2080Ti | Turing | 7.5 | 10+ | | | RTX3090 | Ampere | 8.6 | 11+ | As an example, if you are only interested in V100 and A100: gencode arch=compute70,code=sm70 gencode arch=compute80,code=sm80"
rc-website-fork/content/userinfo/hpc/software/abinit.md,"Description {{< moduledescription }} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions The current installation of {{% softwarename %}} incorporates the most popular packages. To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }} Build Your Own Version Users may build their own versions of {{% softwarename %}} if they wish to use a different compiler/MPI combination. Instructions are available on the {{% softwarename %}} website. If using the Intel compiler, you need to add the heaparrays flag to the Fortran compiler options. Slurm Script Example To run {{% softwarename %}} on the HPC system, a script similar to the following can be used. {{% softwarename %}} has many options so only a basic example is shown. {{% pullcode file=""/static/scripts/abinit.slurm"" lang=""nohighlight"" %}} For this example, the text file files has the following content: gw.in gw.out gwi gwo gwt ../pseudo/scsg15.oncvpsp.psp8 ../pseudo/sesg15.oncvpsp.psp8 ../pseudo/mosg15.oncvpsp.psp8 ../pseudo/wsg15.oncvpsp.psp8"
rc-website-fork/content/userinfo/hpc/software/cellranger-atac.md,"Description {{% moduledescription %}} Software Category: {{% modulecategory %}} For detailed information, visit the {{% softwarename %}} website. Available Versions To find the available versions and learn how to load them, run: module spider {{< modulename }} The output of the command shows the available {{% softwarename %}} module versions. For detailed information about a particular {{% softwarename %}} module, including how to load the module, run the module spider command with the module's full version label. For example: module spider {{% modulefirstversion %}} {{< moduleversions }}"
rc-website-fork/content/userinfo/hpc/ood/fileexplorer.md,"Open OnDemand provides an integrated file explorer to browse and manage small files. Rivanna and Afton have multiple locations to store your files with different limits and policies. Specifically, each user has a relatively small amount of permanent storage in his/her home directory and a large amount of temporary storage (/scratch) where large data sets can be staged for job processing. Researchers can also lease storage that is accessible on Rivanna. Contact Research Computing or visit the storage website for more information. The file explorer provides these basic functions: Renaming of files Viewing of text and small image files Editing text files Downloading & uploading small files To see the storage locations that you have access to from within Open OnDemand, click on the Files menu. The dropdown list will show your Rivanna/Afton /home directory and possibly other leased storage volumes like /project that your group has access to. Clicking on any of them will open a new tab with a file browser for that storage location. The user interface is divided into two panes: The left pane shows the directory tree at the current storage location. The right pane shows the content of a particular directory that has been selected in the left window pane. Above the right window pane are rows of buttons that allow you to execute specific file operations. Renaming Files To rename an existing file or directory, click on it in the right window pane and click the Rename button. In the popup window, modify the file/directory name and click OK. Viewing Files To view existing text and image files (.png, .tif, etc.), select the file in the right window pane and clicking on the View button. The file content or image is shown in a popup window. Editing Files To edit an existing text"
rc-website-fork/content/userinfo/hpc/ood/fileexplorer.md,"file, e.g. source code file or a job script, select the file in the right window pane and click the Edit button. The file is opened in a new browser tab labelled as Editor. The editor shows line numbers and supports syntax highlighting for common programming languages (e.g. Python, R, Matlab, XML, markdown, Bash, etc.). A specific syntax highlighting can be chosen under the Mode dropdown menu. To save any changes, click on the Save button in the top left corner of the Editor tab. Opening Files in Terminal Clicking on the Open Files in Terminal button opens a terminal window in a new web browser tab. The current directory is set to the location directory or file that was selected in the File Explorer's left or right window pane. Note that this terminal is not able to start graphical applications such as the Matlab desktop; for applications such as those you must use FastX. Navigating to other Storage Locations To navigate to other file locations on Rivanna/Afton, you can use the Go To button to enter a specific storage volume and directory path. For example, if you are in your home directory and want to go to your /project directory, enter /project/ and click OK. This will show a list of all project directories including those of other research groups. You can also enter the full path to your Project storage, e.g. /project/mystorage, to go straight to your group's storage. To find out about the full path of all your leased storage locations, run the hdquota command in a Rivanna terminal window. You can only access project directories associated with your leased storage and MyGroup. File Transfer The Open OnDemand file explorer should only be used to transfer small files such as your source code and job scripts. File"
rc-website-fork/content/userinfo/hpc/ood/fileexplorer.md,"Upload to the HPC System To upload files from your current workstation (i.e. the computer that the web browser runs on) to the HPC system, choose the directory on Rivanna/Afton to which the files should be uploaded. Then click on the Upload button at the top of the File Explorer window. This will produce a small popup window with a Choose Files button. Clicking the Choose Files button opens a file browser window showing the storage locations accessible on your local workstation. Select a single file or multiple files and click the Choose or OK button. This will initiate the file transfer and close the file browser window on your local workstation. The uploaded files will appear as new or updated files in the current directory shown in the right pane of the Open OnDemand File Explorer. File Download from the HPC System To download files from the HPC system to your local workstation, select the files or directories from download in the main (right) window pane of the Open OnDemand File Explorer and click the Download button. The selected files and directories are immediately downloaded in the usual way depending on your Web browser; typically it will copy the files to your Download directory. Alternative File Transfer Tools Large files ( 2GB) should be transferred with scp/sftp either from a standard shell or from an application such as MobaXterm (Windows), or Cyberduck or Filezilla (Windows, Mac, Linux), or via the Globus transfer tool. Globus is a browserbased file transfer tool optimized for fast, faulttolerant file transfers that run in the background once started. To use Globus with the HPC system please follow the instructions at our Globus documentation page. A link to the Globus web application can be found on the top of the file explorer window."
rc-website-fork/content/userinfo/hpc/ood/desktop.md,"Overview The Open OnDemand Desktop app provides a full Linux Desktop environment launched on userspecified allocated hardware resources which may include a compute node equipped with graphical processing units (GPUs). {{% callout %}} This is the preferred mechanism to start compute intensive applications that require a graphical user interface (GUI) on the HPC system. {{% /callout %}} Accessing the Desktop App To access the app and start a desktop session, connect to our Open OnDemand portal: Open a web browser and go to https://ood.hpc.virginia.edu. Use your Netbadge credentials to log in. On the top right of the menu bar of the Open OnDemand dashboard, click on Interactive Apps. In the dropdown box, click on Desktop. Requesting an Instance Your instance of the Desktop app will run on a HPC compute node. So it will need a list of resources, such as partition, time, and allocation. If you are new to UVA HPC, you may want to read the HPC User Guide to learn more about the partitions. After connecting to JupyterLab through Open OnDemand, a form will appear where you can fill in the resources for the Desktop session. Partition: UVA HPC has different types of compute nodes that are organized in partitions based on the type of processing they can do. Most of the time you will select the Standard or Dev partition. If you are running machine or deeplearning models that support GPUs, you will want to use the GPU partition. Number of hours: The number of hours defines the amount of time that your session will be active. Bewarewhen time runs out the session will end without warning. Allocation (SUs): An allocation is a special Grouper (requires VPN connection) group that holds the service units you may use for your computation. You may be a member of"
rc-website-fork/content/userinfo/hpc/ood/desktop.md,"multiple allocation groups. When done filling in the resources, click on the blue “Launch” button at the bottom of the form. It may take some time for the system to find and allocate the requested resources. When the resources are ready a Launch Desktop button will appear. Click on the button and the Desktop session will open in a new tab. The Desktop Environment"
rc-website-fork/content/userinfo/hpc/ood/jobcomposer.md,"Open OnDemand allows you to submit Slurm jobs to the cluster without using shell commands. The job composer simplifies the process of: Creating a script Submitting a job Downloading results Submitting Jobs We will describe creating a job from a template provided by the system. Open the Job Composer tab from the Open OnDemand Dashboard. Go to the New Job tab and from the dropdown, select From Template. You can choose the default template or you can select from the list. Click on Create New Job. You will need to edit the file that pops up, so click the light blue Open Editor button at the bottom. Replace your allocation with your group name and click Save. Open OnDemand creates a unique directory for each job. In most cases, you will need to upload or move files into the job directory, so when you have finished editing the script, return to the Job Composer tab and click the darker blue Open Dir button at the bottom of the page. You may now use the File Explorer to upload or move the files you will need to run the job. When you have finished preparing your job, click the Submit button. Your job will be submitted. Any errors will appear at the top of the page. The Job Composer main panel allows you to monitor your job status. When your job has completed, select the job from the list so that it is highlighted. Click the Open Dir button again to enter the directory. There you may view or download your result files."
rc-website-fork/content/userinfo/hpc/ood/_index.md,"Overview Open OnDemand is a graphical user interface that allows access to UVA HPC via a web browser. Within the Open OnDemand environment users have access to a file explorer; interactive applications like JupyterLab, RStudio Server & FastX Web; a command line interface; and a job composer and job monitor. Logging in to UVA HPC The HPC system is accessible through the Open OnDemand web client at https://ood.hpc.virginia.edu. Your login is your UVA computing ID and your password is your Netbadge password. Some services, such as FastX Web, require the Eservices password. If you do not know your Eservices password you must change it through ITS by changing your Netbadge password (see instructions). Open OnDemand can be accessed from off Grounds without the UVA VPN client, but FastX Web requires it. The Dashboard Once you log in, you are automatically redirected to the Open OnDemand dashboard. The dashboard may contain information about upcoming changes and maintenance work that can affect your jobs, so please read all announcements carefully. Keep the dashboard tab open until you are ready to end your session and log out. The dashboard provides access to all Open OnDemand services and applications. These include File Explorer Interactive Applications Command Line Interface (Shell) Job Composer & Job Monitor These services and applications are accessible through dropdown boxes on the menu bar. When you click on any of the dropdown options, a new tab will open in your browser. File Explorer File Explorer makes browsing and managing small files easy. Rivanna and Afton have multiple locations to store your files with different limits and policies. Specifically, each user has a relatively small amount of permanent storage in his/her home directory and a large amount of temporary storage (/scratch) where large data sets can be staged for job processing. Users"
rc-website-fork/content/userinfo/hpc/ood/_index.md,"can also lease storage that is accessible on Rivanna. Contact Research Computing or visit the storage page for more information. The file explorer provides these basic functions: Renaming of files Viewing of text and small image files Editing of text files Downloading & uploading of small files Visit our File Explorer guide for detailed instructions. Interactive Applications Open OnDemand provides access to interactive applications that provide a full graphical HPC desktop environments, JupyterLab for running Jupyter notebooks, RStudio Server, Matlab, a simple terminal shell, and a variety of other research apps. Desktop The Desktop app provides a full Linux Desktop environment launched on userspecified allocated hardware resources which may include a compute node equipped with graphical processing units (GPUs). This is the preferred mechanism to start compute intensive applications that require a graphical user interface (GUI). Please read the Open OnDemand Desktop documentation for detailed instructions on how to specify resources and start a desktop session. FastX Web FastX Web enables users to start an X11 desktop environment on a remote system. When launched through Open OnDemand, FastX Web provides access to a HPC frontend. The FastX Web desktop environment can be used to open conventional shell terminals or launch applications with a graphical user interface. Please read our FastX Web documentation for a detailed description of this remote desktop environment. FastX Web sessions are not suitable for running compute intensive applications or codeOpen OnDemand Desktop is intended for such purpose. JupyterLab JupyterLab provides an environment that has become popular for interactive code development and debugging. JupyterLab sessions run on userspecified allocated hardware resources which may include compute nodes equipped with graphical processing units (GPUs). Please read the JupyterLab documentation for detailed instructions on how to start JupyterLab sessions and specify hardware resource requests. After starting a JupyterLab session, you're"
rc-website-fork/content/userinfo/hpc/ood/_index.md,"taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner. RStudio Server RStudio provides an environment specifically designed for interactive R script development and debugging. Please read the RStudio Server documentation for detailed instructions on how to start RStudio sessions and specify hardware resource requests. After starting an RStudio session, you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner. Matlab The Matlab interface provides an environment for interactive Matlab script development and debugging. Please read the Matlab documentation for detailed instructions on how to start Matlab sessions and specify hardware resource requests. After starting an interactive Matlab session, you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner. Blender Through this interface users can run interactive Blender sessions on a dedicated GPU node. Please read the Blender documentation for detailed instructions on how to start Blender sessions and specify hardware resource requests. After starting an interactive Blender session, you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner. ParaView Through this interface users can run interactive ParaView sessions on a dedicated GPU node. Please read the ParaView documentation for detailed instructions on how to start ParaView sessions and specify hardware resource requests. After starting an interactive ParaView session,"
rc-website-fork/content/userinfo/hpc/ood/_index.md,"you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner. Command Line Interface (Shell) To open a conventional command line terminal window, click on the Clusters dropdown menu and select Rivanna Shell Access. A new tab opens that provides a Bash command line environment. This is similar to logging in through ssh but with the limitation that it cannot start graphical (X11) applications. You must use FastX for X11 applications such as the Matlab desktop. Utilities In addition to interactive apps, UVARC offers several static applications found in the 'Utilities' dropdown menu on Open OnDemand. These applications serve as wrappers for existing bash commands and scripts users can run in the shell. Disk Usage The Disk Usage app provides information on the remaining storage in each project directory available to the user, as well as in their /home and /scratch directories. Acting as a wrapper for the hdquota command, it displays the size and available storage in each quota. Queue Status The Queue Status app provides detailed information on jobs queued and running in each partition. It acts as a wrapper for the qlist command and displays data on the total cores, free cores, jobs running, jobs pending, time limits, and SU charges for each partition. Check Scratch For Purge According to UVA RC policy, files in the /scratch directory that have not been accessed for over 90 days will be permanently deleted or 'purged'. The Check Scratch For Purge app allows you to see which files are at risk of being purged and download a list of their filenames. It displays the output of the command checkscratchforpurge, showing a list of files ordered"
rc-website-fork/content/userinfo/hpc/ood/_index.md,"from the oldest last accessed to the most recently accessed. Slurm Script Generator The Slurm Script Generator helps users write Slurm scripts for their jobs. It is available on Open OnDemand and the RC Website. This tool assists users in creating a Slurm script tailored to the specifications of a given job. It also calculates the estimated amount of Standard Units (SUs) needed to run the job. Job Composer The Job Composer is an easy way to submit generalpurpose jobs. You can copy preexisting templates and modify them for your application, then submit a job at the click of a few buttons. It works with the File Explorer to allow you to upload or move files you need for your job, and to download your results. Visit our Job Composer documentation for details."
rc-website-fork/content/userinfo/hpc/logintools/fastx.md,"Overview FastX is a commercial solution that enables users to start an X11 desktop environment on a remote system. It is available on the UVA HPC frontends. Using it is equivalent to logging in at the console of the frontend. Using FastX for the Web We recommend that most users access FastX through its Web interface. To connect, point a browser to: https://fastx.hpc.virginia.edu {{< offcampus }} Login Screen After entering your computing ID and Netbadge password, you will see a launch screen. Launch In this example, we have no preexisting sessions so we must create one. Click the Launch Session button. This will bring up a screen showing the options. Launch MATE Most users will choose the MATE desktop. Click on the green MATE icon. Text showing the choice will appear in the box below it. Click the Launch button to start your session. If you are running a popup blocker in your browser, a request will appear that you unblock this site. Once you do so, you can click the button to continue to your session. After a short wait, your desktop will appear. Desktop The toolbar at the top controls FastX behavior. If the desktop does not automatically expand to the browser screen, the user can click the double arrow. The pushpin pins the toolbar to the screen. Logout When you are done, you can log out by selecting Logout from the System menu. This will terminate your FastX Web session. Resume If you close the browser tab with the desktop, your session will be suspended rather than terminated. You can go back to the launch tab and click the thumbnail of your desktop. To resume the session, click the arrow (play) button. Terminate Session To terminate the session, either select Terminate from the Actions dropdown menu, or"
rc-website-fork/content/userinfo/hpc/logintools/fastx.md,click the close symbol. Please terminate sessions if you do not plan to use them in the near future.
rc-website-fork/content/userinfo/hpc/logintools/cl-data-transfer.md,"Standard Linux tools can efficiently transfer a small to moderate quantity of data to or from Rivanna/Afton. scp scp uses the secure shell (SSH) protocol to transfer files between your local machine and a remote host, or between two remote hosts. The following syntax enables copying from local to remote or vice versa. In both cases we are starting from the local system. By default, scp works from the level of the directory in which it is invoked. Copying from local to remote: scp sourcefile mst3k@hostaddress:targetfile Copying from remote to local: scp mst3k@hostaddress:sourcefile targetfile The following examples detail how to transfer data between your local computer and /project storage on Rivanna/Afton. In these examples myfile is the file you would like to transfer mst3k is your computing ID mygroupname is the name of your /project directory. mydirectory is the directory to which you wish to copy the file. To copy a file From your computer to /project storage: scp myfile mst3k@login.hpc.virginia.edu:/project/mygroupname From /project storage to mydirectory on your computer: scp mst3k@login.hpc.virginia.edu:/project/mygroupname/myfile /mydirectory scp accepts wildcards. In this example, the mycode directory must exist in your scratch directory. scp cxx mst3k@login.hpc.virginia.edu:/scratch/mst3k/mycode scp Options The r option recursively copies directories. From your computer to /project storage: scp r mydirectory mst3k@login.hpc.virginia.edu:/project/mygroupname From /project storage to your computer: scp r mst3k@login.hpc.virginia.edu:/project/mygroupname /targetdirectory The p option preserves modification time, access time, and ownership from the original file. The q option suppresses the progress and debugging messages. Useful for scripts. sftp Secure FTP or sfpt is an interface built on top of scp to mimic the behavior of ftp. To connect to UVA HPC with sftp, execute the following in the command line interface: sftp mst3k@login.hpc.virginia.edu When prompted, enter your password. Once the connection succeeds, you will see the sftp prompt: sftp Navigating Directories You can"
rc-website-fork/content/userinfo/hpc/logintools/cl-data-transfer.md,"access both your local and remote file systems with sftp. The following table lists how to execute the following commands for both your local and remote systems. | Action |On Remote System | On Local System | | : |:: | :: | |Print Working Directory | pwd | lpwd | |List Contents of Directory | ls | lls | |Change Directory | cd | lcd | File Transfer from Local to Remote To transfer files from your computer to the Rivanna/Afton file system, use the put command: sftp put myfile To transfer a folder from your computer to Rivanna/Afton, use put r. A folder with the same name must also exist on Rivanna. An example is shown below: sftp mkdir /project/mygroupname/myfolder sftp cd /project/mygroupname sftp put r myfolder File Transfer from Remote to Local To transfer files from Rivanna/Afton to your computer, use the get command: sftp get myfile To transfer a folder from Rivanna/Afton to your computer, use get r: sftp get myfolder Terminating the Connection To terminate the sftp connection, use exit. sftp exit rsync Remote sync is a powerful tool for copying files. It is most widely used to transfer multiple files and/or directories. In this example, we have a local directory ldir and a remote directory rdir and we wish to copy the contents of ldir to rdir. rsync r ldir/ mst3k@login.hpc.virginia.edu:rdir The trailing / after ldir is important. Without it, ldir and its contents would be placed under rdir. Unlike scp, if the target directory does not exist, rsync will create it. It is more common to use the a (archive) option. This option preserves symbolic links, special files, ownership, permissions, and timestamps. rsync a ldir/ mst3k@login.hpc.virginia.edu:rdir Show a progress bar and keep partially transferred files rsync Pa ldir/ mst3k@login.hpc.virginia.edu:rdir Delete files not present"
rc-website-fork/content/userinfo/hpc/logintools/cl-data-transfer.md,"on the source directory if they are present on the target directory rsync Pa delete ldir/ mst3k@login.hpc.virginia.edu:rdir Have rsync print the list it will transfer without carrying out the transfers. Especially important when using delete. rsync Pa delete dryrun ldir/ mst3k@login.hpc.virginia.edu:rdir AWS CLI The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services, including data transfer. Learn more about the AWS CLI tools. Globus CLI The Globus commandline interface can also be used to orchestrate the transfer of large datasets, or to script regular transfers in or out of systems. Read more about the Globus CLI. Usage from Off Grounds {{< offcampus }}"
rc-website-fork/content/userinfo/hpc/logintools/rivanna-ssh.md,"The secure shell ssh is the primary application used to access the HPC system from the command line. Connecting to a Remote Host For Windows, MobaXterm is our recommended ssh client; this package also provides an SFTP client and an X11 server in one bundle. Mac OSX and Linux users access the cluster from a terminal through OpenSSH, which are preinstalled on these operating systems. Open a terminal (on OSX, the Terminal application) and type ssh Y mst3k@login.hpc.virginia.edu where mst3k should be replaced by your user ID. You will generally need to use this format unless you set up your user account on your Mac or Linux system with your UVA ID. Please note that ssh will not echo your password or move your cursor as you type. Mac users will need to install XQuartz in order to use graphical applications through a shell (the Y option will permit this). Passwordless ssh using keys Sometimes you will need to enable passwordless ssh. We allow passwordless ssh to frontend nodes from UVA IP addresses. Key authentication works by matching two halves of an encrypted keypair. The ""public"" key is placed within your home directory on the remote server and the ""private"" key is kept safely on your own workstation. You should treat private keys as securely as you would any password. Windows In MobaXterm, click the Tools icon or menu and select MobaKeyGen. Keep it as RSA and leave the passphrase blank. Save the public key under a name of your choice. MobaXterm will display the public key. Copy this key to your clipboard. Continue as for ""All Operating Systems."" Mac OSX and Linux Open a terminal and type sshkeygen Accept all defaults. When it asks for a passphrase, hit Enter to keep it blank. Open the file idrsa.pub and copy"
rc-website-fork/content/userinfo/hpc/logintools/rivanna-ssh.md,"its contents to your clipboard. Graphical Installation, All Operating Systems Log in to UVA HPC, cd .ssh Note the period in front of ssh. Then, using a text editor, open the file authorizedkeys. Append the key you copied previously. Use the middle mouse button or scroll wheel to paste it into the authorizedkeys file if you are using MobaXterm. Otherwise rightclick and select paste. Be sure there are no line breaks in the key. CommandLine Transfer (Mac and Linux) Transfer the idrsa.pub file to the HPC system with scp: scp ~/.ssh/idrsa.pub mst3k@login.hpc.virginia.edu:~/.ssh/mykey.pub Log in to UVA HPC through a terminal, then type cat ~/.ssh/mykey.pub ~/.ssh/authorizedkeys Passwordless ssh Between Nodes If you are permitted to use passwordless ssh between HPC compute nodes, such as for ANSYS, follow the instructions for Mac and Linux but generate the key directly on a UVA HPC frontend. Use the cat command to append the key to your authorizedkeys file. Troubleshooting When you log in to a new host, ssh will ask whether you wish to accept the host key. You must answer yes explicitly in order to proceed. When off Grounds, you must use the UVA Anywhere client in order to connect to onGrounds resources. If you do not, your attempt to use ssh will hang with no messages. A relatively short period of inactivity may cause ssh connections to time out. Mac OSX and Linux users can reduce this by setting a configuration value. At the terminal change to your ~/.ssh directory cd ~/.ssh Use a text editor to create a file called config. Place the following lines in it: Host ServerAliveInterval 60 There should be one or more spaces at the beginning of the second line. MobaXterm users should see the documentation for instructions to enable KeepAlive. When in doubt, you can obtain"
rc-website-fork/content/userinfo/hpc/logintools/rivanna-ssh.md,"more information by running ssh with the v (verbose) flag. ssh v Y mst3k@login.hpc.virginia.edu A common error message from ssh is when a host key changes, such as after an upgrade. This will appear as a message containing lines such as @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (maninthemiddle attack)! It is also possible that a host key has just been changed. MobaXterm will typically detect this and ask whether you want to change the host key; you may answer yes. On Mac OSX or Linux, from a terminal go to your ~/.ssh directory and use a text editor to open the file knownhosts. Remove all lines that might refer to UVA HPC. Alternatively, just delete the entire file; it will be recreated as you log in to different hosts. If you are unfamiliar with using a command line on Mac, you must cd cd .ssh After that you must either edit the knownhosts file with a text editor to remove the invalid lines, either through a commandline editor or with open knownhosts (The above command only works on Macs, not Linux.)"
rc-website-fork/content/userinfo/hpc/logintools/mobaxterm.md,"MobaXterm is the recommended login tool for Windows users. It bundles a tabbed ssh client, a graphical draganddrop sftp client, and an X11 window server for Windows, all in one easytouse package. Some other tools included are a simple text editor with syntax coloring and several useful Unix utilities such as cd, ls, grep, and others, so that you can run a lightweight Linux environment on your local machine as well as use it to log in to a remote system. Download To download MobaXterm, click the link below. Select the ""Home"" version, ""Installer"" edition, Download MobaXterm Run the installer as directed. Connecting When you start MobaXterm you can create a new session, restart a saved one, or attach to an existing one. To start a new one, begin with ssh. Remember to connect to login.hpc.virginia.edu with your Eservices username and password. SSH key authentication is also supported. You can prevent premature ssh timeouts by accessing SettingsSSH and making sure the box labeled ""SSH keepalive"" is checked. When you start an ssh session, MobaXterm will automatically start a sftp session with a file browser. You can doubleclick files on the remote host and they will open if the appropriate application is found on your local computer. MobaXterm by default automatically allows X11 to pass through ssh; no special options are required when you log in. You can run individual X11 (graphical) applications simply by starting them. Remember to type an ampersand (&) at the end of the command. For example, to edit a simple text file using gedit, type gedit & This can be slow, especially off Grounds, so for extensive work with graphical applications you may prefer FastX. When you are logged in to a Unix system (like the UVA HPC system), MobaXterm will utilize the Unix X11 convention"
rc-website-fork/content/userinfo/hpc/logintools/mobaxterm.md,"for cut and paste. Highlighting text with the left mouse button selects it automatically. Clicking the middle mouse button pastes it. If you do not have a middle mouse button or scroll wheel, such as on a laptop, clicking both left and right buttons simultaneously emulates a middle mouse button. Access from Off Grounds {{< offcampus }}"
rc-website-fork/content/userinfo/hpc/logintools/filezilla.md,"Filezilla is a crossplatform data transfer tool. The free version supports FTP, FTPS, and SFTP. Only SFTP can be used with UVA HPC. Download Download Filezilla Connecting to the HPC System and File Transfer Launch FileZilla. After launching FileZilla, the user interface will open. In the left panel, you should see your local file system and files listed in the left side panels. You will enter your login credentials in the fields highlighted in the figure below. Enter Your Credentials. Fill in the Host, Username, Password, and Port fields. Host: login.hpc.virginia.edu Username: your computing ID Password: your Eservices password Port: 22 When completed, click Quickconnect. Click OK on Warning. When connecting for the first time, a warning like the one shown below. Check the box next to “Always trust this host, add this key to the cache”, and then click OK. After successfully connecting, your home directory and files on the HPC system should appear in the rightside panels, as shown below. To transfer a file, simply doubleclick the filename in the panel or rightclick on the file and select the Upload option. To transfer a folder, rightclick on the folder and select Upload. Access from Off Grounds {{< offcampus }}"
rc-website-fork/content/userinfo/hpc/logintools/graphical-sftp.md,"Several options are available to transfer data files between a local computer and the HPC system through userfriendly, graphical methods. {{< offcampus }} MobaXterm MobaXterm is a Windows application that combines an ssh client for logging in, a graphical securecopy client for easy draganddrop file transfer, and an X11 server for displaying graphical applications. MobaXterm FileZilla FileZilla is a crossplatform application for draganddrop data transfer. It is available for Windows, Mac OSX, and Linux. FileZilla Cyberduck Cyberduck is another crossplatform application for draganddrop data transfer. It is available for Mac OSX and Windows. Cyberduck Open OnDemand The File Explorer in Open OnDemand can be used to upload and download small files. Open OnDemand can be accessed from any Web browser through Netbadge, and does not require the installation of an application, but the number and size of files that can be transferred is limited. Open OnDemand Remote Mounting for Research Project and Research Standard Storage Shares Leased storage (Research Project and Research Standard) can be mounted as remote drives to Windows and Mac OSX computers."
rc-website-fork/content/userinfo/hpc/logintools/cyberduck.md,"Cyberduck is a transfer tool for Windows and Mac. It supports a large number of transfer targets and protocols. Only SFTP can be used with Rivanna/Afton. The free version will pop up donation requests. Download Download Cyberduck Connecting to the HPC System and File Transfer Launch Cyberduck. After launching Cyberduck, the user interface will open. To initiate a connection to UVA HPC, click the Open Connection button. Enter Your Credentials. From the dropdown menu, select SFTP (SSH File Transfer Protocol). Then enter the appropriate information in the following fields: Host: login.hpc.virginia.edu Username: your computing ID Password: your UVA HPC password Port: 22 When completed, click Connect. After successfully connecting to UVA HPC, the contents of your UVA HPC home directory will appear in the user interface. Navigate to the directory to which you would like to transfer the files.To move to the higher level directories, use the highlighted dropdown menu. Transfer your local file or directory to Rivanna/Afton by dragging and dropping it to the Cyberduck user interface. There are two ways to transfer files from Rivanna/Afton to your local host. Method 1: Drag and Drop Simply drag your desired file or directory from the Cyberduck user interface to the target directory on your local machine. Method 2: Download To… Rightclick on the file or directory that you want to transfer. Select the Download To… option. Select the target directory on your machine where you want to download the file. Once your transfer is initiated, a popup window will then appear to inform you of the status of your transfer. Access from Off Grounds {{< offcampus }}"
rc-website-fork/content/userinfo/howtos/ivy/secure-globus-transfer.md,"Login In to Globus and Choose Ivy Collection Disconnect your High Security VPN – it will interfere with the high security network that Globus uses. Open https://www.globus.org/ and sign in through the University of Virginia using NetBadge. Once you are on the Transfer Files page, at the first ""Collection"" type ivy to search. Find the uvaivyDTN collection and select it. You will then be asked to authenticate. Once signed in, simply click through the names of your shares until you find the source or destination for your file transfers. Navigate to Your Personal Collection In another pane repeat the process of searching for a collection, but use the name of your personal collection. NOTE: Ivy is designed for security for research on sensitive data. Sensitive data should never be stored on or transmitted from personal laptops. Once you have moved your data to HighSecurity Research Standard Storage using Globus, you may need to copy it to your Ivy VM Storage. Doubleclick the Desktop icon labeled “HighSecurity Research Standard Storage” to see the files you transferred with Globus (a file explorer window will appear). Doubleclick the Desktop icon labeled “VM Storage” to see the files that currently exist in your VM storage. To copy data from HighSecurity Research Standard Storage to the VM, you can drag files from the HighSecurity Research Standard Storage file explorer window to the VM storage file explorer window, or you can use traditional copy+paste functions. The decision to work off of your data on the local hard drive versus on the HighSecurity Research Standard Storage drive should be driven by the size of your data, and how much your research needs to read from and write to it. For More Information To learn more about setting up and using a Globus personal collection, please see our"
rc-website-fork/content/userinfo/howtos/ivy/secure-globus-transfer.md,documentation.
rc-website-fork/content/userinfo/howtos/ivy/_index.md,Transfer Files to or from Ivy Using Globus
rc-website-fork/content/userinfo/howtos/general/redis.md,"« Return to Databases {{% callout %}} redis is an inmemory, key/value store. Think of it as a dictionary with any number of keys, each of which has a value that can be set or retrieved. However, Redis goes beyond a simple key/value store as it is actually a data structures server, supporting different kinds of values. Some fundamental concepts: Can be used as a database, cache, or message broker Supports multiple data types and structures Builtin replication Keys can be up to 512MB in size {{% /callout %}} {{< ref ""/userinfo/howtos/general/databases.md"" }} Getting Started HPC nodes can connect to external Redis databases hosted in Kubernetes or a public cloud (AWS, Azure, GCP, etc.) To use Redis from the commandline, use the rediscli. In the HPC system, this is a module: $ module load rediscli You can now create a connection to the server. Use port 6379 (the default port). No password is required: $ rediscli h redis.uvarc.io Basic Operations As a dictionary, Redis allows you to set and retrieve pairs of keys and values. Think of a ""key"" as a unique identifier (string, integer, etc.) and a ""value"" as whatever data you want to associate with that key. Values can be strings, integers, floats, booleans, binary, lists, arrays, dates, and more. {{% callout %}} Note! The ""value"" half of a Redis key/value pair can be quite large 512MB. This is considerably larger than other popular NoSQL databases such as DynamoDB or MongoDB. {{% /callout %}} To view all keys (once you have established a connection to the Redis server): redis.uvarc.io:6379 keys 1) ""hello"" 2) ""1234"" 3) ""1a2b3c"" 4) ""124a"" Then use a specific key to fetch its value: redis.uvarc.io:6379 get hello ""world"" To set a new key/value: redis.uvarc.io:6379 set herman melville OK Set an expiring key/value (EX in seconds,"
rc-website-fork/content/userinfo/howtos/general/redis.md,"PX in milliseconds) redis.uvarc.io:6379 set jane eyre EX 30 OK Delete a key/value: redis.uvarc.io:6379 del herman OK Working Alongside Other Users Redis allows for the creation and management of multiple databases, called ""indexes"". By default new connections are attached to index 0 but this can be changed to the integer of another index. Keys/values stored in one index are unavailable to another index. Use select to move between indexes. There are 64 total indexes in this implementation. redis.uvarc.io:6379 select 0 OK redis.uvarc.io:6379 set hello world OK redis.uvarc.io:6379 get hello ""world"" redis.uvarc.io:6379 select 1 OK redis.uvarc.io:6379[1] get hello (nil) Indexes need not be created in order. We suggest you select a high arbitrary number (0 to 63) for a private index. Populate and empty it as you find necessary. However, in the standard security environment remember that your keys/values are visible to other UVA HPC users. To connect to the Redis endpoint and specify an index other than 0, use the n flag with the integer of the index. The rediscli prompt will indicate when you are using a nonzero index: $ rediscli h redis.uvarc.io n 17 redis.uvarc.io:6379[17] Advanced Operations Data Types & Structures Values are not constrained Lists Sets Incremental Counters Command Repetition Random Keys Data Types & Structures In addition to strings and integers, Redis supports the following data types and data manipulations: Lists Sets Hashes Increments Command repetition Random Keys Sorted sets Secondary indexes Scripts Values are not constrained Remember that the ""value"" half of a key/value pair does not have to contain only a single value. It can essentially be populated with multiple, separated values, so long as you can anticipate the order, and identity of those values. In this way a key/value is akin to a ""row"" of a commaseparated data file. To implement this functionality,"
rc-website-fork/content/userinfo/howtos/general/redis.md,"you have two options: Use a hash. Hashes in Redis store multiple objects within the same key, i.e. sets of key/value pairs within a single key/value pair. Hashes are named, then subkeys and their values are defined: redis.uvarc.io:6379 hset hashkey subkey1 value1 subkey2 value2 OK Then fetch all values: redis.uvarc.io:6379 hgetall hashkey 1) ""subkey1"" 2) ""value1"" 3) ""subkey2"" 4) ""value2"" Fetch a specific field: redis.uvarc.io:6379 hget hashkey subkey2 ""value2"" Store your payload as JSON. Redis will store your JSON data as one long string, which you can then parse: rediscli h redis.uvarc.io raw redis.uvarc.io:6379 set jsonkey '{""eventType"": ""purchase"", ""amount"": 5, ""itemid"": ""XXX""}' OK redis.uvarc.io:6379 keys ""jsonkey"" redis.uvarc.io:6379[5] get jsonkey {""eventType"": ""purchase"", ""amount"": 5, ""itemid"": ""XXX""} Note the use of the raw flag when invoking therediscli` tool. This ensures that response data is decoded back to UTF8 instead of bytes. Lists Create a list by pushing a value into it: redis.uvarc.io:6379 LPUSH dbs redis (integer) 1 redis.uvarc.io:6379 LPUSH dbs mongodb (integer) 2 redis.uvarc.io:6379 LPUSH dbs mysql (integer) 3 redis.uvarc.io:6379 LPUSH dbs mysql (integer) 4 Pushing a new value into a list gives the new value the 0 position of the list. (To add new values to the end of the list use the RPUSH command.) List values can be duplicated within the list. Get a list range back by defining the min and max indices you want: redis.uvarc.io:6379 LRANGE dbs 0 10 1) ""mysql"" 2) ""mysql"" 3) ""mongodb"" 4) ""redis"" redis.uvarc.io:6379 LRANGE dbs 0 1 1) ""mysql"" 2) ""mysql"" You can also LPOP, LPUSH, and LTRIM as well as RPOP, RPUSH, and RTRIM with Redis lists. Sets You can populate a set within a single key. Set members already present cannot be duplicated within the set: redis.uvarc.io:6379 sadd set1 bananas (integer) 1 redis.uvarc.io:6379 sadd set1 apples (integer) 1 redis.uvarc.io:6379 sadd set1"
rc-website-fork/content/userinfo/howtos/general/redis.md,"grapes (integer) 1 redis.uvarc.io:6379 sadd set1 bananas (integer) 0 Then retrieve the set members: redis.uvarc.io:6379 smembers set1 1) ""grapes"" 2) ""apples"" 3) ""bananas"" Incremental Counters Use Redis as a counter or tracker: redis.uvarc.io:6379 set counter 1 OK redis.uvarc.io:6379 incr counter (integer) 2 redis.uvarc.io:6379 incr counter (integer) 3 Increment by integers other than 1: redis.uvarc.io:6379 get counter 1 redis.uvarc.io:6379 incrby counter 3 (integer) 4 redis.uvarc.io:6379 incrby counter 6 (integer) 10 Command Repetition If you need the same command to be repeated N times, simply preface your command with that integer: redis.uvarc.io:6379 set counter 1 OK redis.uvarc.io:6379 5 incr counter (integer) 2 (integer) 3 (integer) 4 (integer) 5 (integer) 6 Random Keys Using a database populated with keys and values, some workflows could make use of this as a queue for jobs or batches to be processed when order does not matter. Your process can fetch a random key: redis.uvarc.io:6379 randomkey ""herman"" redis.uvarc.io:6379 get herman ""melville"" Working with redis in Code Redis has many available SDKs for most modern languages. Every operation available via the cli is available in those SDKs. Some popular choices: Python C++ R MATLAB Perl Go Others Use redis in Your Research We are frequently asked by researchers how to incorporate databases into their work. Here are four suggestions for how Redis might help your research:: Queue Have a list of datafiles or batches that need processing? Redis supports queues in two ways: Load a Redis index with identifiers and let jobs retrieve single values at a time. Each job, when complete, removes that key from the table, working its way until the queue is empty. For less demanding processes, write your HPC job to loop through values in a Redis index to fetch identifiers and process them in series as part of one SLURM job. Use Redis"
rc-website-fork/content/userinfo/howtos/general/redis.md,"as a simple Pub/Sub message broker. This model decouples message producers from message receivers, and allows for multiple of each. Cache Store interim results or data for use in later computation. This is a faster and more scalable replacement for temporary text files. Dictionary Use an extended key/value store as an inmemory lookup resource for reference values. Where you may have previously stored reference values in a text file or relational DB table, Redis would likely outperform that pattern. Transactions with Redis are also atomic, which means multiple keys can be set, retrieved, or modified at the same time without risking data concurrency. Other Resources Redis Documentation Try Redis Online Redis Cheatsheet"
rc-website-fork/content/userinfo/howtos/general/sshkeys.md,"{{% callout %}} Users can authenticate their SSH sessions using either a password or an ssh key. The instructions below describe how to create a key and use it for passwordless authentication to your Linux instances. {{% /callout %}} About SSH Keys SSH keys are a pair of encrypted files that are meant to go together. One half of the pair is called the “private” key, and the other half is the “public” key. When users use the private key to connect to a server that is configured with the public key, the match can be verified and the user is signed in. Or, put it more simply, when data is encrypted using one half of the key, it can be decrypted using the other half. The most important thing to remember about SSH key pairs is to NEVER share or distribute the private half. That should remain safely and securely with you. Anyone with possession of that key can potentially sign in to other systems as you. Public keys, by contrast, can be shared widely. Create an SSH keypair From a terminal or command prompt (Linux and macOS) issue this command: sshkeygen If you receive an error, you may need to install the openssl package. This command will prompt you for a name and location of the key pair. By default, the key is usually named idrsa and is placed within a hidden .ssh folder within your personal directory. When creating the key, you will be asked if you would also like to secure it with a password. This is optional, but should be used in high security environments. After key generation you will find two new files in your .ssh directory: idrsa idrsa.pub The .pub file is your public key. Note that the private key has restricted permissions,"
rc-website-fork/content/userinfo/howtos/general/sshkeys.md,"rw (600). Authenticate SSH using keys To use your SSH keypair for authentication, you need to do two things: Copy the public key to your destination server First, cat out your public key, and copy it to your clipboard. Then SSH into your destination server using a password as normal. Within the .ssh/ folder on the remote server, you should find a file named authorizedkeys. (If you do not, create one.) And then paste your public key into that file. Be sure the key is entirely on only one line. Then log out. Use key authentication for SSH connections Second, when you invoke the ssh client from your local workstation, use the i flag to specify your identity file (i.e. ssh key). So while a normal SSH connection looks like this (prompting you for a password): ssh foo9b@login.hpc.virginia.edu You should now instead use something like this (that requires no password): ssh i ~/path/to/file foo9b@login.hpc.virginia.edu You can add an alias in as a new line in your .bashrc file for easy logins, for example: rivanna='ssh i ~/path/to/file foo9b@login.hpc.virginia.edu' Key Expiration One risk of SSH keys is that they have no expiration date or specific lifespan. Be sure to rotate out older keys on a regular basis. We suggest swapping out keys every 90180 days. Windows Users GitBash Download and install GitBash 1, which allows you to run Linuxstyle commands such as ssh (for secure shell connections) and sshkeygen to generate keypairs. SSH in Chrome Browser Run an SSH client within the Chrome browser. Install the extension here 2 and launch. Works with Linuxcompatible SSH keys, and profiles can be saved. PuTTY Download the full PuTTy package and generate an RSA keypair at least 2048 bits in size using the PuTTYGen. The files it creates are in a unique format but work"
rc-website-fork/content/userinfo/howtos/general/sshkeys.md,perfectly well when used with the PuTTY SSH client.
rc-website-fork/content/userinfo/howtos/general/mysql.md,"« Return to Databases {{}} {{% callout %}} MySQL is an opensource relational database management system. It supports standard SQL syntax and models. Some important concepts are: Tables Rows Keys Schemas Data types Selects Joins Indexes Other CRUD operations {{% /callout %}} Getting Started After submitting a request for a MySQL database, a username and password be created for you, this information along with your endpoint name will be sent to you via our ticketing system. Store this information somewhere secure, and do not share this information with others. User: <yourdbusername Pass: <yourdbpassword Host: <mysqlsharedendpointname Port: 3306 The MySQL service is backed by a pair of servers in HA replication mode. One serves as the primary for READS and WRITES, and the readreplica can be used for READ queries only. These endpoints are available only within the HPC networks and cannot be accessed from elsewhere in the UVA WAN. You cannot use MySQL tools remotely (from University offices, labs, or home offices over VPN). To use MySQL from the commandline, use the mysqlclient module on the HPC system: $ module load mysqlclient Or use the appropriate library for the language you are coding in to establish a connected client. You can now create a connection to the server. Use port 3306 (the default port): $ mysql h <mysqlendpointname u <yourusername p Password:"
rc-website-fork/content/userinfo/howtos/general/docker-basics.md,"{{% callout %}} Note that Docker requires sudo privilege and therefore it is not supported on the HPC system. To use a Docker image you will need to convert it into Apptainer. More information can be found here on our website. {{% /callout %}} What Is Docker? ""Docker is a set of platformasaservice (PaaS) products that use OSlevel virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through welldefined channels. All containers are run by a single operatingsystem kernel and are thus more lightweight than virtual machines. The service has both free and premium tiers. The software that hosts..."" Continue reading on Wikipedia Click to watch on YouTube: Install Docker Docker is available for Windows, Mac, and Linux. Download the appropriate Docker Edition for your platform directly from Docker. We suggest the CE “Community Edition.” Finding Containers There are thousands of prebuilt containers already available for common use cases. If you need a web server, a database instance, or portions of a genomics pipeline, there is probably a container ready for you to use. Here are some good places to search for container images or docker files. Docker Hub BioContainer GitHub Running Containers If you have found a container you would like to try, download it (using the nginx web server as an example): docker pull nginx View a list of all container images you have pulled: Run a container image: docker run d nginx This runs the container as a daemon (service). But you may want to expose the container to a specific port locally, so that you can interact with it. For example, if you wanted to expose nginx locally over port 80, enter this: docker run d"
rc-website-fork/content/userinfo/howtos/general/docker-basics.md,"p 8080:80 nginx The p 8080:80 flag publishes your local computer’s port 8080 with the container’s port 80. Another useful flag for runtime is a volume mapping, so that your running container can read or write to portions of your local computer’s filesystem. So, extending the earlier command: docker run d p 8080:80 v /User/local/dir:/var/www/html nginx View all running containers: You can also run containers interactively (i.e. logging in) instead of running as a service. This allows you to explore the structure, features, or configuration of a container, or modify how it works: docker run it nginx /bin/bash This runs the container interactively (i) in a pseudoTTY (t), and instantiates a shell for your session to use. Once you are done, simply exit the shell and you will leave the container and return to your local computer’s shell. If you have made any changes to the container, be sure to save it using docker commit (see here for more info). https://asciinema.org/a/108394 Creating Containers If you cannot find just the right container, you can always build your own. There are two ways to do this: Pull Images and Customize Download a container image, run it and log into it, and customize as if it were your own custom virtual machine. Then, save the container for later deployment. Instructions for interactively logging into a container can be found above. Pull a base container you want to start with, such as Ubuntu, CentOS, Amazon Linux, Yocto, etc. Run the container interactively so that you can install packages and code, and customize the image from within. Finally, when you exit the container and stop it, save it using the docker commit command. At this point your updated container is versioned (much like a git repository) and can be pushed to Docker Hub if you"
rc-website-fork/content/userinfo/howtos/general/docker-basics.md,"want to share or store it. Write your own Dockerfile Alternatively, you can write a custom Dockerfile and build the container from scratch, using docker build. More on Docker files and builds can be found at https://docs.docker.com/reference/dockerfile/. This allows Dockerfiles to be shared as snippets of code rather than as full container images, comparable to a bootstrapping script you might use when instantiating a virtual server instance. Step 1 Create a text file called Dockerfile with contents such as: Step 2 Then build your container based on your Dockerfile: docker build t mycontainer . Tutorials Play with Docker Classroom Handson labs Docker for Beginners Covers the basics of container management, execution, modification, etc. Docker Training Docker documents this process in great detail, and provides a stepbystep overview of their container system. Next Steps Learn about Docker Swarms for deploying containers in high availability. Design Docker Stacks for complex solutions of services. Learn how to convert Docker images into Apptainer to run on the HPC system."
rc-website-fork/content/userinfo/howtos/general/_index.md,User Guides Docker Basics Authentication with SSH Keys
rc-website-fork/content/userinfo/howtos/rivanna/bioinfo-on-rivanna.md,"{{% lead %}} The UVA research community has access to numerous bioinformatics software installed directly or available through the bioconda Python modules. Click here for a comprehensive list of currentlyinstalled bioinformatics software. {{% /lead %}} Popular Bioinformatics Software Below are some popular tools and useful links for their documentation and usage: Tool Version Description Useful Links BEDTools 2.26.0 BEDTools utilities allow one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widelyused genomic file formats such as BAM, BED, GFF/GTF, VCF. Homepage Tutorial BLAST+ 2.7.1 BLAST+ is a suite of commandline tools that offers applications for BLAST search, BLAST database creation/examination, and sequence filtering. Web BLAST Manual BWA 0.7.17 BWA is a software package for mapping lowdivergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWAbacktrack, BWASW and BWAMEM Homepage Manual Bowtie2 2.2.9 Bowtie2 is a memoryefficient tool for aligning short sequences to long reference genomes. Homepage Manual FastQC 0.11.5 FastQC is a Java application that generates a comprehensive quality control report for raw sequencing data. Homepage Documentation GATK 4.0.0.0 The Genome Analysis Toolkit provide tools for variant discovery. In addition to SNP and INDEL identification in germline DNA and RNAseq data, GATK tools include somatic short variant calling, as well as tackle copy number and structural variation. User Guide Picard 2.1.1 Picard is a set of command line tools for manipulating highthroughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF. Homepage Documentation SAMTools 1.7 SAMTools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a perposition format. Homepage Manual SPAdes 3.10.1 SPAdes provide pipelines for assembling genomes from Illumina and IonTorrent reads, as well as hybrid assemblies using PacBio, Oxford Nanopore and Sanger reads. It supports"
rc-website-fork/content/userinfo/howtos/rivanna/bioinfo-on-rivanna.md,"pairedend reads, matepairs and unpaired reads. Homepage Manual STAR 2.5.3a Spliced Transcripts Alignment to a Reference (STAR) is a RNAseq aligner based on an algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. Homepage vsearch 2.7.1 VSEARCH (stands for Vectorized Search) is a toolkit for nucleotide sequence analyses, including database search and clustering algorithms. It supports clustering, chimera detection, database searching, merging of pairedend reads, and other sequence manipulation tools. Homepage Bioinformatics Modules To get an uptodate list of the installed bioinformatics applications, log on to UVA HPC and run the following command in a terminal window: module keyword bio If you know which package you wish to use, you can look for it with module spider <software For example, module spider bcftools This returns Available versions may change, but the format should be the same. To obtain more information about a specific module version, including a list of any prerequisite modules that must be loaded first, run the module spider command with the version specified; for example: module spider bcftools/1.3.1 Using a Specific Software Module To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module. After loading a module, you are ready to run the application(s) provided by the module. For example: module load bcftools/1.3.1 bcftools version Output: bcftools 1.3.1 Using htslib 1.3.1 Copyright (C) 2016 Genome Research Ltd. License GPLv3+: GNU GPL version 3 or later This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. You will need to"
rc-website-fork/content/userinfo/howtos/rivanna/bioinfo-on-rivanna.md,"include the appropriate module load commands into your Slurm script. General Considerations for Slurm Jobs Most bioinformatics software packages are designed to run on a single compute node with varying support for multithreading and utilization of multiple cpu cores. Many can run on only one core. In that case, please request only a single task. Some software is multithreaded. Usually it communicates the number of threads requested through a commandline option. In this case the Slurm job scripts should contain the following two SBATCH directives: ` SBATCH N 1 request single node SBATCH cpuspertask= request multiple cpu cores Replace` with the actual number of cpu cores to be requested. Requesting more than 8 cpu cores does not provide any significant performance gain for many bioinformatics packages. This is a limitation due to code design rather than a UVA HPC constraint. Please be certain that the number of cores you request matches the number you communicate to the software. To be certain, you can often use the environment variable SLURMCPUSPERTASK. For example, biofoo n ${SLURMCPUSPERTASK} You should only deviate from this general resource request format if you are absolutely certain that the software package supports execution on more than one compute node. Reference Genomes on the HPC system {referencegenomesonhpcsystem} Research Computing provides a set of readytouse reference sequences and annotations for commonly analyzed organisms in a convenient, accessible location on Rivanna: /project/genomes/ The majority of files have been downloaded from Illumina's genomes repository (iGenomes), which contain assembly builds and corresponding annotations from Ensembl, NCBI and UCSC. Each genome directory contain index files of the whole genome for use with aligners like BWA and Bowtie2. In addition, STAR2 index files have been generated for each of Homo Sapiens (human) and Mus musculus (mouse) genomic builds. Click the radio button for the genome"
rc-website-fork/content/userinfo/howtos/rivanna/bioinfo-on-rivanna.md,"of your choice, then click the clipboard icon to copy it. On Rivanna please use the right click method to paste. {{% referencegenomes %}}"
rc-website-fork/content/userinfo/howtos/rivanna/docker-images-on-rivanna.md,"Docker requires sudo privilege and therefore it is not supported on the HPC system. To use a Docker image you will need to convert it into Apptainer. Convert a Docker image There are several ways to convert a Docker image: Download a remote image from Docker Hub Build from a local image cached in Docker daemon Build from a definition file (advanced) Instructions are provided in each of the following sections. Docker Hub Docker images hosted on Docker Hub can be downloaded and converted in one step via the apptainer pull command: module load apptainer apptainer pull docker://account/image Use the exact same command as you would for docker pull. Docker daemon (Taken from Apptainer 3.5 User Guide) You can convert local Docker images into Apptainer (e.g. on your personal computer). Suppose you have the godlovedc/lolcow:latest image cached by Docker: $ sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE godlovedc/lolcow latest 577c1fe8e6d8 16 months ago 241MB You can build an Apptainer image from it via: $ apptainer build lolcowfromdockercache.sif dockerdaemon://godlovedc/lolcow:latest INFO: Starting build... Getting image source signatures Copying blob sha256:a2022691bf950a72f9d2d84d557183cb9eee07c065a76485f1695784855c5193 119.83 MiB / 119.83 MiB [==================================================] 6s Copying blob sha256:ae620432889d2553535199dbdd8ba5a264ce85fcdcd5a430974d81fc27c02b45 15.50 KiB / 15.50 KiB [====================================================] 0s Copying blob sha256:c561538251751e3685c7c6e7479d488745455ad7f84e842019dcb452c7b6fecc 14.50 KiB / 14.50 KiB [====================================================] 0s Copying blob sha256:f96e6b25195f1b36ad02598b5d4381e41997c93ce6170cab1b81d9c68c514db0 5.50 KiB / 5.50 KiB [======================================================] 0s Copying blob sha256:7f7a065d245a6501a782bf674f4d7e9d0a62fa6bd212edbf1f17bad0d5cd0bfc 3.00 KiB / 3.00 KiB [======================================================] 0s Copying blob sha256:70ca7d49f8e9c44705431e3dade0636a2156300ae646ff4f09c904c138728839 116.56 MiB / 116.56 MiB [==================================================] 6s Copying config sha256:73d5b1025fbfa138f2cacf45bbf3f61f7de891559fa25b28ab365c7d9c3cbd82 3.33 KiB / 3.33 KiB [======================================================] 0s Writing manifest to image destination Storing signatures INFO: Creating SIF file... INFO: Build complete: lolcowfromdockercache.sif Note that this requires sudo privilege. You can then transfer the image to the HPC system. Definition file If you are building from a definition file, you can bootstrap from a Docker base container. Bootstrap: docker From: account/image:version"
rc-website-fork/content/userinfo/howtos/rivanna/docker-images-on-rivanna.md,"... Use a Docker image After you have obtained the converted .sif Apptainer image, you can use it by: module load apptainer apptainer run|exec|shell image.sif Please visit this page for more information on run, shell, and exec."
rc-website-fork/content/userinfo/howtos/rivanna/load-module-in-jupyter.md,"Users cannot load modules inside a JupyterLab session. If you need access to modules, please request a desktop session instead of JupyterLab. Fill out the form as you normally would for JupyterLab. After you get to a desktop, open a terminal (next to Firefox in the top bar) and type these commands: module load jupyterlab module load ... your modules here jupyterlab This should start up Firefox shortly. If you accidentally close the window, rightclick on the link in the terminal and choose ""open link"" to restart. An example of using LaTeX inside a JupyterLab session is shown in the screenshot below. Note: While you can load modules in the native terminal window within JupyterLab, it only applies to the terminal tab and has no effect on the notebook tab."
rc-website-fork/content/userinfo/howtos/rivanna/make.md,"Overview Make is a program used primarily on Unix systems to manage compiling and linking (building) programs written in C, C++, Fortran, or other compiled languages. Make operates on targets using rules to create those targets. It has a set of builtin rules but users may write their own or override the default rules. Make scans the dependencies of each target looking for files newer than the target. If it finds them, it recreates the target. Targets may and usually do depend on other targets; make will work its way through the chain to rebuild the final target, which is typically an executable. To utilize the program, the user types make Make looks first for a file called makefile. If it does not find that it will look for Makefile. If neither is present, it will attempt to use its default rules, which is seldom successful. Users can name their file other than makefile or Makefile but then must invoke make with the f option make f filename The name Makefile (capital M) is most frequently used on Unix because it will stand out in a directory where other files are entirely or mostly lower case. Make is often used in conjunction with autoconf. Autoconf uses a script called configure to generate a Makefile. In most cases, the configure script will be provided by the developer of a particular program. The configure script usually takes several optional arguments, including an option prefix=/path/to/installation which will allow users to install to a location other than the default, which is usually /usr/bin and is not writeable by ordinary users. Another popular build system is cmake. Cmake is more similar to autoconf than to make, since on Unix it creates a Makefile which must then be executed. Basics of Make The Makefile must follow"
rc-website-fork/content/userinfo/howtos/rivanna/make.md,"a rigid format. The target must start in the first column of a line and must be terminated with a colon (:). Any dependencies, i.e. files required to create this target, must follow the colon as a spaceseparated list on a single line. The rules required to create the target from the dependencies must follow on separate lines and each rule line must begin with a tab character. Example: myexec: file1.o file2.o <tabg++ file1.o file2.o file1.o: file1.cxx <tab g++ c file1.cxx file2.o: file2.cxx <tab g++ c file2.cxx Because some patterns occur repeatedly, make supports suffix rules, which describe how to create targets from certain files. For example, a suffix rule to compile any Fortran file ending in .f90 would be written ` .SUFFIXES .f90: .f90.o: gfortran c $< The$<` special variable stands for the current target. Make supports variables. Normally collected at the top of the Makefile, these are conventionally written in all capitals. F90=gfortran Our suffix rule could then be expressed as .f90.o: <tab $(F90) c $< Variables make it easy to Make on the HPC system Users who write their own code and need to generate a Makefile can start with the makemake script. It is local to the HPC system and should automatically be in the path so it is sufficient to type makemake This will create a skeleton Makefile. The user must at minimum assign a value to the PROG variable PROG=myexec Usually it will also be necessary to change the compiler names to that actually used, especially for Fortran programs. The version of makemake installed on the HPC system attempts to create a Makefile valid for C, C++, and Fortran programs. Any lines in the Makefile not pertinent to the user's application (such as C++ for a Fortran program or vice versa) may be deleted."
rc-website-fork/content/userinfo/howtos/rivanna/make.md,"It is important to note that makemake is not intelligent. It simply collects all files it finds in a directory that end in the suffices .f, .f90, .c, .cxx, and a few others. Any of those files that are not compilable, for example because they are included into another source file, must be removed from the SRCS and OBJS lists. The makemake script also creates a special target, called a dummy, clean. Typing make clean removes the executable and all object (.o) files, as well as any .mod files for Fortran. Users should make clean every time compiler options are changed. Special note for Fortran Programs Modern Fortran programs typically use modules. Make is not very good at determining correct dependency chains with modules and may not rebuild when modules are changed. If this happens it will be necessary to make clean when module files are altered."
rc-website-fork/content/userinfo/howtos/rivanna/mpi-howto.md,"Building an MPI Code All implementations provide wrappers around the underlying compilers that simplify compilation. As it is very important to use the headers that correspond to a given library, users are urged to make use of the wrappers whenever possible. For OpenMPI and MVAPICH2 these are: mpicc (C) mpicxx (C++) mpif90 (Fortran free or fixed format) For Intel MPI these use gcc/g++/gfortran by default, which is generally not recommended; to use the Intel compilers the corresponding wrappers are: mpiicc mpiicpc mpiifort Note: At this time, we recommend MPI users build with Intel 18.0 and IntelMPI 18.0 nohighlight module load intel/18.0 module load intelmpi/18.0 Most MPI programs are distributed for Linux with a Makefile or a means to create a Makefile, such as configure or cmake. If the Makefile can be edited directly, it usually contains variables such as CC, CXX, FC or F90, or similar that are set to the compiler to be used. It is only necessary to use the appropriate wrapper as the compiler. For configure or cmake, it may be necessary to export environment variables, e.g. export CC=mpicc before running the command. Users should refer to the installation documentation for the code they wish to build. The same wrappers should also be used to link the program, since they automatically link the correct MPI library for the chosen compiler and implementation. When using the wrappers as the linker, any Makefile variables such as MPILIB should be left blank. Users who have difficulty building an MPI code not already present on the system can contact RC for assistance. Running MPI Codes MPI programs are invoked under the control of an executor; without invoking an executor only a single process will be instantiated, so it is equivalent to running a serial executable. Running on Uncontrolled Systems On a"
rc-website-fork/content/userinfo/howtos/rivanna/mpi-howto.md,"system not controlled by Slurm, the executors are usually mpirun or mpiexec (these are typically equivalent). They take a number of options, including the number of processes and a file containing a list of the hostnames on which to run them. Users are permitted to run short (approximately 10 minutes or less), small (48 processes) testing/debugging runs on the UVA HPC frontends. Multinode frontend runs are not permitted. It is not necessary to specify a hostfile if all processes are run on the same system. To run a test job on a frontend with four processes, first load the appropriate modules and then type mpirun np 4 ./mycode On the frontends the processes will not be assigned to specific cores and may be competing with other processes, so performance may be poor. To use a debugger with an MPI program, compile with the g flag as for a serial code. We provide the Totalview graphical debugger for MPI and OpenMP applications. Totalview requires that the mpiexec executor be in your path before you invoke it. If you need to debug for a longer time, with a large number of cores, or with multiple nodes, you can use Totalview through the Open OnDemand Desktop. Please request all cores for the node whether you use them or not, because Totalview cannot use the mpirun command as the executor. Running Under Slurm When running with Slurm, the mpirun command must be used as the executor. Load the appropriate modules in your script, then invoke mpirun ./mycode Do not specify the number of processes or the list of hosts since mpirun will obtain that information from your request to Slurm and will distribute your processes on the nodes and cores to which your job was assigned. This example is a Slurm job command file"
rc-website-fork/content/userinfo/howtos/rivanna/mpi-howto.md,"to run a parallel (MPI) job using the OpenMPI implementation: ` !/bin/bash SBATCH nodes=2 SBATCH ntaskspernode=16 SBATCH time=12:00:00 SBATCH output=outputfilename SBATCH partition=parallel SBATCH A mygroup module load gcc module load openmpi mpirun ./parallelexecutable In this example, the Slurm job file is requesting two nodes with sixteen tasks per node for a total of 32 processes. Both OpenMPI and IntelMPI are able to obtain the number of processes and the host list from Slurm, so these are not specified. In general, MPI jobs should use all of a node so we'd recommendntaskspernode=20` on the parallel partition, but some codes cannot be distributed in that manner so we are showing a more general example here. Please see our MPI software page for examples of Slurm scripts for more complex situations, including running hybrid MPI/OpenMP codes. For more detailed instructions on building and running compiled codes on the HPC system, please see our online tutorial."
rc-website-fork/content/userinfo/howtos/rivanna/mpi-tutorial.md,"Tutorials and books on MPI A helpful online tutorial is available from the Lawrence Livermore National Laboratory. The following books can be found in UVA libraries: Parallel Programming with MPI by Peter Pacheco. Using MPI : Portable Parallel Programming With the MessagePassing Interface by William Gropp, Ewing Lusk, and Anthony Skjellum. Using MPI2: Advanced Features of the MessagePassing Interface by William Gropp, Ewing Lusk, and Rajeev Thakur. MPI: The Complete Reference : The MPI Core by Marc Snir, Steve Otto, Steven HussLederman, David Walker, and Jack Dongarra. MPI: The Complete Reference : The MPI2 Extensions by William Gropp, Steven HussLederman, Andrew Lumsdaine, Ewing Lusk, Bill Nitzberg, and Marc Snir. A free HTML version of the first edition of the Complete Reference is available at Netlib. Example Code The MPI interface consists of nearly two hundred functions but in general most codes use only a small subset of the functions. Below are a few small example MPI programs to illustrate how MPI can be used. The first code prints a simple message to the standard output: program MPIoutput use mpi implicit none integer :: ierr character(len=100):: filename character(len=10) :: digitstring integer :: p, myrank call MPIInit(ierr) ! Get myrank call MPICommrank(MPICOMMWORLD, myrank, ierr) !Get the number of Processes call MPICommsize(MPICOMMWORLD, p, ierr) call seedrandomnumbers( myrank) ! Convert myrank to a charactoer string write(digitstring,'(i4.4)') myrank ! Concat ""file"" and the string for myrank filename='file.'//digitstring(1:lentrim(digitstring) ! !Open a file for each process for I/O open(unit=7,file=filename,status='unknown') write(7,)'Hello World from ',myrank call MONTECARLO() !Do actual work of the code close(7) call MPIFinalize(ierr) end program MPIoutput `"
rc-website-fork/content/userinfo/howtos/rivanna/launch-rserver.md,"Rocker provides many software containers for R. Due to the default permission settings of our file system, launching an RStudio Server session is not straightforward. If you are interested in using their containers on the HPC system, please follow these steps. Pull container Use Apptainer to pull the container. We will use geospatial in this example. bash module load apptainer apptainer pull docker://rocker/geospatial You should see geospatiallatest.sif in your current directory. Onetime setup The commands in this section are to be executed as a onetime setup on the frontend. You may need to repeat the steps here when running a new rocker container. Create a directory where you have write permissions, e.g. under $HOME: bash TMPDIR=~/rstudiotmp your choice mkdir p $TMPDIR/tmp/rstudioserver uuidgen $TMPDIR/tmp/rstudioserver/securecookiekey chmod 600 $TMPDIR/tmp/rstudioserver/securecookiekey mkdir p $TMPDIR/var/{lib,run} These directories will be bindmounted at runtime when you launch the container. Launch script You must be consistent with the bindmount paths that you set up in the previous section. We recommend putting the following commands in a script (e.g. runrserver.sh) so that you will not need to type every time you launch RStudio Server. Change the script into an executable: {{< codesnippet }} chmod +x runrserver.sh {{< /codesnippet }} Launch We recommend launching this in a FastX Web (MATE) session for short runs or debugging on the frontend. For production runs you can request a Desktop interactive app. Both FastX and the Desktop can be accessed at our Open OnDemand portal. Once in either FastX or a remote Desktop, start a terminal window. To launch RStudio Server, execute: {{< codesnippet }} ./runrserver.sh {{< /codesnippet }} Nothing will happen in the terminal, which is normal. Open a browser (Firefox is available through the MATE desktop menu) and go to localhost:8787. Your server should be running there."
rc-website-fork/content/userinfo/howtos/rivanna/image-processing.md,"Available Software To get an uptodate list of the installed image processing and visualization tools, log on to UVA HPC and run the following command in a terminal window: module keyword vis To get more information about a specific module version, run the module spider command, for example: module spider blender/2.78c List of Image Processing and Visualization Software Modules {{< rivannasoftware moduleclasses=""vis"" }} Running Interactive Visualizations Many of the provided image processing and visualization applications provide a graphical user interface (GUI). In order to use a GUI on the HPC system, users must log in through a client capable of displaying X11 graphics. We recommend FastX Web which provides a GPU to accelerate rendering. To start an applications GUI in an X11enabled terminal, first load the software module and then run the GUI application executable, e.g. module load blender When connected to UVA HPC via FastX Web, rendering of the graphical user interface can be accelerated by executing this command: module load blender vglrun c proxy blender & The ampersand & returns the terminal to input mode while the application is running."
rc-website-fork/content/userinfo/howtos/rivanna/custom-jupyter-kernels.md,"You can create custom kernels from a conda environment or an Apptainer container. In both cases you'll need to install the ipykernel package. Jupyter kernel based on a conda environment To create a custom kernel of the conda environment myenv that uses Python 3.7: module load miniforge conda create n myenv python=3.7 ipykernel <otherpackages source activate myenv python m ipykernel install user name myenv displayname ""My Env"" Note: You can customize the display name for your kernel. It is shown when you hover over a tile in JupyterLab. If you do not specify a display name, the default Python [conda env:<ENVNAME] will be shown. A custom kernel cannot be created from the terimnal within an interactive JupyterLab session. This will create the kernel in an incorrect folder and the new tile will not be visible. Perform the above commands either in a FastX terminal, SSH connection, or using HPC Shell Access within Open OnDemand. For more information on Miniforge, please visit here. Jupyter kernel based on Apptainer container For this to work, the ipykernel Python package must be installed within the Apptainer container. To create a Jupyter kernel for the container, you can either use our automated script jkrollout or do it manually. Automated script Replace /path/to/sif with the actual image name or path: jkrollout /path/to/sif ""My kernel"" If GPU is supported: jkrollout /path/to/sif ""My kernel"" gpu Manual Custom kernels are stored under ~/.local/share/jupyter/kernels. If this directory does not already exist, run mkdir p ~/.local/share/jupyter/kernels Next, cd into it and create a directory for your specific kernel, e.g. mykernel: mkdir mykernel Create two files in that directory, kernel.json and init.sh. (The former must be exactly kernel.json. The latter can be customized as long as you are consistent.) kernel.json: { ""argv"": [ ""/home/yourid/.local/share/jupyter/kernels/mykernel/init.sh"", ""f"", ""{connectionfile}"" ], ""displayname"": ""My kernel"", ""language"": ""python"""
rc-website-fork/content/userinfo/howtos/rivanna/custom-jupyter-kernels.md,"} (Remember to replace yourid with your user ID.) init.sh: (Remember to use the actual path of your Apptainer image.) If the container has GPU support, add a nv flag in the last line: apptainer exec nv /path/to/apptainer/image python m ipykernel $@ Change init.sh into an executable: chmod +x init.sh You will see your custom kernel ""My kernel"" next time you use JupyterLab."
rc-website-fork/content/userinfo/howtos/rivanna/wdl-bioinformatics.md,"{{% callout %}} WDL (pronounced widdle) is a workflow description language to define tasks and workflows. WDL aims to describe tasks with abstract commands that have inputs, and once defined, allows you to wire them together to form complex workflows. Learn More {{% /callout %}} {{% callout %}} CROMWELL is the execution engine (written in Java) that supports running WDL scripts on three types of platforms: local machine (e.g. your laptop), a local cluster/compute farm accessed via a job scheduler (e.g. Slurm, GridEngine) or a cloud platform (e.g. Google Cloud or Amazon AWS). Learn More {{% /callout %}} Introduction Prerequisites: This tutorial assumes that you have an understanding of the basic structure of a WDL script. Learn here This tutorial will walk you through steps for creating a WDL script, and executing it on the HPC system. For the purpose of this document, we will write a (very) basic realworld workflow that does something useful! Our workflow: The processing with bwamem contains two tasks: Alignment of sequence files to reference genome using bwa, followed by SAM to BAM format conversion using picard. The tasks are joined together using linear chaining, with output from bwa step used as input to the picard step. Inputs: Sample pairedend FASTQ files hg38 reference fasta and BWA index files UVA HPC modules: wdltool cromwell bwa picard Set up your Working Environment Login to UVA HPC and create a root working directory in your /scratch folder: cd /scratch/$USER/ mkdir wdltutorial cd wdltutorial Get the Sample FASTQ files. Copy the sample pairedend fastq files to this folder. For this tutorial, we will use reads for NA12878, downloaded from here. You can download the dataset, or use DNAseq data for any sample of your choice. Get the reference genome files. We will use the hg38 reference fasta and"
rc-website-fork/content/userinfo/howtos/rivanna/wdl-bioinformatics.md,"BWA indexes from the genomes repo on the HPC system at /project/genomes/Homosapiens/UCSC/hg38/Sequence/BWAIndex/. All UVA HPC users have read access to these reference genomes, no need to download/copy them to the working directory! WDL script First, let's write our workflow! Open a blank text file in your favorite text editor and save it as bwaAln.wdl. Workflow Let’s begin with the workflow skeleton. Our workflow let's name it bwamem calls two tasks: align and samSort. workflow bwamem { call align { input: } call samSort { input: } } Next, we need to tell cromwell how to link the tasks together. We will tell samSort task to take the outsam from align task as its input. So let's update the workflow definition: workflow bwamem { call align { input: } call samSort { input: insam = align.outsam } } Finally, let’s add the inputs for our first task, as well as few variables each task is going to need. Our workflow definition in bwaAln.wdl is complete! Tasks 1. align This task will align the pairedend reads to hg38 build of human reference genome, using bwa mem algorithm. Here's the skeleton definition: task align { Inputs/Variables command {...} runtime {...} output {...} } Now let's define the alignment command and the variables we need to execute it: We are passing the pairedend fastq files for our sample, the reference fasta and its BWA indexes, as well as the number of threads for alignment as inputs to the task. Notice how to reference the variables in the command, using ${variablename}. Next, add runtime attributes, i.e. the number of cpus (same as num of threads passed as variable) and memory (16GB) required for the task. ... runtime { cpus: threads requestedmemorymb: 16000 } ... Finally, lets define out output: ... output { File outsam ="
rc-website-fork/content/userinfo/howtos/rivanna/wdl-bioinformatics.md,"""${samplename}.hg38bwamem.sam"" } ... Our task is complete and should look like this: 2. sortSam This task will take the output from alignment step in SAM format, convert it to BAM, sort it on coordinates and create the index using picard SortSam utility. This the task skeleton: task sortSam { Inputs/Variables command {...} runtime {...} output {...} } Add variables … task sortSam { String samplename File insam ... } Add command … ... command <<< java jar $EBROOTPICARD/picard.jar \ SortSam \ I=${insam} \ O=${samplename}.hg38bwamem.sorted.bam \ SORTORDER=coordinate \ CREATEINDEX=true ... } {{% callout %}} Note: picard is available as a module on the HPC system. When you load the module to your environment (using module load picard), it also defines the $EBROOTPICARD environment variable, which defines the full path to the jar file for calling picard utilities. {{% /callout %}} Add output … ... output { File outbam = ""${samplename}.hg38bwamem.sorted.bam"" File outbamidx = ""${samplename}.hg38bwamem.sorted.bai"" } .... For this task, we don’t need custom runtime attributes, and will be using the default described in the backend configuration file for UVA HPC! The complete sortSam task definition should look like this: task sortSam { String samplename File insam command <<< java jar $EBROOTPICARD/picard.jar \ SortSam \ I=${insam} \ O=${samplename}.hg38bwamem.sorted.bam \ SORTORDER=coordinate \ CREATEINDEX=true output { File outbam = ""${samplename}.hg38bwamem.sorted.bam"" File outbamidx = ""${samplename}.hg38bwamem.sorted.bai"" } } Complete WDL script bwaAln.wdl Validate Next, we will validate our script, make sure there are no syntax errors. We will use wdltool utility toolkit that includes a syntax validation function. It is available as a module on the HPC system module load wdltool This will define a global environment variable, $WDLTOOLPATH, that stores the root directory path for the jar file. To validate our script, we simply call the validate function: java jar $WDLTOOLPATH/wdltool0.14.jar validate bwaAln.wdl This function"
rc-website-fork/content/userinfo/howtos/rivanna/wdl-bioinformatics.md,"parses the WDL script and alerts us to any syntax errors such missing curly braces, undefined variables, missing commas and so on. It will resolve imports, but note that it is not able to identify errors like typos in commands, specifying the wrong filename, or failing to provide required inputs to the programs that are run in the workflow. No messages will be thrown if the syntax is valid! Specify Inputs Now, we will create a JSON file of inputs for our workflow. We will use the inputs function in the wdltool package to create a template to populate the values java jar $WDLTOOLPATH/wdltool0.14.jar inputs bwaAln.wdl bwaAln.inputs.json This will create a file called bwaAln.inputs.json that lists all the inputs to all the tasks in your script following the pattern below: { ""<workflow name.<task name.<variable name"": ""<variable type"" } This saves you from having to compile a list of all the tasks and their variables manually! Now, populate the variables using your favorite text editor. You can verify the file's content with the cat command: cat bwaAln.inputs.json Output in terminal: {{% callout %}} Please provide FULL PATH to fastq files! There is no need to change file paths to hg38 reference fasta and BWA index files! {{% /callout %}} Execute At the moment, Cromwell is the only fullyfeatured execution engine that supports WDL. It is available as a module on the HPC system. module load cromwell Basic syntax: java jar $CROMWELLPATH/cromwell30.1.jar <action <parameters Backend In order to run each task of our workflow as a slurm job, we need to configure a SLRUM backend. Create an empty text file, cromwellrivanna.conf, and copy the contents described in this post. Slurm batch submission script Finally, we will write a simple Slurm submission script to execute our workflow. Create an empty file submitbwaAln.sh and"
rc-website-fork/content/userinfo/howtos/rivanna/wdl-bioinformatics.md,copy the below contents to it Note: This assumes that the rivannacrowell.conf backend configuration file is located in your home directory. Learn more about how to create this Cromwell configuration file here. Submit the job submit submitbwaAln.sh Monitor job progress squeue u $USER Outputs The outputs of the workflow will be written to /call/execution/ folder! Please explore the directory structure for relevant files!
rc-website-fork/content/userinfo/howtos/rivanna/convert-jupyter-pdf.md,"Users cannot load modules inside the OpenOnDemand App for JupyterLab. Therefore it is not possible to convert a Jupyter Notebook to a PDF directly inside the JupyterLab Interactive App on OpenOnDemand. There are 2 ways to convert a Jupyter Notebook to a PDF: Both methods require Jupyter to be installed within a conda environment. The following example will install Jupyter into a conda environment named 'jupyter': module load miniforge conda create n jupyter source activate jupyter mamba install jupyter y Directly from the command line. ssh from your terminal and type the following: If you want to use GUI, please request a Desktop session. Fill out the form as you normally would for JupyterLab. After you get to a desktop, open a terminal (black box next to Firefox in the top bar) and type these commands: This will pull up JupyterLab. Now you will be able to use the Save and Export Notebook As function in JupyterLab."
rc-website-fork/content/userinfo/howtos/rivanna/jupyter-to-python-script.md,"Sometimes it may be useful to convert a Jupyter notebook into a Python executable script. Once your notebook is opened in OOD you can select File Export Notebook As ... Export Notebook to Executable Script: This will download a Python executable with a '.py' extension into your local computer's Downloads folder. Your notebook may also show ""Download as"" instead of ""Export Notebook As ..."". Either of these selections will allow you to download a Python executable. This script can be copied to the HPC system in the working directory where JupyterLab was accessing the notebook. Information on transferring files to and from Rivanna can be found here. Notebooks can also be converted directly on the command line. This can be done by loading jupyterlab and running jupyter nbconvert to script /path/to/ipynb, where /path/to/ipynb is the location of the notebook file: module load jupyterlab jupyter nbconvert to script /path/to/ipynb A Slurm submission script is required to execute the Python executable. JupyterLab on OOD provides a web form that requests resources for your job to run on a compute node. Let's look at the web form from OOD: We see that the form requests a partition, the time in hours, the number of cores, the requested memory, the working directory, and your UVA HPC allocation. Once you click ""Launch"", your job is submitted and requests these specifications on a compute node. The same can be done by writing a Slurm submission script. The script below mirrors the specifications set in the web form. We'll call this script submitjup.slurm. Now that everything is ready, all that needs to be done is to submit submitjup.slurm. This is accomplished by typing sbatch submitjup.slurm on the command line in the same directory of the Slurm script and Python script. Your job will launch on a compute"
rc-website-fork/content/userinfo/howtos/rivanna/jupyter-to-python-script.md,"node and write its output to a Slurm output file. Since this script only prints standard output the Slurm output file will contain the same information as the notebook. Hello World with 3.8.8 | packaged by condaforge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0] More information on using Slurm on the HPC system can be found here. If your code requires a specific conda environment, you can specify source activate <environment name below the module commands in the Slurm script to activate the environment:"
rc-website-fork/content/userinfo/howtos/rivanna/clear-ood-session-files.md,"To clear OOD Session files, the HPC system will need to be accessed via a terminal. See documentation for information on how to access via SSH. You can find the session files and logs for all Open on Demand apps at: ~/ondemand/data/sys/dashboard/batchconnect/sys Under this directory you will see subdirectories for the Open on Demand applications that you have used before. Under each subdirectory you can find the files that are created when you launch a new session. To quickly clear all session files for OnDemand from your /home directory run: rm rf ondemand Other directories related to Open on Demand such as .jupyter and .rstudiodesktop can be removed in the same fashion: rm rf .jupyter removes JupyterLab session files rm rf .rstudiodesktop removes RStudio session files"
rc-website-fork/content/userinfo/howtos/rivanna/migrate-python.md,"Scenario You have installed Python packages locally in one version and now wish to use them in a different version. For example, you have been using Python 3.6 but it is obsolete and will be removed soon, so you need to set up those packages for Python 3.8. There are several ways to accomplish this, depending on the package manager. In this howto we will discuss pip and conda. You will need to load the module for the newer Python version. For this example, {{< codesnippet }} module load miniforge {{< /codesnippet }} Pip The Python packages are installed in a hidden location under your home directory: ~/.local/lib/pythonx.y/sitepackages where x and y are the major and minor Python versions, respectively. Preserve individual package versions To preserve the versions for all individual packages, first freeze the environment into a file, say requirements.txt: {{< codesnippet }} pip freeze path ~/.local/lib/python3.6/sitepackages requirements.txt {{< /codesnippet }} Next, install the packages: {{< codesnippet }} pip install user r requirements.txt {{< /codesnippet }} Use the latest versions or whichever are mutually compatible If you have no preference on the package versions, you can remove the version requirements: {{< codesnippet }} pip freeze path ~/.local/lib/python3.6/sitepackages | sed 's/==.$//g' requirements.txt {{< /codesnippet }} Install the packages: {{< codesnippet }} pip install user r requirements.txt {{< /codesnippet }} Conda You can create/load a conda environment that uses a different Python version with the Miniforge module. Suppose the environment name is myenv. You can either update the existing environment or create a new one. Update Python in the old environment bash source activate myenv conda install python=3.8 Note that if you have many packages in the environment, such an update could take very long due to conda's slow dependency resolution. Individual package versions are not preserved. Create the new environment"
rc-website-fork/content/userinfo/howtos/rivanna/migrate-python.md,It is better to create a new environment and let the dependency solver do its work from scratch: bash conda create n mynewenv python=3.8 <list of packages Use the syntax <package=<version if you have version requirements. Run conda list n myenv to get a list of all packages in myenv. You can use the following command to show the same list in one line without version information: {{< codesnippet }} conda list n myenv | awk '{if($1 !~ /^/) print $1}' | tr '\n' ' ' {{< /codesnippet }} Please also visit this page for more information.
rc-website-fork/content/userinfo/howtos/rivanna/compiler-howto.md,"Building your Application Creating an executable from source with a compiled language requires two steps, compiling and linking. The combination of these is generally called building. The output of the compiler is generally an object file, which on Unix will end in a .o suffix. Object files are machine code and are not humanreadable, but they are not standalone and cannot be executed. The linker, which is usually invoked through the compiler, takes all object files, along with any external libraries, and creates the executable (also called a binary). Compilers are invoked on source files with a line such as <compiler <options code.<suffix You must know the name of the compiler you wish to use as well as its options. Most compilers offer a large number of options that can control very detailed properties of the resulting executable, but the average user need only know a few of them. Please see our compiler documentation for information about the available compilers on the HPC system. For building and running parallel code, see the documentation. Debugging and Profiling To use a debugger you must compile with a special flag. On all Unix compilers this is g. Fortran programmers can add a flag to check that array accesses fall within the declared bounds. For gfortran this flag is fboundscheck, for ifort it is CB, and for pgfortran it is C or Mprof. Since arraybounds errors are the most common cause of segmentation violations in Fortran, this can be a very valuable flag, but it slows down the execution. Examples: gcc g mycode.c gfortran g fboundscheck mycode.f90 Options for profiling vary more by compiler. For Gnu compilers it is a combination flag pg. Intel uses separate options p g. The PGI compiler uses Mprof. Using debuggers and profilers is covered separately here. If you"
rc-website-fork/content/userinfo/howtos/rivanna/compiler-howto.md,"write your own code, profiling is useful to increase the performance of your code. Optimizing Once your code is working, you should remove all debugging flags and compile from source. Debugging flags inhibit optimizations and can cause your code to waste SUs. The general optimization flag for all compilers is O. With no integer it will set optimization at the default level, which varies by compiler. You can specify different levels of optimization (including none) with an integer immediately after O. The number of available levels and where the default lies varies by compiler. Gnu and Intel have three levels and the default is O1. PGI has four levels and the default is O2. The flag O0 disables all optimizations, which can be useful for debugging; the g flag may or may not imply O0. Examples: gcc O mycode.c icc O2 mycode.c gfortran O0 mycode.f90 As noted above, it is particularly important for Fortran programmers to remove the boundschecking flag for production runs, as that can slow down execution considerably. Compilers have many more options to finetune optimization levels. However, users should be clear on what they need and why, and should avoid flags like fast that may bind the executable too tightly to a specific architecture, since HPC nodes are of different ages and architectures. Renaming the Executable Unless otherwise specified, the name of your executable will be a.out. To change that, add the flag o <name. It is critical to include the name between the o and the source file, or the compiler will overwrite your source file. Examples: g++ o mycode mycode.cxx ifort o mycode mycode.f90 pgcc o mycode mycode.c Separating Compiling and Linking Unless otherwise specified, the compiler will attempt to compile all the source files name to the end of the line, then to link"
rc-website-fork/content/userinfo/howtos/rivanna/compiler-howto.md,"them into an executable. If there are no dependencies in the source files this can work. However, frequently there are dependencies or other special circumstances, so that the compilation and link steps must be performed separately. To suppress the link step use the c option. To execute the link step, the compiler must be provided with a list of all the resulting .o files that are needed to create the executable. Example: ifort c O mycode.f90 ifort c O mymod.f90 ifort o mycode mycode.o mymod.o Linking Libraries Executables for more advanced programs frequently need to link external libraries, such as numerical packages or datamanagement libraries. If the libraries are in the compiler's default search path, they are specified by adding flags after the last object file. For known paths the flag is l followed by the ""root"" name of the library, which drops the ""lib"" in front and everything including and after a period. So we would specify libblas.so as lblas. Order matters because the linker resolves references from left to right. Therefore if we also use lapack, we must have a line like gfortran o mycode mycode.o mymod.o llapack lblas If these libraries are not located in the compiler's search path, the paths for both header or module files and library files must be provided to it. Header paths must go on compile lines, whereas library paths go on the link line. In the example above, if we wanted to link a library in the user home directory, we'd require the I flag for including and the L flag for the library: gfortran c O I/home/msk3k/myblas mycode.f90 gfortran c O mymod.f90 gfortran o mycode L/home/mst3k/myblas mycode.o mymod.o lopenblas Managing the Build Typing a compiler line for each source file is tedious and errorprone. There are several systems to help"
rc-website-fork/content/userinfo/howtos/rivanna/compiler-howto.md,"manage building. The most widely used on Unix for C, C++, and Fortran is make. Make uses rules to determine how to generate a particular file, called a target, from its dependencies. It has a rather peculiar syntax so a more complete discussion can be found here. Make does not set up paths to external libraries, or specify a compiler other than the default, or enable or disable different build options. Other systems have been written to handle these initialization steps. The autoconf/configure system is widely used on Unix. Another popular system is CMake, which is crossplatform and works on Windows and Mac OSX as well as on Unix. Autoconf A code that uses autoconf will provide a script named configure which should be run before building. Since the current directory is not in the default user path, usually this is invoked with ./configure <options Options to the configure script will vary depending on the software package. To see a full list, type ./configure help The default installation prefix is usually /usr or /usr/local. UVA HPC users are not allowed to write to either of these directories, so an alternative must be provided. Normally you should use a directory in your home directory. The minimum configure command would thus be ./configure prefix=/home/yourid/your/directory/path Other options to configure generally involve adding or removing options or specifying a path to libraries that it may not be able to find on its own. Library paths are particularly important in a modulesbased system such as ours, because those directories won't always be visible to configure. The module should set an environment variable for the path that you can use. For example, if you wish to link the HDF5 library corresponding to your toolchain, you can run printenv | grep HDF In this case we might"
rc-website-fork/content/userinfo/howtos/rivanna/compiler-howto.md,"specify a configure line such as ./configure prefix=/home/mst3k/softpack withhdf=${HDF5ROOT} Configure generates a Makefile. Once configure has completed, run make If that succeeds run make install CMake CMake is an increasingly popular build system. Whereas autoconf is generally tied to Unix, CMake is crossplatform. It has several important differences from autoconf. Most importantly, it pays no attention to user paths for binaries or libraries. Each package has specific environment variables that must be set if it is to find binaries or libraries not in its internal default search path. These environment variables can be set in the shell where cmake is run, or it can be specified at the cmake command line. CMake usually expects to build packages in a separate directory that typically must be created by the user. The user must change to this directory before invoking the cmake command. This command looks for a file CMakeLists.txt, so it takes a commandline argument of the directory where that file is located. The equivalent to the prefix of autoconf is CMAKEINSTALLPREFIX. The equivalent to the configure example above for our hypothetical package would be (assuming a separate build directory) mkdir build cd build cmake DCMAKEINSTALLPREFIX=/home/mst3k/softpack .. This assumes that HDF5ROOT is already in the environment. If it is not, another option DHDF5ROOT=/path/to/hdf5 must be included. CMake often requires that you specify the compiler. It does not follow paths, so even if you load a gcc module, it will use a hardcoded path that will generally pick up the system compiler, which is often quite old. You can modify this with the flags DCMAKECCompiler DCMAKEFortranCompiler DCMAKECXXCompiler For example cmake DCMAKECCOMPILER=gcc DCMAKEINSTALLPREFIX=/home/mst3k/softpack .. Other options can be set in different ways. In order to see a list of the userchangeable options, you can run ccmake, which will bring up a simple textbased"
rc-website-fork/content/userinfo/howtos/rivanna/compiler-howto.md,"graphical user interface, where you can turn options on or off, set paths, and so forth. More information is available at Kitware's CMake website. Regardless of how CMake is run, it will generate a CMakeCache.txt file that will not be overwritten if cmake is run again; it must be removed in order to redo the configuration. As for autoconf, on Unix the output of cmake is a Makefile, so make make install is generally the recipe to build and install the program. The default cmake on the HPC system is fairly old and most users will need to load a newer cmake module. If any newer version will work, module load will suffice. Otherwise module spider cmake will show the options. For more detailed instructions on building and running compiled codes on the HPC system, please see our online tutorial."
rc-website-fork/content/userinfo/howtos/rivanna/add-packages-to-container.md,"Basic Steps Strictly speaking, you cannot add packages to an existing container since it is not editable. However, you can try to install missing packages locally. Using pythonpip as an example: module load apptainer apptainer exec <container.sif python m pip install user <package Replace <container.sif with the actual filename of the container and <package with the package name. The Python package will be installed in your home directory under .local/lib/pythonX.Y where X.Y is the Python version in the container. If the installation results in a binary, it will often be placed in .local/bin. Remember to add this to your PATH: export PATH=~/.local/bin:$PATH You should be able to use the new package/binary in the container, as your entire home directory is mounted at runtime. Handling Errors Installation may fail in more complicated scenarios, where additional libraries are needed. As a first step, load related modules (e.g. if you see ""GLIBC not found"" then load gcc) and try again. You are always welcome to reach out to us for support. To expedite the process, please let us know what you have tried and include any error messages that you encountered. Advanced Users Our Dockerfiles are hosted at https://github.com/uvarc/rivannadocker. Please feel free to customize them and build your own version. For more information about our use of Docker Hub, please visit here."
rc-website-fork/content/userinfo/howtos/rivanna/gpu-training.md,"Training Deep learning models on GPU(s) Running large scale machine learning models typically requires the user leverage GPU's to speed up computation. In this howto we overview how to use gpu's , and in particular multiple gpu's, to run large tensorflow and pytorch machine learning models more efficiently. Tensorflow Tensorflow is capable of automatically detecting if there are GPU devices available, and will run on them by default. To check what devices, including GPU's, are available to tensorflow in a given computing environment, you can run devicelist = tf.config.listphysicaldevices(devicetype=None) Then specify which device you want to run computations on, merely set the environment variable CUDAVISIBLEDEVICES to the index of the device you want to run on in the devices list. ex: Tensorflow multigpu with Tf.distribute Tensorflow GPU paralellization is principally handled through the Tf.distribute API. We overview how to modify a tensorflow script to run on multiple GPU's.The below code defines a simple NN with 1 hidden layer for classification. In order to distribute this model onto multiple GPU's it suffices to wrap our model definition in the mirroredstrategy.scope() class provided by Tf.distribute, as below: To run on selected GPU's pass a list containing those device names into the MirroredStrategy invocation, Ex: mirroredstrategy = tf.distribute.MirroredStrategy([[""GPU:0"", ""GPU:1""]]) The 'strategy' in MirroredStrategy refers to a particular way of distributing computations between devices. mirrorstrategy will be the most common used in mutligpu training, it creates a version of each model variable on each device, and assigns some subset of the data to each. All mirror versions of each variable are used collectively to update the aggregate 'full model' version of the variable. There are other such strategies available through the API, including TPUStrategy for training on TPU's MultiWorkerMirroredStrategy, which is like MirroredStrategy except every worker can be assigned to more than 1 GPU,"
rc-website-fork/content/userinfo/howtos/rivanna/gpu-training.md,"and others. tf.distribute is typically used with the model.fit training functionality from keras, but also provides supports custom training loops thought this requires more extensive code modification. For instruction on using tf.distribute with custom training loops, as well as other additional docs, see : tf.distribute docs Pytorch Unlike tensorflow, pytorch does not automatically detect GPU devices and set models run on them preferentially. In order to get pytorch models onto GPU(s) one must manually assign the model and all related tensors, to a GPU device. Example code is given below: We detect if a GPU device are available, and if so assign it to the 'device' variable Now we assign the model and all relevant tensors to the device with the .to method: Pytorch multigpu with PytorchLightning The easiest way to parallelize training of pytorch models to multiple GPU's and even multiple nodes is to use the Pytorch lighting API. This is a wrapper on top of pytorch which automates a lot of the boiler and facilitates parallelization. We explain the structure of Pytorch lighting code below. The basis for the code is to create our model as a derived class of the pl.LightningModule class we include the standard init and forward methods, as well as other methods delineated by pytorch lightning. The above training step would correspond to this training loop: The trainingstep method delineates the content of the training loop, including last minute preprocessing the forward pass of a batch of data through the model, and calculation of loss. The backward pass is taken care of automatically. The setup and preparedata methods are used in paralellization. preparedata is called once, on one device. This should be used for any computationally intensive data preprocessing or acquisition which should not be repeated on all devices/gpu's. The setup method is called"
rc-website-fork/content/userinfo/howtos/rivanna/gpu-training.md,"on every GPU. Its function is to set up that gpu's instance of the data. The dataloader methods function to load the data into batches, both for training and validation. The validation step method is the analog to trainingstep, but for the validation loop. The validationepochend method is called at the end of a validation epoch, calculates metrics such as loss returns in dict. The configureoptimizers method sets the optimizer, in this case Adam. After defining the pl.LightningModule class with the methods above, and whatever other custom or pl.LightningModule are needed, the model is trained using the pytorchlightning.Trainer class. This class allows one to automate parellelization to as many gpus and nodes as one has available. See additional docs at: pytorchlightning docs"
rc-website-fork/content/userinfo/howtos/rivanna/_index.md,Guides Building compiled code Using make Building and running MPI Code Bioinformatics on UVA HPC Clear OOD Session Files Convert Jupyter Notebook to PDF Convert Jupyter Notebook to Python Script Custom Jupyter kernels Loading Modules in Jupyter Docker images on UVA HPC Adding packages to a container Migrate Python packages Launch RStudio Server from an Apptainer container More Documentation Connecting Using SSH Using a browser Using FastX Jobs / Slurm / Queues Slurm Overview Queues Storage and File Transfer Storage overview Data transfer methods Allocations Allocations Overview
rc-website-fork/content/userinfo/howtos/storage/globus-cli.md,"Install the Globus CLI The Globus CLI is available through the pip installer: If you have administrator privileges type {{< code }} pip install globuscli {{< /code }} Otherwise use {{< code }} pip install user globuscli {{< /code }} The project is open source, so you can also download the source at https://github.com/globus/globuscli If you would like to use the CLI from UVA HPC, please follow these directions below. Authenticate using the Globus CLI Log in against your institutional authentication provider. In the case of UVA, we use NetBadge for signing in: {{< code }} globus login {{< /code }} This will open a page in your web browser where you select your institution and proceed to log in: Select “University of Virginia” and then click Continue. You are then taken to an authorization page where you agree to allow the Globus CLI to access your Globus account. Click on Allow. You may now close your browser window. After you log in, there is one more step to complete authentication. Now, use the following command: {{< code }} globus session consent ""urn:globus:auth:scope:transfer.api.globus.org:all[https://auth.globus.org/scopes/af187d15768f44498670d00e1eb1ce6a/dataaccess]"" {{< /code }} This will open the same browser that pops up when you log in. Follow the same steps to complete the authentication. Once done, you may close the browser. Your commandline tools are now authenticated and ready to use. Basic Commands Issue the base command for the tools and you will see the primary set of commands: {{< code }} globus {{< /code }} {{< code }} jmespath, jq TEXT A JMESPath expression to apply to json output. Takes precedence over any specified 'format' and forces the format to be json processed by this expression maphttpstatus TEXT Map HTTP statuses to any of these exit codes: 0,1,5099. e.g. ""404=50,403=51"" Commands: bookmark Manage endpoint bookmarks config"
rc-website-fork/content/userinfo/howtos/storage/globus-cli.md,"Manage your Globus config file. (Advanced Users) delete Submit a delete task endpoint Manage Globus endpoint definitions getidentities Lookup Globus Auth Identities listcommands List all CLI Commands login Log into Globus to get credentials for the Globus CLI logout Logout of the Globus CLI ls List endpoint directory contents mkdir Make a directory on an endpoint rename Rename a file or directory on an endpoint task Manage asynchronous tasks transfer Submit a transfer task update Update the Globus CLI to its latest version version Show the version and exit whoami Show the currently loggedin identity. {{< /code }} For a full list of commands available: {{< code }} globus listcommands {{< /code }} List/Search Endpoints using the Globus CLI Find a Globus endpoint. Here is how you might find the UVA Main DTN: {{< code }} globus endpoint search ""uvamain"" ID | Owner | Display Name | | c4d80096761211e78b5e22000b9923ef | uva@globusid.org | uvamainDTN {{< /code }} Or search more broadly for all UVA endpoints in Globus: {{< code }} globus endpoint search ""uva"" ID | Owner | Display Name | | c4d80096761211e78b5e22000b9923ef | uva@globusid.org | uvamainDTN 67b9cb38301c11e7bcac22000b9a448b | uva@globusid.org | uvaportableDTN e1c6b1956d0411e5ba4622000b92c6ec | uvastro@globusid.org | uvastroalmuhit 31a68704242211e6bfeb22000b1701d1 | uvastro@globusid.org | uvastroscandium 7bb92d806d0411e5ba4622000b92c6ec | uvacse@globusid.org | uvacsefir de463ced6d0411e5ba4622000b92c6ec | uvastro@globusid.org | uvastrotupungato de463ce46d0411e5ba4622000b92c6ec | uvastro@globusid.org | uvastrohelix a9a9ae5d6d0411e5ba4622000b92c6ec | uvacse@globusid.org | uvacsecooper df70ec7d6d0411e5ba4622000b92c6ec | uvastro@globusid.org | uvastrocavi 24b0ca0c301311e7bcab22000b9a448b | ars9ac@virginia.edu | UVA Portable DTN {{< /code }} For transfers and file operations, reference endpoints by their unique ID. Names are only convenient tags to help humans differentiate between endpoints. Traverse Directory Trees using the Globus CLI Once you know the ID of a specific endpoint, you can list directories visible to you. Here are some paths open to users of the HPC cluster: {{< code }} globus ls c4d80096761211e78b5e22000b9923ef home/ nv/"
rc-website-fork/content/userinfo/howtos/storage/globus-cli.md,"scratch/ {{< /code }} To drill deeper, append directories to the endpoint ID with a colon : {{< code }} globus ls c4d80096761211e78b5e22000b9923ef:home/mst3k/ directory1/ directory2/ archive1.tar.gz file1.txt file2.txt myfile.txt {{< /code }} If we would like to transfer a file, we will need the full Globus ID and path of the source file: {{< code }} c4d80096761211e78b5e22000b9923ef:home/mst3k/archive1.tar.gz {{< /code }} Transfer Files using the Globus CLI To transfer a file you will also need the ID and path to a destination directory and new filename – the place to which you would like to copy the remote file from above. It should look something like this: {{< code }} 39e0bf8a303711e7bcae22000b9a448b:/home/user1/archive1.tar.gz {{< /code }} With the full path to a source file and a full path to a destination, we can now request a transfer. The simplest form of a transfer request looks like this: {{< code }} globus transfer c4d80096761211e78b5e22000b9923ef:home/mst3k/myfile.txt c4d80096761211e78b5e22000b9923ed:nv/vol179/staff/mynewfile.txt Message: The transfer has been accepted and a task has been created and queued for execution Task ID: 94d159809c9411e7acbc22000a92523b {{< /code }} Note: If you wish to encrypt your transfer, add the encrypt flag: {{< code }} globus transfer encrypt c4d80096761211e78b5e22000b9923ef:home/mst3k/myfile.txt c4d80096761211e78b5e22000b9923ed:nv/vol179/staff/mynewfile.txt {{< /code }} Get the Status of a Transfer Using the Task ID returned from a request, you can get the status of a task: {{< code }} globus task show 94d159809c9411e7acbc22000a92523b Label: None Task ID: 94d159809c9411e7acbc22000a92523b Is Paused: False Type: TRANSFER Directories: 0 Files: 1 Status: SUCCEEDED Request Time: 20170918 17:12:43+00:00 Faults: 0 Total Subtasks: 1 Subtasks Succeeded: 1 Subtasks Pending: 0 Subtasks Retrying: 0 Subtasks Failed: 0 Subtasks Canceled: 0 Subtasks Expired: 0 Completion Time: 20170918 17:12:44+00:00 Source Endpoint: uvamainDTN Source Endpoint ID: c4d80096761211e78b5e22000b9923ef Destination Endpoint: uvamainDTN Destination Endpoint ID: c4d80096761211e78b5e22000b9923ed Bytes Transferred: 2812 Bytes Per Second: 2468 {{< /code }} Script Transfers Against the"
rc-website-fork/content/userinfo/howtos/storage/globus-cli.md,"Globus CLI Using the commands above, automated file transfers should not be difficult if run under a user account that has already authenticated. A simple bash script run via cron should work well for automated file shipments. Each shipment will trigger an automatic success/failure email, so there is no need to set up additional notifications. Single file transfers Transfer a single file at a time to another DTN, via script: {{< gist nmagee d9f606ff7edfe1710ce81f1eb23ca654 }} Folder sync transfers Synchronize an entire folder and all contents with another DTN, via script: {{< gist nmagee 6f4ad4d32dbd0415528d1fb11242fd09 }} Run your script: {{< code }} user@host$ ./syncdirectories.sh {{< /code }} Either operation should result in a confirmation message like this: {{< code }} Message: The transfer has been accepted and a task has been created and queued for execution Task ID: 5ffe3058554311e890ce0a6d4e044368 {{< /code }} Automating your scripts In a Unix/Linux/macOS environment, you can set any script or application to run on any schedule using cron. In Windows, we recommend writing the above into a PowerShell script, which can then be scheduled using the “Task Scheduler” tool from the Windows menu. More Information See our main page introducing Globus Connect See the indepth Globus CLI Documentation or Globus CLI Reference Globus also has an API and Python SDK. For other technical details, see Globus Documentation. Use the GlobusCLI from your UVA HPC {usetheglobusclifromyourhpcaccount} Load the globuscli module and its dependencies: {{< code }} module load gcc openmpi globuscli {{< /code }} Authorize globuscli Run this command: {{< code }} globus login {{< /code }} You will then be given an Oauth2 login URL. Start a Web browser, either through FastX or through an X11 server on your local computer. Copy and paste this URL into the web browser, and authorize your connection as instructed"
rc-website-fork/content/userinfo/howtos/storage/globus-cli.md,"in the topic above. Be sure to give your authorization a useful name, such as rivanna, i.e. mst3krivanna. This will help you distinguish it in your list of Globus authorizations. {{% callout %}} Return to the top of the page for information about using the Globus CLI generally. {{% /callout %}} When referencing the globus binary in scripts, you may want to issue a which globus command to find its path as that may change over time with new versions. If used with backticks this can be used to populate a variable in a script: {{< code }} globus=which globus {{< /code }} Please note that users are not permitted to run cron jobs on the HPC system. Scheduling should be done from another system."
rc-website-fork/content/userinfo/howtos/storage/aws-s3.md,"Setup You will need to install and configure the awscli package in order to access objects in S3. Install the AWS CLI The AWS CLI is available through the pip/pip3 installer: If you have administrator privileges type {{< codesnippet }} pip install awscli {{< /codesnippet }} Otherwise use {{< codesnippet }} pip install user awscli {{< /codesnippet }} The project is open source, so you can also download the source at https://github.com/aws/awscli {{% alertgreen %}} UVA HPC Users have two options: Load the awscli module: {{< codesnippet }} module load awscli {{< /codesnippet }} If you need a different version, install it in your user directory: {{< codesnippet }} pip install user awscli==1.19.29 {{< /codesnippet }} {{% /alertgreen %}} Authenticate the CLI to Amazon Web Services Once the aws package is installed, you will need to configure it: {{< codesnippet }} aws configure {{< /codesnippet }} You will be prompted to enter four values: Your AWS access keys: awsaccesskeyid / awssecretaccesskey: These can be obtained from within your AWS account. See this AWS documentation for how to retrieve these two keys. If you are the root account owner, go to your account security settings to retrieve these. It is highly advised NOT to use root credentials for access in this way. Your default AWS region: Unless you know your S3 bucket is scoped to another region, enter useast1. Your default output format: Your options are text, json and table output. The AWS account you enter in these steps must have at least read permissions to access the resources you want to download. Access S3 using the AWS CLI aws s3 ls List Buckets aws s3 mb Make a new bucket aws s3 mb s3://mybucket3 Remember that S3 bucket names must be globally unique from all other AWS customers. aws s3"
rc-website-fork/content/userinfo/howtos/storage/aws-s3.md,"rm Remove a bucket aws s3 rm s3://mybucket3 Remember that S3 buckets must be emptied of all contents before they can be removed. Once removed the bucket name is available for other users. aws s3 ls List the contents of a bucket aws s3 ls s3://mybucket1/ PRE keys/ PRE status/ PRE zip/ 20200626 09:50:08 10451 index.json 20200626 09:50:09 64 robots.txt {{% alertblue %}} FOLDERS IN S3 Contrary to how it appears, S3 is not a file system in the ordinary sense. Instead, it is a webbased, APIdriven object storage service containing KEYS and VALUES. The key (name) of a file (object) is arbitrary after the name of the bucket itself, but must obey certain rules such as using no unusual characters. The typical form of grouping objects under ""subfolders"" uses the same naming convention as regular filesystems with a ""key"" such as: mybucket1/folder/subfolder/filename.txt The value (contents) of that key are the actual contents of the file itself. But it is important to remember that folders as they appear in the path of an S3 object are simply a mental convenience. {{% /alertblue %}} aws s3 cp Download a file aws s3 cp s3://mybucket1/robots.txt ./ aws s3 cp Upload a file aws s3 cp localfile.txt s3://mybucket1/ To upload a file and make it publicly available via HTTPS, add an acl property to it: aws s3 cp acl publicread localfile.txt s3://mybucket1/ {{}} Files that have been made publicreadable can be retrieved using other commandline tools such as curl and wget. S3 is an HTTPS web endpoint, and without the need for authentication you can work with them as if they were regular public web resources: curl O https://mybucket1.s3.amazonaws.com/path/to/myfile.txt {{}} aws s3 sync Synchronize to/from an S3 bucket aws s3 sync ./localdir s3://mybucket1/remotedir/ You can synchronize between any source/destination so long as at"
rc-website-fork/content/userinfo/howtos/storage/aws-s3.md,"least one of them is S3: Sync from local workstation to S3 Sync from S3 to local workstation Sync from S3 bucket to another S3 bucket aws s3 rm Remove a file from S3 aws s3 rm s3://mybucket1/filenotwanted.pdf aws s3 mv Move a file within S3 aws s3 mv s3://mybucket1/originalfile.csv s3://mybucket1/movedfile.csv aws s3 presign Presign an S3 URL In some cases users want to share a file with a remote party without creating access keys or for a limited amount of time. The presign feature is useful in this case since it creates a unique signed URL that expires after a set amount of time. To set the expiry time, calculate the length of time you want the signature to last in seconds. This value will be used with the expiresin flag. {{}} NOTE: This URL works regardless of who uses it, and requires no authentication. Therefore, be careful with the distribution of signed URLs, and keep their expiry time as short as possible. {{}} Access S3 using boto3 in Python The boto3 package is the standard library enabling programmatic access to AWS using Python. boto3 can access all AWS services and is helpful for creating, managing, or removing remote resources and infrastructure dynamically. The steps below refer to using boto3 for working with files in S3. Install boto3 {{< codesnippet }} pip install boto3 {{< /codesnippet }} boto3 will obtain its credentials from one of a few various locations: Hardcoded credentials within the application code itself. This is not recommended. Inherited credentials from the ~/.aws/ directory within your home directory. This is common for remote development. Injected as environment variables of the environment in which your code is running. Inherited credentials from the IAM role of the EC2 instance running your code. This is a best practice for production"
rc-website-fork/content/userinfo/howtos/storage/aws-s3.md,"systems in AWS. Use boto3 Import the library as you would for any other Python package, and set up a client or resource for the AWS service: Upload a file to S3 {{< gist nmagee c2be9caa4479bb11bb1b6097d7269946 }} Download a file from S3 {{< gist nmagee a8b42a126235a0366f7472efd4965d18 }} More Information about boto3 Documentation is available here."
rc-website-fork/content/userinfo/howtos/storage/drive-mapping.md,"Research Standard and Research Project storage can be mapped to your Windows or Mac computer as a remote drive. If you are off Grounds you must be running a VPN, such as the UVA Anywhere or the More Secure VPN from ITS. We recommend the More Secure VPN if that is available to you. Windows Open a File Explorer page. In the left column, rightclick on This PC. In the dropdown box that appears, look for the Map Network Drive option. If you do not see this option, click on Show more Options, and then click on Map network drive.... When the Map Network Drive dialog box appears, select a letter for the drive.This will be the location on your PC where you will be able to access your storage. Also in the Map Network Drive dialog box, type the path for your storage location in the Folder field. For Research Standard storage, the path starts with \\standard.hpc.virginia.edu\ followed by your storage share name, for example \\standard.hpc.virginia.edu\mylabstorage If you have ceph storage (standard.hpc.virginia.edu) you may have to enter ESERVICES\mst3k (with your own user ID) rather than your user ID alone. For Research Project storage, the path starts with \\project.hpc.virginia.edu followed by your storage share name, for example \\project.hpc.virginia.edu\mylabstorage. If you want the mapped drive to remain after a shutdown or reboot, check the box for Reconnect at signin If your laptop does not use you UVA ID and password for logging in, check the box for Connect using different credentials. When it asks you to authenticate, use your UVA id (e.g. mst3k) and your Eservices password. Select Finish. {{% callout %}} Note that you must use backslashes even if the path provided to you used forward slashes. {{% /callout %}} Mac OSX From the Finder menu, select GoGo To FolderConnect"
rc-website-fork/content/userinfo/howtos/storage/drive-mapping.md,"To Server. A dialog box should appear with smb:// filled in. Type the path you were given. For Research Standard storage the path starts with //standard.hpc.virginia.edu/ followed by your storage share name, for example smb://standard.hpc.virginia.edu/mylabstorage If you have ceph storage (standard.hpc.virginia.edu) you may have to enter eservices\mst3k (with your own user ID) rather than your user ID alone. For Research Project storage the path starts with //project.hpc.virginia.edu followed by your storage share name, for example smb://project.hpc.virginia.edu/mylabstorage. Enter your Eservices credentials when prompted, then click Connect. {{% callout %}} Note that for Mac OSX we use forward slashes. {{% /callout %}} Linux Special arrangements must be made with Research Computing to export shares to a Linux workstation."
rc-website-fork/content/userinfo/howtos/storage/_index.md,Map your Research Standard or Research Project Storage to your Desktop Use Globus from the Command Line Work with files in Amazon S3
rc-website-fork/content/service/dtc/contact.md,To contact the DTC for any reason: email rcdtc@virginia.edu submit a ticket to the Digital Technology Core
rc-website-fork/content/service/dtc/grants.md,"DTC Mission and Seed Grants The Digital Technology Core's mission is to provide UVA researchers with support for research utilizing mobile apps and wearable devices. As part of this mission the DTC offers seed grants to research faculty with two primary aims: 1. Launch new research projects in pursuit of competitive grants 2. Develop new DTC software functionality to support UVA researchers Seed Grant Application Guidelines Eligibility: At least one principal investigator must hold faculty status at UVA to be eligible for a DTC seed grant. Maximum Duration: DTC seed grant projects should have a one year timeline; however, they may last two or three years if a strong case can be made. Approved Activities: The proposal should be related to: (1) adding functionality to the DTC ecosystem insupport of research (e.g., collecting data from a new wearable device or new functionality in the DTC app) or (2) running a study in the existing DTC ecosystem. Scope: DTC seed grants are intended to provide approximately $10,000 in DTC services. In practice, this provides enough support for one new feature in the DTC ecosystem (e.g., data collection from a new device or a new kind of user interaction in DTC apps) and direct support for a two month study. Consultation for Seed Grant Applications Before you apply we highly recommend that you have a consultation meeting with the DTC. We can help you improve your application and appropriately scope your proposal. You can request a consultation by contacting us through any avenue on our contact page. Application and Response Timelines Applications for DTC seed grants can be submitted at any time. We will respond to your proposal within one month. Application Content and Template To apply for a DTC seed grant, the PI will submit an application (2 pages maximum not"
rc-website-fork/content/service/dtc/grants.md,"including references) that includes: The PI's name, computing ID, and department A title for the project A description of the project and its goals A description of how the seed grant, and the DTC, will support the goals of the project A timeline for completing the proposed project A plan for how the seed grant will lead to future support (e.g., external grants) for this or subsequent projects We provide a seed grant application template. You are not required to use this template. Evaluation of Applications Applications will be evaluated on: The completeness of the submission The clarity of the descriptions the reviewers may not be familiar with the PI's discipline How this grant will advance your research or will enhance the community of researchers at UVA Disbursement of Seed Grants Seed grant recipients will not receive any cash or vouchers from the DTC. Instead, DTC staff will work with recipients to allocate DTC staff time to your project. DTC staff will continue to work with you until all proposed software functionality has been provided and/or the research study has been completed. Required Publicity for Application Awards PI's must acknowledge the DTC (and its parent organization Research Computing) in any publication, presentation, or poster that results from the work done for the seed grant. Additionally, at the end of the award, the PI's must agree to have a blurb about the research project posted on the DTC website."
rc-website-fork/content/service/dac/awards.md,"The mission of the Data Analytics Center (DAC) is to provide UVA researchers with support for the management and analysis of large datasets on Research Computing platforms. To fulfill this mission and to empower faculty in advancing their research, we offer Awards for access to Research Comuting (RC) resources. These Awards aim to i) launch new research projects, ii) promote interdisciplinary collaborations, iii) cultivate communities of researchers with shared interests in data and analysis methods. The Awards The DAC offers two types of Awards: Small Awards (up to $10,000 worth of services) and Large Awards (up to $100,000 of services). Recipients of the Awards will receive vouchers that may be used toward RC services, such as collaborations with DAC team members or purchase of data storage, or for support from any of the UVA Cores. Small Data Analytics Resource Awards The Small Analytics Resource Awards are intended to support research tasks for an individual PI or lab. Large Data Analytics Resource Awards The Large Analytics Resource Awards are aimed to build data or analytics infrastructure with the potential to impact multiple researchers at UVA. Funding up to $100,000 in resources, this award aims to enable the hosting of data collections, the movement of data or the development of analytics infrastructure that will enable the research workflows of a broad community at UVA. In addition award funds may be allocated for indepth collaborations with DAC team members. Award Guidelines Eligibility: The principal investigator must hold faculty status at UVA to be eligible for an Analytics Research Award. Funding Limit: Although the Awards will be in the form of vouchers, the use of the voucher may not exceed the amount of the Award. Maximum Proposal Duration: Eighteen months; however, Large Awards may last two or three years if a strong case may"
rc-website-fork/content/service/dac/awards.md,"be made. Approved Activities: The activities in the proposal should be related to the acquisition, storage, or analysis of data as it relates to a research project. Priority will be given to proposals that request collaborative services of the Research Computing team. Timelines for Small Awards: We accept Small Award proposals at any time throughout the year; however, your best chance for receiving an award will be if you submit it at the end of a quarter. The following table shows the upcoming deadlines for proposals. Proposal Deadline Award Announcements Award Start Date 30 Jun. 2025 21 Jul. 2025 11 Aug. 2025 29 Sep. 2025 20 Oct. 2025 10 Nov. 2025 29 Mar. 2026 19 Apr. 2026 9 May 2026 29 Jun. 2026 20 Jul. 2026 10 Aug. 2026 Timelines for Large Awards: We will accept proposals for the Large Awards on specified dates. The following table provides the deadlines for the upcoming Large Award submissions. Proposal Deadline Award Announcements Award Start Date 20 Oct. 2025 8 Dec. 2025 12 Jan. 2026 1 Jun. 2026 20 Jul. 2026 24 Aug. 2026 Before you apply: We highly recommend that you have a consultation meeting with a DAC member to discuss the services that you are planning and what the budget will look like for those services. You can request a consultation by submitting this online support request form . Application Requirements for Small Awards: To apply for the Small Analytics Research Award, the PI will submit a proposal (2 pages maximum, Calibri 11 pt. font) that includes: The PI’s name, computing ID, and department, A title for the project, A description of the project and its goals, A description of the type(s) of data collected for the project and how the data will be used to meet the goals of the"
rc-website-fork/content/service/dac/awards.md,"project, A description of how the Analytics Research Award will support the goals of the project, including the use of vouchers to obtain services, A plan for how the Analytics Research Award will lead to future support (e.g., external grants or other funding) for this or subsequent projects. Priority will be given to proposals that state how a DAC team member will be included in future funding proposals. An attachment with a budget showing the services and expected costs that the Award would cover. The attachment will not count toward the twopage limit. Application Requirements for Large Awards: The objective of the Large Award is to develop comprehensive data resources that will foster a collaborative spirit among researchers at UVA. Therefore, preference will be given to projects that include principal investigators or coinvestigators from diverse fields. For the Large Analytics Research Award, the proposal must include the following: A cover page that includes the project title, and name, title, department, and email address of each the PI(s) and CoI(s). The Principal Investigator(s) will be the individual(s) responsible for the scientific or technical direction of the project. If more than one PI is included, the first one listed will have primary responsibility for communications with the DAC and the submission of reports. Any listed CoIs will not have overall responsibility or spending authority as the PI. A project summary or abstract, limited to 1 page A research plan, limited to 3 pages and addresses the following: Specific objectives of the project, including how the project spans labs, departments, or schools Approach and workplan, including the responsibilities of DAC team members or the need for resources, such as data storage Outcomes, including planned proposals for external funding Budget. Please contact the DAC (RCDAC@virginia.edu) to request a consultation for preparing the budget. Timeline"
rc-website-fork/content/service/dac/awards.md,"(include milestones) for completing the scope of the work Works Cited/References List of targeted funding sources for followup work (for each source include funding agency, anticipated funding request, deadline, link to RFP if available) Please use Calibri, 11point font, singlespaced with 1inch page margins. All pages, except for the cover page, should be numbered consecutively throughout the proposal. Submission Process: Email your proposal, in PDF format, to RCDAC@virginia.edu. The subject line for the email must be ""Analytics Resource Proposal"". You may include a brief cover letter in the body of the email. We encourage you to follow these instructions precisely to ensure that your proposal is received and processed correctly. After your email is received, we will secure a copy of your proposal and review that copy. We will not consider any changes that you make to the proposal after we have received it. Evaluation Criteria: The proposals will be evaluated on: The completeness of the submission, The clarity of the descriptions – the reviewers may not be familiar with the PI’s discipline, The diversity of the data, How this Award will advance your research or will enhance the community of researchers at UVA. Voucher Disbursement After the Awards recipients have been announced, the Data Analytics Center will work with the PI to determine the disbursement of the Award vouchers. Publicity & Communication The PI must acknowledge Research Computing in any publication, presentation, or poster that results from the work done on the Award. At the end of the Award, the PI must agree to have a blurb about the research project posted on the Research Computing website."
rc-website-fork/content/service/dac/past_awardees.md,"2025 Douglas Bayliss, Department of Pharmacology Characterizing RTN Population Activity Following Hypercapnia and Drub Challenges Using in vitro Calcium Imaging Zezhou Cheng, Department of Computer Science Detect, Track, and Relate Anythin in 3D Nick Guagliardo, Department of Pharmacology Analysis of Calcium Activity in Ex Vivio Adrenal Slices Yangfeng Ji, Department of Computer Science Legal Document Analysis via Large Language Models: A Preliminary Study on Constitutional Court of Colombia Sheng Li, School of Data Science Towards Efficient and Reliable Reasoning with Large Language Models Thomas Loughran, Division of Hematology & Oncology LGL Leukemia Medical Data Extraction and Search Program for Translational Research Nikhil Shukla, Charles L. Brown Department of Electrical and Computer Engineering, Department of Materials Science and Engineering Scalable Combinatorics Accelerator Inspired by Physics Bon Trinh, Division of Experimental Pathology Virtual Compound Screening for CancerCausing Proteins 2024 Alan Bergland, Department of Biology Drosophila Evolution through Space and Time 2.0 Data Repository Updates Josh Colston, Division of Infectious Diseases & International Health Pipeline Feasibility Study and Dashboard Implementation for the Planetary Child Health & Enterics Observatory Plan (PlanEO) Matthew Crawford, Division of Infectious Diseases & International Health Research Computing to Guide the Functional Optimization of Antibacterial Peptides Gretchen Gamrat, McIntire School of Commerce To Buy or Not to Buy: Effectiveness of Boycotts in Consumer Goods Andres Norambuena, Department of Biology Using AlphFold2 to Unveil Seminal Molecular Dysfunctions in Alzheimer's Disease Alexander Podboy, Division of Gastroenterology and Hepatology Outcomes of Dexmedetomidine during ERCP on postERCP Pancreatitis David Rekosh, Department of Microbiology, Immunology, and Cancer Biology Analysis of ONT LongRead Data to Elucidate Differential mRNA Isoform Expression Induced by Viral and Cellular Proteins Heman Shakeri, School of Data Science CLEARiOP: Comprehensive Learning Engine for Assessment of Risk in IntraOcular Pressure Sarah Siegrist, Department of Biology Identifying Cell Types within Three Dimensional Tissues across"
rc-website-fork/content/service/dac/past_awardees.md,"Populations Jeffrey Smith, Department of Biochemistry and Molecular Genetics Dissecting the Role of AcetylCoenzyme A Synthetase Coding Genes in Calori RestrictionDriven Longivity Cynthia Tong, Department of Psychology Intensive Longitudinal Data Analysis and Compositional Predictors Justin Yaworsky, Department of Emergency Medicine STEMI Database Development and Deep Learning ECG Algorithm for STEMI Diagnosis in Emergency Department Patients 2023 Steven Johnson, McIntire School of Commerce Distribution and Discovery of Digital Information (3Di) Jingjing Li, McIntire School of Commerce ResponsiblebyDesign: Combating Biases in Generative AI Applications"
rc-website-fork/content/service/dac/ai.md,"AI is more than ChatGPT Artificial Intelligence, or AI, is a combination of computing platforms and programs that emulate human thought processes for making decisions and solving complex problems. The technology behind AI is much more than GenAI (i.e., applications like ChatGPT or CoPilot). While ChatGPT is a popular example of AI because it simulates conversations with people, AI itself includes many other technologies and applications that can be used to explore research data. Other terms that may be used with AI, but have subtle differences, are Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). Using AI to Explore Your Data AI can be used to explore research data by employing sophisticated algorithms to analyze vast datasets efficiently and accurately. Through machine learning techniques, AI can identify complex patterns, correlations, and trends that might be overlooked by human analysis. This enables researchers to generate new hypotheses, validate existing theories, and uncover novel insights. For instance, in medical research, AI can sift through millions of patient records to discover potential treatment effects, predict disease outbreaks, and personalize patient care. Additionally, AIpowered tools can automate data collection, organization, and visualization, thereby streamlining the research process and allowing scientists to focus on interpreting the results and advancing their fields. How RC can help you Available Platforms For AI algorithms to perform efficiently, they should run on computers with graphics processing units (GPUs). Although GPUs were originally designed to process images, their ability to process thousands of mathematical computations makes them ideal for simulating the relationships within your data. Research Computing has many GPUs available on our standard cluster (i.e., Rivanna/Afton) and on our high security systems (i.e., Ivy/Rio). Access to the GPUs on Rivanna/Afton is available at no cost to all researchers with an allocation. For those who have been"
rc-website-fork/content/service/dac/ai.md,"provisioned on our high security systems, the GPUs are available at no additional cost. If you are writing a grant proposal for research where you will be using GPUs, you can include our facilities statement to describe the resources available to you. Available Software RC has a variety of AI software packages installed on our standard and high security systems. These packages were built to use the GPUs on our systems. However, you will need to ensure that you are running the software on a compute node that has GPUs available. For many of the packages, you will need to customize the software to work with your data. RC does have a team of consultants who can help with these tasks. To request assistance with using AI software, please submit a support request form. Available Workshops and Tutorials RC conducts workshops on how to use our platforms and software. The workshops tend to be offered at the start of each semester. You can check on upcoming workshops here. Many of our previous workshops have been converted to tutorials. You can find several of the AI tutorials here. A good starting point for someone new to AI is the tutorial Machine Learning for Python. Available Support RC provides a variety of support options. You can submit questions to our ticketing system by filling out our Support Request Form. RC also has virtual office hours on Tuesdays and Thursdays. The times and Zoom links are available on our Support Options webpage. Finally, you may request a consultation or collaboration through the Data Analytics Center (RCDAC@virginia.edu). You can even discuss with us the possibility of writing into your grant proposals support from one of the DAC team members. Additional Information The UVA library has great information about GenAI. Plus, any member of the"
rc-website-fork/content/service/dac/ai.md,"UVA community (i.e., an individual with Netbadge capabilities) has access to Coursera For UVA) and O’Reilly books, where you can learn more about AI. Courses that you may want to consider are Bayesian Machine Learning from UC Santa Cruz, and any of the Machine Learning courses from Stanford. If you prefer books, you could start with Machine Learning with PyTorch and ScikitLearn by Sebastian Raschka, Yuxi Liu, and Vahid Mirialili, and Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf."

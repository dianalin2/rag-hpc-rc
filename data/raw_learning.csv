file_path,text
rc-learning-fork/content/home/slider.md,"+++
Slider widget.
widget = ""slider""  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = false  # Activate this widget? true/false
weight = 1  # Order that this section will appear.
Slide interval.
Use false to disable animation or enter a time in ms, e.g. 5000 (5s).
interval = false
Slide height (optional).
E.g. 500px for 500 pixels or calc(100vh - 70px) for full screen.
height = """"
Slides.
Duplicate an [[item]] block to add more slides.
[[item]]
  title = ""Hello""
  content = ""I am center aligned :smile:""
  align = ""center""  # Choose center, left, or right.
# Overlay a color or image (optional).
  #   Deactivate an option by commenting out the line, prefixing it with #.
  overlay_color = ""#666""  # An HTML color value.
  overlay_img = ""headers/bubbles-wide.jpg""  # Image path relative to your static/img/ folder.
  overlay_filter = 0.5  # Darken the image. Value in range 0-1.
# Call to action button (optional).
  #   Activate the button by specifying a URL and button label below.
  #   Deactivate by commenting out parameters, prefixing lines with #.
  cta_label = ""Get Academic""
  cta_url = ""https://sourcethemes.com/academic/""
  cta_icon_pack = ""fas""
  cta_icon = ""graduation-cap""
[[item]]
  title = ""Left""
  content = ""I am left aligned :smile:""
  align = ""left""
overlay_color = ""#555""  # An HTML color value.
  overlay_img = """"  # Image path relative to your static/img/ folder.
  overlay_filter = 0.5  # Darken the image. Value in range 0-1.
[[item]]
  title = ""Right""
  content = ""I am right aligned :smile:""
  align = ""right""
overlay_color = ""#333""  # An HTML color value.
  overlay_img = """"  # Image path relative to your static/img/ folder.
  overlay_filter = 0.5  # Darken the image. Value in range 0-1.
+++"
rc-learning-fork/content/home/hero.md,"+++
Hero widget.
widget = ""hero""  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 10  # Order that this section will appear.
title = ""UVA Research Computing Learning Portal""
Hero image (optional). Enter filename of an image in the static/img/ folder.
hero_media = ""hero-academic.png""
[design.background]
  # Apply a background color, gradient, or image.
  #   Uncomment (by removing #) an option to apply it.
  #   Choose a light or dark text color by setting text_color_light.
  #   Any HTML color name or Hex value is valid.
# Background color.
  # color = ""navy""
# Background gradient.
  #gradient_start = ""#4bb4e3""
  #gradient_end = ""#2b94c3""
# Background image.
image = ""learning-bg.jpg""  # Name of image in static/img/.
image = ""scott_stadium_afc_aerial_ss_01.png""
image = ""rotunda_lawn_aerial_old_cabell_fall_ss_01.jpg""

image_darken = 0.1  # Darken the image? Range 0-1 where 0 is transparent and 1 is opaque.
  # image_size = ""cover""  #  Options are cover (default), contain, or actual size.
  # image_position = ""center""  # Options include left, center (default), or right.
  # image_parallax = true  # Use a fun parallax-like fixed background effect? true/false
# Text color (true=light or false=dark).
  text_color_light = true
Call to action links (optional).
Display link(s) by specifying a URL and label below. Icon is optional for [cta].
Remove a link/note by deleting a cta/note block.
[cta]
  url = ""/landing""
  label = ""Get Started""
  icon_pack = ""fas""
+++
Research Computing provides computational, scientific programming, and data analysis workshops, many of which are offered in partnership with the UVA Library Research Data Services and UVA Health Sciences Library websites. This site compiles RC workshop and tutorial materials as well as interactive exercises for students to review or study at their own pace."
rc-learning-fork/content/home/index.md,"+++
Homepage
type = ""widget_page""
headless = true  # Homepage is headless, other widget pages are not.
+++"
rc-learning-fork/content/home/tags.md,"+++
Tag Cloud widget.
widget = ""tag_cloud""  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 120  # Order that this section will appear.
title = ""Topics""
subtitle = """"
[content]
  # Choose the taxonomy from config.toml to display (e.g. tags, categories)
  taxonomy = ""tags""
# Choose how many tags you would like to display (0 = all tags)
  count = 20
[design]
  # Minimum and maximum font sizes (1.0 = 100%).
  font_size_min = 1.0
  font_size_max = 1.0
+++"
rc-learning-fork/content/landing/_index.md,"+++
title = ""Topics""
subtitle = """"
+++
Tutorials are perfect for introducing yourself to a new topic.
Some tutorials are offered as in-person and/or Zoom workshops regularly.  Find the notes or slides and download materials here.
For a list of upcoming workshops, please see our page at the UVARC Website.
Short courses are longer and more in-depth than tutorials or workshops.

{{< categories >}}"
rc-learning-fork/content/courses/_index.md,
rc-learning-fork/content/notes/_index.md,
rc-learning-fork/content/tutorials/_index.md,"+++
title= ""Tutorials""
subtitle=""""
date= ""2020-05-13T00:00:00""  # Add today's date.
type = ""widget_page""  # Page type is a Widget Page
widget = ""blank""
active= true
headless = false  # This file represents a page section.
... Put Your Section Options Here (title etc.) ...
[content]
  # Page type to display. E.g. post, talk, or publication.
  page_type = ""talk""
# Choose how much pages you would like to display (0 = all pages)
  count = 0
# Choose how many pages you would like to offset by
  offset = 0
# Page order. Descending (desc) or ascending (asc) date.
  order = ""desc""
# Filter posts by a taxonomy term.
  [content.filters]
    tag = """"
    category = """"
    publication_type = """"
    exclude_featured = false
    exclude_past = false
    exclude_future = false
[design]
  # Toggle between the various page layout types.
  #   1 = List
  #   2 = Compact
  #   3 = Card
  #   4 = Citation (publication only)
  view = 2
Optional header image (relative to static/img/ folder).
header:
caption: """"
image: """"
+++"
rc-learning-fork/content/authors/uvarc/_index.md,"The University of Virginia Research Computing learning site is a repository for our online short courses, tutorials, and workshops."
rc-learning-fork/content/authors/kal/_index.md,
rc-learning-fork/content/authors/abd/_index.md,
rc-learning-fork/content/authors/cag/_index.md,
rc-learning-fork/content/authors/gka/_index.md,
rc-learning-fork/content/authors/ab/index.md,
rc-learning-fork/content/authors/rs/_index.md,
rc-learning-fork/content/authors/kah/_index.md,
rc-learning-fork/content/authors/teh/_index.md,
rc-learning-fork/content/authors/khs/_index.md,
rc-learning-fork/content/authors/mab/_index.md,
rc-learning-fork/content/authors/as/_index.md,
rc-learning-fork/content/authors/jmh/_index.md,
rc-learning-fork/content/authors/wtr/_index.md,
rc-learning-fork/content/authors/pbo/_index.md,
rc-learning-fork/content/slides/site-info/index.md,"Basic Info
README

Workflow

Same as for the main rc.virginia.edu site
Always check out and modify staging
Clone into an appropriate directory, then cd to it and run /wherever/hugo server.  Example
(I don't know how to do this on Windows)
website
rc-learning
rc-website
hugo (copy of hugo-extended)


Theme

Formerly called Academic, now called Wowchemy
Do not modify anything under the themes subdirectory
Make changes into the corresponding folder under the top level
e.g. layouts/partials/page-footer.html modified from themes/academic/layout/partials/page_footer.html
Hugo looks in top level first, then goes to theme 


Code Highlighting
Code block
python
porridge = ""blueberry""
if porridge == ""blueberry"":
    print(""Eating..."")

Math
In-line math: $x + y = z$
Block math:
$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$

Diagrams

Mermaid (https://mermaid-js.github.io/mermaid/#/)
Example

markdown
{{< diagram >}}
flowchart LR
   subgraph Users
       A(User) --> F((Internet))
       B(User) --> F
       C(User) --> F
    end
    subgraph Cluster
       F --> G(Frontend)
       G --> H{{Interconnection Network}}
       H --> K(Node)
       H --> L(Node)
       H --> M(Node)
       H --> N(Node)
       H --> S[(Storage)]
    end
{{< /diagram >}}

Mermaid
View example here

Shortcodes

code.html
shows code from a file
code-snippet.html
copies a bit of code to user's clipboard 
code-download
allows user to download a file containing code


Site Design

Three categories
short courses
tutorials
workshops
probably should reorganize but we'll deal with that later


Tutorials and Workshops

Basically the same thing (hence the need to reorganize)
Have a ""landing page""
Landing page has options for links/downloads (in ovals)
notes
slides
pdf


Front Matter for Tutorials and Workshops ""Landings""

notes
directs to content/notes/samename
slides
directs to content/slides/samename
expects Reveal.js slides
url_slides
full (relative to Hugo paths or full URL) URL must be provided, e.g.
url_slides: /slides/r-intro-r-intro-slides.pdf
pdf
expects to find a pdf in the same directory
url_pdf
similar to url_slides
url_dataset
data/whatever
or longer URL
url_code
like url_dataset


Short courses

For longer material
No ""landing page""
Be sure to include tags and categories


Images

Can be in /static/images if used more than once
Can be local within course directory
Reference like
{{< figure src=""/courses/parallel_computing_introduction/img/SMP.png"" caption=""Schematic of an SMP system."" >}}
figure shortcode is a Hugo built-in


Document Types

article
Can have a menu layout with ""subchapters""
Chapter parent menus must be a file that isn't empty (a Hugo thing)
book
no subchapters


Aesthetics

The title in the frontmatter will be Header 1
I tend to prefer to avoid Header 1 subsequently unless you have only one file
"
rc-learning-fork/content/slides/example/index.md,"Create slides in Markdown with Academic
Academic | Documentation

Features

Efficiently write slides in Markdown
3-in-1: Create, Present, and Publish your slides
Supports speaker notes
Mobile friendly slides


Controls

Next: Right Arrow or Space
Previous: Left Arrow
Start: Home
Finish: End
Overview: Esc
Speaker notes: S
Fullscreen: F
Zoom: Alt + Click


Code Highlighting
Inline code: variable
Code block:
python
porridge = ""blueberry""
if porridge == ""blueberry"":
    print(""Eating..."")

Math
In-line math: $x + y = z$
Block math:
$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$

Fragments
Make content appear incrementally
{{%/* fragment */%}} One {{%/* /fragment */%}}
{{%/* fragment */%}} **Two** {{%/* /fragment */%}}
{{%/* fragment */%}} Three {{%/* /fragment */%}}
Press Space to play!
{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} Two {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}

A fragment can accept two optional parameters:

class: use a custom style (requires definition in custom CSS)
weight: sets the order in which a fragment appears


Speaker Notes
Add speaker notes to your presentation
markdown
{{%/* speaker_note */%}}
- Only the speaker can read these notes
- Press `S` key to view
{{%/* /speaker_note */%}}
Press the S key to view the speaker notes!
{{< speaker_note >}}
- Only the speaker can read these notes
- Press S key to view
{{< /speaker_note >}}

Themes

black: Black background, white text, blue links (default)
white: White background, black text, blue links
league: Gray background, white text, blue links
beige: Beige background, dark text, brown links
sky: Blue background, thin dark text, blue links



night: Black background, thick white text, orange links
serif: Cappuccino background, gray text, brown links
simple: White background, black text, blue links
solarized: Cream-colored background, dark green text, blue links


{{< slide background-image=""/img/boards.jpg"" >}}
Custom Slide
Customize the slide style and background
markdown
{{</* slide background-image=""/img/boards.jpg"" */>}}
{{</* slide background-color=""#0000FF"" */>}}
{{</* slide class=""my-style"" */>}}

Custom CSS Example
Let's make headers navy colored.
Create assets/css/reveal_custom.css with:
css
.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}

Two Columns
This is a local addition to the features.


    This is how you make a slide with two columns.  You can put an image on the other side.
 




---

# Questions?

[Ask](https://spectrum.chat/academic)

[Documentation](https://sourcethemes.com/academic/docs/managing-content/#create-slides)"
rc-learning-fork/content/slides/accord-intro/index.md,"


Intro to ACCORD


Overview
ACCORD is a web-accessible secure platform which allows researchers from Virginia public universities to analyze their sensitive data in a central location


ACCORD projects



ACCORD is project-based:

Investigators can have multiple projects

Example: kidney research and an RNA-seq study

Projects are isolated, you cannot transfer or access data between them

Invite co-investigators

To add researchers to your project, submit a request here: Add a researcher







---

### Storage on ACCORD





ACCORD projects come with:


Home directory of 50GB
Project directory of 1TB
Additional storage can be purchased. Please submit a request here







---

### Data on ACCORD




ACCORD supports:

De-identified PII
FERPA
Business Confidential
Other types of sensitive data


ACCORD does not support:

Identifiable HIPAA
CUI
FISMA
PCI





    Questions about whether your data is suitable for ACCORD? Submit a support ticket here


---

### Data retention



- Data is stored on the system for 6 months.
- To extend your project, please fill out a request

---

### Globus data transfer



- Data transfer is processed through Research Computing staff for the time being.
- Please fill out a request here for data transfer

---

### Requirements to access ACCORD

- To access ACCORD, you need:
    - A modern web browser such as Chrome, Firefox, Safari, or Edge
    - You must be logged into your institution’s VPN
        - If you have a sponsored account or are a UVA researcher, you will need the HSVPN
    - Install and register OPSWAT, a posture-checking client

![](img/ACCORD_Intro_2021102211.png)

---

### ACCORD Portal



  * ACCORD can be accessed via:[https://accord.uvarc.io/](https://accord.uvarc.io/)

  * The ACCORD website has a User Guide, FAQ, and additional documentation:[https://www.rc.virginia.edu/userinfo/accord/overview/](https://www.rc.virginia.edu/userinfo/accord/overview)

![](img/ACCORD_Intro_202110222.jpg)

---

### Logging into ACCORD







To access ACCORD, you need to log in through InCommon

Select your home institution from the dropdown menu (or UVA if you have a sponsored account)



---

### Logging into ACCORD







Login using your home institution’s credentials

In this example, UVA will ask you to login using NetBadge. If you're from another institution, this will be different



---

### ACCORD dashboard

  * Once you log in, you will see the ACCORD dashboard
  * Your name will appear in the top right corner, along with any recent or currently running sessions

![](img/ACCORD_Intro_2021102215.png)

---

### Start a new session

  * A session is an individual instance running one of the available containers (RStudio, JupyterLab, etc.)
  * To create a new session, click on the ""*Start A New Session*"" button in the top right

![](img/ACCORD_Intro_2021102216.png)

---

### Select a project

![](img/ACCORD_Intro_2021102217.png)

  * To create a new session, you need to select a project
  * Projects are isolated. You can only access data you’ve uploaded to the project you’ve selected

---

### Select an environment

![](img/ACCORD_Intro_2021102218.png)

  * After selecting a project, select the environment you want to use
  * To start your new environment, click on the “*Start*” button

---

### Connecting to a session

![](img/ACCORD_Intro_2021102219.png)

  * Your new session will be in the “*Current Sessions*” section
  * __Note:__ Your session may show pending as the system waits for resources to become available
  * Once your session is ready, click the “*Connect*” button to launch your session

---

### Stopping a session

![](img/ACCORD_Intro_2021102221.png)

  * When you’re finished working in a session, __always__ click on the “**Stop**” button to delete it. This will free up resources for the system
  * Failing to delete sessions will slow down the system and create long wait times for researchers

---

### Recent sessions

![](img/ACCORD_Intro_2021102222.png)

  * After stopping a session, it will be moved to the “*Recent Sessions*” section
  * You can re-launch any session by clicking the “*Launch*” button

---

### Want to learn more?


* The ACCORD website has additional documentation, FAQs, and a user guide:
  * [https://www.rc.virginia.edu/userinfo/accord/overview/](https://www.rc.virginia.edu/userinfo/accord/overview/)
* Have issues or questions? Fill out a support ticket here

![](img/ACCORD_Intro_2021102223.jpg)"
rc-learning-fork/content/courses/pytorch-hpc/overview.md,"Background
PyTorch is a widely used deep learning platform known for its flexibility and speed. Originally developed by Facebook AI (now Meta AI), it has grown into one of the most popular frameworks for deep learning research and applications. Researchers and industry professionals widely adopt it due to its ease of use, dynamic computation graph, and easy integration with GPU acceleration.
PyTorch evolved from the Torch library, which was an open-source deep learning framework written in C. Although Torch is no longer actively developed, many of its libraries and functionalities have been incorporated into PyTorch. Today, PyTorch is used by corporations, laboratories, and universities to develop software like Autopilot and Full Self-Driving (FSD) models (Tesla), ChatGPT (OpenAI), reinforcement learning models for robotics (Boston Dynamics), and much more.

Overview of PyTorch and Its Core Components
PyTorch provides a comprehensive set of tools and features that enable efficient deep learning model development and training. Below are some of its core components:
1. Torch Tensors
A tensor is a multi-dimensional array, similar to NumPy arrays, but with the added benefit of GPU acceleration and automatic differentiation.Tensors are the core data structure in PyTorch, used to store and manipulate data for deep learning models. Every input in PyTorch is represented as a tensor; features, responses, parameters, etc. PyTorch does not accept numpy arrays as input like Keras/TensorFlow does, but numpy arrays are easily converted to and from Torch tensors.
Creating Tensors
```python
import torch
Creating a tensor from a list
x = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float)
Creating a random tensor
rand_tensor = torch.rand(3, 3) # 3x3 matrix 
Creating a tensor filled with zeros or ones
zero_tensor = torch.zeros(5, 5) # 5x5 matirx
one_tensor = torch.ones(5,5) # 5x5 matirx
Make a row tensor a column tensor
column = torch.tensor([1,2,3,4,5]).view(-1,1)
Convert a Numpy array to a tensor
array = np.array([1,2,3,4])
tensor = torch.tensor(array)
or
tensor = torch.from_numpy(array)
Convert back to array
array = tensor.numpy()
```
Tensor Operations
```python
x = torch.tensor([2,4,6])
y = torch.tensor([1,3,5])
Matrix Addition
z = x + y
z = x * y
Matrix Multiplication
z = torch.matmul(x, torch.t(y))
```
Automatic Differentiation
Automatic differentiation (autograd) is a technique used by PyTorch to compute derivatives (gradients) automatically, making it easier to train deep learning models. It dynamically builds a computational graph and efficiently applies the chain rule for differentiation.
```python
Create tensor with gradient tracking
x = torch.tensor(2.0, requires_grad=True)
```
If a tensor has requires_grad = True, PyTorch keeps track of all operations performed on it.
Computing Gradients
When .backward() is called on a scalar loss, PyTorch traverses the graph in reverse (backpropagation) and computes derivatives using the chain rule.
The computed gradients are stored in the .grad attribute of each tensor. The gradients are then used to update model parameters (e.g., via gradient descent). We'll talk more about backpropogation on the next page.
Key Features of Autograd

Dynamic Computation Graph: Built at runtime, allowing flexibility in model design.
Efficient Backpropagation: Computes gradients only for required tensors.
Automatic Chain Rule Application: Saves time and avoids manual derivative computation.


2. Activation Functions
Activation functions introduce non-linearity into neural networks, enabling them to learn and model complex patterns.



      {{< figure src=""/courses/pytorch-hpc/img/sigmoid.png"" caption=""Sigmoid"" width=""400px"" >}}
    

      {{< figure src=""/courses/pytorch-hpc/img/tanh.png"" caption=""Tanh"" width=""400px"" >}}
    



      {{< figure src=""/courses/pytorch-hpc/img/relu.png"" caption=""ReLU"" width=""400px"" >}}
    

      {{< figure src=""/courses/pytorch-hpc/img/leakyrelu.png"" caption=""Leaky ReLU"" width=""400px"" >}}
    


Common Activation Functions in PyTorch
```python
import torch.nn as nn
Sigmoid Activation
sigmoid = nn.Sigmoid()
ReLU Activation
relu = nn.ReLU()
Tanh Activation
tanh = nn.Tanh()
Leaky ReLU
lrelu = nn.LeakyReLU(negative_slope=0.01)
```
More Activation Functions

3. Loss Functions
A loss function measures how well or poorly a model performs by comparing the model's output to the true labels
Common Loss Functions
```python
mse_loss = nn.MSELoss()  # Mean Squared Error
ce_loss = nn.CrossEntropyLoss()  # Cross-Entropy Loss
bce_loss = nn.BCELoss()  # Binary Cross-Entropy Loss
nll_loss = nn.NLLLoss() # Negative Log-Likelihood (NLL) Loss
hu_loss = nn.SmoothL1Loss() # Huber Loss (Smooth L1 Loss): 
```
More Loss Functions

4. Optimizers
Optimizers are algorithms used to update the weights of a neural network during training to minimize the loss function.
They adjust model parameters based on gradients computed during backpropagation to improve the model's performance.
Common PyTorch Optimizers
```python
import torch.optim as optim
Stochastic Gradient Descent (SGD)
sgd = optim.SGD(model.parameters(), lr=0.01)
Adam Optimizer
adam = optim.Adam(model.parameters(), lr=0.001)
RMSprop: (Root Mean Square Propagation):
rmsprop = optim.RMSprop(model.parameters(), lr=0.001)
Adagrad:
adagrad = optim.Adagrad(model.parameters(), lr=0.01)
```
The model parameters are passed through the optimizer. This is essential for training.
More Optimizers

5. Transforms
PyTorch Transforms are operations used to preprocess and augment data before feeding it into a neural network. They are commonly used in computer vision tasks to prepare images for training. Other data types have other augmenters.
Example Image Transformations
```python
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])
```
More Transforms

6. Data Handling with Dataset and DataLoader
PyTorch provides powerful tools for handling datasets efficiently using torch.utils.data.Dataset and torch.utils.data.DataLoader. These utilities make it easy to preprocess and load data in batches, which is essential for training deep learning models effectively.
Using Dataset to Create Custom Datasets
torch.utils.data.Dataset is an abstract class representing a dataset. PyTorch Datasets provide an easy way to load, preprocess, iterate through, and manage data in PyTorch.Your custom dataset should inherit Dataset and override the following methods:

init: point to your own dataset
len : ensure len(dataset) returns the correct length of the dataset.
getitem: to iterate through data

```python
from torch.utils.data import Dataset
class CustomDataset(Dataset):
    def init(self, data, labels):
        self.data = data
        self.labels = labels
def __len__(self):
    return len(self.data)

def __getitem__(self, idx):
    return self.data[idx], self.labels[idx]

```
Using DataLoader to Load Data Efficiently
The DataLoader class provides an efficient way to load and iterate through the dataset in mini-batches, enabling better performance and faster training.
```python
from torch.utils.data import DataLoader
dataloader = DataLoader(CustomDataset(data, labels), batch_size=32, shuffle=True)
Example: Iterating through batches
for batch in dataloader:
    inputs, targets = batch
    print(inputs.shape, targets.shape)
```
Key Features of DataLoader

Batching: Automatically divides data into batches to optimize training.
Shuffling: Randomly shuffles data at each epoch to improve model generalization.
Parallel Processing: Uses multiple workers to speed up data loading.
Pin Memory: Optimizes memory transfer between CPU and GPU for better performance.

Using Dataset and DataLoader correctly can significantly enhance the efficiency of deep learning pipelines, making it easier to work with large-scale datasets.

Advantages of PyTorch
PyTorch is a widely used deep learning framework known for its flexibility, ease of use, and strong ecosystem. Key advantages include:


Dynamic Computation Graphs (Define-by-Run)
Unlike TensorFlow’s older static graphs, PyTorch builds computation graphs dynamically, enabling intuitive debugging, flexible model modifications, and support for variable-length inputs—essential for NLP and reinforcement learning. Learn more about dynamic computation.


Seamless GPU Acceleration
PyTorch easily utilizes GPUs with .to(device), supports mixed precision training (torch.cuda.amp), and scales efficiently with DataParallel and DistributedDataParallel for multi-GPU training.


Integration with the Python Ecosystem
PyTorch works seamlessly with NumPy, SciPy, and Pandas, allowing easy data preprocessing and interoperability with scikit-learn, TensorBoard, and wandb.


Strong Community & Ecosystem
With extensive documentation, active forums, and widespread industry adoption, PyTorch benefits from constant improvements and extensive open-source contributions.


Excellent Support for Computer Vision & NLP
With TorchVision, TorchText, and TorchAudio, PyTorch simplifies deep learning applications in vision, language processing, and audio analysis.

"
rc-learning-fork/content/courses/pytorch-hpc/transfer.md,"What is Transfer Learning?
Transfer learning allows us to reuse a pre-trained model (trained on a large dataset) for a new but related task instead of training from scratch.Typically we use foundation models for transfer learning. A foundation model is a large-scale, general-purpose AI model trained on large corpora. It serves as a base model that can be fine-tuned for specific tasks, reducing the need for extensive labeled data. Examples include GPT-4 (text generation), BERT (NLP tasks), and ResNet (computer vision). 
Why Use Transfer Learning?


Reduces Training Time: Pre-trained weights are a better starting off point than random initialization. Since the model has already learned useful features, training takes significantly less time.


Powerful Even with Small Datasets: Even with limited labeled data, transfer learning improves generalization.  

Superior Performance: Many pre-trained models outperform custom-trained models on similar tasks.  
Reduced Development Time: Leverages the architecture of models proven to work well on similar data.

Common scenarios:
 - Using a pre-trained image classifier (e.g., ResNet, VGG) for a new image classification task.
 - Applying a pre-trained language model (e.g., BERT, GPT) for sentiment analysis or named entity recognition.

Loading and Preprocessing Datasets
Data are available online from places like PyTorch (torchvision, torchtext, timm) , Huggingface, Paperswithcode, etc. For this course, we'll work with torchvision.
Image Datasets with torchvision
PyTorch provides torchvision, which includes popular datasets and transformations for preprocessing images.
Loading a Dataset (Example: CIFAR-10)
```python
import torch
import torchvision
import torchvision.transforms as transforms
Define transformations (resize, normalize)
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images for pre-trained models
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
Load the training dataset
train_dataset = torchvision.datasets.CIFAR10(root=""./data"", train=True, transform=transform, download=True)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
```

Leveraging Pre-Trained Models for New Tasks
Instead of training from scratch, we can use foundation models from torchvision.models.
```python
from torchvision import models
Load pre-trained ResNet model
resnet = models.resnet50(pretrained=True)
Print the model architecture
print(resnet)
```
The last layer (fully connected) is task-specific and needs to be modified for a new dataset.
Modifying the Last Layer
Since ResNet is trained on ImageNet (1000 classes), we modify the final layer for our dataset.
```python
import torch.nn as nn
num_classes = 10  # For CIFAR-10 classification
resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)
```
Fine-Tuning Models Efficiently with PyTorch
Fine-tuning involves unfreezing some layers of a pre-trained model and training it on a new dataset.
Freezing Some Layers
To avoid losing learned features, we freeze most layers and train only the last few.
```python
for param in resnet.parameters():
    param.requires_grad = False  # Freeze all layers
Unfreeze only the last layer
for param in resnet.fc.parameters():
    param.requires_grad = True
Define loss function and optimizer
criterion = nn.CrossEntropyLoss()  # Loss function for classification
optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)  # Train only last layer
Training the Model
epochs = 5
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
resnet.to(device)
for epoch in range(epochs):
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
    optimizer.zero_grad()
    outputs = resnet(images)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

print(f""Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}"")

Testing
correct = 0
total = 0
resnet.eval()  # Set to evaluation mode
with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = resnet(images)
        _, predicted = torch.max(outputs, 1)  # Get highest probability class
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f""Accuracy: {100 * correct / total:.2f}%"")
```
NOTE: For best model performance, it is important that your data is in the same format as the data used to train the model and that you perform the same transformations on your training data as the original model. For example, ResNet was originally trained on the ImageNet dataset which makes the standard input of Resnet a 224x224 pixel image in RGB."
rc-learning-fork/content/courses/pytorch-hpc/simple_nn.md,"Neural Network Construction
A neural network consists of multiple layers, each performing specific transformations on the input data.
{{< figure src=""/courses/pytorch-hpc/img/nn.png"" caption=""An Artificial Neural Network"" width=""500px"" >}}
Frequently used Layers in PyTorch:
```python
import torch.nn as nn
fc_layer = nn.Linear(in_features=128, out_features=64)      # Fully Connected Layers
conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)  # Convolutional Layers
rnn_layer = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True) # Recurrent Layer
dropout_layer = nn.Dropout(p=0.5)
bn_layer = nn.BatchNorm1d(num_features=64)
maxpool_layer = nn.MaxPool2d(kernel_size=2, stride=2)
avgpool_layer = nn.AvgPool2d(kernel_size=2, stride=2)
layer_norm = nn.LayerNorm(normalized_shape=128)
```
Variations:
- nn.Conv1d, nn.Conv2d, nn.Conv3d 
- nn.LSTM → Handles long dependencies.
- nn.GRU → Simpler alternative to LSTM.
- nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d
- nn.Dropout1d, nn.Dropout2d, nn.Dropout3d
- nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d
- nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d
- nn.AdaptiveAvgPool2d → Fixed output size pooling.
- nn.LayerNorm, nn.InstanceNorm2d

Building a Model and Forward Proporgation
There are two ways to define a neural network, the Sequential Class and the Module Class. 
Defining a Model with the Sequential Class
The Sequential class is a PyTorch object used to simplify the creation of NN. It allows stacking layers sequentially in the order they are defined without explicitly writing a forward() method. It's best used for simple models or prototyping.
```python
import torch.nn as nn
model = nn.Sequential(
    nn.Linear(784, 128),  # Input Layer
    nn.ReLU(),            # Activation Layer
    nn.Linear(128, 64),   # Hidden Layer
    nn.ReLU(),            # Activation Function
    nn.Linear(64,10)      # Output Layer
)
```
Defining a Model with the Module Class
The Module class is the base class for all NN in PyTorch. It provides a framework for defining and organizing the layers of a neural network and enables easy tracking of parameters, gradients, and training. The Forward function defines how the input data is processed through the layers. The .parameters() method (inherited from the Module class) returns the model parameters and is essential for training.
```python
import torch.nn as nn
class SimpleModel(nn.Module):
    def init(self):
        super(SimpleModel, self).init()
        # Define Layers
        self.fc1 = nn.Linear(in_dim,hidden_dim)
        self.fc2 = nn.Linear(hidden_dim,out_dim)
def forward(self, x):
    # Define forward pass
    x = self.fc1(x)
    x = nn.ReLU(x)
    x = self.fc2(x)
    return x

```
Backpropogation
Backpropagation updates the network’s weights based on the gradient of the loss function.
```python
Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_function = nn.MSELoss()
Example backpropagation
optimizer.zero_grad()  # Clear previous gradients
loss = loss_function(output, torch.tensor([[0.5]]))  # Compute loss
loss.backward()  # Backpropagate
optimizer.step()  # Update weights
```

Putting it All Together: Training and Testing
We've loaded our data into dataloaders
```python
from torch.utils.data import DataLoader, TensorDataset
Dummy dataset
X_train = torch.rand(100, 10)
y_train = torch.rand(100, 1)
dataset = TensorDataset(X_train, y_train)
dataloader = DataLoader(dataset, batch_size=10, shuffle=True)
We've also constructed our model, selected the best loss function and optimizer for our problem, and decided how long to trainpython
import torch.nn as nn
import torch.optim as optim
model = SimpleModel()
loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Pass model.paramters()
epochs = 20
```
Now we define our training and testing loops.
Training
In PyTorch, we explicitly define how we want to perform training with a training loop. The loop represents one forward and backward pass of one batch. 
```python
model.train()
for epoch in range(epochs):                             # loop over epochs
    for x_train, y_train in trainloader:            # loop over batches
    # Send data to GPU (code is the same if no GPU available)
    x_train, y_train = x_train.to(device) , y_train.to(device)

    # Forward Pass
    predicted = model(x_train)

    # Backpropagation - tweak the weights/biases of the NN
    optimizer.zero_grad()                            # Clear previous gradients
    loss = loss_fn(predicted, y_train.reshape(-1,1)) # Compute the loss   
    loss.backward()                                  # Backpropogate
    optimizer.step()                                 # Update weights

```
Testing
After training, we test the model with unseen data to estimate its general performance. we use torch.no_grad() to tell PyTorch not to compute the gradient. We pass test data through the network once.
```python
model.eval()
num_batch = len(testloader)
loss = 0
with torch.no_grad():
    for x_train, y_train in testloader: # loop over batches
    # send data to GPU (code is the same if no GPU available)
    x_train, y_train = x_train.to(device), y_train.to(device)

    # predict NN output
    predicted = model(x_train)

    # calculate loss
    loss +=loss_fn)predicted,y_train.reshape(-1,1).item()
    # calculate other metrics here

print result
print('avg loss: {}\t avg accuracy:{}'.format(loss/num_batch, acc/num_batch))
```
Using a Validation Set
A validation set helps tune hyperparameters and evaluate the model's generalization ability before final testing. You can use a test loop like the one above to evaluate the validation set after each training epoch.
```python
from sklearn.model_selection import train_test_split
Splitting data
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
Creating DataLoaders
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)
```"
rc-learning-fork/content/courses/pytorch-hpc/project.md,"Structuring a PyTorch Project
A well-organized project structure makes debugging, collaboration, and experimentation easier.
pytorch_project/
├── data/                 # Dataset files
├── logs/                 # TensorBoard/WandB logs
├── models/               # Trained models
├── notebooks/            # Jupyter notebooks
├── src/                  # Source code
│   ├── dataset.py        # Custom dataset loaders
│   ├── model.py          # Neural network architectures
│   ├── train.py          # Training loop
│   ├── evaluate.py       # Model evaluation
│   ├── utils.py          # Utility functions (checkpointing, logging)
├── requirements.txt      # Dependencies
├── config.yaml           # Hyperparameter config
├── train.py              # Main training script
├── slurm_train.sh        # SLURM script for HPC training
└── README.md             # Documentation

Managing Experiments with Logging Frameworks
Tracking experiments is essential for understanding model performance.
Using TensorBoard for Monitoring
TensorBoard is Tensorflow's visualization toolkit. It is compatible with PyTorch and enbles you to keep track of your experiments' metrics like lossand accuracy and easily visualize them. For more information visit: https://www.tensorflow.org/tensorboard
```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter(""logs/experiment_1"")
Log loss values
for epoch in range(10):
    loss = 0.1 * epoch  # Example loss
    writer.add_scalar(""Loss/train"", loss, epoch)
writer.close()
To visualize logs, run:
~~~sh
tensorboard --logdir=logs
~~~
Using Weights & Biases for Experiment Tracking. [Learn More](https://docs.wandb.ai/)python
import wandb
wandb.init(project=""pytorch-experiments"")
for epoch in range(10):
    wandb.log({""loss"": 0.1 * epoch})
```

Creating Reproducible Experiments
Ensure reproducibility by setting random seeds.
```python
import torch
import random
import numpy as np
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
set_seed(42)
```
Ensuring Deterministic Data Pipelines
In PyTorch's DataLoader, the shuffle parameter controls whether the data is randomly shuffled before each epoch. When shuffle is set to True, the data is reshuffled at the beginning of each epoch, ensuring that the model sees the data in a different order during training. Use shuffle for training sets and ensure that it is set to False for test and validation sets. 
python
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
rc-learning-fork/content/courses/pytorch-hpc/gpu.md,"Why Use GPUs?
Neural Networks can grow large and contain many million if not billions of parameters. ChatGPT free version (based on GPT-3.5) has 175 billion parameters, and ChatGPT paid version (based on GPT-4) has over 1 trillion parameters. Because training often involves millions of operations, we need a form of parallelization to speed up the process. GPUs are optimized for parallel computations, making them ideal for training deep learning models. They accelerate matrix operations, which are the core of neural network computations. Training large models on CPUs can take days or weeks, whereas GPUs can reduce training time significantly. All the major deep learning Python libraries (Tensorflow, PyTorch, Keras, Caffe,…) support the use of GPUs and allow users to distribute their code over multiple GPUs. New processors have been developed and optimized specifically for deep learning, like Google's Tensor Processing Unit.
Key Features

Thousands of cores designed for high throughput optimal for parallel computations
High memory bandwidth for efficient data transfer
Optimized for tensor operations (e.g., matrix multiplications)

If you’re working with a small model or smaller dataset, you may find using a GPU slows down your work.This is mainly due to the overhead cost of transfering data back and forward to the CPU. To find out if your task could benefit from using GPUs, it’s important to benchmark and profile your code. Learn more about Benchmarking and Profiling here: https://learning.rc.virginia.edu/notes/benchmark-parallel-programs/ https://learning.rc.virginia.edu/courses/python-high-performance/

PyTorch and GPUs
What is CUDA?


CUDA (Compute Unified Device Architecture) is NVIDIA’s parallel computing platform and API. It allows PyTorch to leverage GPU acceleration efficiently.


cuDNN (CUDA Deep Neural Network Library) is a GPU-accelerated library for deep learning primitives.It provides optimized implementations for convolutions, activation functions, and other operations.


These tools come installed with the GPU version of PyTorch. To use PyTorch with a CUDA compatible device (Nvidia GPUs), ensure that you install the correct version from the pytorch website. The good news; You don't need to know how to use them to take advantage of GPU acceleration!
Using GPUs in PyTorch
PyTorch makes it easy to move computations between CPU and GPU.
We need to explicitly tell PyTorch to use the CPU or the GPU. If a GPU is available, it will not automatically use it.
``` python
if torch.cuda.is_available():
    device = ""cuda""
else:
    device = ""cpu""
or simply
device = ""cuda"" if torch.cuda.is_available() else ""cpu""
When working with GPUs, inputs must be sent to the device explicitly. This includes your data and the NN model.python
Move model inputs to device
x_train, y_train = x_train.to(device) , y_train.to(device)
Move model to device
model = MyNeuralNetwork()
model.to(device)
```

Writing SLURM Scripts for PyTorch Jobs
Our High-Performance Computing (HPC) clusters use SLURM (Simple Linux Utility for Resource Management) to manage jobs. To run full training jobs submit a slurm script to the GPU partition. When using the GPU partition, your SLURM script must include the --gres=gpu option.
Example SLURM Script for PyTorch Training
Save the following script as train.slurm:
~~~sh
!/bin/bash
SBATCH -A mygroup
SBATCH -p gpu
SBATCH --gres=gpu:1
SBATCH -c 1
SBATCH -t 00:01:00
SBATCH --job-name=pytorch_training
SBATCH --output=logs/output_%j.log   # Save standard output
SBATCH --error=logs/error_%j.log     # Save error output
module purge
module load apptainer pytorch/2.0.1 
Optional: Activate virtual environment
source activate myenv
apptainer run --nv $CONTAINERDIR/pytorch-2.0.1.sif pytorch_script.py 
~~~
Then submit your job:
~~~sh
sbatch train.slurm
~~~
To show current jobs:
~~~sh
show job status squeue -u $computing_id
or
sstat job_id
~~~
Alternatively you can use the SLURM Script Generator to create your script.
For more information on SLURM visit: https://www.rc.virginia.edu/userinfo/hpc/software/pytorch/, https://www.rc.virginia.edu/userinfo/hpc/slurm/
For information on multi-GPU use: https://pytorch.org/tutorials/beginner/ddp_series_multigpu.html"
rc-learning-fork/content/courses/pytorch-hpc/optimize.md,"Optimizers and Learning Rate Adjustments
Optimizers control how a model updates its weights during training. Most optimizers are based on SGD. Choosing the right optimizer and learning rate can significantly impact training efficiency.
Types of Optimizers
| Optimizer  | Description |
|------------|------------|
| SGD (Stochastic Gradient Descent) | Standard optimizer, may converge slowly but generalizes well. |
| Momentum-Based SGD | Uses past gradients to accelerate learning. |
| Adam (Adaptive Moment Estimation) | Adjusts learning rate dynamically, commonly used. |
| RMSprop | Handles non-stationary learning rates, useful for RNNs. |
Learning Rate Schedulers
Schedulers adjust learning rates during training to improve convergence.
```python
from torch.optim.lr_scheduler import StepLR
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR every 10 epochs
```
Common Learning Rate Strategies

Step Decay: Reduces learning rate at regular intervals.
Exponential Decay: Gradually decreases learning rate.
Cyclic Learning Rates: Alternates between high and low learning rates.


Other Techniques for Avoiding Overfitting
Overfitting occurs when a model performs well on training data but poorly on new data. Below are effective strategies to prevent overfitting.
A. Dropout Regularization
Randomly drops neurons during training, forcing the network to learn more robust features.
```python
import torch.nn as nn
model = nn.Sequential(
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Dropout(0.5),  # 50% dropout
    nn.Linear(128, 10)
)
```
B. Batch Normalization
Normalizes activations across mini-batches, improving training stability.
python
model = nn.Sequential(
    nn.Linear(256, 128),
    nn.BatchNorm1d(128),  # Normalize layer activations
    nn.ReLU(),
    nn.Linear(128, 10)
)
C. L2 Regularization (Weight Decay)
Adds a penalty to large weight values, reducing overfitting.
python
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

Importance of Validation Sets
A validation set is crucial for tuning hyperparameters and detecting overfitting by assessing a model's generalization ability on unseen data. Unlike the training set, which optimizes model parameters, the validation set helps select hyperparameters such as learning rate, batch size, and regularization strength using techniques like grid search or Bayesian optimization. It also helps identify overfitting by comparing training and validation performance—if the model performs well on training data but poorly on validation data, overfitting is likely. Strategies such as early stopping, dropout, and regularization can help mitigate this. Additionally, early stopping prevents excessive training once validation performance stops improving. After tuning, a separate test set evaluates the final model’s true performance. A validation set is essential for optimizing hyperparameters, preventing overfitting, and ensuring strong generalization.
A common data split:
 - Training Set (70-80%) – Used to train the model.
 - Validation Set (10-20%) – Used to tune hyperparameters.
 - Test Set (10-20%) – Used for final evaluation.
Using a Validation Set in PyTorch
```python
from sklearn.model_selection import train_test_split
train_data, val_data = train_test_split(train_dataset, test_size=0.2, random_state=42)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False)
```

Data Augmentation Techniques
Data augmentation artificially expands training data by applying transformations. For image data these augmentations exist within transforms. You can compose a transforms pipeline that adds noise to your data, making it more likely to generalize well.
Data Augmentation with torchvision.transforms
```python
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # Flip image horizontally
    transforms.RandomRotation(30),  # Rotate image
    transforms.ColorJitter(brightness=0.2),  # Adjust brightness
    transforms.ToTensor()
])
```
Other data types have their own augmentations.

Handling Imbalanced Data
Class imbalance (e.g., detecting rare diseases) can lead to biased predictions.
Using Weighted Loss Functions
Assign higher weights to minority classes.
python
class_weights = torch.tensor([0.1, 0.9])  # Adjust for class imbalance
criterion = nn.CrossEntropyLoss(weight=class_weights)
Oversampling Minority Classes
Duplicate underrepresented samples.
```python
from torch.utils.data import WeightedRandomSampler
class_counts = [1000, 200]  # Example: Majority vs. Minority class
weights = 1. / torch.tensor(class_counts, dtype=torch.float)
sampler = WeightedRandomSampler(weights, len(weights))
train_loader = DataLoader(dataset, batch_size=32, sampler=sampler)
```
SMOTE (Synthetic Minority Over-sampling Technique)
Creates synthetic samples for the minority class.
```python
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)
```"
rc-learning-fork/content/courses/pytorch-hpc/_index.md,"Overview
This short course provides a practical introduction to building artificial neural networks using PyTorch, a powerful and flexible deep l
earning framework. The course covers the fundamentals of PyTorch, including tensors, automatic differentiation, and model building. Participants will 
learn how to construct, train, and optimize neural networks while exploring real-world applications. The course also includes best practices for experiment tracking and project setup. By the end of the course, participants will have a solid foundation in PyTorch and be able to develop deep learning models for a variety of tasks.
Table of Contents
1. Introduction to PyTorch

Background
Overview of PyTorch and its core components
Advantages of PyTorch

2. Building a Simple Neural Network

Neural Network Construction
Training and Testing
Validation Sets

3. GPU Acceleration for Deep Learning

Why use GPUs?
PyTorch and GPUs
Best practices for usig GPUs
Writing SLURM scripts for PyTorch jobs

4.Transfer Learning

What is Transfer Learning?
Loading and Preprocessing Datasets
Leaveraging Pre-Trained Models for New Tasks

5. Optimizing Neural Networks

Optimizers and learning rate adjustments  
Techniques for avoiding overfitting (dropout, batch normalization)  
Validation Sets
Data augmentation techniques  
Handling imbalanced data 

6. PyTorch Experimentation and Project Setup

Structuring a PyTorch project  
Managing experiments with logging frameworks (TensorBoard, Weights & Biases)  
Creating reproducible experiments
Using torch.utils.data.DataLoader for efficient batch processing  
Debugging PyTorch models

This course is not an introduction to deep learning. It will not cover deep learning theory or mathematical concepts. This course will cover how to develop ANNs and CNNs.
You are strongly encouraged to repeat the examples and do the exercises.
Prerequisites

Deep Learning Theory 
Python Programming
"
rc-learning-fork/content/courses/multicore-gpu-introduction/index.md,"This short course is an introduction to programming for shared-memory devices, mostly for multicore systems but with some GPU programming. Attendees should be proficient at programming in Fortran, C/C++, or Python.
Please download the slides here
Also download the labs for your language:
Fortran/C/C++ OpenMP and OpenACC
Python Multiprocessing and GPU Lab
Bonus topic: Serial optimization
Optimization
Optimization Lab
Code can be copied on Rivanna from /share/resources/tutorials/shmem
Files for optimization studies are at /share/resources/tutorials/hpc/optimization"
rc-learning-fork/content/courses/matlab-programming/effective_programming.md,"This workshop provides hands-on experience using the features in the MATLAB language to write efficient, robust, and well-organized code. These concepts form the foundation for writing full applications, developing algorithms, and extending built-in MATLAB capabilities. Details of performance optimization, as well as tools for writing, debugging, and profiling code are covered. Topics include:

Utilizing development tools
Verifying application behavior
Creating robust applications
Structuring code
Structuring data
Managing data efficiently

Beginners should complete the MATLAB Fundamentals tutorial before beginning this workshop.
This workshop uses selected topics from the online self-paced course from the MathWorks in the link below. Participants must have a Mathworks account in order to access the links in this document.
Matlab Academy: Matlab Programming Techniques"
rc-learning-fork/content/courses/matlab-programming/getting_started.md,"Entering Commands
Exercise: Order of Operations
Exercise: Valid Variable Names and Cleaning Up
Exercise: Change Output Display
Getting Data into Matlab
Video: Import Tool
Exercise: Using the Variable Editor
Exercise: Saving Modifications
Obtaining Help
Video: Using MATLAB's Documentation
Exercise: Open and Use Function Documentation
Plotting and Common Modifications
Plotting
Exercise: Plotting Gasoline Prices
Exercise: Plot Options
Annotating Plots
Exercise: Axis Labels and Title
Exercise: More Plot Annotation Options
Axis Control
Exercise: Modify Axis Properties
Exercise: Modify Gasoline Price Plot Axes
Working with Live scripts
Create and Run a Script
Exercise: Modifying and Running Scripts
Code Sections
Exercise: International Gasoline Prices
Comments and Text
Exercise: International Gasoline Prices
Creating and Manipulating Arrays
Manually Entering Arrays
Exercise: Creating Vectors
Exercise: Creating Matrices
Creating Evenly-Spaced Vectors
Exercise: Use Colon Operator and Linspace
Concatenating Arrays
Exercise: Concatenating Arrays
Exercise: Creating and Concatenating Arrays
Array Creation Functions
Exercise: Creating Arrays
Exercise: Creating Arrays of Random Numbers
Reshaping Arrays
Exercise: Reshaping a Matrix
Exercise: Overall Average Electricity Revenue
Accessing Data in Arrays
Indexing into Vectors
Exercise: Accessing and Modifying Vector Values
Exercise: Index Using Variables and Keywords
Accessing Multiple Elements
Exercise: Index with Vectors
Exercise: Modify Multiple Elements
Accessing Data in Matrices
Exercise: Row,Column Indexing
Exercise: Extract a Single Element
Exercise: Matrix Indexing with Vectors
Exercise: Extract Multiple Elements
Mathematical and Statistical Operations with Arrays
Performing Operations on Arrays
Exercise: Basic Array Arithmetic & Functions
Video: Element-wise vs. Matrix Operations
Exercise: Element-wise Arithmetic
Exercise: Operating on Compatibly-Sized Arrays
Matrix Multiplication
Exercise: Matrix Operations
Exercise: Extract Multiple Elements
Calculating Statistics of Vectors
Exercise: Basic Statistics
Exercise: Average Gasoline Prices
Using Statistical Operations on Matrices
Exercise: Descriptive Statistics
Exercise: Statistics for Each Row
Exercise: Correlation
Visualizing Data in 2D and 3D
Identifying Available Vector Plot Types
Exercise: stairs, stem, and area
Exercise: Gasoline Prices Scatter Plot
Creating Arrays of Text
Exercise: Creating Character Arrays
Exercise: Creating Multiline Annotations
Customizing Plot Properties
Exercise: Customizing Plot Properties
Exercise: Specifying Color
Plotting Multiple Columns
Exercise: Plotting Matrix Columns
Exercise: Plotting Matrix Columns Against a Vector
Visualizing Matrices
Exercise: Surface, Mesh, and Contour
Exercise: Visualize Electricity Data
Conditional Data Selection
Logical Operations and Variables
Exercise: Logical Operations
Exercise: Logical Operations with Arrays
Exercise: Finding Winning Team Records
Counting Elements
Exercise: Count true Values in a Logical Vector
Exercise: Identify Locations of true Values
Logical Indexing
Video: Logical Indexing
Exercise: Logical Indexing
Exercise: Selecting Teams with Winning Home Record
Tables of Data
Storing Data in a Table
Exercise:  Import Table Data
Exercise: Create a Table from Workspace Variables
Sorting Table Data
Exercise: Using sortrows
Extracting Portions of a Table
Exercise: Numeric Indexing
Exercise: Variable Name Indexing
Exercise: Table Indexing
Extracting Data from a Table
Exercise: Indexing with Dot Notation
Exercise: Indexing with Curly Braces
Exporting Tables
Exercise: Using writetable
Organizing Data
Combining Tables
Exercise: Concatenating Tables
Exercise: Join Tables
Table Properties
Exercise: Table Properties
Exercise: Accessing Table Properties
Indexing into Cell Arrays
Exercise: Indexing Into Cell Arrays
Exercise: Extract Multiple Elements
Working with Dates and Times
Video: Data Types for Dates and Times
Exercise: Applying Functions to Datetimes
Operating on Dates and Times
Exercise: Operating on Dates and Times
Exercise: Working with Durations
Representing Discrete Categories
Exercise: Converting to and Operating on Categoricals
Exercise: Category Names and Ordinals
Preprocessing Data
Normalizing Data
Exercise: Normalize Matrices
Exercise: Calculate Average Daily Electricity Usage
Working with Missing Data
Exercise:  Ignoring NaNs in calculations
Exercise: Locating Missing Data
Exercise: Removing Rows with Missing Data
Interpolating Missing Data
Exercise: Filling Missing Values
Exercise: Interpolating Irregularly-Spaced Data
Common Data Analysis Techniques
Moving Window Operations
Exercise: Moving Average
Exercise: Smoothing Electricity Data
Linear Correlation
Exercise: Plotting Electricity Usage
Exercise: Correlations in Electricity Usage
Polynomial Fitting
Exercise: Fit a Line
Exercise: Fit a Polynomial to Electricity Usage
Programming Constructs
User Interaction
Exercise: Gathering Input
Exercise: Dialog Box Output
Decision Branching
Exercise: Using if-else
Exercise: Using if-elseif-else
Exercise: Using switch-case
Determining Size
Exercise: Using size and numel
Exercise: Using length
For Loops
Exercise: Looping Through a Vector
Exercise: Finding the Fibonacci Sequence
While Loops
Exercise: Using a While Loop
Exercise: Finding eps
Increasing Automation with Functions
Creating and Calling Functions
Exercise: Create and Call a Function
Exercise: Electricity Usage Analysis Function
Function Files
Exercise: Compare Using a Tolerance
Exercise: Custom Statistics
Function Workspaces
Quiz: Function workspaces
MATLAB Path and Calling Precedence
Exercise: MATLAB Calling Precedence
Troubleshooting Code
Code Analyzer
Exercise: Remove Code Analyzer Warnings
Debugging Run-Time Errors
Quiz: Matrix Indexing with Vectors
Additional Resources
Resources available through the MathWorks"
rc-learning-fork/content/courses/matlab-programming/fundamentals.md,"This tutorial provides a comprehensive introduction to the MATLAB® technical computing environment. No prior programming experience or knowledge of MATLAB is assumed. Themes of data analysis, visualization, modeling, and programming are explored throughout the course. Topics include:

Working with the MATLAB user interface
Entering commands and creating variables
Analyzing vectors and matrices
Visualizing vector and matrix data
Working with data files
Working with data types
Automating commands with scripts
Writing programs with branching and loops
Writing functions

This tutorial uses selected topics from the online self-paced course from the MathWorks in the link below. Participants must have a Mathworks account in order to access these materials."
rc-learning-fork/content/courses/matlab-programming/techniques.md,"Matlab Programming Review
The section is a review of the basic programming constructs presented in the
Matlab Fundamentals tutorial.
Logical operations

Documentation: if,elseif,else
Exercise: Logical Operations
Decision Branching
{{< figure src=""/courses/matlab-programming/branching1.png""  >}}


Exercise: Using if-elseif-else
The switch-case Construction
Exercise: Using switch-case
For Loops
Video: For Loops

{{< figure src=""/courses/matlab-programming/forLoop1.png""  >}}


Exercise: Looping Through a Vector
While Loops
To use a for loop, you need to know in advance how many iterations are required. If you want to execute a block of code repeatedly until a result is achieved, you can use a while-loop.


Slide: The while-Loop Construction


{{< figure src=""/courses/matlab-programming/whileLoop1.png""  >}}
{{< figure src=""/courses/matlab-programming/whileLoop2.png""  >}}


Exercise: Finding eps
Creating and Calling Functions

Slide: Creating and Calling Functions


{{< figure src=""/courses/matlab-programming/function1.png""  >}}


Exercise: Create and Call a Function
Slide: Creating Functions Files


Calling Function Files
{{< figure src=""/courses/matlab-programming/function2.png""  >}}


Exercise: Compare Using a Tolerance


{{< figure src=""/courses/matlab-programming/function3.png""  >}}


Storing Heterogeneous Data
Matlab Data Types

Slide: MATLAB Data Types
Exercise: Creating Variables of a Specific Data Type
Table Basics

Documentation: Tables
Exercise: Create a Table from Workspace Variables
Extracting Data from a Table

Exercise: Extracting Data from a Table: Dot Notation
Exercise: Extracting Data from a Table: Curly Braces
Cell Array Basics

Documentation: Cell Arrays
Exercise: Creating and Concatenating Cell Arrays
Exercise: Cell Array Extraction
Structures

Documentation: Structures
Exercise: Create a Structure and Access Fields
Exercise: Create a Structure Array
Structuring Heterogeneous Data
Structuring Data Considerations

Slide: Structuring Data Considerations
Exercise: Cell Array
Exercise: Structure Array
Exercise Table
Extracting Multiple Elements from Cell and Structure arrays

Slide: Gathering Output
Exercise: Multiple Elements from a Cell Array
Exercise: Multiple Elements from a Structure Array

Function Handles

Using Function Handles
Exercise: Optimization

Applying Scalar Functions to Arrays

{{< figure src=""/courses/matlab-programming/cellfun1.png""  >}}



Video: Applying Scalar Functions to Arrays
Exercise: Applying Functions to Groups
Exercise: cellfun Basics
Converting Data Types

Slide: Converting Data Types
Exercise: Numeric to Cell Array
Exercise: Converting Strings
Managing Data Efficiently

Slide: Datatypes and Memory
Quiz 1
Quiz 2
Video: Preallocation
Slide: Preallocating Numeric, Cell, and Structure Arrays
Exercise: Preallocation Experiment
Vectorization

{{< figure src=""/courses/matlab-programming/vectorization1.png""  >}}
{{< figure src=""/courses/matlab-programming/vectorization2.png""  >}}
Copy-on-write with Function Parameters

Slide: Copy-on-write Behavior
In-place Optimizations

Slide: In-place Optimizations
Nested Functions
{{< figure src=""/courses/matlab-programming/nestedFunction1.png""  >}}

Slide: Nested Functions
Exercise: Create a Nested Functions
Creating Flexible functions

Video: Creating Flexible Function Interfaces
Creating Multiple Interfaces with Wrapper Functions

Slide: Separating the Interface from the Algorithm
Exercise: Create a Wrapper Function with a Fixed Interface
Setting Default Input Values

Slide: Setting Default Input Values
Missing Input Arguments

Slide: Missing Input Arguments
Slide: Empty Input Arguments
Exercise: Skipping the Input
Allowing Any Number of Inputs

Slide: Functions with a Variable Number of Inputs
Slide: Variable Length Input Argument List
Slide: Passing Argument Lists to Another Function
Allowing a Variable Number of outputs

Slide: Defining Different Behaviors Based on Outputs
Exercise: Matrix or Vector Output
Changing the Function Interface with Anonymous Functions

Slide: Modifying Function Interfaces
Slide: Wrapping Functions with Anonymous Functions
Exercise: Write and Use an Anonymous Function
Slide: Common Function Handle Uses
Exercise: Change Function Interface with an Anonymous Function
Creating Robust Applications
Restricting Access Using Private Functions
{{< figure src=""/courses/matlab-programming/privateFunction1.png""  >}}

Slide: Making Functions Private


{{< figure src=""/courses/matlab-programming/privateFunction2.png""  >}}
Writing Local Functions
{{< figure src=""/courses/matlab-programming/localFunction1.png""  >}}
{{< figure src=""/courses/matlab-programming/localFunction2.png""  >}}
{{< figure src=""/courses/matlab-programming/localFunction3.png""  >}}
Comparison of Functions
{{< figure src=""/courses/matlab-programming/compareFunction1.png""  >}}

Exercise: Create Local Functions
Validating Function Inputs
{{< figure src=""/courses/matlab-programming/validateInput1.png""  >}}
{{< figure src=""/courses/matlab-programming/validateInput2.png""  >}}

Quiz
Verifying Application Behavior
Why Use a Testing Framework?

Video: Why Use a Testing Framework
What is a Test
{{< figure src=""/courses/matlab-programming/test1.png""  >}}

Slide: Elements of a Test
""is"" Functions
{{< figure src=""/courses/matlab-programming/isFunction.png""  >}}

Exercise: isequal Versus ==
Test Response
{{< figure src=""/courses/matlab-programming/assertFunction.png""  >}}

Exercise: assert
Writing and Running a Test Script
{{< figure src=""/courses/matlab-programming/testScript1.png""  >}}
Writing a Test Script
{{< figure src=""/courses/matlab-programming/testScript2.png""  >}}

Exercise: Write a Test Script
Running a Test Script

Slide: Running a Test Script
Exercise: Run a Test Script
Exercise: Fix Broken Tests
Utilizing Development Tools
Developing and Maintaining Code
{{< figure src=""/courses/matlab-programming/devTools1.png""  >}}
Folder Reports

Slide: Folder Reports
Errors and Debugging

Video: Different Kinds of Errors
Code Analyzer
{{< figure src=""/courses/matlab-programming/codeAnalyzer1.png""  >}}

Slide: Suppressing and Fixing Code Analyzer Warnings
Exercise: Remove Code Analyzer Warnings
Debugging Runtime Errors
{{< figure src=""/courses/matlab-programming/debugRun1.png""  >}}

Slide: Debugging Run-Time Errors
Measuring Performance
{{< figure src=""/courses/matlab-programming/performance1.png""  >}}

Slide: Tic and Toc
Finding Bottlenecks
{{< figure src=""/courses/matlab-programming/bottleneck1.png""  >}}

Video: The MATLAB Profiler"
rc-learning-fork/content/courses/matlab-programming/_index.md,"MATLAB is an integrated technical computing environment from the MathWorks that combines array-based numeric computation, advanced graphics and visualization, and a high-level programming language. Separately licensed toolboxes provide additional domain-specific functionality.
Participants must have a Mathworks account in order to access the links to the online tutorials in this document.
Matlab Academy:Fundamentals of Matlab
Video: Fundamentals of  Matlab
Quick Reference Guide (used for presentation)"
rc-learning-fork/content/courses/r-intro/index.md," We will introduce the R statistical computing environment as well as RStudio. We will cover very basic functionality, including variables, functions, importing data, and the fundamentals of inspecting data frame objects. The course assumes little to no experience with statistical computing or R.
{r, echo=FALSE, message=FALSE, eval=TRUE, warning=FALSE}
library(knitr)
opts_chunk$set(message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE, results=""show"", cache=FALSE)
opts_knit$set(root.dir = ""../../static/data/"")
options(digits=3)
options(max.print=200)
.ex <- 1 # Track ex numbers w/ hidden var. Increment each ex: `r .ex``r .ex=.ex+1`
RStudio
Let's start by learning about RStudio. R is the underlying statistical computing environment, but using R alone is no fun. RStudio is a graphical integrated development environment that makes using R much easier.

Panes in RStudio. There are four panes, and their orientation is configurable under ""Tools -- Global Options."" You don't have to do it this way, but I usually set up my window to have:
Editor in the top left
Console top right
Environment/history on the bottom left
Plots/help on the bottom right.  


Projects: first, start a new project in a new folder somewhere easy to remember. When we start reading in data it'll be important that the code and the data are in the same place. Creating a project creates a Rproj file that opens R running in that folder. This way, when you want to read in dataset whatever.txt, you just tell it the filename rather than a full path. This is critical for reproducibility, and we'll talk about that more later.
Code that you type into the console is code that R executes. From here forward we will use the editor window to write a script that we can save to a file and run it again whenever we want to. We usually give it a .R extension, but it's just a plain text file. If you want to send commands from your editor to the console, use CMD+Enter (Ctrl+Enter on Windows).
Anything after a # sign is a comment. Use them liberally to comment your code.

Basic operations
R can be used as a glorified calculator. Try typing this in directly into the console. Make sure you're typing into the editor, not the console, and save your script. Use the run button, or press CMD+Enter (Ctrl+Enter on Windows).
{r calculator}
2+2
5*4
2^3
6/7
R Knows order of operations and scientific notation.
{r calculator2}
2+3*4/(5+3)*15/2^2+3*4^2
5e4
However, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator <- and the value we want to give it:
{r assignment}
weight_kg <- 55
<- is the assignment operator. It assigns values on the right to objects on the left, it is like an arrow that points from the value to the object. Mostly similar to = but not always. Learn to use <- as it is good programming practice. Using = in place of <- can lead to issues down the line. The keyboard shortcut for inserting the <- operator is Alt-dash.
Objects can be given any name such as x, current_temperature, or subject_id. You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they represent the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it's allowed, it's best to not use other function names, which we'll get into shortly (e.g., c, T, mean, data, df, weights). In doubt check the help to see if the name is already in use. It's also best to avoid dots (.) within a variable name as in my.dataset. It is also recommended to use nouns for variable names, and verbs for function names.
When assigning a value to an object, R does not print anything. You can force to print the value by typing the name:
{r printAssignment}
weight_kg
Now that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight in pounds (weight in pounds is 2.2 times the weight in kg).
{r modAssignment}
2.2 * weight_kg
We can also change a variable's value by assigning it a new one:
{r newAssignment}
weight_kg <- 57.5
2.2 * weight_kg
This means that assigning a value to one variable does not change the values of other variables. For example, let's store the animal's weight in pounds in a variable.
{r calculationWithVar}
weight_lb <- 2.2 * weight_kg
and then change weight_kg to 100.
{r modAssignment2}
weight_kg <- 100
What do you think is the current content of the object weight_lb? 126.5 or 220?
You can see what objects (variables) are stored by viewing the Environment tab in Rstudio. You can also use the ls() function. You can remove objects (variables) with the rm() function. You can do this one at a time or remove several objects at once. You can also use the little broom button in your environment pane to remove everything from your environment.
{r rm, eval=FALSE}
ls()
rm(weight_lb, weight_kg)
ls()
weight_lb # oops! you should get an error because weight_lb no longer exists!

EXERCISE r .ex``r .ex=.ex+1
What are the values after each statement in the following?
{r ex1}
mass <- 50              # mass?
age  <- 30              # age?
mass <- mass * 2        # mass?
age  <- age - 10        # age?
mass_index <- mass/age  # massIndex?

Functions
R has built-in functions.
```{r fns}
Notice that this is a comment.
Anything behind a # is ""commented out"" and is not run.
sqrt(144)
log(1000)
```
Get help by typing a question mark in front of the function's name, or help(functionname):
help(log)
?log
Note syntax highlighting when typing this into the editor. Also note how we pass arguments to functions. The base= part inside the parentheses is called an argument, and most functions use arguments. Arguments modify the behavior of the function. Functions some input (e.g., some data, an object) and other options to change what the function will return, or how to treat the data provided. Finally, see how you can nest one function inside another (here taking the square root of the log-base-10 of 1000).
{r log}
log(1000)
log(1000, base=10)
log(1000, 10)
sqrt(log(1000, base=10))

EXERCISE r .ex``r .ex=.ex+1
See ?abs and calculate the square root of the log-base-10 of the absolute value of -4*(2550-50). Answer should be 2.

Data Frames
There are lots of different basic data structures in R. If you take any kind of longer introduction to R you'll probably learn about arrays, lists, matrices, etc. We are going to skip straight to the data structure you'll probably use most -- the data frame. We use data frames to store heterogeneous tabular data in R: tabular, meaning that individuals or observations are typically represented in rows, while variables or features are represented as columns; heterogeneous, meaning that columns/features/variables can be different classes (on variable, e.g. age, can be numeric, while another, e.g., cause of death, can be text). 
Recommended reading: Review the Introduction (10.1) and Tibbles vs. data.frame (10.3) sections of the R for Data Science book. We will initially be using the read_* functions from the readr package. These functions load data into a tibble instead of R's traditional data.frame. Tibbles are data frames, but they tweak some older behaviors to make life a little easier. These sections explain the few key small differences between traditional data.frames and tibbles. 
Our data
The data we're going to look at is cleaned up version of a gene expression dataset from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. This data is from a gene expression microarray, and in this paper the authors are examining the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that: 

Raise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of >25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions. 
Respond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient.

You can download the cleaned up version of the data at the link above. The file is called brauer2007_tidy.csv. Later on we'll actually start with the original raw data (minimally processed) and manipulate it so that we can make it more amenable for analysis. 
Reading in data
dplyr and readr
There are some built-in functions for reading in data in text files. These functions are read-dot-something -- for example, read.csv() reads in comma-delimited text data; read.delim() reads in tab-delimited text, etc. We're going to read in data a little differently here using the readr package. When you load the readr package, you'll have access to very similar looking functions, named read-underscore-something -- e.g., read_csv(). You have to have the readr package installed to access these functions. Compared to the base functions, they're much faster, they're good at guessing the types of data in the columns, they don't do some of the other silly things that the base functions do. We're going to use another package later on called dplyr, and if you have the dplyr package loaded as well, and you read in the data with readr, the data will display nicely. 
First let's load those packages.
{r loadpkgs}
library(readr)
library(dplyr)
If you see a warning that looks like this: Error in library(packageName) : there is no package called 'packageName', then you don't have the package installed correctly.
read_csv()
Now, let's actually load the data. You can get help for the import function with ?read_csv. When we load data we assign it to a variable just like any other, and we can choose a name for that data. Since we're going to be referring to this data a lot, let's give it a short easy name to type. I'm going to call it ydat. Once we've loaded it we can type the name of the object itself (ydat) to see it printed to the screen. 
{r loaddata}
ydat <- read_csv(file=""brauer2007_tidy.csv"")
ydat
Take a look at that output. The nice thing about loading dplyr and reading in data with readr is that data frames are displayed in a much more friendly way. This dataset has nearly 200,000 rows and 7 columns. When you import data this way and try to display the object in the console, instead of trying to display all 200,000 rows, you'll only see about 10 by default. Also, if you have so many columns that the data would wrap off the edge of your screen, those columns will not be displayed, but you'll see at the bottom of the output which, if any, columns were hidden from view. If you want to see the whole dataset, there are two ways to do this. First, you can click on the name of the data.frame in the Environment panel in RStudio. Or you could use the View() function (with a capital V).
{r view, eval=FALSE}
View(ydat)
Inspecting data.frame objects
Built-in functions
There are several built-in functions that are useful for working with data frames.

Content:
head(): shows the first few rows
tail(): shows the last few rows


Size:
dim(): returns a 2-element vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object)
nrow(): returns the number of rows
ncol(): returns the number of columns


Summary:
colnames() (or just names()): returns the column names
str(): structure of the object and information about the class, length and content of each column
summary(): works differently depending on what kind of object you pass to it. Passing a data frame to the summary() function prints out useful summary statistics about numeric column (min, max, median, mean, etc.)



{r data_frame_functions}
head(ydat)
tail(ydat)
dim(ydat)
names(ydat)
str(ydat)
summary(ydat)
Other packages
The glimpse() function is available once you load the dplyr library, and it's like str() but its display is a little better.
{r}
glimpse(ydat)
The skimr package has a nice function, skim, that provides summary statistics the user can skim quickly to understand your data. You can install it with install.packages(""skimr"") if you don't have it already.
{r, eval = FALSE}
library(skimr)
skim(ydat)
Accessing variables & subsetting data frames
We can access individual variables within a data frame using the $ operator, e.g., mydataframe$specificVariable. Let's print out all the gene names in the data. Then let's calculate the average expression across all conditions, all genes (using the built-in mean() function).
```{r}
display all gene symbols
ydat$symbol
mean expression
mean(ydat$expression)
```
Now that's not too interesting. This is the average gene expression across all genes, across all conditions. The data is actually scaled/centered around zero:
{r histogram_expression_values, echo=F}
library(ggplot2)
ggplot(ydat, aes(expression)) + 
  geom_histogram(bins=100)+ 
  xlab(""Expression"") + 
  ggtitle(""Histogram of expression values"") +
  theme_bw()
We might be interested in the average expression of genes with a particular biological function, and how that changes over different growth rates restricted by particular nutrients. This is the kind of thing we're going to do in the next section.

EXERCISE r .ex``r .ex=.ex+1

What's the standard deviation expression (hint: get help on the sd function with ?sd).
What's the range of rate represented in the data? (hint: range()).


BONUS: Preview to advanced manipulation
What if we wanted to show the mean expression, standard deviation, and correlation between growth rate and expression, separately for each limiting nutrient, separately for each gene, for all genes involved in the leucine biosynthesis pathway?
{r, results='hide'}
ydat %>% 
  filter(bp==""leucine biosynthesis"") %>% 
  group_by(nutrient, symbol) %>% 
  summarize(mean=mean(expression), sd=sd(expression), r=cor(rate, expression))
{r, echo=FALSE}
ydat %>% 
  filter(bp==""leucine biosynthesis"") %>% 
  group_by(nutrient, symbol) %>% 
  summarize(mean=mean(expression), sd=sd(expression), r=cor(rate, expression)) %>% 
  mutate_each(funs(round(., 2)), mean:r)
```{r makedata, include=F, eval=F}
Here's how I made the data
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw())
read original data
(ydat <- read_delim(""data/brauer2007_orig.txt"", delim=""\t""))
change variable WS||WS to ::
(ydat <- ydat %>% mutate(NAME=gsub(""\s\|\|\s"", ""::"", NAME, perl=TRUE)))
separate on :: into gene symbol, go-bp, go-mf, syssymbol, and somenumber
(ydat <- ydat %>% separate(NAME, c(""symbol"", ""bp"", ""mf"", ""systematic_symbol"", ""somenumber""), sep = ""::""))
replace missing with NA
(ydat[ydat==""""] <- NA)
remove things where syssymbol is missing
(ydat <- ydat %>% filter(!is.na(systematic_symbol) & systematic_symbol!=""""))
write out go terms to join to later
ydat %>% select(systematic_symbol, bp, mf) %>% distinct %>% write_csv(""data/brauer2007_syssymbol2go.csv"")
unite the symbol column again
(ydat <- ydat %>% unite(NAME, symbol, systematic_symbol, somenumber, sep=""::"") %>% select(-bp, -mf))
write out the messy data that you'll use for data cleaning class
ydat %>% write_csv(""data/brauer2007_messy.csv"")
rm(ydat)
(ydat <- read_csv(""data/brauer2007_messy.csv""))
(syssymbol2go <- read_csv(""data/brauer2007_syssymbol2go.csv""))
ydat <- ydat %>% 
  separate(NAME, c(""symbol"", ""systematic_symbol"", ""somenumber""), sep = ""::"") %>%
  select(-somenumber, -GID, -YORF, -GWEIGHT) %>%
  gather(sample, expression, G0.05:U0.3) %>%
  separate(sample, c(""nutrient"", ""rate""), sep = 1) %>%
  filter(!is.na(expression), systematic_symbol != """") %>% 
  inner_join(syssymbol2go, by=""systematic_symbol"") %>% 
  mutate(nutrient = plyr::mapvalues(nutrient, 
                                    from=c(""G"", ""L"", ""P"", ""S"", ""N"", ""U""), 
                                    to=c(""Glucose"", ""Leucine"", ""Phosphate"", ""Sulfate"", ""Ammonium"", ""Uracil"")))
ydat %>% write_csv(""data/brauer2007_tidy.csv"")
```
```{r, include=FALSE, eval = TRUE}
find packages attached during for this Rmd session and their versions
package <- installed.packages()[names(sessionInfo()$otherPkgs), ""Package""]
version <- installed.packages()[names(sessionInfo()$otherPkgs), ""Version""]
thesepkgs <- data.frame(package, version)
if no non-base packages installed skip
if(nrow(thesepkgs) == 0) invisible()
find package csv file
fp <- ""../../packages.csv""
if it exists find read in contents ... combine with attached pkgs and dedupe
if(file.exists(fp)) {
pkgs <- read.csv(fp)
pkgs <- 
    rbind(pkgs, thesepkgs)
pkgs <- pkgs[!duplicated(pkgs),]
} else{
pkgs <- thesepkgs
}
write out new package.csv file
write.table(pkgs, 
          file = fp,
          sep = "","",
          row.names = FALSE)
```"
rc-learning-fork/content/courses/r-shiny/_index.md,Shiny is a package that lets you build interactive Web apps using the R language.
rc-learning-fork/content/courses/python-high-performance/mpi-parallelization.md,"The most widely used general-purpose communications library for distributed parallelization is MPI, the Message Passing Interface.  
MPI works on multicore systems as well as multi-node, but the programming model is still different from threads.  MPI starts a specified number of independent copies of the program, which then communicate with one another through the MPI library.
In MPI each process has an ID called its rank.  Ranks are numbered from 0 to n-1, for n processes. No process shares memory with any other process whether running on the same node or not.  All communications occur over the network.  To use MPI the programmer must manage the distribution of the data to different processes and the communication among the processes. Each process runs the same script or program, so any difference in behavior by rank must be programmed.  
MPI messages are identified by an ""envelope"" of metadata. This consists of the destination, the source (the ""return address""), a communicator (a group of processes that will be exchanging information), and optionally a tag.  A communicator consisting of all processes, called COMM_WORLD, is set up when the MPI program is initiated.
The process with rank 0 is usually called the root process.  Since the minimum number of processes is 1, the root process is the only one that is guaranteed to be present.  For this reason it is usually used to manage various bookkeeping tasks as well as input/output.
The mpi4py Package
The most popular direct way to use MPI with Python is the mpi4py package.  If you are running on your own multicore system you can install it directly with conda as usual. If you are running on an HPC cluster that uses a resource manager such as Slurm, the process is more complicated.
When installing it into your environment in an HPC cluster, you should not use conda install because conda will install precompiled binaries and you must use a version of MPI that will correctly communicate with the system network and the resource manager. We suggest creating a conda environment for your MPI programs.
bash
conda create -n ""mpienv"" python=3.11
In a cluster environment, it is best to install mpi4py from the conda-forge channel following their instructions here. 
This will install dummies into your environment that will be replaced by the external library when the package is imported.  Do not try to install both MPICH and OpenMPI; use the one most appropriate to your system. In our example, we will install mpi4py with OpenMPI.
First load a compiler. Generally you will want to use a version of gcc that is compatible with that used for your installation of Python. Running the python interpreter from the command line will show this
```
python
Python 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.



For the above example, with the versions of gcc available on our system, this is gcc 11.4.0. Then check for available versions of OpenMPI (please use OpenMPI on UVA HPC systems) withbash
module spider openmpi
Now view the available external versions of OpenMPI from conda-forgebash
conda search -f openmpi -c conda-forge
For this example, we will use OpenMPI 4.1.4.bash
conda install -c conda-forge ""openmpi=4.1.4=external_*""
``
You *must* use theexternal` hook so that mpi4py will link to a version of MPI that will communicate with the cluster resource manager.  Do not install openmpi directly from conda-forge.



Once this installation has completed, you can install mpi4py
conda install -c conda-forge mpi4py
MPI consists of dozens of functions, though most programmers need only a fraction of the total.  The mpi4py package has implemented most of them using a ""Pythonic"" syntax, rather than the more C-like syntax used by other languages.  One peculiarity of mpi4py is that only particular types may be communicated; in particular, only NumPy NDArrays or pickled objects are supported.  To simplify our discussion, we will ignore the versions for pickled objects.  The requirement that NumPy arrays be sent means that even single values must be represented as one-element NumPy arrays.
MPI requires more advanced programming skills so we will just show an example here.  Our Monte Carlo pi program is well suited to MPI so we can use that. For much more information about programming with MPI in Python, as well as C++ and Fortran, please see our short course.
{{% code-download file=""/courses/python-high-performance/codes/MonteCarloPiMPI.py"" lang=""python"" %}}
The first invocation of MPI is the call to Get_rank.  This returns the rank of the process that calls it.  Remember that each MPI process runs as a separate executable; the only way their behaviors can be controlled individually is through the rank.  This call also initializes MPI; a separate MPI.Init is not required. The next line allows us to find out how many processes are in COMM_WORLD.  The number of processes for MPI programs is always set outside the program, and should never be hardcoded into the source code.
Next we divide up the number of ""throws"" into roughly equal chunks, just as we did in the corresponding Multiprocessing example.  The same list is generated, but we use myrank to select the element of the list that each rank will use.
After this each process invokes the pi routine using myNumPoints for its rank.  We need to collect all the estimates and average them.  The reduction collects the results from each rank (in rank order) and applies the ""op"" (operation).  A reduction must be a binary operation that returns another object of the same type; the operation is applied along the sequence.  In this case we want the sum so we add the result from rank 0 to that from rank 1, then we take that sum and add the result from rank 2, and so forth.  Dividing by the number of processes provides the average.  
The Reduce function returns each result to the process regarded as the root, which would normally be 0 as it is in this case, and root carries out the operation.  Thus only the root process knows the final result.  We then select root to print out the value.
This example reruns the problem without distributing the throws in order to obtain a serial time for comparison purposes.  Of course, a real application would not do that.
Running the MPI Python Program on an HPC System
In order to launch multiple tasks (or processes) of our program, we run this program through the MPI executor.  On our HPC cluster this is srun.
srun python MonteCarloPiMPI.py 1000000000
Note: you cannot launch the MPI program with srun on the login nodes.  In order to execute our program on designated compute node(s), we need to write a simple bash script that defines the compute resources we need.  We call this our job script.  For our example, the job script pimpi.sh looks like this:
{{% code-download file=""/courses/python-high-performance/codes/pympi.slurm"" lang=""bash"" %}}
The #SBATCH directives define the compute resources (-N, --ntasks-per-node, -p), compute wall time (-t), and the allocation account (--account) to be used. -N 1 specifies that all MPI tasks should run on a single node.  We are limiting the number of nodes for this workshop so that everyone gets a chance to run their code on the shared resources. Be sure to edit the script to activate your environment by its correct name.
Submitting the job:
Open a terminal window. Do not load any modules. Execute this command:
sbatch pympi.slurm 
Checking the job status:
Check the job status with the squeue -u or sacct commands as described in the Multiprocessing section. 
Checking the output file:
Open the slurm-XXXXXX.out files in a text editor and record the total run time for the job.
Exercise
Rerun the MPI job using 1 or 4 cpu cores by changing the --ntasks-per-node option.
Scaling
On the cluster, the timings are very similar to Multiprocessing on the workstation.
{{< table >}}
| CPU Cores | Run Time |
| --- | --- |
| 1 (serial) | 400 sec|
| 4 | 102 sec |
| 8 | 60 sec |
| 16 | 31 sec |
{{< /table >}}
Dask-MPI
Dask can use mpi4py on a high-performance cluster.  First install mpi4py according to the instructions in the previous section, then pip install --user dask-mpi. 
Schedulers
We have not discussed Dask schedulers previously. The scheduler is a process that managers the workers that carry out the tasks.  We have been implicitly using the single-machine scheduler, which is the default. Within the single-machine scheduler are two options, threaded and processes.  The threaded single-machine scheduler is the default for Dask Arrays, Dask Dataframes, and Dask Delayed.  However, as we discussed with Multiprocessing, the GIL (Global Interpreter Lock) inhibits threading in general.  Most of NumPy and Pandas release the GIL so threading works well with them.  If you cannot use NumPy and Pandas then the processes scheduler is preferred.  It is much like Multiprocessing.
To use Dask-MPI we must introduce the Dask distributed scheduler. The distributed scheduler may be preferable to processes even on a single machine, and it is required for use across multiple nodes. 
Running Dask-MPI
We will discuss here only the batch interface for Dask MPI.  Dask-MPI provides an initialize function that initializes MPI and sets up communications. You must then start a Client cluster to connect the workers.  Dask-MPI uses rank 0 for the manager process and rank 1 to mediate between manager and workers, so you must request at least 3 processes.  Close the cluster at the end of your script to avoid error messages about terminated processes.  
Example
Convert the ""timeseries"" example to Dask MPI.
{{< code-download file=""/courses/python-high-performance/codes/dask_df_mpi.py"" lang=""python"" >}}
Run this simple example with
{{< code-download file=""/courses/python-high-performance/codes/run_dask_mpi.slurm"" lang=""bash"" >}}
The OMPI_MCA environment variable suppresses a warning message that is seldom relevant.
Using Dask-MPI is not difficult, especially in batch mode, but users interested in trying it should be sure to first understand the distributed scheduler, then study the online examples carefully.  Dask-MPI does not require explicit calls to MPI but with the convenience comes some loss of control; the algorithm must be suited to the distributed scheduler.  More information may be found in the documentation.
Full documentation for Dask-MPI is here."
rc-learning-fork/content/courses/python-high-performance/multiprocessing.md,"Sometimes you cannot sufficiently speed up your program even with all optimization tricks.  You may be able to take advantage of modern multicore processors to distribute the work across different cores.  One popular programming model for multicore is threads.  Threads are subprocesses launched by the initial process (the executable in most cases).  Threads can be created and destroyed.  Each thread should be assigned to its own core.  Threads share a memory space and can also access the global memory of the system.  
The Global Interpreter Lock (GIL)
Standard Python implements a GIL (global interpreter lock). Threads cannot be started within a single interpreter.
A variety of workarounds to the GIL exist.  For instance, Python 3 provides the threading module, which implements the Thread class. However, unless the programmer is familiar with low-level thread operations and is very careful, it is more likely to slow down the code than to speed it up.
The Process Class
In most cases, it is better to just start another Python process.  The multiprocessing package handles this and manages communication among the processes.  For the purpose of this tutorial we will experiment with a few different multiprocessing approaches. A detailed description can be found on the official Multiprocessing website.
One difference between true threads and a Multiprocessing process is that threads directly share memory and processes do not.
Import the package
python
from multiprocessing import Process
Define a function
```python
def f(name): 
    print('hello from '+name)
if name == ""main"": 
    ncpus=4
    for i in range(ncpus):
        p=Process(target=f,args=(str(i),)) 
        p.start()
The result may be something likepython
hello from 0
hello from 2
hello from 3
hello from 1
```
Notice that the responses are not in numerical order.  In general, parallel programs do not guarantee ordering unless the library or the programmer forces it.  In this case the processes results are printed as they arrive.
Note that multiprocessing requires a main() function or section and must be run inside it.  This means that some examples, such as the multiprocessing.Pool examples, will not work in the interactive interpreter. 
The Pool Class
For manager-worker problems, we can start a pool of workers.  You can define a pool using an instance of the Pool class.  
Pools work through data parallelization.  The map method is analogous to the corresponding standard built-in ""map"" functional but distributes the data across the processes.
```python
from multiprocessing import Pool 
def f(x): 
  return x*x 
if name == ""main"": 
   pool = Pool(processes=4) 
   result = pool.map(f, range(1,11))
# Print result
   print(result)  
#Close out pool and have threads rejoin
   pool.close()
   pool.join()
```
In this example we created a Pool of four workers (Pool(processes=4)). The pool.map call submits a workload to the Pool of workers.  The first parameter is the name of the function, in this case f, and the second argument defines the sequence of argument(s) that need to be passed to the specified function f. Each element of the sequence is passed to f on one of the cores in use.
The map function is blocking; execution will not continue until the result is returned.  Another version of map, map_async, is nonblocking. Execution continues while the computations are carried out.  The communication is terminated when get is invoked.
```python
from multiprocessing import Pool
def f(x):
  return x*x
if name == ""main"":
   pool = Pool(processes=4)
   result = pool.map_async(f, range(1,11))
#Do other things
# Print result
   print(result.get())                    
   pool.close()
   pool.join()
``
The map method accommodates only one argument to the function.  For Python 3.3 and later,starmap` is available for multiple arguments.  An efficient way to generate the required iterator of tuples is to use the zip() function.
{{< code-download file=""/courses/python-high-performance/codes/mpstarmap.py"" lang=""python"" >}}
Another set of functions is apply and apply_async.  The difference between the apply group and map/map_async is that apply returns the result from only one element of the pool.  Like starmap, apply supports multiple arguments, but there is no starmap_async so if we need a nonblocking routine equivalent, we should use apply_async.  We will need to collect the results ourselves.
```python
from multiprocessing import Pool 
def f(x,y): 
  return x**y
if name == ""main"": 
   pool = Pool(processes=4) 
   results=[]
   for x in range(1,11):
       results.append(pool.apply_async(f, (x,3)))
   allresults=[result.get() for result in results]
   # Print results
   print(allresults)
```
Here is a more realistic example.  Let’s parallelize our Monte Carlo pi solver (MonteCarloPiMC.py).
Map requires an iterator for its second argument. We will manually divide the total number of ""data throws"" into chunks of roughly equal size on each process and store the result into a list myNumPoints. The Pool map method will then distribute the elements of the list, one to each cpu.  This is called load balancing in parallel computing terms.  Maximum efficiency generally occurs when each process performs approximately the same quantity of work.
We also do not hard-code the number of processes, but will set an environment variable NUM_PROCS outside to select the core count. 
{{% code-download file=""/courses/python-high-performance/codes/MonteCarloPiMC.py"" lang=""python"" %}}
Running on a Local Computer
Most modern personal computers, including laptops, are multicore.  If you are running on your own computer, test the code for a fairly small number of ""dart throws."" You may change ncpus to a fixed integer corresponding to your computer's core count.  Start with 10000 and increase to 100000, then to 1000000.  You may find that for a small number of throws, the serial time is faster than the multicore time.  This is due to overhead, which includes the additional time required to set up the multiple processes and communicate between them.  The result on one computer running Linux was
no-highlight
$ python MonteCarloPiMC.py 10000
ncpus=4
Points: [2500, 2500, 2500, 2500]
3.0968
Parallel time on 4 cores:0.0068
3.13
Serial time:0.0056
$ python MonteCarloPiMC.py 100000
ncpus=4
Points: [25000, 25000, 25000, 25000]
3.14844
Parallel time on 4 cores:0.0245
3.1366
Serial time:0.0586
$ python MonteCarloPiMC.py 1000000
ncpus=4
Points: [250000, 250000, 250000, 250000]
3.1436
Parallel time on 4 cores:0.1963
3.14234
Serial time:0.5596
As we might expect, the time for the serial run increases roughly linearly with the number of points.  The parallel time seems to obey the same rule after the first test run; for larger runtimes the additional time to set up Multiprocessing becomes less significant.  The value of $\pi$ also becomes more accurate as the number of ""throws"" increases.
Running the Program on a Cluster
For those who have access to a high-performance computing cluster such as UVA's HPC, Python scripts can be run in batch mode.  Our example assumes the SLURM resource manager.
In order to execute our program on designated compute node(s), we need to write a simple bash script that defines the compute resources we need.  We call this our job script.  For our example, the job script pimc.sh looks like this:
{{% code-download file=""/courses/python-high-performance/codes/pimc.sh"" lang=""bash"" %}}
You can view this script in a text editor on an HPC frontend.  If you are connected through a FastX Mate session, go to the menu Applications -> Accessories --> Pluma Text Editor.
The #SBATCH directives define the compute resources (-N, --cpus-per-task, -p), compute wall time (-t), and the allocation (-A) to be used. -N 1 specifies that the job runs on a single node. With --cpus-per-task we request the number of cpu cores for the job.  By increasing the number for --cpus-per-task we can take advantage of multiple cpu cores and set up a bigger pool of workers. Ideally we want to match the worker pool size with the number of cpu cores.
Submitting the job
The job must be submitted to the job scheduler with a specific command. On our HPC system we use the Simple Linux Utility Resource Manager (SLURM) and the sbatch command for job submission.
Open a terminal window and execute this command:
bash
sbatch pimc.sh 
After submission you should see output like this:
bash
Submitted batch job 9024339
The integer number resembles a unique job id.
Checking the Job Status
You can check the status of your jobs by running either one of these commands:

squeue -u YOUR_ID
sacct

The squeue command show all active jobs, either pending or running.  The sacct command shows the history of your jobs whether they are pending, running, completed, cancelled or failed.
You can find more details about SLURM and job management on our website.
Checking the Output File
SLURM creates output files for each job that log the information that the program prints to stdout and stderror during the job run.  The file(s) will also include information in case the job run was aborted.  By default the name of the SLURM output file is slurm-<JOB_ID>.out.  Check the directory in which you executed the sbatch command for a SLRURM output file and open it in a text editor.
Exercises: Rerun the job using 2, 4, or 8 cpu cores.  In order to do this, open the pimc.sh, change the --cpus-per-task option accordingly, save the file, and resubmit the job script from the terminal window with the sbatch command. 
Scaling
When we run the exercise with 10^9 points we may obtain results like these (on one particular workstation):
{{< table >}}
| CPU cores | Run time | Scaling
| --- | --- | --- |
| 1 (serial) | 402 sec | 1 |
| 4 | 109.5 sec | 3.67 |
| 8 | 60.5 sec | 6.64  |
| 16 | 32.5 sec | 12.37  |
{{< /table >}}
If we plot time versus number of cores we obtain the following graph.  The orange line is ideal scaling, where the total time is the serial time divided by the number of cores used.  The blue line shows the actual runtime and speedup achieved.
{{< figure src=""/courses/python-high-performance/mp-scaling.png"" caption=""Scaling performance for the Multiprocessing Monte Carlo Pi example."" >}}
Our actual scaling in this case is quite close to perfect.  This has a lot to do with our problem; the amount of time taken is mostly proportional to the number of throws to be calculated.  Not all problems scale this well.
Combining Approaches
If we have installed Numba,  we can use it and Multiprocessing together.  On the same workstation this reduced the serial time to 12.8 seconds and the time on 4 cores to 5.8 seconds.  The poorer scaling here could be due to the time required being so small that the overhead became dominant.
Further Information
The official documentation for Multiprocessing is here.  
This tutorial has some examples for using the Process class.
A good tutorial for both Processing and Pool is here.  It also introduces the Queue, which we have not discussed."
rc-learning-fork/content/courses/python-high-performance/serial-optimization.md,"We can represent the optimization process with a flowchart:
{{< diagram >}}
graph TD;
A(Profile or time) --> B(Tune the slowest section);
B --> C{Performance increase?};
C -- Yes --> D(Go to next slowest);
C -- No --> E(Try a different solution);
D --> A
{{< /diagram >}}
There are many approaches to speeding up sections, some specific to Python and some more generic.  In this section we will consider several possibilities.
Avoid for Loops
Strategy 1.
Use Python-specific constructions such as list comprehensions.  Use generators whenever possible.  Functionals such as map() can also be faster than for loops. 
List comprehensions compress a for-loop into a single line, with an optional conditional.
python
import math
a_list=[-10.,-8.,-6.6,-3.,0.,2.3,4.5,7.1,8.9,9.8]
y = [x**2 for x in a_list]
sqrts = [math.sqrt(x) for x in a_list if x>=0]
The three functionals take a function as their first argument, and an iterator as the second argument.
The map() functional remains available in Python 3, along with filter(). The reduce() functional must be imported from the functools module.  In Python 3 they return iterators and must be cast to a list if desired.  They may be used with a predefined function; they are frequently used with ""anonymous"" or lambda functions.
python
y2 = list(map(lambda x:x**2,a_list))
sqrts2 = list(map(lambda x:math.sqrt(x),filter(lambda x:x>=0,a_list)))
A generator is a function that returns an iterator.  Rather than creating all the results and storing them, they create but do not store values, returning them as they are needed.  In Python 3, range is a generator.  You can write your own generators by using yield rather than return.  Each value must be ""yielded"" as it is produced.
python
def square(x):
    yield x**2
A list comprehension can be converted to a generator by using parentheses rather than square brackets.
python
yg = (x**2 for x in a_list)
The result is a generator object.  This can save both memory and time.
Example
The following code tests the speed of map, list comprehension, and loop.
{{% code-download file=""/courses/python-high-performance/codes/replace_forloop_comp.py"" lang=""python"" %}}
The result on one particular system:
python
Time for map 4.9825
Time for comprehension 5.3274
Time for loop 5.6446
Strategy 2. Convert everything you can to use NumPy array intrinsics.
NumPy provides a large library of functions on NumPy arrays that take the place of loops.  This is referred to as vectorizing a code.
Exercise: 
Nested for loops are very inefficient (loops.py)
{{% code-download file=""/courses/python-high-performance/codes/loops.py"" lang=""python"" %}}
Eliminating for loops is much faster (aops.py)
{{% code-download file=""/courses/python-high-performance/codes/aops.py"" lang=""python"" %}}
Results with Python 3.6.9 on one particular system:
loops.py  1.197 sec 
aops.py  .211 sec
Extreme Example
```python
assume numpy array with n x n elements
for i in range(1,n-1):
      for j in range(1,n-1):
           u[i,j]=0.25(u[i+1,j]+u[i-1,j]+u[i,j+1]+u[i,j-1])
Replace with a single linepython
u[1:-1,1:-1]=0.25(u[2:,1:-1]+u[:-2,1:-1]+u[1:-1,2:]+u[1:-1,:-2] 
```
Example
Our ""dummy"" function is a ufunc, so we can run a trial with little modification to the previous code.  The ""setup"" code is not timed by timeit.
{{% code-download file=""/courses/python-high-performance/codes/replace_forloop.py"" lang=""python"" %}}
The difference is remarkable.  Remember that times for different runs may vary somewhat even on the same system, but the basic result will be similar.
Time for map 5.0804
Time for comprehension 5.4944
Time for loop 5.9716
Time for numpy 0.0885
More Information

List comprehensions
map/reduce/itertools

Avoid Copying
Bad:
python
s = """"
for x in mylist:
    s += string_function(x)
Better:
python
slist = [string_function(el) for el in mylist]
s = """".join(slist)
Not only does the first version have a for loop, but since strings are immutable each concatenation requires copying.  A join is much faster.
Note: string concatenation is faster in Python 3 than in Python 2.
Minimize Use of Dynamically Sized Objects
Use dynamically sized objects when appropriate, but do not append if you don’t have to do so.  Especially avoid inserting.
Simplify Mathematical Expression
Mathematical functions are very slow, in general.  When possible, simplify mathematical expressions.  For example,
$$ e^a e^b=e^{a+b} $$
Reducing two exponential evaluations to one can save a significant amount of time, especially if the expression is repeated many times.
Use Functions
Due to some quirks of Python, functions are faster than straight code.  
This implies you should use a main() function even if you never import your file as a module:
```python
def main():
     solve_problem()
if name==""main"":
      main()
```
Concluding Advice for Serial Optimization

Do not sacrifice readability for optimization.  Human time is much more expensive than computer time.
Do simple optimizations first.  Profile before undertaking extensive optimization efforts.
Use timing functions to obtain finer-grained information about bottlenecks as needed.
"
rc-learning-fork/content/courses/python-high-performance/gpu_acceleration.md,"Certain tasks can be greatly accelerated if run on a graphics processing unit (GPU).  A GPU can be regarded as a device that runs hundreds or thousands of threads.  The memory per thread is usually fairly limited but has a very high bandwidth.  Data must be moved to and from the host computer's memory to the GPU's memory.
GPU programming is an advanced topic and we will mention here only a few basics, and some resources for the interested reader.
In what follows, the host is the computer in which the GPU is installed, and the GPU itself is the device.  The GPU has its own memory system and cannot access the RAM of the host; similarly the host cannot directly access GPU memory. Data must be copied back and forth between them.
In most cases, it may be advisable to set up a separate environment for different Python+CUDA packages.
CuPy
CuPy is an implementation of many of the features of NumPy and SciPy that takes advantage of the GPU.  It is one of the simplest introductions to GPU programming.
It can be installed with conda through the conda-forge channel.
For installing from the command line, a terminal in MacOS or Linux or a Miniforge or Anaconda shell on Windows , use
bash
conda install -c conda-forge cupy
You can also use pip to install CuPy.
Alternatively, use a Docker container.
You must set the CUDA_PATH environment variable for CuPy to be able to accelerate your code properly. If you are working with your own computer, CUDA is installed from NVIDIA packages. 
For example, on a local Linux workstation, NVIDIA installs into /usr/local/cuda so you should set this as your CUDA_PATH.
bash
export CUDA_PATH=/usr/local/cuda
Refer to NVIDIA's instructions for other operating systems.
On a system such as UVA's HPC environment, the CUDA module will set the CUDA_PATH environment variable.
bash
module load cuda
Methods invoked through the CuPy module will be carried out on the GPU.  Corresponding NumPy methods will be processed by the CPU as usual.  Data transfer happens through streams.  The null stream is the default.
CuPy provides several packages.  In this example we show its FFT implementation.
{{% code-download file=""/courses/python-high-performance/codes/cupy_example.py"" lang=""python"" %}}
Exercise
Providing enough work to fill the GPU's threads is critical to overcoming the overhead of copying data.  
Reduce the length of the array in the example to 10000, then increase it until you find a size for which the GPU is faster.
PyCUDA
CUDA is a package from NVIDIA that enables access to the GPU through the C programming language.  With appropriate bindings it can be called from other languages.  PyCUDA is a package that implements CUDA bindings to Python.
Like CuPy, it is available through conda-forge.
bash
conda install -c conda-forge pycuda
On Linux the PATH variable must include the location of the nvcc compiler. If you have your own Linux workstation you must first locate nvcc. It should be in the folder indicated by the CUDA_PATH variable, with a ""bin"" appended.
bash
ls $CUDA_PATH/bin
Then add this location to your path, e.g.
bash
export PATH=$CUDA_PATH/bin:$PATH
Example
This script is copied directly from PyCUDA's examples.
{{% code-download file=""/courses/python-high-performance/codes/pycuda_example.py"" lang=""python"" %}}
Much as we saw when discussing using compiled code, we must define our function in C style.  This block of code to be executed on the device is called a kernel.  PyCUDA compiles the kernel, uses its interface with NumPy to allocate memory on the device, copy the Ndarrays, carry out the computation, then copy the result from the device to the dest array.
Notice that the Ndarrays are declared to be dtype=float32.  Very few GPUs support double-precision (float or float64 in Python) hardware, so using doubles may be slow as the computations must be done in software on the device.
Numba
When used for GPU acceleration, the package relies on a conda package cudatoolkit.
bash
conda install cudatoolkit
If you must use pip, you must also install the NVIDIA CUDA SDK.
Numba can be used with PyCUDA so adding it to the PyCUDA environment, which should already contain cudatoolkit, might be advisable. This example is from the PyCUDA tutorial.
{{% code-download file=""/courses/python-high-performance/codes/pycuda_numba.py"" lang=""python"" %}}
Numba Vectorization
Numba CUDA can ""vectorize"" a universal function (ufunc) by compiling it and running it on the GPU.  Vectorization is implemented through a decorator.
For best performance, the signature of the function arguments must be specified.  If more than one type should be supported, all must be passed as a list.
Example
From the Numba documentation:
{{% code-download file=""/courses/python-high-performance/codes/numba_vectorize.py"" lang=""python"" %}}
The run may emit a warning about under-utilization:
no-highlight
Grid size (1) < 2 * SM count (40) will likely result in GPU under utilization due to low occupancy.
This is because efficient use of a GPU requires that the device threads be as filled as possible.  If too many threads are idle, GPU code can be slower than the equivalent CPU code.
Another requirement for efficient GPU utilization is memory management.  Using NumPy arrays will result in copying from the host memory to the device memory.
Calling a ufunc with NumPy arrays can result in a large amount of copying of data back and forth between host and device.
We can instead declare arrays that will be set up in the GPU memory.  The todevice method will copy the host array to the device array.  We can also declare an output array on the device for the result.  When we are done with our calculations, we explicitly copy the result back to the host memory.
{{% code-download file=""/courses/python-high-performance/codes/numba_vectorize_todevice.py"" lang=""python"" %}}
If you time these two scripts, you may see a small speedup even for this relatively low-efficiency problem when copying is minimized.  Of course, we should aim for a large amount of work on the device arrays before we return the output to the host, and we should strive to make the problem large enough to fill the GPU's threads.
Numba GPU Kernels
Numba has a cuda.jit decorator that can be used like the jit equivalent, but defines a CUDA kernel. Some accommodations for the CUDA programming model must be made.  For the best performance, the number of threads on the hardware must be divided into thread blocks and the optimal number may depend on the device. Similarly to PyCUDA, Numba over CUDA makes use of NumPy arrays.
Example
This example of a matrix-multiplication kernel is taken from the Numba CUDA documentation.
{{% code-download file=""/courses/python-high-performance/codes/numba_cuda_example.py"" lang=""python"" %}}
RAPIDS
RAPIDS is a set of libraries released by NVIDIA for its GPU architectures (Pascal or later).  It builds upon CuPY and introduces a GPU DataFrame cuDF, and a package cuML that mostly replicates scikit-learn.
See our RAPIDS workshop to learn more about using RAPIDS."
rc-learning-fork/content/courses/python-high-performance/distributed-parallelization.md,"Nearly all recent computers, including personal laptops, are multicore systems.  The central-processing units (CPUs) of these machines are divided into multiple processor cores.  These cores share the main memory (RAM) of the computer and may share at least some of the faster memory (cache).  This type of system is called a shared-memory processing or symmetric multiprocessing (SMP) computer.  
{{< figure src=""/courses/python-high-performance/SMP.png"" caption=""Schematic of an SMP system"" >}}
A computing cluster consists of a group of computers connected by a network.  High-performance clusters nearly always have a network that is faster than the Ethernet used by consumer devices.  Many use a network called InfiniBand.  A cluster is thus a distributed-memory processor (DMP).  Each computer, usually called a node, is independent of the others and can exchange data only through the network.
Multiprocessing works only on a single computer with multiple computing cores (SMP). If you have access to a computing cluster you can use distributed parallelization to run your program on multiple computers (DMP) as well as multiple cores per computer.  This requires a communications library.  
{{< figure src=""/courses/python-high-performance/DMP.png"" caption=""Schematic of a DMP system"" >}}
Before considering parallelizing your program, it is highly desirable to spend some time optimizing the serial version.  Particularly if you can utilize NumPy and Pandas effectively, you may not need to try to parallelize the code.  If you still need to do so, a well-optimized, clearly-written script will make the work much easier.
Programming Note
We have seen that Multiprocessing requires the presence of a main function or section. Other parallelization packages may or may not require this, but it never hurts to include it, and it is a good idea to structure all parallel programming scripts in this manner.
```python
import package
def main():
    parallel invocations
if name==""main"":
    main()
orpython
import package
def func1():
   code
def func2():
   code
if name==""main"":
    parallel invocations
```
Dask
Dask is a framework for distributed applications.  It works with NumPy, Pandas, and Scikit-Learn, as well as some less common packages that have been customized to utilize it.  It is primarily used to distribute large datasets.  
Dask breaks the work into tasks and distributes those tasks.  It can use multicore systems easily. With some care, it can also use multiple nodes on a cluster if that is available.  A task may be reading a file, or doing some work on a portion of a large dataframe, or processing JSON files.  To accomplish this, Dask constructs a task graph.  This lays out the order in which tasks must occur.  Independent tasks can be run in parallel, which can considerably speed up the time to solution.  Dask is lazy and results are collected only as needed.
Our examples are taken from the Dask documentation.  We cover only the basics here; a more complete tutorial can be downloaded as Jupyter notebooks.
Dask is best utilized for problems for which the dataset does not fit in memory.  Some overhead is associated with it, so if your optimized NumPy or Pandas code can handle the problem, it is better to use those standard packages.
Dask Arrays
Dask arrays are much like NumPy arrays, but are decomposed into subunits that Dask generally calls chunks.  In the language of parallel computing, we call this data decomposition.
```python



import dask.array as da
x = da.random.random((10000, 10000), chunks=(1000, 1000))
```
Here we have created a two-dimensional array and broken it into 100 1000x1000 subarrays.  The full-sized array is not actually created so we do not need the memory required for it.  This is a general rule; avoid creating large global arrays.  Large, memory-hungry arrays can be slow to process or, if they do not fit in any available memory, can make the problem impossible to solve on most computers.



Most, but not all, NumPy functions and methods are available for Dask arrays.
```python



x.mean()
This returnsno-highlight
dask.array
This is because the Dask functions set up a computation but most do not execute it until told to do so.  To get our answer we must invoke the `compute` method.python
x.mean().compute()
0.5000237776672359
``
Certain operations that require the full result, such as plotting, will automatically invokecompute` but it can always be done explicitly.



To visualize the task graph for a given Dask object, be sure that graphviz is installed (python-graphviz for conda).  The visualize method for a Dask object will then produce the graph.  If run at the command line, the result should be written to a file.  Within a Jupyter notebook, the image will be embedded.  
Example 
{{% code-download file=""/courses/python-high-performance/codes/dask_viz.py"" lang=""python"" %}}
{{< spoiler text=""Expected result"" >}}
{{< figure src=""/courses/python-high-performance/codes/da_mean.png"" height=720 >}}
{{< /spoiler >}}
Dask Dataframes
Dask also has an infrastructure built atop Pandas. As for NumPy, many but not all Pandas functions and methods are implemented. 
```python



import dask
import dask.dataframe as dd
df = dask.datasets.timeseries()
df
This is also lazy:no-hightlight
Dask DataFrame Structure:
                   id    name        x        y
npartitions=30
2000-01-01      int64  object  float64  float64
2000-01-02        ...     ...      ...      ...
...               ...     ...      ...      ...
2000-01-30        ...     ...      ...      ...
2000-01-31        ...     ...      ...      ...
Dask Name: make-timeseries, 30 tasks
```
Only the structure of the dataframe has been established.



We can invoke Pandas functions, but as for arrays we must finalize the results.
```



df2 = df[df.y > 0]
df3 = df2.groupby('name').x.std()
df3.compute()
```



Exercise
In a Jupyter notebook or Spyder interpreter, run the Dataframe examples above. Now run
```python



import matplotlib.pyplot as plt
or in Jupyterpython
%matplotlib inline
Thenpython
df[['x', 'y']].resample('1h').mean().head()
df[['x', 'y']].resample('24h').mean().compute().plot()
Runpython
plt.show()
```
if necessary.



The Dask version of read_csv can read multiple files using wildcards or globs.
```python



df = dd.read_csv('myfiles..csv')
When creating a Dataframe, Dask does attempt to assign a type to each column.  If it is reading multiple files it will use the first one to make this determination, which can sometimes result in errors.  We can use `dtype` to tell it what types to use.python
df = dd.read_csv('myfiles..csv',dtype={'Value':'float64'})
```



Example
{{% code-download file=""/courses/python-high-performance/codes/dask_df_example.py"" lang=""python"" %}}
Dask Delayed
Another example of lazy evaluation by Dask is delayed evaluation. Sometimes functions are independent and could be evaluated in parallel, but the algorithm cannot be cast into an array or dataframe. We can wrap functions in delayed, which causes Dask to construct a task graph. As before, results must be requested before actual computations are carried out.
Example 
{{% code-download file=""/courses/python-high-performance/codes/dask_delayed.py"" lang=""python"" %}}
It is also possible to invoke delayed through a decorator.
```python
@dask.delayed
def inc(x):
    time.sleep(random.random())
    return x + 1
@dask.delayed
def dec(x):
    time.sleep(random.random())
    return x - 1
@dask.delayed
def add(x, y):
    time.sleep(random.random())
    return x + y
```
Dask Bags
Dask Bags implement collective operations like mapping, filtering, and aggregation on data that is less structured than an array or a dataframe.
Example
Download the data.  Unzip it where the Python interpreter can find it.  If using Jupyter make sure to set the working folder and then provide the path to the data folder.
python
import dask.bag as db
b = db.read_text('data/*.json').map(json.loads)
b
b.take(2)  # shows the first two entries
c=b.filter(lambda record: record['age'] > 30)
c.take(2)
b.count().compute()
Exercise
Run the above example.  Why does the expression b not show any values?
Xarray
Xarray is a project that extends Pandas dataframes to more than two dimensions. It is able to use Dask invisibly to the user, with some additional keywords.
```python



import xarray as xr
ds = xr.tutorial.open_dataset('air_temperature',
                              chunks={'lat': 25, 'lon': 25, 'time': -1})
da = ds['air']
da
``
The result shows the chunks layout but not the actual data.  Without thechunks` argument when the dataset was initiated, we would have an ordinary Xarray dataset.



Xarray is particularly well suited to geophysical data in the form of NetCDF files, but can also handle HDF5 and text data.
Dask and Machine Learning
Machine learning is beyond our scope here, but we will make a few comments.  Dask can integrate with scikit-learn in the form of Dask-ML. You may need to use pip to install dask-ml.
```python



import numpy as np
import dask.array as da
from sklearn.datasets import make_classification
X_train, y_train = make_classification(
       n_features=2, n_redundant=0, n_informative=2,
       random_state=1, n_clusters_per_class=1, n_samples=1000)
X_train[:5]
N = 100
X_large = da.concatenate([da.from_array(X_train, chunks=X_train.shape)
                             for _ in range(N)])
y_large = da.concatenate([da.from_array(y_train, chunks=y_train.shape)
                             for _ in range(N)])
X_large
from sklearn.linear_model import LogisticRegressionCV
from dask_ml.wrappers import ParallelPostFit
clf = ParallelPostFit(LogisticRegressionCV(cv=3), scoring=""r2"")
clf.fit(X_train, y_train)
y_pred = clf.predict(X_large)
y_pred
clf.score(X_large, y_large)
```
Dask can also be used with Pytorch.  See the documentation for an example.


"
rc-learning-fork/content/courses/python-high-performance/profiling-timing.md,"The first step is usually to profile the code.  Profiling can help us find a program's bottlenecks by revealing where most of the time is spent.  Keep in mind that most profilers work per function.  If you do not have your code well separated into functions, the information will be less useful.
Profilers are statistical in nature.  They query the program to find out what it is doing at a snapshot during the run, then compile the results into a kind of frequency chart.  
Python Profiler in the Spyder IDE
The Spyder IDE provides an integrated Profiler that is easy to use. To profile your Python code follow these steps:

Open the Python script in the Editor pane.  If your program contains multiple files, open the script that contains the main function.
In the Spyder menu, go to Run -> Profile.

The results will be shown in the Profiler pane. 
Exercise: 
Open the fibonacci.py file and execute it with the Spyder Profiler.  The code deliberately uses an inefficient algorithm.  Let's look at the output in the Profiler pane.  What function was called most frequently and has the largest cumulative run time?
{{% code-download file=""/courses/python-high-performance/codes/fibonacci.py"" lang=""python"" %}}
{{< figure src=""/courses/python-high-performance/fibonacci-profiler.png"" caption=""Profiler output for the Fibonacci example"" >}}
A more detailed description of the Profiler option for Spyder can be found here.
Another popular IDE (integrated development environment) for Python is Pycharm by JetBrains.  The profiler option is only available in the Professional (paid) version, but some UVA departments have a license for this version.
Profiling in Jupyter
To invoke the Python profiler in Jupyter, use the ""magic"" prun:
python
%prun code_line
for one line of code.  To profile a code block use
python
%%prun
for i in range(10):
    a[i]=b[i]
    c[i]=a[i]**2
This runs the cProfile profiler.
Prun will print a plain-text summary similar to the Profile window of Spyder.
Using the cProfile and pstats Modules in your script
Python includes the cProfile and profile packages.  In Python 2, profile is a pure Python module with significant more overhead than the C extensions of cProfile (read the details).  In Python 3, profile is cProfile by default.  These packages can be used from the command line or in your Python scripts.
You can use the cProfile and pstats modules in your script to profile specific functions and save the profiling results to a file. Let's assume that we have a module MyModule that defines a function myFunc to profile: 
python
import MyModule
import cProfile
cProfile.runctx(""MyModule.myFunc()"", globals(), locals(), ""Profile.result"")
Here globals() defines a set of global variables to be set for the profiling run.  The locals() is a dictionary for arguments passed to the specific function to be profiled, i.e. MyModule.myFunc. The last argument, Profile.result, defines the file that the profiling output is written to.
The following code snippet can be used to implement profiling of the fib_seq function in the fibonacci.py script.
```python
import cProfile,pstats
cProfile.runctx(""fib_seq"", globals(), {'n':30}, ""Profile.result"")
s = pstats.Stats(""Profile.result"") 
s.strip_dirs().sort_stats(""time"").print_stats()
``
Note the passing of{'n':30}to pass local arguments to thefib_seqfunction. The profiling stats are read from theProfile.resultfile, the path information of the involved Python modules is removed, and the output is sorted bytime` values before it is printed.
Other Tools
If you are running on your own system, you can install the snakeviz package.  This enables you to view profiler output in a browser window.  First you must generate the profile file.  You can then invoke snakeviz from the command line with
snakeviz Profile.result
To use snakeviz from within an iPython shell or Jupyter notebook, first load it
python
load_ext snakeviz
Then use the ""magics""
python
%snakeviz code_line
for a single line, or
python
%%snakeviz
a line
for i in stuff:
   more
for multiple lines.
Exercise: 
Import the fibonnaci.py code to JupyterLab.  Run prun.  Try snakeviz if you wish to do so.  Snakeviz requires Internet access while it is running; prun does not.
You can also install the line_profiler and memory_profiler tools. It may be preferable to install from the conda-forge channel for memory_profiler.
no-highlight
conda install line_profiler
conda install -c conda-forge memory_profiler
You can use pip if you are not working with conda. From the command line, be sure to use the conda-forge channel with -c conda-forge if it is not the default channel (as it is for Miniforge). If using the Anaconda Navigator GUI, switch to the conda-forge channel before installing.
The line_profiler tool provides a special version of the Python interpreter, kernprof and defines a decorator @profile.  To use it from the command line, add the decorator to any function you wish to profile.
```python
@profile
def fib(n):
    # from http://en.literateprograms.org/Fibonacci_numbers_(Python)
    if n < 2:
        return n
    else:
        return fib(n-1) + fib(n-2)
@profile
def fib_seq(n):
    results = [ ]
    if n > 0:
        results.extend(fib_seq(n-1))
    results.append(fib(n))
    return results
You can then run on the command linepython
kernprof -l fibonacci.py
This produces a file `script.py.lprof`; in this specific case we now haveno-highlight
fibonacci.py.lprof
We must run it through the profiler to visualize itno-hightligh
python -m line_profiler script.py.lprof
```
Once installed, line_profiler can be invoked in Jupyter with the lprun magic:
python
%lprun -f fib fib_seq(30)
The functions to be profiled must be specified by the -f option to lprun.
Exercise
Run the line_profiler on the Fibonacci script. 
Another potentially useful tool is the memory profiler.  It is similar to line_profiler but for memory.  To use memory_profiler rather than line_profiler, import from the module
python
from memory_profiler import profile
Now the @profile decorator will apply the memory profiler.  Run with the
-m option to Python
no-highlight
python -m memory_profiler fibonacci.py
This will print to the console, so be prepared to redirect the output.
In Jupyter the corresponding magics are memit and mprun.
The memit magic will print the maximum memory usage, whereas mprun will 
generate the profile.
python
%load_ext memory_profiler
%memit fib_seq(30)
The mprun magic must work on modules imported from outside the notebook. We already have a fibonacci.py script so we can use that.  However, we'll need to change directory since Jupyterlab starts at the top-level folder.
python
import os
os.chdir(""/home/myid/myfolder"")
from fibonacci import fib
%mprun -f fib fib_seq(30)
We are using the fib_seq function that is in scope in Jupyter, but the fib 
function from the module.
Timing
Often we want to know exactly how long a particular code, or portion of the code, ran.  Profiling adds some execution overhead and works only for functions; even line_profiler is function-based.  For finer-grained data, we must measure the elapsed time directly.
The time module is available for longer segments of code.
```python
import time
start_time=time.clock()
do stuff
end_time=time.clock()
print (""Elapsed time is "",end_time-start_time)
Python 3 offers the `timeit` module for short snippets.python
import timeit
print(timeit.timeit('""-"".join(str(n) for n in range(100))', number=10000))
``
Since a single repetition of a command in Python may run too quickly to measure easily, we specify a number of repetitions (number=10000`).  
The Jupyter/iPython magics for timing are time and timeit.
python
%timeit fib_seq(10)
19.3 µs ± 108 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
Timeit in Jupyter makes its own decision about how many repetitions to run.
It also factors out system overhead.  The time magic is like the Linux time command.
python
%time fib_seq(10)
CPU times: user 81 µs, sys: 5 µs, total: 86 µs
Wall time: 89.6 µs
With some tools in hand, we can now investigate how we can speed up our scripts."
rc-learning-fork/content/courses/python-high-performance/compiled_code.md,"Broadly speaking, interpreted languages tend to be slow, but are relatively easy to learn and use.  Compiled languages generally deliver the maximum speed, but are more complex to learn and use effectively.  Python can utilize libraries of compiled code that are appropriately prepared, or can invoke a compiler (standard or ""just in time"") to compile snippets of code and incorporate it directly into the execution. 
Wrapping Compiled Code in Python
Function libraries can be written in C/C++/Fortran and converted into Python-callable modules.  
In order to wrap code written in a compiled language, you must have a compiler for the appropriate language installed on your system.  
Windows
If you do not use Fortran, you can install MS Visual Studio. A community edition is available free for personal use and includes C and C++ compilers. If you might use Fortran, a good option is MinGW-64. This may also provide good compatibility with a Python installation such as Miniforge, even if you do not expect to use Fortran.  MinGW-64 provides several options for builds of the gcc (Gnu Compiler Collection).  The ucrt build is recommended but may be a little rough around the edges, at least for Fortran users.  The older mingw64 build may be more suitable.  Either or both can be installed on the same system; the path will select the compiler used by Python or the IDE.  A nice tutorial on installing MingGW-64 and using it with the free VSCode IDE is here. You must install VSCode extensions for C/C++ and, if appropriate, Fortran. To install the mingw64 version, simply substitute that name for ucrt in the pacman instructions. For Fortran, after the basic toolchain is installed, run 
no-highlight
pacman -S mingw-w64-x86_64-gcc-fortran
Now go to Settings and edit your system environment variables to add C:\msys2\mingw64\bin to path.  Once that is done, you can use a command line or a local PowerShell, such as the Miniforge shell, to run f2py as shown below for Linux. After that move the resulting library to an appropriate location in your PYTHONPATH.
Mac OS
Install XCode from the Mac App Store for the C/C++ compilers, then if appropriate install gfortran from the Wiki.  MinGW-64 is also an option for macOS. Once installed you can run commands in a Terminal shell. In newer macOS versions the shell is zsh and not bash, but the commands shown for Linux should work without modification.
Linux
The gcc compiler should be installed by default but you may have to add the corresponding g++ and gfortran compilers. Refer to the documentation for your Linux distribution and package manager.
Wrapping Fortran

If you have Fortran source code you can use f2py.  It is included as part of NumPy.  It can work for C as well, but requires some knowledge of Fortran interfaces to do so.  It can wrap nearly all legacy Fortran 77 and some newer Fortran 90 constructs, in particular, modules. It must be used from a command line, which is simple on Linux and macOS but a little more complicated on Windows. 

http://docs.scipy.org/doc/numpy-dev/f2py/
We will illustrate with a simple (and very incomplete) example of code to work with fractions.
Example
Download the example Fortran code fractions.f90 to try this yourself.
First create a Python signature file.
f2py fractions.f90 -m Fractions -h Fractions.pyf
We then use the signature file to generate the Python module.
f2py -c -m Fractions fractions.f90
The original Fortran source consisted of a module Fractions.  Examining the signature file, we see that f2py has lower-cased it and created the Python module Fractions.  Under Linux the module file is called Fractions.cpython-39-x86_64-linux-gnu.so but we can drop everything past the period when we import it.
```



from Fractions import fractions
fractions.adder(1,2,3,4)
array([10,  8], dtype=int32)
```
One significant weakness of f2py is limited support of the Fortran90+ standards, particularly derived types.  One option is the f90wrap package.



It is also possible to wrap the Fortran code in C by various means, such as the F2003 ISO C binding features, then to use the Python-C interface packages, such as ctypes and CFFI, for Fortran.  More details are available at fortran90.org for interfacing with C and Python.
Wrapping C
The [CFFI] (https://cffi.readthedocs.io/en/latest/overview.html) package can be used to wrap C code.  CFFI (C Foreign Function Interface) wraps C libraries into Python code. To use it, prepare a shared (dynamic) library of functions.  This requires a C compiler, and the exact steps vary depending on your operating system.  Windows compilers produce a file called a DLL, Unix/Linux shared libraries end in .so, and macOS shared libraries end in .dylib.  
CFFI is not a base package, but is often included in Python distributions such as Miniforge. It may also be included as an add-on for other installations such as system Pythons, since some other package such as a cryptography library may require it. Before installing CFFI, first attempt to import it
python
import cffi
If that fails you can pip install cffi.
Much as for f2py, the user must prepare some form of signature for the C functions. For CFFI that usually means writing a header (.h) file containing the function prototypes.
CFFI has several modes.  We will work with the API (Application Programming Interface) since it is not too complicated and performs well.
See the documentation for a discussion of the various modes.
Example
Download the arith.c file and its corresponding arith.h header file, which implements some trivial arithmetic functions.  
Now download the build_arith.py script
{{< code-download file=""/courses/python-high-performance/codes/build_arith.py"" lang=""python"" >}}
We must repeat the function prototypes in the cdef method. The set_source method takes several arguments, not all of which are shown in this example. The first is the name of the shared library that will be generated. Next is all preprocessor statements within triple-double quotes. The sources argument is a list of the source file or files.  Note that if the path is not in the current search path, it must be specified.  Our example shows a Unix-like path.  Finally, we invoke the compiler to create the library. CFFI will use the default system library.
Run the script.
python
python build_arith.py
On Linux the name may be lengthy, such as _arithlib.cpython-39-x86_64-linux-gnu.so.  When importing we may use only the first part _arithlib.
We now utilize it from the interpreter as follows:
```



from _arithlib import ffi, lib
lib.sum(11.1,12.8)
23.9
lib.difference(11.1,12.8)
-1.700000000000001
lib.product(11.1,12.8)
142.08
lib.division(11.1,12.8)
0.8671874999999999
```



CFFI supports more advanced features.  For example, structs can be wrapped into Python classes.  See here for an example.  
CFFI does not support C++ directly.  Any C++ must be ""C-like"" and contain an extern C declaration.
Wrapping C++
One of the most popular packages that deals directly with C++ is PyBind11.  Setting up the bindings is more complex than is the case for ctypes or CFFI, however, and the bindings are written in C++, not Python.  Pybind11 will have to be installed through conda or pip.
One option is to use CMake, since it can be configured to generate the fairly complex Makefile required, and it also works on Windows.  A somewhat simpler method is to use the Python package invoke.  This can be installed through pip or through a manager such as a Linux operating system package manager.  The Python header file Python.h must also be accessible.
Example
We will wrap the fractions.cxx file.  It also requires the fractions.h header. These files implement a very incomplete Fractions class, similar to the Fortran example above.

Write the bindings wrap_fractions.cxx.

{{% code-download file=""/courses/python-high-performance/codes/wrap_fractions.cxx"" lang=""c++"" %}}

Compile fractions.cxx into a shared library.  Invoke can be used for this, but a simple command line is also sufficient here.
g++ -O3 -Wall -Werror -shared -std=c++11 -fPIC fractions.cxx -o libfractions.so
Run the command
no-highlight
python -m pybind11 --includes
in order to determine the include path.  On a particular system it returned
no-highlight
-I/usr/include/python3.11 -I/usr/include/pybind11
Take note of the include file paths, which will vary from one system to another.  Move into Python and run invoke
{{% code-download file=""/courses/python-high-performance/codes/tasks.py"" lang=""python"" %}}

This will create a module whose name begins with py_fractions (the rest of the name is specific to the platform on which it was created, and is ignored when importing).  Test that it works:
```python



from py_fractions import Fraction
f1=Fraction(5,8)
f2=Fraction(11,13)
f1.addFracs(f2)
[153, 104]
```



Pybind11 requires C++ code that adheres to the C++11 standard or higher.  Another option is the Boost library Python bindings Boost.Python.  Pybind11 is a fork of these bindings; the Boost version is more general, and can handle many older C++ codes, but it is more complex to use.
Cython
Cython is a package that allows Python code to be compiled into C code.  Some rewriting is required because C requires statically-typed variables.  Cython defines two additional keywords to declare functions, cdef and cpdef. cdef is basically C and can produce the fastest code, but cdef declared functions are not visible to Python code that imports the module. cpdef is mix of C with dynamic bindings for passing of Python objects which makes it slower than cdef (read the details).
Exercise: integrate.py 
Suppose we start with
{{% code-download file=""/courses/python-high-performance/codes/integrate_cyf.pyx"" lang=""python"" %}}
Save the above code as integrate_cyf.pyx.  Now create a setup.py file:
{{% code-download file=""/courses/python-high-performance/codes/setup.py"" lang=""python"" %}}
On the command line run python setup.py build_ext --inplace to build the extension.
This will create a file integrate_cyf.c in your local directory. In addition you will find a file called integrate_cyf.so in unix or integrate_cyf.pyd in Windows. Now you can import your module in your Python scripts.
python
import integrate_cyf as icyf
print(icyf.integrate_f(1.,51.,1000))
More detailed information describing the use of Cython can be found here.
Numba
Numba is available through the Miniforge Python distribution.   It compiles selected functions using the LLVM compiler.  Numba is accessed through a decorator.  Decorators in Python are wrappers that modify the functions without the need to change the code.
Exercise:
A well-known but slow way to compute pi is by a Monte Carlo method.  Given a circle of unit radius inside a square with side length 2, we can estimate the area inside and outside the circle by throwing “darts” (random locations).  Since the area of the circle is pi and the area of the square is 4, the ratio of hits inside the circle to the total thrown is pi/4.  
Open the MonteCarloPi.py script.
{{% code-download file=""/courses/python-high-performance/codes/MonteCarloPi.py"" lang=""python"" %}}
Running with $10^9$ points takes 6 minutes and 21 seconds on one particular system.
Now add another import statement.
python
from numba import jit
Add the decorator above the pi function:
python
@jit
def pi(numPoints):
No other changes are required.  The time is reduced to only 14.7 seconds!"
rc-learning-fork/content/courses/python-high-performance/_index.md,"Python, like most interpreted languages, can be very slow. But there are best practices and some programming tricks that can speed it up considerably.  This can make the difference between finishing the work in an acceptable time, or being unable to finish a project.
First Things First: Always start with a working, correct code. Make it as clean and readable as possible.
For this tutorial, it is assumed that you have experience with programming in Python. We will explore examples for different Optimization Strategies, including

Serial Optimization: Replacing inefficient code constructs with more efficient ones (single process).
Multiprocessing: Executing multiple processes on a single computer (shared memory).
Distributed Parallelization: Executing multiple processes across multiple computers (distributed memory, HPC cluster).
GPU Acceleration: A minimal introduction to programming for a GPU with Python.


Setup
To follow along for the Serial Optimization and Multiprocessing examples, you can execute the code examples on your own computer or on UVA's high-performance computing cluster.  Examples described in the last section, Distributed Parallelization, are best executed on UVA's high-performance computing platform.
If you are using your local computer for your personal applications, not related to work, you can install the Anaconda distribution to run the code examples. Anaconda provides multiple Python versions, an integrated development environment (IDE) with editor and profiler, Jupyter notebooks, and an easy-to-use package environment manager.  If you will or might use the installation for work, or just prefer a more minimal setup that you can more easily customize, we suggest Miniforge (https://github.com/conda-forge/miniforge).
If you are using UVA HPC, follow these steps to verify that your account is active:
Check your Access to UVA HPC


In your web browser, go to fastx.hpc.virginia.edu.  This takes you to our FastX web portal that lets you launch a remote desktop environment on a frontend.  If you are off Grounds, you must be connected through the UVA Anywhere VPN client.


Log in with your UVA credentials and start a MATE session.  You can find a more detailed description of the FastX login procedure here.

User name: Your UVA computing id (e.g. mst3k; don't enter your entire email address)

Password: Your UVA Netbadge password 


Starting Spyder: You must first activate an environment and install Spyder into it.  Open a terminal window and type
module load miniforge
python -V
You will obtain a response like
Python 3.11.3
If your environment does not include it, install the package
conda install spyder
Now type
spyder &


For Jupyterlab you can use Open OnDemand.  Jupyterlab is one of the Interactive Apps.  Note that these apps submit jobs to compute nodes. If you need to use Jupyterlab outside of the OOD interactive app, you should install it into your environment similarly to installng Spyder.
conda install jupyterlab nbconvert
The nbconvert paackages allows Jupyter to export your cells to various formats, including Python scripts.  You can then invoke it with
jupyter-lab &
It will open in the default Web browser.
Please note that parallelization methods may not work well or at all in Jupyter.
"
rc-learning-fork/content/courses/opencv/index.md,"
Introduction
From the OpenCV project documentation:

OpenCV (Open Source Computer Vision Library: http://opencv.org) is an open-source library that includes several hundreds of computer vision algorithms.

This workshop assumes a working knowledge of the Python programming language and basic understanding of image processing concepts.
Introductions to Python can be found here and here.

Getting Started
Python code examples
The Python scripts and data files for this workshop can be downloaded from here. On your computer, unzip the downloaded folder and use it as working directory for this workshop.
Python programming environment
The Anaconda environment from Anaconda Inc. is widely used because it bundles a Python interpreter, most of the popular packages, and development environments. It is cross-platform and freely available. There are two somewhat incompatible versions of Python; version 2.7 is deprecated but still fairly widely used. Version 3 is the supported version. 
Note: We are using Python 3 for this workshop.
Option 1: Using the UVA HPC platform
If you have a Rivanna account, you can work through this tutorial using an Open OnDemand Desktop session.


Go to https://rivanna-portal.hpc.virginia.edu.


Log in with your UVA credentials.


Go to Interactive Apps > Desktop


On the next screen, specify resources as shown in this screenshot: 


Note: Workshop participants may specify rivanna-training in the Allocation (SUs) field. Alternatively, you may use any other Rivanna allocation that you are a member of.  



Click Launch at the bottom of the screen. Your desktop session will be queued up -- this may take a few minutes until the requested resources become available.


Option 2 - Use your own computer


Visit the Anaconda download website and download the installer for Python 3 for your operating system (Windows, Mac OSX, or Linux). We recommend to use the graphical installer for ease of use.


Launch the downloaded installer, follow the onscreen prompts and install the Anaconda distribution on your local hard drive.


The Anaconda Documentation provides an introduction to the Anaconda environment and bundled applications. For the purpose of this workshop we focus on the Anaconda Navigator and Spyder. 
Using Anaconda
Navigator
Once you have installed Anaconda, start the Navigator application: 
* Instructions for Windows
* Instructions for Mac
* Instructions for Linux
You should see a workspace similar to the screenshot, with several options for working environments, some of which are not installed. We will use Spyder which should already be installed. If not, click the button to install the package.

Spyder
Now we will switch to Spyder. Spyder is an Integrated Development Environment, or IDE, aimed at Python. It is well suited for developing longer, more modular programs. 

To start it, return to the Anaconda Navigator and click on the Spyder tile. It may take a while to open (watch the lower left of the Navigator). 
Once it starts, you will see a layout with an editor pane on the left, an explorer pane at the top right, and an iPython console on the lower right. This arrangement can be customized but we will use the default for our examples. Type code into the editor. The explorer window can show files, variable values, and other useful information. The iPython console is a frontend to the Python interpreter itself. It is comparable to a cell in JupyterLab.


Installation of OpenCV
It is recommended to install the opencv-python package from PyPI using the pip install command. 
On your own computer:
Start the Anaconda Prompt command line tool following the instructions for your operating system.
* Start Anaconda Prompt on Windows
* Start Anaconda Prompt on Mac, or open a terminal window.
* Linux: Just open a terminal window.
At the prompt, type the following command and press enter/return:
bash
pip install opencv-python matplotlib scikit-image pandas
This command will install the latest opencv-python package version in your current Anaconda Python environment.  The matplotlib package is used for plotting and image display. It is part of the Anaconda default packages. The scikit-image and pandas packages are useful for additional image analysis and data wrangling, respectively. 
On Rivanna (UVA's HPC platform):
Rivanna offers several Anaconda distributions with different Python versions. Before you use Python you need to load one of the Anaconda software modules and then run the pip install command in a terminal. 
bash
module load anaconda
pip install --user opencv-python matplotlib scikit-image pandas

Note: You have to use the --user flag which instructs the interpreter to install the package in your home directory. Alternatively, create your own custom Conda environment first and run the pip install opencv-python matplotlib pandas command in that environment (without the --user flag) 

To confirm successful package installation, start the Spyder IDE by typing the following command in the terminal:
bash
spyder &
In the Spyder IDE, go to the IPython console pane, type the following command and press enter/return:
python:
import cv2
print (cv2.__version__)
If the package is installed correctly, the output will show the openCV version number.
Example scripts and images
Download the example scripts and images from this link. Unzip the downloaded file and start your Python IDE, e.g. Spyder.
If you are on Rivanna, run the following command to copy the examples to your home directory:
bash
cp -R /share/resources/tutorials/opencv-examples ~/

Basic Operations
Loading Images
The imread function is used to read images from files. Images are represented as a multidimensional NumPy arrays. Learn more about NumPy arrays here. The multidimensional properties are stored in an image's shape attribute, e.g. number of rows (height) x number of columns (width) x number of channels (depth).
```python:
import cv2
load the input image and show its dimensions
image = cv2.imread(""clown.png"")
(h, w, d) = image.shape
print('width={}, height={}, depth={}'.format(w, h, d))
```
Output:
width=320, height=200, depth=3
Displaying Images
We can use an openCV function to display the image to our screen. 
```python:
open with OpenCV and press a key on our keyboard to continue execution
cv2.imshow('Image', image)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

The cv2.imshow() method displays the image on our screen. The cv2.waitKey() function waits for a key to be pressed. This is important otherwise our image would display and immediately disappear before we even see the image. The call of destroyAllWindows() should be placed at the end of any script that uses the imshow function.

Note: Before you run the code through the Spyder IDE, go to Run > Run configuration per file and select Execute in dedicated console first. Then, when you run the code uyou need to actually click the image window opened by OpenCV and press a key on your keyboard to advance the script. OpenCV cannot monitor your terminal for input so if you press a key in the terminal OpenCV will not notice.  

Alternatively, we can use the matplotlib package to display an image.
```python:
import matplotlib.pyplot as plt
plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))
```

Note that OpenCV stores channels of an RGB image in Blue, Green, Red order. We use the cv2.cvtColor(image,cv2.COLOR_BGR2RGB) function to convert from BGR --> RGB channel ordering for display purposes. 

Saving Images
We can use the imwrite() function to save images. For example:
python:
filename = 'clown-copy.png'
cv2.imwrite(filename, image)
Accessing Image Pixels
Since an image's underlying pixel information is stored in multidimensional numpy arrays, we can use common numpy operations to slice and dice image regions, including the images channels.
We can use the following code to extract the red, green and blue intensity values of a specific image pixel at position x=100 and y=50.
```
(b, g, r) = image[100, 50]
print(""red={}, green={}, blue={}"".format(r, g, b))
````
Output:
red=184, green=35, blue=15

Remember that OpenCV stores the channels of an RGB image in Blue, Green, Red order.  

Slicing and Cropping
It is also very easy to extract a rectangular region of interest from an image and storing it as a cropped copy. Let's extract the pixels for 30<=y<130 and 140<=x<240 from our original image. The resulting cropped image has a width and height of 100x100 pixels.
python:
roi = image[30:130,140:240]
plt.imshow(cv2.cvtColor(roi,cv2.COLOR_BGR2RGB))
Resizing
It is very easy to resize images. It just takes a single line of code. In this case we are resizing the input image to 500x500 (width x height) pixels.
python:
resized = cv2.resize(image,(500,500))
Note that we are forcing the resized image into a square 500x500 pixel format. To avoid distortion of the resized image, we can calculate the height/width aspect ratio of the original image and use it to calculate the new_height = new_width * aspect ratio (or new_width = new_height / aspect ratio).
```python:
resize width while preserving height proportions
height = image.shape[0]
width = image.shape[1]
aspect = height/width
new_width = 640
new_height = int(new_width * aspect)
resized2 = cv2.resize(image,(new_width,new_height))
print (image.shape)
print (resized2.shape)
```
```python:
display the two resized images
_,ax = plt.subplots(1,2)
ax[0].imshow(cv2.cvtColor(resized, cv2.COLOR_BGR2RGB))
ax[0].axis('off')
ax[1].imshow(cv2.cvtColor(resized2, cv2.COLOR_BGR2RGB))
ax[1].axis('off')
```

Splitting and Merging of Color Channels
The split() function provides a convenient way to split multi-channel images (e.g. RGB) into its channel components.
```python:
Split color channels
(B, G, R) = cv2.split(image)
create 2x2 grid for displaying images
_, axarr = plt.subplots(2,2)
axarr[0,0].imshow(R, cmap='gray')
axarr[0,0].axis('off')
axarr[0,0].set_title('red')
axarr[0,1].imshow(G, cmap='gray')
axarr[0,1].axis('off')
axarr[0,1].set_title('green')
axarr[1,0].imshow(B, cmap='gray')
axarr[1,0].axis('off')
axarr[1,0].set_title('blue')
axarr[1,1].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
axarr[1,1].axis('off')
axarr[1,1].set_title('RGB')
```

Let's take the blue and green channel only and merge them back into a new RGB image, effectively masking the red channel. For this we'll define a new numpy array with the same width and height as the original image and a depth of 1 (single channel), all pixels filled with zero values. Since the individual channels of an RGB image are 8-bit numpy arrays, we choose the numpy uint8 data type.
```python:
import numpy as np
zeros = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
alternative:
zeros = np.zeros_like(B)
print (B.shape, zeros.shape)
merged = cv2.merge([B, G, zeros])
_,ax = plt.subplots(1,1)
ax.imshow(cv2.cvtColor(merged, cv2.COLOR_BGR2RGB))
ax.axis('off')
```

Exercises

In the clown.png image, inspect the pixel value for x=300, y=25.
Crop the clown.png image to a centered rectangle with half the width and half the height of the original.
Extract the green channel, apply the values to the red channel and merge the original blue, original green and new red channel into a new BGR image.  Then display this image as an RGB image using matplotlib.


Filters
Denoising
OpenCV provides four convenient built-in denoising tools:

cv2.fastNlMeansDenoising() - works with a single grayscale images
cv2.fastNlMeansDenoisingColored() - works with a color image.
cv2.fastNlMeansDenoisingMulti() - works with image sequence captured in short period of time (grayscale images)
cv2.fastNlMeansDenoisingColoredMulti() - same as above, but for color images.

Common arguments are:
* h: parameter deciding filter strength. Higher h value removes noise better, but removes details of image also. (10 may be a good starting point)
* hForColorComponents: same as h, but for color images only. (normally same as h)
* templateWindowSize: should be odd. (recommended 7)
* searchWindowSize: should be odd. (recommended 21)
Let's try this with a noisy version of the clown image. This is a color RGB image and so we'll try the cv2.fastNlMeansDenoisingColored() filter. Here is the noisy input image clown-noisy.png.

The denoising.py script demonstrates how it works.
```python:
import cv2
from matplotlib import pyplot as plt
noisy = cv2.imread('clown-noisy.png')
define denoising parameters
h = 15
hColor = 15
templateWindowSize = 7
searchWindowSize = 21
denoise and save
denoised = cv2.fastNlMeansDenoisingColored(noisy,None,h,hColor,templateWindowSize,searchWindowSize)
cv2.imwrite('clown-denoised.png', denoised)
display
plt.subplot(121),plt.imshow(cv2.cvtColor(noisy, cv2.COLOR_BGR2RGB), interpolation=None)
plt.subplot(122),plt.imshow(cv2.cvtColor(denoised, cv2.COLOR_BGR2RGB), interpolation=None)
plt.show()
```

Additional useful filters for smoothing images:
 * GaussianBlur - blurs an image using a Gaussian filter
 * medianBlur - blurs an image using a median filter
There are many other image smoothing filters described here. 
Morphological Filters
Morphological filters are used for smoothing, edge detection or extraction of other features. The principal inputs are an image and a structuring element also called a kernel.
The two most basic operations are dilation and erosion on binary images (pixels have value 1 or 0; or 255 and 0). The kernel slides through the image pixel by pixel (as in 2D convolution). 

During dilation, a pixel in the original image (either 1 or 0) will be considered 1 if at least one pixel under the kernel is 1. The dilation operation is implemented as  cv2.dilate(image,kernel,iterations = n).
During erosion, a pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (set to zero). The erosion operation is implemented as  cv2.erode(image,kernel,iterations = n). 

The erode-dilate.py script provides an example:
```python:
import cv2
import numpy as np
import matplotlib.pyplot as plt
image = cv2.imread('morph-input.png',0)
create square shaped 7x7 pixel kernel
kernel = np.ones((7,7),np.uint8)
dilate, erode and save results
dilated = cv2.dilate(image,kernel,iterations = 1)
eroded = cv2.erode(image,kernel,iterations = 1)
cv2.imwrite('morph-dilated.png', dilated)
cv2.imwrite('morph-eroded.png', eroded)
display results
_,ax = plt.subplots(1,3)
ax[0].imshow(image, cmap='gray')
ax[0].axis('off')
ax[1].imshow(dilated, cmap='gray')
ax[1].axis('off')
ax[2].imshow(eroded, cmap='gray')
ax[2].axis('off')
```
Original             | Dilation               | Erosion
:-------------------:|:----------------------:|:----------------------:
 |  |  |

By increasing the kernel size or number of iterations we can dilate or erode more of the original object.

Elementary morphological filters may be chained together to define composite operations.
Opening is just another name of erosion followed by dilation. It is useful in removing noise. 
python:
opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)
Closing is reverse of opening, i.e. dilation followed by erosion. It is useful in closing small holes inside the foreground objects, or small black points on the object.
closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
Original             | Opening               | Closing
:-------------------:|:----------------------:|:----------------------:
 |  |  |

Note how the opening operation removed the small dot in the top right corner.  In contrast the closing operation retained that small object and also filled in the black hole in the object on the left side of the input image.

Morphological Gradients can be calculated as the difference between dilation and erosion of an image. It is used to reveal object's edges.
python:
kernel = np.ones((2,2),np.uint8)
gradient = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)
Original             | Gradient (edges)
:-------------------:|:----------------------:
 | 
Exercises

Experiment with different kernel sizes and number of iterations. What do you observe?


Segmentation & Quantification
Image Segmentation is the process that groups related pixels to define higher level objects. The following techniques are commonly used to accomplish this task:

Thresholding - conversion of input image into binary image
Edge detection - see above
Region based - expand/shrink/merge object boundaries from object seed points
Clustering  - use statistical analysis of proximity to group pixels as objects
Watershed - separates touching objects
Artificial Neural Networks - train object recognition from examples

Let's try to identify and measure the area of the nuclei in this image with fluorescent labeled cells. This is the fluorescent-cells.png image in the examples folder. We will explore the use of morphology filters, thresholding and watershed to accomplish this.
The complete code is in the segmentation.py script.

Preprocessing
First, we load the image and extract the blue channel which contains the labeling of the nuclei. Since OpenCV reads RGB images in BGR order, the blue channel is at index position 0 of the third image axis.
```python:
import cv2
image = cv2.imread('fluorescent-cells.png')
nuclei = image[:,:,0] # get blue channel
```

To eliminate noise, we apply a Gaussian filter with 3x3 kernel, then apply the Otsu thresholding algorithm. The thresholding converts the grayscale intensity image into a black and white binary image. The function returns two values, we store them in ret (the applied threshold value) and thresh (the thresholded black & white binary image).  White pixels represent nuclei; black pixel represent background. 
```python:
apply Gaussian filter to smoothen image, then apply Otsu threshold
blurred = cv2.GaussianBlur(nuclei, (3, 3), 0)
ret, thresh = cv2.threshold(blurred,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
```
Next, we'll apply an opening operation to exclude small non-nuclear particles in the binary image. Furthermore we use scikit's clear_border() function to exclude objects (nuclei) touching the edge of the image. 
```python:
fill small holes
import numpy as np
from skimage.segmentation import clear_border
kernel = np.ones((3,3),np.uint8)
opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel,iterations=7)
remove image border touching objects
opening = clear_border(opening)
```
The resulting image looks like this.

A tricky issue is that some of the nuclei masks are touching each other.  We need to find a way to break up these clumps. We do this in several steps. First, we will dilate the binary nuclei mask. The black areas in the resulting image represent pixels that certainly do not contain any nuclear components. We call it the sure_bg.
```python:
sure background area
sure_bg = cv2.dilate(opening,kernel,iterations=10)
```

The nuclei are all fully contained inside the white pixel area. The next step is to find estimates for the center of each nucleus. Some of the white regions may contain more than one nucleus and we need to separate the joined ones. We calculate the distance transform to do this.

The result of the distance transform is a graylevel image that looks similar to the input image, except that the graylevel intensities of points inside foreground (white) regions are changed to show the distance to the closest boundary from each point.

We can use the intensity peaks in the distance transform map (a grayscale image) as proxies and seeds for the individual nuclei. We isolate the peaks by applying a simple threshold. The result is the sure_fg image.
```python:
calculate distance transform to establish sure foreground
dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)
ret, sure_fg = cv2.threshold(dist_transform,0.6*dist_transform.max(),255,0)
```
Distance Transform               | Sure Foreground (nuclei seeds)
:-------------------------------:|:------------------------:
|  |  |
By subtracting the sure foreground regions from the sure background regions we can identify the regions of unknown association, i.e. the pixels that we have not assigned to be either nuclei or background.
```python:
sure_fg is float32, convert to uint8 and find unknown region
sure_fg = np.uint8(sure_fg)
unknown = cv2.subtract(sure_bg,sure_fg)
```
Sure Background         | Sure Foreground (nuclei seeds)  | Unknown
:----------------------:|:-------------------------------:|:-----------------------:
 |          |  |
Watershed
Now we can come back to the sure foreground and create markers to label individual nuclei. First, we use OpenCV's connectedComponents function to create different color labels for the set of regions in the sure_fg image. We store the label information in the markers image and add 1 to each color value. The pixels that are part of the unknown region are set to zero in the markers image. This is critical for the following watershed step that separates connected nuclei regions based on the set of markers. 
```python:
label markers
ret, markers = cv2.connectedComponents(sure_fg)
add one to all labels so that sure background is not 0, but 1
markers = markers + 1
mark the region of unknown with zero
markers[unknown==255] = 0
markers = cv2.watershed(image,markers)
```
Lastly, we overlay a yellow outline to the original image for all identified nuclei.
image[markers == -1] = [0,255,255]
The resulting markers (pseudo-colored) and input images with segmentation overlay look like this:
Markers                 | Segmentation
:----------------------:|:-------------------------:
 |  |
Measure
With the markers in hand, it is very easy to extract pixel and object information for each identified object. We use the scikit-image package (skimage) for the data extraction and pandas for storing the data in csv format.
```
from skimage import measure, color
import pandas as pd
compute image properties and return them as a pandas-compatible table
p = ['label', 'area', 'equivalent_diameter', 'mean_intensity', 'perimeter']
props = measure.regionprops_table(markers, nuclei, properties=p)
df = pd.DataFrame(props)
print data to screen and save
print (df)
df.to_csv('nuclei-data.csv')
```
Output:
label    area  equivalent_diameter  mean_intensity    perimeter
0       1  204775           510.614951       72.078891  6343.194406
1       2    1906            49.262507      218.294334   190.716775
2       3    1038            36.354128      204.438343   148.568542
3       4    2194            52.853454      156.269827   215.858910
4       5    2014            50.638962      199.993545   177.432504
5       6    1461            43.130070      185.911020   168.610173
6       7    2219            53.153726      170.962596   212.817280
7       8    1837            48.362600      230.387044   184.024387
8       9    1032            36.248906      228.920543   135.769553
9      10    2433            55.657810      189.083436   218.781746
10     11    1374            41.826202      214.344978   167.396970
11     12    1632            45.584284      191.976716   196.024387
12     13    1205            39.169550      245.765145   141.639610
13     14    2508            56.509157      153.325359   229.894444
14     15    2086            51.536178      195.962608   244.929978
15     16    1526            44.079060      243.675623   163.124892
16     17    1929            49.558845      217.509072   174.124892
17     18    1284            40.433149      165.881620   150.710678
18     19    2191            52.817306      174.357827   190.331998
19     20    2218            53.141747      170.529306   210.260931
20     21    2209            53.033821      164.460842   203.858910
21     22    2370            54.932483      193.639241   206.296465
22     23    1426            42.610323      249.032959   157.296465
23     24    2056            51.164250      194.098735   181.396970
Exercises

Experiment with different kernel sizes during the preprocessing step.
Experiment with different iteration numbers for the opening and dilation operation. 
Experiment with different thresholding values for isolating the nuclei seeds from the distance transform image.
Change the overlay color from yellow to magenta. Tip: magenta corresponds to BGR (255,255,0).

How do these changes affect the segmentation?

Resources

Introduction to OpenCV
OpenCV Python Tutorial
"
rc-learning-fork/content/courses/fortran-introduction/character_intrinsics.md,"Character Functions
These intrinsic functions operate on individual characters.
fortran
! Returns the integer in the character sequence corresponding to the single
! character c
ICHAR(C [,KIND])
! Same as ichar
IACHAR(C [,KIND])
! Returns the character corresponding to integer i
CHAR(I [,KIND])
! Same as char
ACHAR(I [,KIND])
String Functions
These intrinsic functions operate on strings of any length.
Length and Trimming
fortran
! Length of string (returns declared size for fixed-length strings)
LEN(STR)
! Length of string with trailing blanks removed
LEN_TRIM(STR)
! Returns new string with trailing blanks removed from argument
TRIM(STR)
! Concatenate multiple copies of the same string
REPEAT(STR,NCOPIES)
Alignment
These functions adjust the appearance of a string.
fortran
! Adjust left by removing leading spaces
ADJUSTL(STR)
! Adjust right by removing trailing spaces. Pad front if necessary.
ADJUSTR(STR)
Example
fortran
character(len=15) :: lang
lang=""Fortran""
print *, lang
print *, adjustl(lang)
print *, adjustr(lang)
The default is to left justify the string.
Searching
Search for characters and substrings.
fortran
! In these, the BACK option, if present, must be logical.
! Return starting indext of SUBSTR from left (from right with BACK)
!   or zero if SUBSTR not present.
INDEX(STR, SUBSTR[, BACK [, KIND]])
! If any of SET is in STR, return leftmost (rightmost with BACK) position in STR
SCAN(STR, SET[, BACK [, KIND]])
! Check whether all the characters of SET are in STR, return first location 
   of a character _not_ in SET, or zero if all are present.
! VERIFY(STR, SET[, BACK [, KIND]])
Example
fortran
character(len=15) :: lang
character(len=5) :: ort
lang=""Fortran""
ort=""ort""
print *, index(lang,'f')
print *, index(lang,'F')
print *, scan(lang, ort)
print *, verify(lang, ort)
Character Comparison Operators
Strings can be compared to one another. 
These operators use lexical ordering, which is based on the ordering in the character set along with rules for comparing multiple-character strings one 
character at a time.  The usual operators ==,/=,<,<=,>,>= may be used, as well as the functions below.  The standard operators use ASCII ordering, whereas the functions use the character set on a particular platform, which may not use ASCII ordering.
String comparisons are case-sensitive.
fortran
! Returns .true. if stringA is lexically greater than or equal to stringB, 
!   otherwise returns .false.
LGE(STRINGA,STRINGB)
! Lexically greater than to stringB, 
LGT(STRINGA,STRINGB)
! Returns .true. if stringA is lexically less than or equal to stringB, 
!  otherwise returns.false.
LLE(STRINGA,STRINGB)
! Lexically less than string B 
LLT(STRINGA,STRINGB)
Example
fortran
print *, lang==""Fortran""
print *, lang==""fortran""
print *, ""Fortran""<""fortran""  !surprise!
print *, ""2."">=""2.0"" 
print *, lle(""2."",""2.0"") 
print *, ""1""<=""10""  
print *, llt(""1"",""10"")  
print *, ""1""<="" 1"""
rc-learning-fork/content/courses/fortran-introduction/subprogram_arrays.md,"Passing scalar arguments is straightforward, but many, perhaps most, Fortran programs are oriented around arrays and must pass them to and from subprograms.
Since Fortran passes all variables by reference, i.e. by passing a pointer, there is no special behavior compared to scalars, unlike many other languages.
Passing Arrays to Subprograms

Arrays may be passed in one of three ways.
Static
Dimensions are declared as fixed numbers in both calling unit and callee.
Automatic
Dimensions may be passed in the argument list
Assumed-Shape
Only the rank is given, with an appropriate number of colons.
Assumed-shape arrays require an interface.

Examples
We will show examples for subroutines but the rules are the same for functions.
Static:
```fortran
real, dimension(100) :: A
call sub(A)
subroutine sub(A)
real, dimension(100) :: A  ! in sub
A variable may be used but it must be declared PARAMETER.  Some compilers, specifically Intel's, may require that the parameter be declared before any arrays are declared that use it.fortran
integer, parameter  :: N=100
real, dimension(N)  :: A
real, dimension(N,N):: B
```
Automatic:
```fortran
real, dimension(100) :: A
call sub(A,n)
subroutine sub(A,n)
real, dimension(n) :: A  ! in sub
Assumed-shape:fortran
integer,parameter  :: n=100
real, dimension(n) :: A
call sub(A)
subroutine sub(A)
real, dimension(:) :: A   ! in sub
```
Though the dimensions need not be known for assumed-shape arrays, the rank must match.
Example
{{< code-download file=""courses/fortran-introduction/codes/pass_arrays.f90"" lang=""fortran"" >}}
Allocating Arrays in a Subprogram
You may allocate an array inside a subprogram and return it.  An interface is required.  The array must be declared allocatable throughout the chain of calling units and must be intent(inout).  
In the example below, no value is assigned initially. Most compilers will initialize the array to zero, but this is not guaranteed.
Exercise
Run the program as is. Correct the lack of initialization in the subroutine.
{{< code-download file=""/courses/fortran-introduction/codes/pass_alloc.f90"" lang=""fortran"" >}}
Local Arrays in Subprograms
Arrays that are local (not in the parameter list) to a subprogram may be sized using an integer passed to the subprogram.
```fortran
double precision function myfunc(A,n)
integer,                        intent(in) :: n
double precision, dimension(n), intent(in) :: A
double precision, dimension(n)             :: B
do things with A and B
  myfunc=whatever
end function
```"
rc-learning-fork/content/courses/fortran-introduction/conditionals.md,"A conditional is a programming construct that implements decisions. 
* If the weather is good then we will go for a walk, else we will stay inside and watch TV.
* If it is cold enough to snow I will wear my heavy coat, else if it is warmer and just rains I will wear my rain jacket.
The expression following each if or else must be true or false, i.e. a logical expression (in Fortran terminology).
Conditional Operators
Conditional operators are used to construct logical (Boolean) expressions.
Numeric Operators
These are used to compare numerical values.
Fortran has two sets, one with letters and one with symbols.  Note that /= has a / for “not.”  The periods around the letter-based operators are required.
{{< table >}}
|   Letters    |   Symbols   |   Meaning  |
|--------------|-------------|------------|
|   .EQ.       |   ==        |   Equality |
|   .NE.       |   /=        | Not equal  |
|   .LT.       |    <        | Less than  |
|   .LE.       |    <=       | Less than or equal  |
|   .GT.       |    >        | Greater than  |
|   .GE.       |    >=       | Greater than or equal to  |
{{< /table >}}
Logical Operators
{{< table >}}
|   Operator    |   Meaning   |
|---------------|-------------|
|   .NOT.       |   Negation of what follows |
|   .AND.       |     and     |
|   .OR.        |     or      |
{{< /table >}}
It is important to note that .OR. is an inclusive or.  It evaluates to .TRUE. if either operand is true.  This is different from many human languages, for which ""or"" is generally, though not always, exclusive.  An exclusive ""or"" is true only if exactly one of the conditions is true.
   You can have cake or ice cream (but not both).
An exclusive or can be constructed with 
fortran
(a .AND. .NOT. b) .OR. (.NOT. a .AND. b)
where a and b are logical expressions.
Conditional Operator Precedence
Like arithmetic operators, conditional operators have a precedence ordering.

.NOT. has the highest rank
>,>=,<,<= are equal and outrank == or /=
==,/= are equal and outrank .AND.
.AND. outranks .OR.

As always, use parentheses to change grouping or to improve clarity.
IF-THEN-ELSE
The ELSEIF/ELSE IF and ELSE are optional. The parentheses around the logical expression are required.
fortran
   IF ( logical ) THEN
      code
   ELSEIF ( logical) THEN
      more code
   ELSE
      yet more code
   ENDIF
Only one branch will be executed.  Once any logical expression is determined to be true, 
the corresponding code will be executed and then the flow will proceed beyond the if block.
Exercise
Experiment with various truth values for bool1 and bool2.
{{< code-download file=""/courses/fortran-introduction/codes/if_demo.f90"" lang=""fortran"" >}}
SELECT CASE
Many ""else ifs"" can become confusing.  The SELECT CASE construct can simplify the statements, under the right conditions.  ""Expression"" refers to any valid
expression that can be evaluated to ""value0"", ""value1"", etc.
fortran
   SELECT CASE (expression)
      CASE(:value0)   ! Expression <= value0
         code
      CASE(value1)
        code
      CASE(value2)
        code
      CASE(value3:)  ! Expression >=value3
        code
      CASE (value4,value5,value6) !Multiples OK
        code
      CASE (value7:value9)
        code
      CASE DEFAULT    ! Optional
        code
   END SELECT
""Expression"" must be character, integer, or logical.
Ranges are only applicable for numeric or character expressions.
DEFAULT is for the action, if any, to be taken if the expression does not evaluate to any of the options available.
Example
{{< code-download file=""/courses/fortran-introduction/codes/selectcase.f90"" lang=""fortran"" >}}
Exercise:
This is the National Football Conference standings in late 2020:
   The Packers only need a win over the Chicago Bears to secure the No. 1 seed in the NFC. A loss or tie by the Seattle Seahawks would also give Green Bay the top spot.  If the Packers lose, though, the Seahawks or New Orleans Saints could claim the top spot. The Saints would secure the No. 1 seed with a Packers loss, a win and a Seattle win. Seattle can get it with a win, a Green Bay loss and a New Orleans loss or tie.
Write a program to determine whether the Packers will win the No. 1 seed.  Given the following conditions (not what happened), who won?
Packers lose to Bears

Seattle wins

The Saints tie 
Hint: code can often be simplified with the introduction of logical variables which are sometimes also called flags.

Hint: if a variable is already a logical it is not necessary to test it against .true. or .false.

Hint: a loss or tie is not a win.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/nfc.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/project7.md,"

Write a Fraction class that implements a representation of a fraction, where each instance consists of a numerator and a denominator. Overload addition, subtraction, and multiplication for this class.  Write a method to format each fraction in the form
no-highlight
5/7
For your first attempt it is not necessary to reduce the fraction, i.e. it is acceptable to have fractions like 6/8. Be sure to check for division by zero in your division procedure.


Add a reduce method that finds the least common multiple to obtain the lowest common denominator and reduce the fraction.


Set fractions with a denominator of 0 to 0/0 (this is arbitrary).


Write a driver to test all your procedures.


{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/fractions.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/make.md,"make is a tool to manage builds, especially with multiple files.
It has a rigid and peculiar syntax.
It will look for a makefile first, followed by Makefile (on case-sensitive systems).
The makefile defines one or more targets .  The target is the product of one or more rules .
The target is defined with a colon following its name.  If there are dependencies those follow the colon.
Dependencies are other files that are required to create the current target.
Targets and Rules
Example
myexec:main.o module.o
<tab>gfortran -o myexecmain.o module.o
The tab is required in the rule.  Don’t ask why.
Macros (automatic targets) for rules:
$@ the file name of the current target
$< the name of the first prerequisite
Variables and Comments
We can define variables in makefiles:
```
F90=gfortran
CXX=g++
``
We then refer to them as$(F90),$(CXX)`, etc.
Common variables: F90, CC, CXX, FFLAGS, F90FLAGS, CFLAGS, CXXFLAGS, CPPFLAGS (for the preprocessor), LDFLAGS.
The continuation marker \ (backslash) can be used across multiple lines. It must be the last character on the line; do not add spaces after it.
Comments are indicated by the hash mark #.  Anything beyond it will be ignored except for a continuation marker, which will extend the comment.
Suffix Rules
If all .f90 (or .cc or whatever) files are to be compiled the same way, we can write a suffix rule to handle them.
It uses a phony target called .SUFFIXES.
.SUFFIXES: .f90 .o
    $(F90) -c $(F90FLAGS) –c $<
Pattern Rules
This is an extension by Gnu make (gmake), but nearly every make is gmake now.
It is similar to suffix rules.  Useful for Fortran 90+:
%.mod: %.o
Pattern for creating the .o:
%.o: %.f90
    $(F90) $(F90FLAGS) -c $<
Example
{{< code-download file=""/courses/fortran-introduction/codes/Makefile"" lang=""make"" >}}
In this example, notice that the suffix rule applies the global compiler flags and explicitly includes the -c option.  If a particular file does not fit this pattern, a rule can be written for it and it will override the suffix rule.  The link rule includes the loader flags and the -o flag.  The compilation suffix rule uses the special symbol for the prerequisite; the link rule applies to the current target.
The example also demonstrates switching back and forth between ""debug"" and ""optimized"" versions.  The ""debug"" version would be created this time.  The -g flag is required for debugging.  The -C flag is a very useful flag specific to Fortran that enables bounds checking for arrays.  Both these flags, but especially -C, will create a slower, sometimes much slower, executable, so when debugging is completed, always recompile with the optimization flag or flags enabled, of which -O is the minimum and will activate the compiler's default level.  You must always make clean anytime you change compiler options.
For further reading about make, see the gmake documentation.
Makemake
Makemake is a Perl script first developed by Michael Wester soon after the introduction of Fortran 90, in order to construct correct makefiles for modern Fortran code.  The version supplied here has been extended.  It is freely licensed but if you use it, please do not remove the credits at the top.
makemake
This version works reasonably well for Fortran, C, and C++.  It will generate stubs for all languages. You may remove any you are not using.  Also note that the output is a skeleton Makefile.  You must at minimum name your executable, and you must fill in any other options and flags you wish to use.  The makemake script blindly adds any files ending in the specified suffixes it finds in the current working directory whether they are independently compilable or not, so keep your files organized, and be sure to edit your Makefile if you have files you need but cannot be compiled individually.
Several other build tools called makemake are available, and not all handle Fortran.  In addition, more tools have been created since the first makemake.  Several options are described at the Fortran Wiki.  The Python makemake can search recursively through subfolders, which the original makemake cannot.
Building with an IDE and a Makefile
Several IDEs will manage multiple files as a ""project"" and will generate a Makefile automatically.  Unfortunately, that Makefile is frequently incorrect for Fortran codes that use modules, so you may have to write your own Makefiles.  The makemake script or one of the newer build tools described above can help.
We will use the NetCDF library as an example.  Environmental sciences still use Fortran a great deal and this is a popular library for data files.  The example code is taken from their standard examples, modified to place the subroutine into a separate file.  The file is simple_xy_wr.f90.
On our test system, the library is installed in a standard location, but the netcdf module is not, so we need to use the -I flag but not the -L flag.
First we run makemake to obtain a skeleton Makefile.
{{< code file=""/courses/fortran-introduction/netcdf_example/Makefile.sample"" lang='make' >}}
We edit it to add the addition information required and to remove unneeded lines.
{{< code-download file=""/courses/fortran-introduction/netcdf_example/Makefile"" lang=""make"" >}}
Exercise 1:
If you have not already done so, download or copy the example.f90 and its required adder.f90.  Place them into a separate folder.  Run makemake.  Edit the Makefile appropriately.  Build the project using Geany or your choice of IDE.
Exercise 2:
If you are working on a system with NetCDF available, download the two files and the completed Makefile into their own folder.  Open Geany and browse to the location of the files.  Open the two source files.  Either select Make from the Build menu, or from the dropdown arrow next to the brick icon choose Make All.
Build the code using the Makefile.  Execute the result."
rc-learning-fork/content/courses/fortran-introduction/project3.md,"Using the cpi.csv data, write a program that will read from the command line the name of a file. Read this file into your program.  Request from the command line a year.  Optionally request on the command line an amount.
Check that you have enough command line input. Stop with a message if you don’t have enough command line values. 
Do not assume you know in advance how many lines the file contains. 
Use the inquire statement to check that the file exists before you attempt to open it. You can add a message to stop, e.g. stop “Invalid file”.  Test that the file can be opened as part of your open statement.
Once the data have been read, check that the requested year is in range.
The ratio of any two years is an estimate of the change in the cost of living.  Compute the change in the cost of living from the year you specify to the last number in the data (2020 in the sample file). Print it out neatly with some informative text. Print the result to 2 decimal places.
In 1954 a color television cost $1295. From your result how much would that be in 2020 dollars?
A rough estimate of the year-over-year inflation rate can be obtained from
fortran
inflation(i)=(cpi(i+1)-cpi(i))/12.
Compute the inflation rate for the data and print with the corresponding year to a comma-separated file.  Use any plotting package with which you are familiar (Excel, Matlab, Python, R, etc.) to plot the data.
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/inflation.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/loops.md,"Much computing is repetitive work.  Evaluate an expression many times with different values.  Read lines of a file.  Update a large number of variables.  To accomplish this, we use loops.
DO Loop
DO is the equivalent of for in languages like C/C++.
DO executes a fixed number of iterations unless explicitly terminated.
DO can iterate only over integer sequences.  Some compilers support real indices as an extension, but they should be avoided.
```fortran
INTEGER   :: L, U, S
INTEGER   :: I
DO I=L,U,S
   code
END DO
``Iis the _loop variable_,Lis the _lower bound_,Uis the _upper bound_, andSis the stride.  It is equal to 1 if not present.
The stride can be negative, in which caseLmust be greater thanU`.
Fortran always starts at the first bound and includes the second bound.
QUIZ
The standard requires that loop variables be integers.  How would I implement loop variables that are real?

How might real loop variables be a problem?
Implied DO
The implied do is used in a few circumstances, specifically input/output and array construction.
(var(iterator),iterator=lbound,ubound,s)
The parentheses are required.
Example
fortran
(a(i),i=1,20)
Implied do loops can be nested.
fortran
((r(i,j),j=1,M),i=1,N)
WHILE Loops
Whereas DO loops execute a particular number of iterations, WHILE loops iterate based on the truth or falsity of an expression.  The WHILE continues as long as the expression is .true. and terminates when it becomes .false.. It is up to the programmer to be sure to add statements to ensure that the expression eventually evaluates to .false. so the loop will end.
fortran
DO WHILE (<logical expression>)
   statement
   statement
   statement somewhere to check expression
END DO
Example
{{< code-download file=""/courses/fortran-introduction/codes/while_demo.f90"" lang=""fortran"" >}}
Exiting Early and Skipping Statements
The EXIT statement leaves the loop immediately.
EXIT is able to break out of only the loop level in which it appears.  It cannot break from an inner loop all the way out of a nested set of loops.  This is a case where goto is better than the alternatives. EXIT is equivalent to
break of several other languages.
CYCLE skips the rest of loop and goes to the next iteration.  It is equivalent to continue of other languages.  Like EXIT, it applies only to the loop level in which it is located.
fortran
x=1.
do while (x>0.0)
x=x+1.
if (x>=10000.0) exit
if (x<100.0) cycle
x=x+20.0
enddo
Repeat-Until
The standard DO and WHILE loops test at the top.  That is, upon entry to the loop, the termination condition is evaluated.  If it is false, the statements of the loop are executed.  Otherwise the loop is exited.
With the ability to break out of the loop at will, we can change this pattern.
One particularly common pattern is called repeat until.
fortran
do
   statement
   statement
   if (<logical expression>) exit
end do
One major reason for repeat-until is that a standard while loop may not be entered if the condition is initially false, whereas a repeat-until will always be executed at least once.
Example
Reading a file of unknown length.  This is not how we usually read a file, since most of the time the length is known, but it is possible to construct a loop to read a file whose length may vary for different runs.
fortran
    nlines=0
    do
        read(unit=iunit, end=10) var
        nlines=nlines+1
    enddo
10  continue
Exercises
1 Loop from 0 to 20 by increments of 2.  Make sure that 20 is included.  Print the loop variable at each iteration.
2 Start a variable n at 1.  As long as n is less than 121, do the following:
  - If n is even, add 3
  - If n is odd, add 5
  - Print n for each iteration.  Why do you get the last value?
3 Set a real value x=0. Loop from 1 to L inclusive by 1.
  - If the loop variable is less than M, add 11. to x.
  - If x > w and x < z, skip the iteration.
  - If x > 100., exit the loop.
  - Print the final value of x.
  - Experiment with different values for the variables.  Start with L=50, M=25, w=9., z=13.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/loops.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/project2.md,"Some languages, such as Python, can split a string on a specified delimiter character and return a list of strings. The delimiter is dropped.  Write a program that can accomplish this.
This should be a function, but we have not yet covered subprograms, so you may write a monolithic program.  If you wish to look ahead, write this as a function.
Ideally we would use varying-length strings, but for them to be really useful for this project, we would need an allocatable array of varying-length strings, which requires a derived type or use of more advanced intrinsics.
Print the result in Python format,
python
[sub1,sub2,sub3]
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/splitter.f90"" lang=""f90"" >}}
{{< /spoiler >}}
Bonus Project
A consultant was given a program that has code to generate a format string dynamically.  In particular, it can print items that might be arrays, with the repeat value generated automatically.
For example, given n1=5, n2=1, n3=3 and a pattern '(#e15.8,#i5,#f8.2)' 
the result would be '(5e15.8,1i5,3f8.2)'. However, it didn’t work if any of the n1, n2, n3 variables were two digits. The algorithm was convoluted and hard to understand.  It did work for two digits if the programmer used two hash marks, e.g. ##e15.8, but that required hand-editing the several files with output routines to find every place she wanted to write out more than 9 array elements.  The author of the original code didn’t use any character or string functions other than substrings. This would surely be implemented more generally with better use of strings.  Your task is to come up with a way to do this.  If you have time, come up with a way to handle a 0 (i.e. skip printing).  Test your program carefully.
Hints.  You do not need to use a variable-length string, but if not, be sure to declare a generously-sized set of strings (32 or even 64 characters, for example).  If using a fixed-length string, remember that you will need to remove blank space.  Test your program for n1=10, n2=2, n3=3.  Try another pattern like 
'(#es12.4,#i2,#f15.7,#i4)'.  Suggestion: use an allocatable array for the coefficients (both numerical and character).  Use array size to make sure they match.  
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/formatter.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/subprogram_args.md,"Pass by Reference and INTENT
Unlike most languages, Fortran passes all arguments by reference.  This effectively means that what is actually passed is the memory location of the argument.  Consequently, any change to the argument in the subprogram, intended or not, will change the variable outside as well. Changing the value of an argument is called a side effect.  Side effects can be legitimate -- subroutines rely on them -- but they should be controlled.  For this reason, Fortran introduced the INTENT attribute for subprogram parameters.
fortran
INTENT(IN)   ! Changing the variable in the subprogram throws a fatal error
INTENT(OUT)  ! Not changing the variable in the subprogram throws a fatal error
INTENT(INOUT)! Indicates that the programmer intends to overwrite the variable
Example
subroutine mysub(x,y,z)
   real, intent(in)    :: x
   real, intent(out)   :: y
   real, intent(inout) :: z
      y=x-z
      z=y+x
end subroutine
As a general rule, all arguments to a FUNCTION should be INTENT(IN).
Saving and Deallocating Subprogram Arguments
According to the standard, the memory used by local variables in a subprogram is freed upon exit from the procedure.
Allocatable local arrays are automatically deallocated (this is a form of “garbage collection”).
If you need some local variables to retain their value from one call to another, use the SAVE keyword
SAVE var1, var2, var3
SAVE
With no variable list it saves all local variables.
Note that allocatable local arrays cannot be SAVEd.
Many compilers do not actually free the memory of non-allocatable local variables and some old programs rely on this behavior.  Compilers have an option to ensure that all local variables are saved.
gfortran -fno-automatic mycode.f90
ifort -save mycode.f90
Optional and Keyword Arguments
Optional Arguments
Subroutines and functions may take optional arguments.   Such arguments need not be passed.  If they are passed, they take on the passed value. They are declared with the OPTIONAL attribute.
fortran
subroutine mysub(x,y,z,w)
implicit none
real, intent(in)           ::x,y
real, intent(in), optional ::z,w
The call to the previously-defined subroutine could be
fortran
callmysub(a,b)
in which case c and d would have no values and the subroutine would need to handle that situation appropriately.  The call could also be
fortran
callmysub(a,b,c)
or
fortran
callmysub(a,b,c,d)
depending on how many of the optional arguments needed to be passed.
Keyword Arguments
Suppose it were desired to pass d but not c in the preceding subroutine.  The c parameter can be skipped by using a keyword argument; the optional argument is called as
dummy=actual
where dummy is its name in the program unit where it is defined, and the actual argument is its name in the calling program unit.
Example
callmysub(aa,bb,w=d)
Positional (non-optional) arguments must appear before any optional or keyword arguments.
The PRESENT Intrinsic
The PRESENT() intrinsic function tests whether a particular optional argument is present in the argument list of the caller.   If it is not present, defaults can be set or other action taken.
Example
fortran
IF (PRESENT(w)) THEN
   dd=w
ELSE
   dd=3.14
ENDIF
Passing Character Variables
Characters declared with a fixed length may be passed to a subprogram using a dummy length.
fortran
character(len=20) :: str
   call mysub(str)
end program
subroutine mysub(str)
   implicit none
   character(len=*), intent(in) :: str
end subroutine
Passing a Subprogram Name
The name of a subprogram can be passed to another subprogram.
Example
a numerical-integration subroutine needs to be able to call the function to be integrated.
subroutine trap(f,a,b,h,n)
where f is a function.
The procedure name to be passed must have an interface in the unit that invokes the subprogram that will take this parameter.  The subprogram must also have an interface for the passed procedure. The older syntax EXTERNAL func may be used for the parameter, but new code should use INTERFACE.  This will result in an INTERFACE block within another INTERFACE, which may look a bit wordy but is the modern way to declare this type of parameter.
fortran
interface
  real function trap(f,a,b,h,n)
      implicit none
      real,    intent(in)   :: a, b, h
      integer, intent(in)   :: n
      interface
         real function f(x)
         implicit none
         real, intent(in) :: x
         end function
       end interface
   end function
   real function f(x)
        implicit none
        real, intent(in) :: x
   end function
end interface
{{< spoiler text=""Full example of passing a subprogram as a dummy variable"" >}}
{{< code-download file=""/courses/fortran-introduction/codes/trap.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Exercise
Write a program for comparing Euclidean distances.  The program should implement a function that takes the coordinates of two points and returns their distance.  Implement a subroutine that invokes this function for three points to determine which of the first two is closer.  Use interfaces.
Each calling unit must have an interface for every subprogram it calls. 
Use intent and implicit none.  Remember that implicit none must be declared in each unit.
You may use arrays to represent the points.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/euclid.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/project6.md,"Modify your bird_data type from Project 5 to make it a class.  Make all the methods private.  
You may use the same file_utils.f90 and sorters.f90 as before.  
You may find that you need to declare your allocatable bird_list array to be of type class.
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/bird_class.f90"" lang=""fortran"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/bird_obs_class.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/overloading.md,"Overloading is when a procedure or operator is able to work on multiple types.  Most languages overload at least basic arithmetic operators to work on all numerical types.  In modern Fortran the mathematical intrinsics are overloaded
to work at least on real and double precision, and when appropriate complex. For example, in older Fortran there were three versions of every trigonometric function when defined for complex numbers.  Cosine was COS for real number, DCOS for doubles, and CCOS for complex.  In modern Fortran COS works for all three.
This is an example of generic programming.
Overloading can be more general than just numerical types.  In some languages, sort is an intrinsic and works on any types for which relational operators (<,<=,>,>=) are defined.
Programmers can overload their own procedures in Fortran.  To make a generic procedure, an interface is required.  In this case the interface block has a name, which will be the generic name, and only the specific procedures should be included.
In all the examples below, our new generic diag for computing the diagonal of a matrix, will accept either real or double precision arrays.  The extension to complex should be obvious if it is needed.
Explicit Interface Block
We can use an explicit interface block in a non-module program unit.
fortran
interface diag
    function rdiag(A)
       use precisions
       implicit none
       real (sp), dimension(:,:), intent(in) :: A
       real (sp), dimension(size(A,2))       :: rdiag
    end function
    function ddiag(A)
       use precisions
       real (dp), dimension(:,:), intent(in) :: A
       real (dp), dimension(size(A,2))       :: ddiag
    end function
end interface
Module Procedure
Overloaded procedures are usually and most appropriately defined in a module.  Since procedures in modules already have an implicit interface, we cannot use the explicit interface above.  We still use a named interface block with MODULE PROCEDURE
fortran
interface diag
    module procedure rdiag
    module procedure ddiag
end interface
The body of the functions would be in the module.
Generic in Type-Bound Procedures
If we wanted to define something with various linear-algebra functions defined on it, we would use the GENERIC keyword.  Note that the specific functions must be written so that their signatures, the number and types of their arguments, are differ, or the compiler will not be able to distinguish them.
fortran
type something
   !variables
   contains
      private
      procedure :: rdiag
      procedure :: ddiag
      generic,public :: diag=>rdiag,ddiag
end type
Operator Overloading
The arithmetic operators +,-,*,and / can be overloaded to work on programmer-defined derived types.  Assignment (=) can also be overloaded when copying can be defined for the type.
Module Procedures
The arithmetic operators are overloaded with a generic interface, but rather than the name of the generic function, the keyword OPERATOR is used, followed by the symbol in parentheses for operators, or ASSIGNMENT(=) for copying.
fortran
interface operator(+)
   module procedure adder
end interface
interface operator(-)
   module procedure subber
end interface
interface assignment(=)
   module procedure assigner
end interface
The procedures that define the operator must be functions, must have two arguments, and must declare those arguments INTENT(IN).  For example, suppose we wished to define adder for an Atom type to add the atomic masses:
fortran
type(Atom) function adder(atom_A,atom_B)
    type(Atom), intent(in) :: atom_A, atom_B
    adder%atomic_mass=atom_A%atomic_mass+atom_B%atomic_mass
end function
For assignment, the procedure must be a subroutine with two arguments.  The first argument must represent the left-hand side and be INTENT(OUT), while the second represents the right-hand side and is INTENT(IN).
fortran
subroutine assigner(new_atom,old_atom)
   type(Atom), intent(out) :: new_atom
   type(Atom), intent(in)  :: old_atom
   new_atom%symbol=old_atom%symbol
   new_atom%eng_name=old_atom%eng_name
   new_atom%atomic_number=old_atom%atomic_number
   new_atom%atomic_mass=old_atom%atomic_mass
   new_atom%electronegativity=old_atom%electronegativity
end subroutine
Type-Bound Operators
Operators may be overloaded to work on class members.  The syntax is somewhat different from that for separate types in modules.
Here is a snippet from a module defining a Fraction class.  The rules for the arguments are the same as for modules, but of course the instance variable must be declared CLASS.
```fortran
private
public :: Fraction
type Fraction
      private
      integer              :: num, denom
      contains
         private
         procedure adder
         procedure subber
         procedure multer
         procedure divver
         procedure copier
         procedure, public :: reduce
         procedure, public :: print=>printer
         generic, public   :: operator(+) => adder
         generic, public   :: operator(-) => subber
         generic, public   :: operator(*) => multer
         generic, public   :: operator(/) => divver
         generic, public   :: assignment(=) => copier
   end type
```"
rc-learning-fork/content/courses/fortran-introduction/scope.md,"Scope is the range over which a particular variable is defined and can have a value.  In Fortran, scope is defined by the program unit.
A calling unit may have a variable named x, and a function may also have a variable named x, and if x is not an argument to the function then it will be distinct from the x in the calling unit.
```fortran
x=20.
call sub(x)
etc.
subroutine sub(y)
real, intent(inout) :: y
real                :: x
x=10.
y=30.
end subroutine sub
```
An interface is also a scoping unit.
fortran
program 
   implicit none
   real   :: x
   interface
      function myfunc(x)
      real, intent(in) :: x
      end function
   end interface
end program
In the above example, the x in the interface is not connected to the x in the interface.
A variable that is only in scope in a particular unit is said to be local to that unit.  Variables that are visible from more than one unit are global to those units.
BLOCK
The Fortran 2008 standard introduced the BLOCK construct.  A BLOCK is a scoping unit that is finer-grained than the program-unit level.  Variables may be declared within a BLOCK. Variables declared within the block are local to it.  Variables declared outside the block are global to it.  
IMPLICIT statements cannot appear in a block, but they do affect variables within the block.
{{< code file=""/courses/fortran-introduction/codes/blocker.f90"" lang=""fortran"" >}}
CONTAINS and Nested Procedures
CONTAINS is a way to nest procedures within another unit.
The CONTAINS keyword extends the scope into the contained program unit.
The end of the “container” must follow the end of the “containee”
A contained subprogram can access all the variables in the container except those that are explicitly passed.
The interface is implicit and should not be made explicit.
Only one level of nesting is permitted.
Example
```
PROGRAM myprog
IMPLICIT NONE
   REAL  ::x,y,z
x=5.; y=10.
   CALL mysub(z)
CONTAINS
  SUBROUTINE mysub(w)
     REAL, INTENT(INOUT) :: w
        w=x+y
  END SUBROUTINE

END PROGRAMm
```
COMMON and INCLUDE
COMMON is a deprecated feature that is frequently seen in older code.  It is a means of providing global variables.  Global variables are variables that are in scope in at least the entire file in which they are declared; they are a frequent source of bugs since they may result in a ""memory"" that should not be present. 
fortran
common /comname/ var1, var2, var3
The variables in the common list will be available to any program unit that includes the above line.  Variables in common between two program units should not be passed as subroutine parameters.
Newer code should use modules rather than COMMON.
Pitfalls with Common
The COMMON statement must be in every program unit that will share the variables, and it must be identical in each one.  It is highly recommended that any code using common put each one into a separate file and use the INCLUDE statement to merge the files. INCLUDEis a standard Fortran statement, not a preprocessor statement; its syntax is
fortran
include 'file.h'
where file.h can be any name; Fortran does not have a rule about file extensions for included files.
COMMON is a frequent source of memory errors.
COMMON makes interface control difficult to impossible.
The recommended first step in updating old code is to replace all COMMON with modules and then gradually to move the variables into the appropriate parameter lists. 
COMMON is a vestige of the early computer era, when main memory was measuring in megabytes or even kilobytes, and every byte was precious.  With COMMON the global variables would occupy a single spot in memory, would not be copied, and could be reused.  In particularly old code it is not unusual for arrays in COMMON to be reshaped implicitly, since COMMON inherently represents a large linear block of memory.  Sometimes the EQUIVALENCE statement is still seen, by which types could be converted implicitly as long as they occupied the same number of bytes."
rc-learning-fork/content/courses/fortran-introduction/intrinsic_modules.md,"Recent revisions of the Fortran standard support several intrinsic modules.
One must be downloaded, while the others can be USEd and will be supplied by the compiler.
ISO_VARYING_STRING
Fortran 95 did not support a variable-length string.  A standardized module was defined to support a type VARYING_STRING.  This module was never incorporated into the standard, so compilers do not include it, but an implementation can be found here.  This module was mostly obsoleted by the variable-string capabilities of Fortran 2003, but does have a few features still lacking in the standard.  A description is here.  In addition to defining the standard character intrinsics for VARYING_STRING, it contains some additional functionality, such as GET and PUT to read and write a character from or into a string, REMOVE, REPLACE, SPLIT, and some other useful procedures.
This module should be USEd like a programmer-written module.
Intrinsic Modules
The Fortran 2003 standard defined several intrinsics modules.  They have a special form of USE:
USE, INTRINSIC :: <module>
ISO_FORTRAN_ENV
This module contains many useful variables for system parameters, some storage parameters, and KIND parameters.  It also provides two intrinsic functions, COMPILER_OPTIONS and COMPILER_VERSION, which respectively return the command-line options and the compiler version used to compile the binary.
```fortran
use iso_fortran_env
print , ""This executable was compiled with "",COMPILER_VERSION()
print , ""The options used were "",COMPILER_OPTIONS()
Particularly useful members of the ISO_FORTRAN_ENV module are predefined KIND parameter for specific types.fortran
USE ISO_FORTRAN_ENV
INTEGER(int64)     :: i,j
```
{{< table >}}
|  KIND Parameter | IEEE Type |
|-----------------|----------------|
|  int8           | 8-bit integer  |
|  int16          | 16-bit integer |
|  int32          | 32-bit integer |
|  int64          | 64-bit integer |
|  real32         | 32-bit floating point |
|  real64         | 64-bit floating point |
|  real128        | 128-bit real          |
{{< /table >}}
Not all the IEEE KINDs may be supported in the hardware, particularly real128.
Not all compilers support all the above KINDs, in which case it should set it to a negative value.
IEEE Modules
The three IEEE modules provide constants and procedures relating to floating-point hardware such as exceptions and arithmetic constants.
A good reference for these modules is provided by Intel for their compiler.  The NAG compiler also has useful documentation.
IEEE Features
This module specifies the definitions of IEEE special bit patterns such as IEEE_Datatype and IEEE_Inf.
IEEE Exceptions
An exception occurs due to an illegal operation.  This may include mathematically illegal operations such as taking the square root of a negative real number, dividing by zero, and so forth.  Due to the finite range of floating-point numbers, other excepts are underflow and overflow.  Mathematically illegal operations result in NaN (Not a Number), whereas overflow results in Inf.  Operations are defined on NaNs, with the result of any arithmetic operation on a NaN being another NaN.  Therefore, NaNs can easily propagate through results and it would be useful to catch them when they first occur.  The IEEE Exceptions module can help with this.
For example, the IEEE_SET_HALTING_MODE,HALTING) intrinsic would be invoked as follows:
```fortran
USE, INTRINSIC :: IEEE_EXCEPTIONS
TYPE(IEEE_FLAG_TYPE) :: flag
LOGICAL              :: halt=.true.
CALL IEEE_SET_HALTING_MODE(flag,halt)
``Flagis a variable of TYPE(IEEE_FLAG_TYPE), also defined in the module.  It can be IEEE_DIVIDE_BY_ZERO, IEEE_INEXACT, IEEE_INVALID, IEEE_OVERFLOW, or IEEE_UNDERFLOW.  TheHALTINGargument is LOGICAL.  If HALTING is set to.true.` the program will stop on occurrence of the specified MODE. Since this is often desired for INVALID (NaN), DIVIDE_BY_ZERO, and OVERFLOW, a predefined array is available.
{{< code-download file=""courses/fortran-introduction/codes/ieee_exc.f90"" lang=""fortran"" >}}
IEEE Arithmetic
The IEEE_ARITHMETIC encompasses and extends the IEEE_EXCEPTIONS module.
Several useful procedures and included. IEEE_SELECTED_REAL_KIND chooses only KINDs corresponding to IEEE-supported types. Other procedures determine whether arithmetic operations conform to IEEE specifications.  
For more elegant handling of errors, the IEEE_IS_NAN, IEEE_IS_FINITE, and some others can be used to test a result and handle it in some manner other than halting the execution.
{{< code-download file=""courses/fortran-introduction/codes/ieee_arith.f90"" lang=""fortran"" >}}
ISO_C_BINDING
Mixed-language programming is common, particularly mixing C with other languages.  In the past, invoking C procedures from Fortran was tedious and error-prone, due to differences in conventions such as character termination, name-mangling of subprograms, and so forth.  The ISO_C_BINDING module was added to simplify this.
A good reference for the content of this module is from gfortran. Note that this module provides C equivalents to Fortran types for a given platform; in particular, the C standard does not specify the length of an int, only a minimum, so on some platforms the default is 16 bits and on others it is 32 bits.  The variables defining the correspondence can be used as KIND parameters.  A subset of the most commonly used might include
{{< table >}}
|  Fortran Type |  Module Name     | C Type |
|-----------------|----------------|--------|
|  INTEGER        | C_INT          | int    |
|  INTEGER(int64) | C_INT64_T     | int64_t |
|  REAL           | C_FLOAT |       float   |
|  REAL(real64)   | C_DOUBLE |      double  |
|  LOGICAL        | C_BOOL   |      _Bool  |
|  CHARACTER      | C_CHAR   |     char     |
{{< /table >}}
If all the types can be matched, a C struct can be mapped to a Fortran type with the BIND(C) attribute.
fortran
USE ISO_C_BINDING
 TYPE, BIND(C) :: myType
   INTEGER(C_INT) :: i, j
   REAL(C_DOUBLE) :: d
   CHARACTER(KIND=C_CHAR) :: c
 END TYPE
The KIND= keyword is required for CHARACTER because the default argument is the LEN.
This would correspond to a C struct
C
struct {
   int i, j;
   double d;
   char c;
 } myType;
Some facts to keep in mind are that C arrays number from 0, and C strings are terminated with a NULL character (C_NULL_CHAR in the module).
Subprograms must also declare the BIND(C) attribute to set up C bindings.
Caution is required since C generally passes by value, which makes a copy, whereas Fortran effectively passes by reference, i.e. the address of the memory location that holds the variable.  Arrays must also receive special treatment. Pointers require particular care. 
For a very simple example, suppose we have a C function
C
int adder(int i, int* j)
and we wish to write Fortran bindings to it.  The first variable is passed by value so we must add the VALUE attribute to its declaration in Fortran.  The second argument is passed by reference, as is the default in Fortran.  The Fortran declaration looks like
fortran
 integer(c_int) function func(i,j)
    use iso_c_binding, only: c_int
    integer(c_int), VALUE :: i
    integer(c_int) :: j
A good general discussion of Fortran-C interoperability, from which the above example is taken, is from gfortran."
rc-learning-fork/content/courses/fortran-introduction/subprogram_atts.md,"Subprograms can be defined with attributes for particular conditions or behaviors.  They can call themselves, or operate elementwise on arrays.
Pure and Elemental Procedures
PURE Functions
Side effects should be avoided in functions.  Fortran offers subroutines to handle situations where changes to the parameters make sense.
The programmer can declare a function PURE to tell the compiler it is free of side effects.
Pure functions must declare all parameters INTENT(IN).
Subroutines may also be PURE.  They may change their parameters but must declare those as INTENT(OUT).  INTENT(INOUT) is not permitted for PURE subroutines.
There are strict rules for PURE procedures.
* PURE procedures must have an interface.
* Any additional procedures they call must also be PURE.
* Neither PURE functions nor PURE subroutines are permitted to
  * Alter any accessible global variables (e.g. from CONTAINS);
  * Perform any IO -- this can make debugging inconvenient;
  * SAVE any variables;
  * Contain any STOP statement.  RETURN is permitted.
fortran
PURE FUNCTION myfunc(x)
INTEGER          ::myfunc
REAL, INTENT(IN) :: x
Since there is PURE, there is also IMPURE, so that the programmer is explicit about the presence of side effects.
ELEMENTAL Procedures
PURE procedures were intended for automatic parallelization.  However, a particularly useful derivative is the ELEMENTAL procedure.
ELEMENTAL procedures operate elementwise on arrays.
All ELEMENTAL procedures must obey the rules for PURE procedures. In addition, all arguments must be scalars.
```fortran
ELEMENTAL FUNCTION f2c(tempF)
REAL             :: f2c
REAL, INTENT(IN) ::tempF
f2c=(tempF-32.)/1.8
END FUNCTION
```
Using ELEMENTAL Procedures
An elemental procedure can be called for any arrays as long as they conform to the requirements for all operations in the procedure.  Each array element is modified by the procedure appropriately.  The procedure can also be called as a normal scalar function.
```fortran
PROGRAM elements
REAL                  ::tempF,tempC
REAL, DIMENSION(100)  ::tempFs,tempCs
REAL, DIMENSION(10,10)::dataF,dataC
INTERFACE
   ELEMENTAL FUNCTION f2c(tempF)
     REAL             :: f2c
     REAL, INTENT(IN) ::tempF
   END FUNCTION
END INTERFACE
! Set up tempFs somehow
tempC= f2c(tempF)
tempCs= f2c(tempFs)
dataC= f2c(dataF)
END PROGRAM
```
Like PURE procedures, ELEMENTAL procedures must have an interface.
Starting with Fortran 2008, it was recognized that the restrictions of PURE were sometimes unnecessary and detrimental to developing ELEMENTAL procedures, which have many uses for which PURE is irrelevant.  They may thus be declared
IMPURE ELEMENTAL myfunc(x)
Absent IMPURE, the procedure must obey the rules for PURE.
RECURSIVE Procedures
Recursive procedures call themselves.  This may seem impossible, but the compiler sets up multiple copies, usually in a section of memory called the stack.  Without some care in implementing the algorithm, recursion can lead to stack overflow.  Even worse, a recursive procedure must have a stopping condition or the result is infinite recursion, at least until the stack overflows and the executable or even the whole computer crashes.
Most recursive algorithms have an iterative (while loop) equivalent, which may perform better, but the recursive algorithm may be simpler and easier to understand.
Both functions and subroutines can be RECURSIVE.  Up to the F2008 standard, recursive functions require a RESULT clause.  Recursive subroutines do not support RESULT and do not require it. Starting with the F2018 standard, the default is for subprograms to be assumed RECURSIVE, so the keyword will no longer be required unless a compiler option is used to change the default behavior.  Another keyword NON_RECURSIVE can be used to make a subprogram explicitly iterative.
One of the most famous examples of a recursive algorithm is the Fibonacci sequence.  To compute the Nth number in the sequence, we can use
$$ F_0 = 0 $$
$$ F_1 = 1 $$
$$ F_{N}=F_{N-1}+F_{N-2} $$
{{< code-download file=""/courses/fortran-introduction/codes/fibonnaci.f90"" lang=""fortran"" >}}
Exercise


Write an elemental function that accepts a single parameter as an angle in degrees and converts it to radians.


Write a program that computes the radian equivalent of 0 to 90 degrees in increments of 5 degrees.  Print the output.  Do it with a loop, and then by creating a one-dimensional array of angles and passing it to the function.


{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/deg_rad.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/declarations.md,"Like most compiled languages, Fortran is statically typed .  All variables must be declared to be of a specific type before they can be used.  A variable’s type cannot be changed once it is declared.
Fortran is (nearly) strongly typed.  Mixed-mode expressions are limited and most conversions must be explicit.
Unlike most languages, Fortran is not case-sensitive.  Variables Mean, mean, and even mEan are the same to the compiler.
Variable names may consist of alphanumeric (letter or digit) characters, plus underscores.  No other characters, including spaces, are permitted.  The first character must be an alphabetical character.  The maximum length of a variable name for modern Fortran as of Fortran 95 is 31 characters.  The 6-character limit of Fortran 77 is long gone.  Some compilers permit up to 127 characters as an extension, though excessively long variable names is not a good programming practice.  
A good descriptive variable name often consists of multiple words or parts of words.  Since Fortran is not case-sensitive, underscores can be used to separate the components.
fortran
is_valid
start_date
num_species
Separation through capitalization is possible with the understanding that different variables cannot be distinguished by ""camel case.""
fortran
subroutine BioGeoChem
type myType
Variables are declared by indicating the type followed by a comma-separated list of variables.
In older code no separator was used.
fortran
INTEGER i, j, k
In newer code, use the double colon to separate the type from the variable list
fortran
INTEGER  :: i, j, k
If there are other attributes on the line the :: will be required .
It is not necessary to write keywords, or any source at all, in all capital letters, but they may be written in capitals here for clarity.
Declarations by Type
{{< table >}}
|   Fortran Name   |    Type   |   Standard?       |
|------------------|-----------|----------------------------|
|     INTEGER      |  32-bit integer |  Yes                  |
|     INTEGER*8   |  64-bit integer |  No, but nearly universal |
|     INTEGER(ik)  |  Integer specified by KIND |  Yes |
|     REAL         |  Single precision floating point | Yes  |
| DOUBLE PRECISION |  Double precision floating point | Yes, but deprecated style |
| REAL*8 |  Double precision floating point |  No, but universal |
| REAL(rk)|  Floating point denoted by KIND |  Yes |
| LOGICAL |  Logical (Boolean)  |  Yes |
| COMPLEX  |  Single precision complex  | Yes |
| COMPLEX*8 |  Double precision complex  | No, but nearly universal |
| CHARACTER  |  One character  | Yes |
| CHARACTER(LEN=10)  | Character variable with 10 characters | Yes |
| CHARACTER*10  | Character variable with 10 characters | Yes, but deprecated style |
|     BYTE         |  One byte  | Yes  |
{{< /table >}}
Other types may be specified through [KIND].
Implicit and Explicit Typing
For historical reasons, Fortran used implicit typing for numerical types.  Any variable starting with the letters A-H or O-Z were floating point.  Variables beginning with the letters I-N were integers.  Note that IN are the first two letters of the word ""integer.""  That is a longstanding mathematical tradition and Fortran was developed to translate mathematical notation (FORmula TRANslation).
Older code often changes the default float to double:
fortran
IMPLICIT DOUBLE PRECISION(a-h,o-z)
However, in modern usage, all variables should be explicitly typed.  This will enable the compiler to catch typographical errors.  If implicit typing is used, a new variable would not need to be declared and would assume the type based on its name, so a misspelling of an existing variable would create a different variable.  Bugs like this can be difficult to track down.
The statement
fortran
IMPLICIT NONE
negates implicit typing.  It must be the first line after a unit declaration unless a USE is present.
Example
fortran
PROGRAM simple
IMPLICIT NONE
INTEGER              ::   I, J
REAL                 ::   R, S, T
DOUBLE PRECISION     ::   D
DOUBLE COMPLEX       ::   Z
LOGICAL              ::   FLAG
CHARACTER (len=20)   ::   C
Line up declarations neatly.
Initializing at Compile Time
Variables can be declared and initialized at the same time:
fortran
real  :: x=1.e-8, y=42.
When variables are initialized in this manner it happens only once , at compile time.  If this takes place in a subprogram it will not happen again upon repeated invocations.
It is equivalent to the older DATA statement:
fortran
DATA x,y/1.e-8,42./
In Fortran 2003 it became possible to initialize using intrinsic functions:
fortran
real  :: pi = 4.0*atan(1.0)
Example
Start your choice of IDE or editor.  Type
fortran
program first
! My first program
! Author:  Your Name
  implicit none
  real     ::x,y
  integer  ::i,j=11
     x=1.0
     y=2.0
     i=j+2
     print *, ""Reals are "",x,y
     print *, ""Integers are "",i,j
end program
PARAMETER
In compiled languages, programmers can declare a variable to have a fixed value that cannot be changed.
In Fortran this is indicated by the PARAMETER attribute.
fortran
REAL, PARAMETER  ::  pi=4.0*ATAN(1.0)
Attempting to change the value of a variable declared to be a parameter will result in a fatal compiler error.
In older code the declaration and parameter statement will be on different lines
fortran
real  pi
parameter (pi=3.14159)"
rc-learning-fork/content/courses/fortran-introduction/statements.md,"If expressions are the ""words,"" then statements are the ""sentences"" of a programming language.  A statement is a combination of expressions and operators such as assignment (= in most languages) which describes a command to the compiler.
A Fortran peculiarity: statements are executable or non-executable .  Non-executable statements are instructions to the compiler (variable declarations, interfaces, etc.)  Executable statements perform some action. All non-executable statements must precede the first executable statements in a program unit.
Indentation is not required but should be used!
No semicolons should be used at the end of the line.
Multiple statements may be written on the same line if they are separated by semicolons.  If this is done each statement should be kept short.
Fixed Format versus Free Format
Prior to the Fortran 90 standard, Fortran code was required to conform to rigid column rules based on the layout of punched cards.
Statements began in column 7 and could extend to column 72.  Column 6 was reserved for continuation marks.  Columns 1-5 were for statement labels.  Columns 73-80 were ignored (and were used to number cards).  Because of the placement restrictions, this is called fixed format.
In Fortran 90 and up, there are no column rules.  This is called free format.
Comments and Continuations
Comments are ignored by the compiler.  They are for the benefit of human readers and writers.
Fixed format comment:
  * C or c in the first column meant the entire line was a comment.
Free format comment:
  * Anything from ! to the end of the line is ignored.
Due to its record-oriented history, Fortran uses the end-of-line marker to terminate a statement.  If the statement is too long to fit, or for aesthetic reasons the programmer wishes to extend the statement over multiple lines, a continuation marker must be used.
Fixed format continuation:
  * A number or printable character in the 6th column.
Free format continuation:
  * Ampersand & at the end of the line to be continued.
The maximum number of statement characters in a line for fixed format can be modified through a compiler option, with the default the traditional 72.  The default maximum line width for free format is 132 columns, but it can also be modified through compiler options in most cases.  For the case of gfortran, see their documentation.  For other compilers, consult their documentation.
Program Statement
A program may optionally begin with a PROGRAM statement which is optionally followed by its name.
PROGRAM myprogram
The program must end with an END statement.  Optionally it may be
END PROGRAM <name>
Use of the longer, more descriptive forms is strongly recommended.
Execution may be terminated with the STOP statement. STOP is required only for abnormal termination.  It can optionally be followed by a message, which it will print to the console.
STOP ""Attempt to divide by zero.""
Statement Labels
In fixed format code, statement labels were often used.
In fixed format statement labels must be integers and must occupy a maximum of five digits, at least one of which must be nonzero.
In free format there is less need for labels and they do not always need to be integers.
Any statement that is not part of a compound (semicolon-separated) statement can be labeled with an integer.
Miscellaneous
The no-op (do nothing) is CONTINUE.
It was often used in old code since do loops required a labeled statement as the terminator.
fortran
      do 100 i=1,n
         statements
100   continue
We use END DO now for this purpose.  However, CONTINUE is a convenient 
target for labels in input/output statements, and occasionally in other circumstances.
Fortran has a GO TO (or GOTO) statement.
fortran
    go to <label>
Modern practice is to avoid go to and some languages, such as Python, do not even provide it.  However, it is still sometimes useful and in some situations 
it is far more readable than the contortions required to replace it!
* As a rule goto should always direct the flow downward, never upward.
Hello World
An improved example:
fortran
program hello
implicit none
   integer  :: i
   real     :: pi=0.25*atan(1.0)
      i=42
      print *, 'Hello! The answer is', &i,' and pi is ',pi
end program
Exercise
* Write a program that will set variables as indicated and will print the expressions indicated.  Use print * as shown above to print to the console. Invoke 
implicit none and declare all your variables.
fortran
x=17.
Xs=11.
num_1=10
num_2=14
! Print the following expressions (remove comment marker)
!x
!Xs/x
!int(Xs/x)
!int(Xs)/int(x)
!Xs/x + x
!Xs/(x+x)
!x/num_1
!num_1/num_2
!num_2/num_1
{{< spoiler text=""Example Solution"" >}}
{{< code file=""courses/fortran-introduction/solns/statements.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/more_classes.md,"Classes are fairly new to Fortran and we will mention only a few more of their features.  Some, such as inheritance, are applicable to all types.  We will describe some additional features below. 
Data Hiding
One of the main purposes of OOP is to prevent outside units from doing anything without “sending a message” to an appropriate instance.
As for modules, we can specify access through the PUBLIC and PRIVATE statements or attributes.  As for modules, the default is PUBLIC.
The previous example violates this principle.  We can make everything private, which means that only members of the module can access the members of the class.  We must then go through an instance of the type/class to invoke the procedures.
Making a type public “exposes” the type name and its type-bound procedures, but not its variables if the module default is set to private.
We will modify the example to accomplish this.
Modified Example
{{< code-download file=""/courses/fortran-introduction/codes/private_class_module.f90"" lang=""fortran"" >}}
In the caller:
fortran
CALL myvar%init(i,j,x,y)
CALL write_class(myvar,11) ! illegal, link error
CALL myvar%write(12)       ! OK
Constructors and Destructors
A constructor is a subprogram that handles the bookkeeping to initialize an instance of a type. This may entail:
  * Assigning values to variables
  * Allocating memory for allocatable arrays
    * This never happens automatically.  If an allocatable is a member of a type, a constructor must be written.
A destructor is a subprogram that releases memory for a type.  This may be required if you allocate in a constructor.  The garbage collection in subprograms will not release memory allocated for a type.
Fortran has no special syntax for a constructor or destructor.  Programmers can define an init function or equivalent, then declare it private to be sure it can be accessed only through a type instance.  Destructors can be similarly written to deallocate arrays.
Inheritance
Inheritance is when a type is derived from another type and has access to its members.  Inheritance is not restricted to classes in Fortran but can be used with types as well.
```fortran
type Parenttype
integer ::my_id
real    ::my_value
end typeParenttype
type, extends (Parenttype) :: Childtype
integer ::my_int
end type Childtype
```
Attribute Inheritance
The child type inherits all the attributes of its parent.
fortran
type(ChildType) :: billy
billy%my_id !is valid, and is equivalent to
billy%ParentType%my_id
But billy%my_int does not refer back to the parent, since that variable occurs only in the extension.
Class Inheritance
When a class is extended, not only the components but the procedures are inherited.
{{< code-download file=""/courses/fortran-introduction/codes/clinherit.f90"" lang=""fortran"" >}}
Exercise

Write a class Atom that contains the following attributes:
Element symbol
Element name
Isotopic mass (mass of a single isotope, not the ""atomic weight"" averaged over a mix of isotopes)
Atomic number
The method should be
Compute and return the number of neutrons from the mass and number (n=mass-number)
"
rc-learning-fork/content/courses/fortran-introduction/classes.md,"A class is a data structure that encapsulates both data (variables) and behavior (procedures).  The only difference between a derived type and a class is that a class may contain procedures as well as variables.
The major difference between a class and a module is that modules cannot be instantiated; that is, variables can be declared of any type it may encompass, but not of the module itself.
OOP Terminology
An instance of a type or class is a variable of that type/class.
fortran
type(mytype)   :: A, B
A and B are instances of mytype.
The variables and procedures that belong to the class are often called members of the class.
Types with Procedures
Types containing type-bound procedures were introduced in Fortran 2003.  They are nearly synonymous with methods in other languages.
If a type-bound procedure is public it can be called in the conventional way from a unit that creates a variable of the type.
If the procedure is private then it can be accessed only via an instance of the type.
As for modules, the default is public.  
Type-bound procedures are declared using CONTAINS and the PROCEDURE keyword.
fortran
TYPE mytype
   REAL      ::x,y
   CONTAINS
      PROCEDURE ::init=>init_class
      PROCEDURE :: write=>write_class
      PROCEDURE :: reset
END TYPE
The => operator is optional.  It means that the procedure on the left-hand side will be used through an instance to invoke the actual procedure on the right-hand side.
fortran
TYPE(mytype) :: v
   v%init(arg1,arg2)
   v%write()
   v%reset(arg)
Instance Parameters
In Fortran the instance variable must be passed explicitly as the first parameter to the procedure, unless the PASS keyword is utilized.
The instance variable must be declared as CLASS rather than TYPE.
When we invoke the procedure we do not explicitly pass the instance argument.
If it does not need to be passed at all (for the equivalent of a class method in other languages), the NOPASS attribute can be added, as discussed below.
Filling out the procedures gives us
{{< code-download file=""/courses/fortran-introduction/codes/class_module.f90"" lang=""fortran"" >}}
There is no reserved word in Fortran for the class instance variable.  Some authors use ""self"" as is conventional in Python.  Others use ""this"" to imitate C++ and Java.  Others prefer their own conventions.
Invoking Class Methods
The write_class procedure is not private so the second two calls are equivalent.
call myvar%init(x,y)
call write_class(myvar,11)
callmyvar%write(12)
Even in languages such as C++, where the ""real"" form write_class(this,param) is not accessible, the compiler still constructs a ""real"" function with a unique name for the method and invokes it in the usual way.  The class name is typically attached as well as the instance variable appearing explicitly.  The ""true"" name of the method is said to be ""mangled.""  Name mangling is also used to distinguish overloaded functions which can seemingly take different types for the same calling sequence.  Various language rules, such as not passing the instance variable, are often called ""syntactic sugar"" by computer scientists, since their aim is to simplify the human-written code.
PASS and NOPASS
If it is desirable to pass the instance variable at some position other than the first, the PASS attribute may be used.
```fortran
module mytype_mod
implicit none
TYPE mytype
   contains
      procedure, pass(z) :: mysub
end TYPE
contains
  subroutine mysub(y,z)
  class(mytype) :: z
  real          :: y
     print *, y,z
  end subroutine

end module
In this case `z` will represent the instance variable, so the procedure will be invoked likefortran
call z%mysub(y)
```
For cases where no instance variable is required at all, the NOPASS attribute may be used.  This creates something analogous to a type of method that is sometimes called a class method in other languages.
```fortran
module mytype_mod
implicit none
TYPE mytype
   contains
      procedure, nopass :: mysub
end TYPE
contains
  subroutine mysub(y,z)
  real          :: y,z
     print *, y,z
  end subroutine

end module
Invoke this withfortran
call t%mysub(w,x)
If `mysub` is public it can even be called withfortran
call mysub(w,x)
```
in which case it is effectively equivalent to a procedure in a module"
rc-learning-fork/content/courses/fortran-introduction/derived_types.md,"Programmer-Defined Datatypes
So far we have used only the predefined types available in the Fortran standard.  However, an important principle of modern software engineering is separation of concerns and encapsulation.  We would like for related data to be connected, and we want each program unit to implement a well-defined set of actions, its ""concern.""  This also allows the programmer to control the interface, the way in which other parts of the program interact with the data.
For example, consider a program to update employee information.  We can define several variables relevant for an employee; for example we might use salary, name of manager, name of department, employee ID number, and so forth.  Each of these is potentially a different type.  Salary would be floating point, the names would be strings, and the ID number would generally be an integer.  We have more than one employee to handle, so we must use some form of list or array.  In most languages we cannot define a single array to accommodate all these fields.
This leads to the need for a way to keep all the information about one employee coordinated.
If we were restricted to predefined types in Fortran, we would have to declare separate arrays for each field of interest.  When processing data, we would have to take pains to ensure that the index of one array was correct for another array.  Suppose we wanted to find all employees making more than a certain amount.  We would have to search the ""salary"" array for the elements that met the criterion, while storing the index into some other array, so that we would have an array whose contents were the indices for other arrays.  This is very prone to errors. 
fortran
integer,            dimension(:), allocatable:: employee_ID
character(len=128), dimension(:), allocatable:: employee_name
character(len=128), dimension(:), allocatable:: employee_manager
character(len=128), dimension(:), allocatable:: employee_dept
real,               dimension(:), allocatable:: employee_salary
We need a different type of data structure.  Programmer-defined datatypes allow the programmer to define a new type containing the representations of a group of related data items.
For example, some languages define dataframes, which are essentially representations of spreadsheets with each column defined as something like an array. This would be an example of a defined datatype, since it must be described relative to basic types available in the language.  This is perhaps easier in languages that use inferred typing, where the interpreter or compiler makes its best guess as to the type of data, as opposed to statically typed languages like Fortran or C++.  But conceptually it is a good example of a programmer-defined datatype.
In Fortran abstract datatypes are called derived types.  The syntax is extremely  simple; in the example, ptype stands for a primitive type.
```
TYPE mytype
   <ptype> var1
   <ptype> var2
   <ptype>, DIMENSION(:), ALLOCATABLE:: var3
   TYPE(anothertype) :: var4
END TYPE mytype
```
Variables ofmytypeare declared as
```
type(mytype) :: x, y
```
We access the fields using the%separator:
```
z=x%var1
w=y%var2
allocate(x%var3(N))
```
where the variableszandw` must be declared to match the type, including attributes such as ALLOCATABLE, of the field of the type.  As shown above, a TYPE may be a member of another TYPE as long as its definition has already been seen by the compiler.  Variables that belong to the type are usually called components in Fortran.
Note that a type is a scoping unit.
We can apply this to our employee example.  The longer name for the fields is not helpful since we declare everything to be pertinent to an ""employee.""
fortran
TYPE employee
   INTEGER             :: ID
   CHARACTER(len=128)  :: name
   CHARACTER(len=128)  :: manager
   CHARACTER(len=128)  :: dept
   REAL                :: salary
END TYPE
We can now declare employees
```fortran
TYPE(employee) :: fred, joe, sally
real           :: raise
raise=0.02
   fred%salary=(1+raise)*fred%salary
```
Arrays and Types
Types may contain arrays and from F2003 onward, those arrays may be allocatable. Very few compilers do not support this standard but if one is encountered, the POINTER attribute must be used.  We will not discuss POINTER further but it may be seen in code written before F2003 compilers were widely available.
In Fortran, the array data structure is a container and the elements of an array may be derived types.
fortran
TYPE(employee), dimension(:), allocatable :: employees
We allocate as usual
fortran
num_employees=126
allocate(employees(num_employees))
Arrays and Modules
We nearly  always  put derived  types  into modules; the module will  define procedures that operate on the type. The module must not have the same name as the derived  type, which can be somewhat inconvenient. 
A derived type may need to be initialized explicitly.
For example, if you need to allocate memory, say for an allocatable array, to create a variable of a given type, this will not happen automatically. You must write a constructor to allocate the memory.
**Example**
This type is a set of observations for birds denoted by their common name.fortran
TYPE bird_data
   CHARACTER(LEN=50)                  :: species
   INTEGER, DIMENSION(:), ALLOCATABLE :: obs
END TYPE 
A constructor-like procedure would befortran
MODULE bird_obs
TYPE bird_data
   CHARACTER(LEN=50)                  :: species
   INTEGER, DIMENSION(:), ALLOCATABLE :: obs
END TYPE 
CONTAINS
SUBROUTINE constructor(bird,species,obs)
      TYPE(bird_data),       INTENT(INOUT) :: bird
      CHARACTER(LEN=50),     INTENT(IN)    :: species
      INTEGER, DIMENSION(:), INTENT(IN)    :: obs
     bird%species=species
     allocate(bird%obs(size(obs)))
     bird%obs=obs

END SUBROUTINE
END MODULE
``
It is important to understand that thespeciesthat is a member of the type is _not_ the same as thespeciesthat is passed in toinit_bird.  In Fortran we can easily distinguish them since we _must_ use the instance variable,birdin this case, as a prefix; not all languages require that.  In C++ we would need to usethis->species(this` is the ""invisible"" instance variable in that language) if an attibute has the same name as a dummy parameter.
Exercise
Write a main program to use the bird_dat module.  Assume you will read the bird data from a CSV (comma-separated values) file with each row consisting of a string for the species and then 10 numbers for observations over 10 years.  Create a file 
""Species"",2000,2001,2002,2003,2004,2005,2006,2007,2008,2009
""BlueJay"", 24, 23, 27, 19, 22, 26, 28, 27, 24, 30
""Cardinal"", 11, 15, 18, 18, 19, 17, 20, 21, 20, 19
Use this file to test your program.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/bird_reader.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/characters.md,"Fortran's support of characters and strings has evolved significantly through the recent standards. The earliest Fortran used Hollerith codes to represent characters.  The character type was introduced in the Fortran 77 standard. The original character type is essentially a fixed-length string.  Variable-length strings were introduced in the Fortran 2003 standard.  
Character Sets and Encodings
Like everything else in the computer, characters must be represented by a sequence of 0s and 1s.  A catalogue of these representations is usually called an encoding.
The basic character set used by Fortran is ASCII, the American Standard Code for Information Interchange. Internationally, it is sometimes known as US-ASCII.  Originally, 7 bits were used to represent data, resulting in a total of 128 (27) available characters.  The first 32 are non-printing characters that were mainly needed by the mechanical devices for which the encoding was developed.  Only a few non-printing characters are still used, among them line feed, carriage return, and even ""bell.""  
Exercise:
fortran
program ringer
    print *, char(7)
end program
ASCII now uses 8 bits (extended ASCII) which doubles the number of available characters, but the first 128 character codes are the same as 7-bit ASCII.  It was not as well standardized as ASCII, though standards exist for Latin alphabets (ISO 8859-1, ISO Latin 1) and Cyrillic (ISO 8859-5).  Nearly all programming languages continue to restrict the characters allowed for statements to the original ASCII, even when larger character sets are the default and may be used for comments and output.
Even extended ASCII accommodates far too few characters to accommodate more than a handful of alphabets, much less other writing systems.  Unicode was created to address this.  The first 128 codes are still the 7-bit ASCII codes even with the millions available through Unicode.  Fortran supports a version of Unicode called ISO_10646 for comments and output, though not all compilers implement it yet.
Example 
from the gfortran documentation. It may compile only with gfortran.
{{< code file=""courses/fortran-introduction/codes/iso.f90"" lang=""fortran"" >}}
Fixed-Length Strings
Declare character variables with
fortran
CHARACTER(len=<N>) :: string
The value of N must be an integer literal and it must be large enough to contain all characters in the string, including whitespace (tab character, space).
Arrays of character variables are permitted.
fortran
CHARACTER(len=3), DIMENSION(10) :: string
This is a 10-element, rank-1 array, each of whose elements is a length-3 character.
Variable-Length Strings
Variable-length strings are declared similarly to allocatable arrays.
fortran
CHARACTER(len=:), ALLOCATABLE :: string
The string must be allocated in the executable code before it is used.
fortran
   num_chars=5
   allocate(character(len=num_chars) :: string)
Allocatable arrays of allocatable strings are possible, but will require creating a derived type.
An allocatable string may be deallocated if necessary with the usual DEALLOCATE intrinsic.
fortran
DEALLOCATE(str)
Prior to Fortran 2003, the standard defined a module iso_varying_string.  Most compilers available now support the 2003 standard so will offer the standard variable string, but the iso_varying_string module provides a number of functions so may still be worthwhile.  We will discuss standardized modules later.
Substrings
Similar to arrays, slices or substrings may be extracted from strings.  Characters are counted starting from 1 at the left and the upper bound of the range is included.
If the lower bound is omitted, the first character is assumed.  If the upper bound is omitted, characters from the specified character to the end of the string are included.  To extract a single character, the range is from its position to the same position.
```fortran
character(len=11) :: message
message=""Hello world""
print *, message(1:5),"" "",message(5:5),"" "",message(7:)
This results in
 Hello o world
```
Concatenation
The only string operator is concatenation //.  Try this out and see what
is printed.  Remember to add the program, implicit none, and end program statements to your program, and indent properly.
```fortran
character(len=5)  :: word1, word2
character(len=20) :: message
word1=""Hello""
word2=""world""
message=word1//"" ""//word2
print , message
Now tryfortran
print , message//"" today""
You should see
Hello world          today
```
String Length
A useful string function, especially for variable-length strings, is LEN(S).
A fixed-length string will always occupy the specified number of characters. The default is to left-justify non-blank characters in the field.  This can be modified with intrinsics.
Exercises
* Declare character variables large enough to hold the indicated strings.  Make full_title at least 5 characters longer than you think necessary.
fortran
title=""Jaws""
subtitle=""The Revenge""
print *,len(title)
full_title=title//"":""//subtitle
print *, full_title
print *,len(full_title)
print *,full_title(2:4)
   1. Change “Jaws” to “Paws” in full_title
   2. Make the strings variable sized.  Use the len function.
{{< spoiler text=""Solution with variable strings."" >}}
{{< code file=""courses/fortran-introduction/solns/var_strings.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/modules.md,"Modules are subordinate program units that can contain multiple subprograms as well as associated variables.
Modules allow you to organize your code into logically-connected units.  It is a form of object oriented programming.
They should contain coherent data+procedures.
Modules permit data hiding.  Variables and subprograms may be kept private from other program units.  This prevents another source of error, by reducing the number of variables an outside program can affect or procedures it can call.
Fortran Modules
Each module has a name that must be unique.  A module begins with
fortran
MODULE modname
and ends with
fortran
END MODULE [NAME]
Modules are typically placed into separate files.  The file name does not need to be the same as the module name, but the module will be referenced by its name and not by the file name.  It is acceptable for short, closely-related modules to be in the same file.  If more than one module is in a file, each must be USEd individually.
Using Modules
Modules are brought in via the USE statement
fortran
USE mymodule
All USE statements must be the first executable statements after the declaration of the program unit (program, function, subroutine), before any IMPLICIT statement and the variable declarations.
There is no distinct ""namespace"" for a Fortran module.  Names imported into the USEing unit do not acquire a distinguishing name.
Variations of USE
Only specified routines can be brought in with ONLY:
fortran
USE mymod, ONLY : f1, f2, s4
Routines can be renamed:
fortran
USE mymod, name-here => name-in-module
USE stats_lib,sprod=>prod
Module Variables.
IMPLICIT NONE at the top applies throughout the module.
All variables declared or types defined before a CONTAINS statement are global throughout the module.
Module symbols (variables and names of procedures) can be private .  You may also explicitly declare them public but that is the default.
The private and public attributes may be added to the declaration, or they may be specified separately with a list following.
Private variables are not directly accessible by program units that use the module.  Only procedures in the same module can access them.
Using PRIVATE or PUBLIC as a separate statement without a list sets or resets the default and may be done only once per module.  Public and private may only be set in the specification (interface) portion of the module, not in the procedure bodies.
We will discuss PUBLIC and PRIVATE in more detail when we cover classes.
Example
MODULE mymod
USE precisions
   REAL, PRIVATE  :: x, y, z
   REAL(sp)       :: r_fun
   REAL(dp)       :: d_fun
   PRIVATE        ::r_fun,d_fun
where we assume the precisions module defines the KIND parameters sp and dp.
Subprograms in Modules
Subprograms defined in a module must follow a CONTAINS.
The FUNCTION or SUBROUTINE keywords after END are not optional, e.g. END SUBROUTINE is required.  The name of the procedure is still optional and some authors recommend not using it, in case it is changed later or to avoid cut and paste errors.
All subprograms in a module have an implicit interface.  You should not write an explicit interface for them, and in fact it’s illegal to do so.
Example
{{< code file=""/courses/fortran-introduction/codes/module.f90"" lang=""fortran"" >}}
Modules and Make
A module must be compiled before any other file that uses it.  This can create a complicated build environment, so make or a similar build manager is usually used.
Exercises

Type the module mymod into a file mymod.f90.
Fortran allows the module and the file to have either the same or a different name, but the name of the module is the name that must appear in the use statement.
Fill out the subroutine mysub to set b to 11., then set x to the sum of corresponding elements of a and b.  Hint: you can use x=a(:)+b(:size(a)) to avoid a loop.
Write a main program main.f90 that uses mymod, initializes A allocatable, allocates it to 1000, sets its values to i+3 in a loop, then passes it to mysub.   Print the value of x that is returned.
Create a Makefile.  If you wish you may copy the example Makefile from the earlier chapter.  Make the appropriate changes to the program name, the names of the source files, and the names of the object files.  Make the dependency line at the end
make
main.o:main.o mymod.o
Run a make project in Geany or your preferred IDE.
"
rc-learning-fork/content/courses/fortran-introduction/file_io.md,"Reading from and writing to the console works for simple programs, and is often used even in more complex codes to print error messages, progress indicators, and the like, but for most purposes we need to read from and write to files.  
Open
A file must be opened before it can be accessed by the executable.  In general, we associate some type of file descriptor with the name of the file, then after making that connection, henceforth the file is referenced by its descriptor.
In Fortran, files are identified with integers called unit numbers. They are not generated automatically, but must be chosen by the programmer.
fortran
OPEN(UNIT=iunit,FILE=fname)
The open command has many other options.  Only UNIT and FILE are required.  If the unit argument is first it does not need the ""UNIT="" keyword.
On Unix file names will be case-sensitive .
In Unix unit 5 is conventionally standard input and unit 6 is standard output.  Standard error is not as uniform, but it is usually unit 2.
Programmers can reassign units 2, 5, and 6, but it is strongly advised that you not do so.
Close
Much of the time, it is not necessary to close a file explicitly.  Files are automatically closed when execution terminates.
If many files are opened, it is good practice to close them before the end of the run.
fortran
CLOSE(UNIT=iunit)
If you wish to reopen a file for some reason, you must first CLOSE it.
Read/Write with Files
The file must first be opened and a unit assigned.
READ(iunit,*)
List-directed output is indicated by an asterisk.  Formatted output requires a format string, or a reference to a labeled FORMAT statement.
WRITE(iunit,*)
WRITE(iunit,'(fmtstr)')
or
WRITE(iunit,label)
label FORMAT(fmtstr)
If the unit identifier is not the first in the list it must be written as UNIT=iunit.  The UNIT= keyword is optional otherwise.
NAMELIST
One of the most convenient I/O statements in Fortran is NAMELIST.  With this statement, parameters in an input file can be specified by name=value pairs and in any order.
The namelist must be declared.  This is a non-executable statement.  The syntax is:
fortran
NAMELIST /name/ var1,var2,var3
The name is chosen by the programmer.
The namelist is read with a special form of the READ statement.
fortran
read(iunit, name)
Namelist Input
The input file containing the namelist must follow a specific format. Namelist was not part of the Fortran 77 standard so there is some variation.  However, thenamelistalways starts with
fortran
&name
The variable list follows, with each variable on a separate line and consisting of the varname=value pair.
In older code, the namelist frequently terminates with another ampersand &, 
or &end.  Also, in Fortran 77 there may be rules about in which column the & can occur.
Namelist was established as part of the standard in Fortran 90. According to the standard, the namelist is terminated with a forward slash /.
Example
In the program
```fortran
NAMELIST /params/ rho,eps, x0
OPEN(10,file='paramlist.txt')
READ(10,params)
The input file (Fortran 90 format) would befortran
&params
rho=1.3
eps=1.e-7
x0=0.0
/
```
Exercises

Write a program that creates a file mydata.txt containing four rows consisting of
1, 2, 3
4, 5, 6
7, 8, 9
10, 11, 12
Close the file, then open it again.  Read the data back.  Write a loop to add 1 to each value and print each row to the console.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/reopen.f90"" lang=""fortran"" >}}
{{< /spoiler >}}

Write a program that reads the params namelist and prints the variables to the console.  Create the paramlist.txt file and test your program.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/nlist.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/operators.md,"Operators are characters or groupings of characters that take some number of variables or literals as operands, apply some specific mathematical, logical, or translational operation, and return a result.  Operators are defined by each programming language, although basic ones are often the same or similar.  The majority are mathematically binary operators, i.e. they take two operands, though nearly all languages have some unitary operators and a few have operators that take three or more operands.  Each operand must be of the specific types for which an operator is valid.
Basic Operators
Arithmetic Operators
These operators are defined on integers, floats, and doubles.  
+ - add, subtract
* / multiply, divide
** exponentiation
Operators are applied in a particular order.  This is called precedence.
First: ** 
Second (equal status):  * /
Third (equal status):  + -
Evaluation is left to right by precedence unless parentheses are used to 
specify a different ordering.
5+11*6=71
(5+11)*6=96
The mnemonic PEMDAS is sometimes applied--ParenthesesExponentsMultiplicationDivisionAdditionSubtraction--but remember that MD and AS are equals within their ranking.
Not all programming languages have an exponent operator.  The base and exponent may both be integers or floating-point numbers.
Handy Trick
Many Fortran compilers will recognize integer exponents and, at least for relatively small ones, will perform a multiplication, whereas floating-point exponents are evaluated with the much slower logarithm functions.  Always remember that a literal like 3.0 is a floating point number, not an integer.
Right: x**3
Wrong: x**3.0
Special Integer Operators
Division.  In Fortran 2/3 is always zero!  Why?
This is because 2 and 3 are both integers, so / is an integer operation that yields an integer result.  This is a frequent source of bugs in compiled languages.
Exercise:
What is 9/5?
Remainders.
MOD(N,M).  The mod intrinsic function returns the remainder of a division.  It is computed as N-(INT(N/M)*M).
MODULO(N,M). The modulo intrinsic function returns N mod M, which is computed as N-FLOOR(N/M)*M. 
Points to note:
  * MOD and MODULO are not the same for negative numbers.
  * MOD is most frequently used though MODULO is closer to other languages' % operator.
  * Use for negatives is uncommon in all languages.
Mod and modulo are defined for negative values and reals, as well as non-negative integers, but the results, while well-defined mathematically, are not generally what most programmers are expecting.  For this reason they should generally be avoided for arguments other than non-negative integers.
Example
{{< code-download file=""/courses/fortran-introduction/codes/testmod.f90"" lang=""fortran"" >}}
Expressions
An expression is a combination of variables, operators, and function invocations that can result in a unique evaluation.
Fortran expressions are much like those of other languages.
fortran
a+3*c
8.d0*real(i,dp)+v**3
phase+cmplx(0.,1.)
sqrt(abs(a-b))
A .or. B
y > 0.0 .and. y < 1.0
myfunc(x,y)
Type Conversions
As we have seen with the example of dividing two integer, operators are defined on specific types and return a specific type.  What if we write 2./3?  The first operand is a real, whereas the second is an integer.  This is called a mixed expression.  For consistency, one type must be converted to match the other before the operator is applied.  Type conversion is also called casting.
Most compilers will automatically cast numeric variables in mixed expressions.  The variables are promoted according to their rank.  Lowest to highest rank, the types are integer, real, double, complex.  Therefore, integers will be converted to float if necessary, floats to double precision, then to complex.
The rules for numerical type conversions may result in some surprises.  For example, when a real is converted to double precision, the extra bits in the significand are filled (""padded"") with zeros.  There is no magic that tells the compiler how to extend it ""correctly.""  To illustrate with a base-10, 5-digit example:
fortran
real             :: r
double precision :: s
r=1./3.
s=r
We would find that, in this (artificial) number system,
fortran
r=.33333
1.d0/3.d0=.3333333333
s=.3333300000
Fortran, like most programming languages, also provides means for the programmer to specify when a type conversion should take place.
Use this explicit casting to be clear, or in circumstances, such as argument lists, where the compiler will not do it.
The new way to cast numbers is via KIND.  Older conversion functions such as dble can still be used and will usually be present in older code.
Logicals cannot be cast to anything, even though they are usually represented internally by integers.
Examples
Explicit casting among numeric types, default kind.
R=REAL(I)
I=INT(R)
Z=CMPLX(r1,r2)
D=DBLE(R)
Using KIND (with predetermined parameters)
R=REAL(I,dp)
D=REAL(R,dp)
Character/Numeric
Fortran has a peculiar way to do this called internal read/write.
Convert numeric to character:
character(len=4)  :: age
integer           ::iage
  iage=39
  write(age,'(i4)') iage
Convert character to numeric:
age='51'
   read(age,'(i4)') iage
The character variable to be converted always appears as the first argument to the read or write.  It is called a buffer.
To remember whether to use read or write, keep in mind that if we wish to convert numeric to character we know the number but not the character, so we will write it to the buffer.  For character to number, we will read the known characters from the buffer and write them into the target numeric variable."
rc-learning-fork/content/courses/fortran-introduction/getting_started.md,"Some History
Fortran and Algol
Algol 60 is of similar age and similar design.
Some sample code (from Wikipedia):
plaintext
procedure Absmax(a) Size:(n, m) Result:(y) Subscripts:(i, k);
   value n, m; array a; integer n, m,i, k; real y;
comment The absolute greatest element of the matrix a, of size n by m, is transferred to y, and the subscripts of this element to i and k;
begin integer p, q;
    y := 0; i := k:= 1;
    for p := 1 step 1 until n do
       for q := 1 step 1 until m do
           if abs(a[p, q]) > y then  
           begin
              y := abs(a[p, q]); i:= p; k := q
           end
end Absmax
Note that in Algol, a begin/end block is logically a single statement.
Similar code in early Fortran would look like
fortran
      SUBROUTINE ABSMAX(A,N,M,I,K,Y)
      INTEGER N, M
      INTEGER A
      DIMENSION A(N,M)
      INTEGER Y
      INTEGER I, K
      INTEGER P, Q
C  THE ABSOLUTE GREATEST ELEMENT OF ARRAY A OF SIZE NxM IS COMPUTED AND RETURNED
C  IN Y. THE CORRESPONDING LOCATION SUBSCRIPTS ARE RETURNED IN I AND K.
      Y=0
      I=1
      K=1
      DO 200 P=1,N
          DO 100 Q=1,M
             IF ( ABS(A(P,Q)) .LE. Y) GO TO 10
                Y=ABS(A(P,Q))
                I=P
                K=Q
   10        CONTINUE  
  100     CONTINUE
  200 CONTINUE
      END
Some of the peculiarities of the first versions of Fortran, such as FORTRAN IV, were due to IBM's use of punch cards as input devices.
{{< figure src=""/courses/fortran-introduction/img/PunchCard.jpg"" width=500px caption=""80-column punch card"" >}}
Punch cards were prepared on machines called keypunches. The character set on a keypunch was limited, so Fortran used all capital letters and few other characters.  There was no opportunity to correct typographical errors, which may account for early Fortran ignoring spacing within keywords. The layout in Fortran was determined by the physical layout of the cards; this, combined with the need to write a simple, memory-conserving compiler, resulted in a strict column-oriented syntax.
This is called fixed format.  It required that the first column be reserved for the C that introduced a comment.  The next four columns were for numerical statement labels.  Column six was reserved for continuation characters.  Statements began in column 7 and could extend through column 72; anything from column 72 to 80 was ignored by the compiler.  These columns were frequently used to number the cards.
Algol, on the other hand, was written for paper tape.  Whereas punch cards are record (i.e. line) oriented, paper tape is a continuous medium, so the semicolon marked the end of a statement.  Algol's descendants, which include C and C++, were also written for paper tape.  Semicolons continue to be used in many languages to mark the end of a sentence; such languages do not use continuation markers.  Record-oriented languages such as Fortran and Python do not require statement markers (though both modern Fortran and Python allow them) and provide for continuations.
Newer Fortran
The language has changed dramatically since 1957 but the name has never changed.
The above subroutine in modern, free format Fortran is
```fortran
subroutine absmax(a,i,k,y)
!  The absolute greatest element of array A of size NxM is computed and returned
!  in Y. The corresponding location subscripts are returned in I and K.
   integer, dimension(:,:), intent(in) :: a
   integer,                 intent(out):: i,k
   integer,                 intent(out):: y
   integer                             :: p,q
y=0; i=1; k=1
   do p=1,size(a,1)
      do q=1,size(a,2)
         if (abs(a(p,q)) > y) then
            y=abs(a(p,q))
            i=p
            k=q
         endif
      enddo
   enddo
end subroutine
```
Strengths and Weaknesses
{{< table >}}
|Fortran        |  C++ (not C)   |
|---------------|----------------|
|(2003/8) Many math function built-ins |Limited mathematical built-ins |
|Multidimensional arrays a first-class data structure, array operations supported| True multidimensional arrays not possible without add-on libraries (Blitz++, Boost)|
|Does not support true strings yet, just character arrays| Good string handling (compared to C) |
|Classes somewhat clunky.  Modules fill much of this role| Straightforward implementation of classes (modules in C++20 standard) |
{{< /table >}}
Compiled Languages
Fortran and C++ are compiled languages.  Readers who are accustomed to 
interpreted languages such as Python, R, and MATLAB should be aware that
compiled languages are generally more complex than interpreted languages and 
require more steps in the development process. 
A compiler produces a stand-alone program for a given platform (cpu+operating system).  The output of a compiler is an object file, represented with a .o suffix on Unix.  Object files are in machine language but cannot be run independently.
A linker takes the .o files and any external libraries and links them into the executable.  Normally the linker is invoked through the compiler.
An interpreter interprets line by line.  The executable that is run is the interpreter itself.  Programs for interpreters are often called scripts.  Scripts are frequently cross-platform, but the interpreter itself must be appropriate to the platform.
Compared to interpreted languages such as Python, compiled languages are:
  * Generally stricter about typing (static typing) and memory allocation.
  * Memory must frequently be managed explicitly by the programmer.
  * Generally produce faster and more efficient runs.
Interpreted languages are:
  * Generally looser about typing (dynamic typing).
  * Generally have dynamically sized data structures built in.
  * Often run very slowly.
The workflow for compiled code consists of compiling, correcting syntax errors if necessary, then linking.  The process of compiling and linking is generally called building.  The product of a build cycle is the executable (also called a binary).  For each change, the entire process must be repeated.  It is easy to forget to recompile, then wonder why the change is not reflected in the output."
rc-learning-fork/content/courses/fortran-introduction/advanced_file_io.md,"The basic file input/output commands previously covered are sufficient for many programs, but far more control is possible through the use of other commands and options.  We will describe on the most common here.
A detailed overview of file IO can be found in Intel's documentation.
OPEN Options
Several options to OPEN are related to error checking and are common among several file IO commands:
fortran
IOSTAT=ios !Returns status into integer variable ios.  Nonzero value for failure.
IOMSG=iomesg !Returns into a character variable msg an informative message on error
ERR=label  !Jumps to the statement labeled `label` if an error occurs.
END=label  !Jumps to the statement labeled `label` on end of file.
The following are specific to OPEN and describe the file type:
fortran
STATUS=stat
The value of stat can be 'OLD', 'NEW' , 'REPLACE', 'SCRATCH', or  'UNKNOWN'.  (As usual, the strings are not case-sensitive.)  The default is 'UNKNOWN' (read/write permission).  If 'OLD' it must exist, and if 'NEW' it must not exist.  A 'SCRATCH' file is automatically deleted after being closed.
fortran
POSITION=pos
Position pos is 'ASIS' (the default), 'REWIND', or 'APPEND'.  REWIND returns the file pointer to the top, which will cause the file to be overwritten by new data.  APPEND leaves it at the end of the file so new data can be added.
fortran
FORM=fmt
The permitted values for fmt are 'FORMATTED' (the default, for a text file) or 'UNFORMATTED' (a system-dependent binary format). This term is not related to whether the text is ""formatted"" for printing or not.
fortran
ACCESS=acc
The access acc can be 'SEQUENTIAL' (the default), 'DIRECT', or 'STREAM'. Direct files must be unformatted.
Unless access is 'STREAM', an unformatted file will have a header and footer that is specific to a compiler and platform and may not be portable.  This is a relic of magnetic tape drives and allowed them to backspace and skip forward in the tape.  For a binary-format file similar to that produced by C/C++ programs and interchangeable with them, use
fortran
OPEN(UNIT=iunit,FILE=fname,ACCESS=stream,FORM=unformatted)
Inquire
The INQUIRE statement tests the status of a file.  Most usually we wish to check whether the file exists, or is already open, before we attempt to open it.
fortran
INQUIRE(UNIT=iunit,options)
or
fortran
INQUIRE(FILE=fname,options)
So we can inquire by unit or name but not both.
Common Options to Inquire
Several of the options to INQUIRE are similar to those of OPEN.  Others are more specific.
fortran
IOSTAT=ios     ! Like open, ios must be integer
ERR=label      ! Like open
EXIST=exists   ! Returns .true. or .false. into logical variable exists
OPENED=is_open ! Returns .true. or .false. into logical variable is_open
READ Options for Files
The options to READ and WRITE include those already described for console IO, as well as some that are only relevant to files.  As an example, for ACCESS=STREAM files only, the position specifier can be used to start reading from a particular location on the file.
fortran
POS=p
See documentation for more options.
CLOSE
The complete form of the CLOSE command is
fortran
CLOSE(UNIT=iunit,IOSTAT=ios,ERR=err,STATUS=stat,IOMSG=iomesg)
STATUS can be 'KEEP' (default) or 'DELETE'.  UNIT, IOSTAT, IOMSG, and ERR are like the corresponding options to OPEN.
REWIND
An open unit can be rewound.  This places the file pointer back to the beginning of the file.
The default is to rewind a file automatically when it is closed.
If you want to rewind the file to reread it, use
fortran
REWIND(iunit)
REWIND is convenient if the program must handle files whose lengths may vary.  Read through the file without storing any variables, count the number of lines, rewind, then allocate any arrays needed.
If your input files will always be of known length this isn’t necessary (or efficient), but often file length could vary with different data.
Example
{{< code file=""/courses/fortran-introduction/codes/fileio.f90"" lang=""fortran"" >}}
QUIZ
Why do I increment nlines after the read?
What would I do if I had one or more header lines?
{{< spoiler text=""Answer"" >}}
I need to wait until the line has been successfully read before I count it. Consider an empty file (zero lines).  I will immediately encounter end of file, so I want to exit then.  For a one-line file, it reads the first line and I count that, then next time around it hits end of file and exits.  Nlines=1 then, which is correct.  The rest follows by induction.  For the second question, if I have header lines then I must use one READ per line to move through them before entering the loop to read data.  I can ignore the contents of the line if they are unneeded by providing no variable to store the data.
{{< /spoiler >}}
Exercise
Read the mydata.txt file but don't assume you know how long it is.  Inquire whether it is present, then read it through just to count the number of lines.  Rewind the file to read the data.  
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/rewind.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/console_io.md,"Programs that cannot read input data and write their results to some medium are
of little use.  We have used print * but there is much more to input/output.
Console Input/Output
Most operating systems have some type of console.  Output written to the console appears as text on a terminal or equivalent.  Geany opens a window for console output when a program is executed.
The console in Fortran can be represented with an asterisk *.  In Unix this corresponds to the standard input stream for reading, and the standard output for writing.  For standard error a file must be associated with it.
Input/output commands can be list directed, where
the compiler handles the spacing and other aspects of the appearance of the output, or the programmer can explicitly format the output.
List-Directed Read/Write from/to the Console
Fortran read from the console.  Values may be separated by commas or whitespace:
fortran
READ(*,*) var1, var2, var3
READ requires the unit identifier as the first option; here the * indicates console input. The second option must indicate how to format the output, with * telling the compiler to make its own choices.  READ can take additional options to check for errors and perform other housekeeping tasks.
fortran
IOSTAT=ios
Returns status into the integer variable ios.  If the result is zero, the statement succeeded, otherwise it failed. Specific nonzero values are system-dependent.
fortran
IOMSG=msg
For IOMSG the specific error message will vary by compiler, and no standard length is specified, but a declared length of 128 should be enough.  Use the trim intrinsic to print it more legibly.
fortran
ERR=label   !Jump to label on error
END=label   !Jump to label on end of file 
EOR=label   !Jump to label on end-of-record (line) (nonadvancing READ only)
Fortran WRITE to the console:
fortran
WRITE(*,*) var1,var2,var3
As for READ, the first option to WRITE is the unit identifier and the second is a format descriptor.  The asterisk as the format argument specifies a list-directed write in which the compiler formats the output based on its defaults.
WRITE has optional arguments for error checking similar to READ: IOSTAT, IOMSG, and ERR.  It also has some optional arguments to control certain aspect of the output appearance, particularly those related to differences in conventional representation of numbers, for example:
fortran
DECIMAL=dec
where dec is a character variable or literal that evaluates to COMMA or POINT.  This controls whether floating-point numbers are printed with a comma or decimal point.
The PRINT statement always writes to the console (standard output for Unix).  The asterisk specifies list-directed IO.
fortran
PRINT *, var1,var2,var3
In Fortran the PRINT statement always writes an end-of-line marker after all variables have been output.  The WRITE statement does as well, unless told otherwise. This is the opposite of the behavior of write in most other languages.
Example
{{< code file=""/courses/fortran-introduction/codes/consoleio.f90"" lang=""fortran"" >}}
Reading from the Command Line
Input values can be read from the command line.  This is usually accomplished in an IDE through an option to Run.
We can read strings only.  You must convert if necessary to a numerical type using internal read/write.  See the discussion earlier.
The COMMAND_ARGUMENT_COUNT intrinsic returns the number of command-line options.  For each one, we must call GET_COMMAND_ARGUMENT with its number and a character buffer variable.
fortran
   nargs=command_argument_count()
   if (nargs .ne. 1 ) then
      stop ""No input specified""
   else
      call get_command_argument(1,nval)
      read(nval,'(i4)') n
      call get_command_argument(2,mval)
      read(mval,'(i4)') m
   endif
Example:
{{< code file=""/courses/fortran-introduction/codes/clio.f90"" lang=""fortran"" >}}
Exercises

In an “infinite” while loop:
Request an integer from the user with non-advancing input/output, e.g.
“Please enter an integer:” 
If the integer is 1, print “zebra”.  If it is 2, print “kangaroo”.  If it is anything else except for zero, print “not found”.  If it is 0, exit the loop.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/console_io.f90"" lang=""fortran"" >}}
{{< /spoiler >}}

Write a program that takes a string as the command-line argument.  Print the string to standard output.  Use trim or any other string operators or function s to make the output neat.  If you read a string from the command line you do not have to do any conversion of the variable.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/command_line.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/arrays.md,"Arrays are one of the most important types in Fortran. It can represent many mathematical entities such as grids, matrices, vectors, and so forth.
The members of an array are frequently called elements.
From the computational point of view, an  array is a data structure that contains data of the same type with each scalar element addressed by indexing into the array.  Indices must be integers.
An array has one or more dimensions .  The bounds are the lowest and highest indexes.  The rank is the number of dimensions.  The size of an array is the number of elements.  The shape is a tuple giving the size in each individual dimension.
Fortran Arrays
Arrays must be declared by type and either by size or by some indication of the number of dimensions.  The simplest case is static arrays, which are declared to be of a fixed size and shape.
fortran
REAL, DIMENSION(100)            :: A
INTEGER, DIMENSION(50,50)       :: INDS
CHARACTER(LEN=3), DIMENSION(12) :: MONTHS
By default the index starts at 1.  However, it can start at any integer less than the upper bound:
fortran
REAL, DIMENSION(-1:101,0:3) :: A0
Arrays may have zero size.
The maximum (standard) dimensions up to the F2003 standard is 7. This increases to 15 in F2008.
Fortran arrays carry metadata (shape, size, bounds, some other data) about themselves and these data can be extracted by intrinsic functions.
Each element can be addressed by its index or indices, which must be enclosed in parentheses.
fortran
A(3)
X(i,j)
Remember that the bounds start at 1 by default.
Compiler Bounds Checking
One of the most common errors in programming, in all languages that support an array or something like it, is attempting to access an element with values for the indices that are outside the declared bounds.  This frequently happens with loops such as
fortran
! Average nearest neighbors
do j=1,m
   do i=1,n
      A(i,j)=0.25*(B(i+1,j)+B(i,j+1)+B(i-1,j)+B(i,j-1))  !OOPS
   enddo
enddo
Most usually this results in an error such as ""Segmentation fault.""
In Fortran, compilers know the size and shape of each array, once allocated, so they are able to check each operation to make sure it is within the bounds.  However, since this can greatly slow down your executable, it must be invoked at compile time.  The name of the option may vary from one compiler to another.
For gfortran and Intel they are:
gfortran -g -fbounds-check mycode.f90
ifort -g -CB mycode.f90
ifort -g -check bounds mycode.f90
Be sure to remove the options -g -<array check> and replace with -O once your program is debugged.
Array Operations
Fortran supports array operations in which an action is applied across an entire array, elementwise.  All arithmetic operators and most of the mathematical functions are overloaded to accept array arguments, for arrays of numerical type.
fortran
T=3.0
A=3.14159*I
B=sin(A)
C=A/B !Watch out for elements that are zero
Array Orientation
Array elements are adjacent in memory and are arranged linearly no matter how many dimensions you declare. If you declare a 3x2 array the order in memory is
(1,1), (2,1), (3,1), (1,2), (2,2), (3,2)
“Orientation” refers to how the array is stored in memory , not to any mathematical properties.
Fortran is column-major oriented. Most other languages are row-major oriented.
Loop indices should reflect this whenever possible (when you need loops).
Fortran: outermost first.  Go right to left.
fortran
    do k=1,nz-1
       do j=1,ny-1
          do i=1,nx
              A(i,j,k) = C(i,j,k) !loop order is do k/do j/do i
          enddo
       enddo
    enddo
Array operations can be used to avoid loops in many cases, and the compiler will be able to optimize them automatically.
fortran
A=C
Obtaining Size and Shape Information
The SIZE(ARRAY [,DIM]) function returns the number of elements in the array if DIM is absent.  If it is present, it returns the number of elements in that dimension, e.g. number of rows or columns.
The SHAPE(ARRAY) function returns a one-dimensional array whose elements are the dimensions of the array.
The RANK(ARRAY) function returns the rank (number of dimensions).
Where
where functions like a “vectorized” loop+conditional.
The clauses must be array assignments.
fortran
where ( A>=0.0 )
   B=sqrt(A)
elsewhere
   B=0.
end where
Exercises

Open a new file (you can call it arrays.f90) and type
program arrays
! Array demofortran
implicit none
   real, dimension(10)    :: A
   integer                :: i, j
      A=[(0.2*real(i),i=1,10)]
end program
```

In arrays.f90
1 Print the size and shape of the array A
2 Declare a new real array W of shape 10x10. Set all values to 1.0 using an array operation. 
3 Declare another 10x10 array Z.  Use a nested loop to set each element of W(i,j) to the sum of its row and column indices. Remember that Fortran loops should run over indices from right to left, e.g.
fortran
do j=1,N
   do i=1,M
      A(i,j)=something
   enddo
enddo
4 Change the fourth column of the second row of Z to 1.1


In arrays.f90

Add a WHERE to set elements to W equal to the corresponding elements of Z if the element of Z is less than 7.  Print W(2,4).

{{< spoiler text=""Example solution"" >}}
{{< code file=""/courses/fortran-introduction/solns/arrays.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/interfaces.md,"If your subprogram is not in a module you should provide an INTERFACE.
The INTERFACE is equivalent to the prototype of some other languages.
Interfaces enable the compiler to check that the number and type of the argument list in invocations agrees with the declared parameter list.
Interfaces are non-executable and should be placed with (or immediately following) variable declarations.
Syntax
Function
fortran
INTERFACE
  FUNCTION myfunc(x,y,z)
     implicit none
     real ::myfunc
     real ::x,y
     complex :: z
  END FUNCTION
END INTERFACE
Subroutine
fortran
INTERFACE
  SUBROUTINE mysub(x,y,z)
     use precisions
     use mymod
     implicit none
     real     :: x
     real(dp) ::y,z
  END SUBROUTINE mysub
END INTERFACE
The simplest way to set up an interface is to copy the first lines of the subprogram.  All statements that may affect the ability of the compiler to check number and type of the arguments must be included.  This encompasses USE (for modules), IMPLICIT, and all declarations of the arguments.  Declarations for local variables are not needed and should not be included.  The END statements must include the FUNCTION or SUBROUTINE keyword as appropriate. 
The interface terminates with END INTERFACE.
Interface Blocks
Only one interface block is required per program unit.
fortran
INTERFACE
  function mysub
    declarations
  end function
  subroutine mysub1
    declarations
  end subroutine
  subroutine mysub2
    declarations
  end subroutine
END INTERFACE"
rc-learning-fork/content/courses/fortran-introduction/subprograms.md,"A subprogram is a self-contained, but not standalone, program unit.  It performs a specific task, usually by accepting parameters and returning a result to the unit that invokes (calls) it.
Subprograms are essential to good coding practice.  Among other benefits, they are
  * Reusable.  They can be called anywhere the task is to be performed.
  * Easier to test and debug than a large, catch-all unit.
  * Effective at reducing errors such as cut-and-paste mistakes.
Other general names for subprograms are routines, procedures, and methods. The word ""method"" is generally reserved for procedures defined within an object, but it is not conceptually different from any other subprogram. 
Subprograms must be invoked or called in order for any of their code to be executed.  
Functions and Subroutines
Unlike most languages, Fortran makes a distinction between functions and subroutines .
Functions take any number (up to compiler limits) of arguments and return one item.  This item can be a compound type.
Functions must be declared to a type like variables, or must be defined in an interface (prototype). 
Subroutines take any number of arguments, up to the compiler limit, and return any number of arguments.  All communication is through the argument list.
If they are in the same file as the calling unit, subprograms follow the caller.
Variables in the argument list are often called dummy arguments since they stand for actual arguments that are defined in the calling unit.
Like any variable, they must be declared explicitly.  In Fortran this is done on separate lines, like for PROGRAM.
Subroutines
The subroutine unit begins with its name and parameter list
fortran
SUBROUTINE mysub(param1,param2,param3)
<type>  :: param1, param1, param3
It must terminate with the END statement.
END [SUBROUTINE] [NAME]
The form END SUBROUTINE is highly recommended.  This can be followed by the name of the subroutine
fortran
END SUBROUTINE mysub
but sometimes this leads to cut-and-paste errors.
A RETURN statement is optional unless a premature return is desired.
Invoking RETURN causes an immediate return of control to the caller.  No other statements in the subprogram will be executed.
A subroutine is invoked through the CALL command
fortran
CALL mysub(var1, var2, var3)
Note that the names of the actual and dummy arguments need not match, but they must agree in number and type.
Functions
Functions are declared in a manner similar to subroutines, but unlike subroutines they have a type.  The type of the function is the type of its return value.
fortran
<type> FUNCTION myfunc(param1,param2,param3)
<type>   :: param1, param2, param3
A function receives its return value by assigning an expression to its name.
fortran
myfunc=param1*param2/param3
As for subroutines, a RETURN statement is optional except for returning control due to some conditional.
They must also terminate with END
END [FUNCTION] [NAME]
Functions are invoked by name.  They can enter into an expression anywhere on the right-hand side of the assignment operator (=).
fortran
z=4.*myfunc(var1,var2,var3)
As for subroutines, the names of the actual arguments need not be the same as those of the dummies, but the number and type must match.
Because functions have a type, they must be declared like a variable in any program unit that invokes them.  Better yet, use an interface.
Subroutines have no return type and cannot be declared.
Example
{{< code-download file=""/courses/fortran-introduction/codes/subprogs.f90"" lang=""fortran"" >}}
Renaming Function Results
Normally the function value is returned by assigning a value to the name of the function.
We can return it in a different variable with the RESULT clause.
fortran
FUNCTION summit(x,y) RESULT(s)
This is especially useful for recursive functions; it is required in this case until the F2008 standard, and not all compilers support F2008 in full yet.
When using RESULT we declare the type of the name of the RESULT rather than the name of the function.  The caller must still declare the function, however (or use an interface).
Example
```fortran
FUNCTION myfunc(param1,param2) RESULT value
             :: value
, INTENT(in) :: param1, param2
, INTENT(in) :: param3, param4
statements
value=whatever
return        !Optional unless premature
```
Exercise
Write a program that evaluates the function
$$f(x)=\frac{1}{\pi (1+x^2)}$$
for 401 values of x equally spaced between -4.0 and 4.0 inclusive.
Put the values into an array variable x.  Use variables for the starting and ending values of x and the number of values.   Use an array operation to fill a variable y.
Write a function to evaluate f(x) for any given real (scalar) value of x and call it each time through your loop.
Print the values and the corresponding function evaluation to a comma-separated-values (CSV) file.  Use software such as Excel, Python, Matlab, or anything else you know to plot the result.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/func.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/advanced_arrays.md,"Array Initialization
Arrays can be initialized to the same quantity by an array operation:
```fortran
A=0.0 !equivalent to A(:)=0.0
For small arrays, an array constructor can be written.fortran
I=[1,0,0,0]
The constructor can use the implied do construct:fortran
X=[(real(i),i=1,100)]
```
If an array of characters is declared, then each element must be initialized to a string of the specified length.  The compiler will not pad with blanks.
Array Slices
Subarrays, also known as slices, may be extracted using the colon operator.
fortran
REAL, DIMENSION(100)      :: A
REAL, DIMENSION(12)       :: B
INTEGER, DIMENSION(20,10) :: N
INTEGER, DIMENSION(20)    :: C
   ! Assign values to A and N
   B=A(1:12)
   C=N(:,i)  !ith column of N
The upper bound of the range is always included. If the first bound is omitted, it starts from 1.  If the second bound is absent, the slice is extracted to the end of the range.  A single colon : represents the full range along a dimension.
Allocatable Arrays
So far we have examined static arrays, whose size is fixed at compile time.
Arrays may be sized at runtime by making them allocatable .  They are declared with an ALLOCATABLE` attribute and a colon for each dimension.
fortran
REAL, ALLOCATABLE, DIMENSION(:)   :: A, B
REAL, ALLOCATABLE, DIMENSION(:,:) :: C
If any dimension is allocatable, all must be.
These arrays must be allocated before they are used, so their size must be known at runtime.  More than one array may be allocated in a single ALLOCATE statement.
ALLOCATE(A(NMAX),B(MMAX),C(NMAX,MMAX))
Check whether an array is allocated with the intrinsic ALLOCATED(A)
if (allocated(A)) then
   do something
else
   allocate(A(some_size))
endif
or if we do not need to take any action if A is allocated:
fortran
if ( .not. allocated(A)) then
   allocate(A(some_size))
Advanced Array Indexing
Arrays can be addressed with arrays of integers (but not logicals).
```
integer, dimension(1)           :: maxtemp
real, dimension(365)            :: temps
character(len=5),dimension(365) :: dates
maxtemp=maxloc(temps)
print *, ""maximum temp was at "",dates(maxtemp)
```
Conditionals with Arrays
Logical arrays can be assigned with conditionals derived from other arrays to construct masks.  The maxval intrinsic finds the (first) maximum value in an array.
```fortran
logical, dimension(365) ::is_max
integer                 :: day
is_max=temps==maxval(temps)
   print , 'Maximum temperature(s) were at'
   do day=1,size(is_max)
      if (is_max(day)) then
         write(,advance='no'), dates(day)
      endif
   enddo
   write(,)
```
Example
Pulling the array indexing capabilities all together we have a complete program:
{{< code-download file=""/courses/fortran-introduction/codes/arrayinds.f90"" lang=""fortran"" >}}
This code contains some features, such as string concatenation, that we will study later.
Exercises


1 Download the program above.  Add the code from the ""Conditionals With Arrays"" section appropriately.  Compare your output to the maxloc (which returns an integer array of the indices of the maximum value).


2 Make all arrays that should be the same size as temps allocatable, leaving temps static.  Allocate all to the size and shape of the temps array.  For convenience you may introduce an integer that represents the size of temps.  This way we can accommodate data for a leap year by changing just the size of temps.    


{{< spoiler text=""Example Solution"" >}}
{{< code file=""courses/fortran-introduction/solns/arrayinds.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/primitive_types.md,"In programming, a variable is similar, but not identical to, the variable familiar from mathematics.  In mathematics, a variable represents an unknown or abstract entity.  In programming, a variable represents a location in memory.
Computer memory consists of individual elements called bits, for bi_nary dig_it.  Each bit is ""off"" or ""on"", represented by 0 and 1.  Bits are usually grouped into units of 8, called a byte.  The bytes are organized into words.  The number of bits in a word determines whether the computer is ""32 bits"" or ""64 bits"".  Nearly all modern hardware is 64 bits, meaning that each word of memory consists of 8 bytes.  Words are numbered, starting from 0.
Each variable has a type.  Types are a way of representing values as patterns of bits.  Some of these types, particularly those that represent numeric values, are defined by hardware operations in the computer's CPU.  Others can be defined by the programmer, but even these derived types are represented as combinations of the primitive types. 
Remember that computers do not use base 10 internally.  
Precision is the number of digits that are accurate, according to the requirements of the IEEE standard.  Please note that compilers will happily output more digits than are accurate if asked to print unformatted values.
Numeric Types
Integer
Integers are quantities with no fractional part.
They are represented by a sign bit followed by the value in binary (base 2).
Fortran does not support the unsigned integers of some other languages.
The default integer type has a size of 32 bits.
The range of this type is -2,147,483,648 to 2,147,483,647.  
Nearly all compilers offer an extension to support 64-bit integers. 
Their range is -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.
Integers are represented exactly as long as they fit within the range.
Floating Point
Floating-point numbers are representations of the mathematical real numbers.
However, due to the inherent finiteness of the computer, they have distinct 
properties.

There is a finite number of floating-point numbers. Therefore, many (indeed, infinite) real numbers will map to the same floating-point number.
They are represented with a form of scientific notation, so their distribution on the number line is not uniform.
They are commutative but not associative or distributive, in general.  That 
is,
$r+s=s+r$
$(r+s)+t \ne r+(s+t)$
$r(s+t) \ne rs+rt$ 

Floating-point numbers are defined by the IEEE 754 standard.  They consist of a sign bit, an exponent, and a significand.  All modern hardware uses base 2 so the exponent is a power of 2.  
For input and output, these binary numbers must be converted to and from decimal (base 10), which usually causes a loss of precision at each
conversion.  Moreover, some numbers can be represented exactly given the available bits in base 10 but not in base 2 and vice versa, which is another source of error.  Finally, most real numbers cannot be represented exactly in the relatively small number of bits in either base 2 or base 10.  
The most common types of floating-point number supported by hardware are single precision, which occupies 32 bits, and double precision, which takes up 64 bits.  
{{< table >}}
|   Precision  |  Exponent Bits |  Significand Bits | Exponent Range (base 2) | Approximate Decimal Range  |  Approximate Decimal Precision |
|--------------|----------------|-------------------|-------------------------|----------------------------|--------------------------------|
| Single       |  8    |  23  |  -126/127 | ±2 x 10-38 to ±3 x 1038 | 7 digits |
| Double       |  11   |  52  |  -1022/1023 |  ±2.23 x 10−308 to ±1.80 x 10308 |  16 digits |
{{< /table >}}
Quad precision (128 bits) is also defined, but rarely supported in hardware by modern computers.  Most compilers support it through software, but this will be slower than hardware operations.
The IEEE 754 standard also defines several special values.  The ones most frequently encountered by programmers are Inf (infinity), which may be positive or negative and usually results from an attempt to divide by zero, and NaN (not a number), which is the defined result of mathematically illegal operations such as $\sqrt{-1}$.
The number of bits is not a function of the OS type.  It is specified by the standard.
Complex
Fortran supports at least one complex type.  A complex number consists of 2 floating-point numbers enclosed in parentheses.
The single-precision type is COMPLEX.  It is represented as z=(r,i).
Most compilers provide the DOUBLE COMPLEX extension as a variable type.
Non-numeric Types
Logical
Boolean variables represent  ""true"" or ""false.""  They are called logical in Fortran.
Their values can be .true. or .false. The periods are required.
In some languages Booleans are actually integers; in Fortran that is not necessarily the case; the internal representation is up to the compiler.
Logicals cannot even be converted to an integer in Fortran.
Character
Characters used in Fortran code are ASCII. Fortran supports Unicode to a very limited extent; it is available only in comments and printing and uses the Universal Coded Character Set, which does not support all features of Unicode, and not all compilers support this.
The length may be declared at compile time with a default length of 1, representing a single symbol.  The Fortran 2003 standard introduces a variable-length character (string).
Literals
Literals are specific values corresponding to a particular type.
Examples
{{< table >}}
| Value    |   Type      |
|----------|-------------|
|     3    | Integer     |
|    3.2   |  Single precision floating point |
|  3.213e0 | Single precision floating point  |
|  3.213d0 | Double precision floating point |
| 3.213_rk | Type determined by kind parameter rk |
|  ""This is a string"" | Character string  |
|  ""Isn’t it true?""  |  Character string  |
|  'Isn''t it true?' |  Character string  |
|  .true.  |  Logical  |
| (1.2,3.5) | Single precision complex  |
| (1.2d0,3.5d0) | Double precision complex (compiler extension)  |
{{< /table >}}
In Fortran the default floating-point literal is single precision.  Double precision literals must include a d/D exponent indicator.  This is different from most languages, included C/C++, for which the default floating-point literal is double precision.
Forgetting to write double-precision literals with D exponent indicator rather than E often causes a significant loss of numerical precision that is hard to find.
KIND
Before the IEEE standard was universally adopted, some computer systems used a 64-bit floating-point number for REAL and included hardware for 128-bit DOUBLE PRECISION, while other systems used 32-bit REALs and 64-bit DOUBLE PRECISION.  This made porting codes back and forth problematic.  KIND was developed to solve this problem.  Rather than requesting REAL or DOUBLE PRECISION, the programmer could specify the minimum precision (for floating point) or range (for integer).  This motivation was mooted with the move to IEEE 754, but KIND remains useful as an abstract way to specify the desired type.  Put simply, an integer can be associated with the different precisions and ranges that are possible for primitive types.  A given compiler does not need to support all possibilities but should return an indication that it does not support a requested type.
The intrinsics SELECTED_REAL_KIND and SELECTED_INT_KIND can be used to specify KIND.  For compilers that support an additional character set, SELECTED_CHARACTER_KIND can be used to print most Unicode characters.
SELECTED_REAL_KIND(P,R)
requests a REAL with a decimal precision of at least P digits and an exponent range of at least R.  Fortran 2008 allows an additional argument RADIX to select the base for the other options.
SELECTED_INT_KIND(R)
requests an INTEGER with a range at least 10-R to 10R.
Both of these intrinsics should return negative values if the request cannot be accommodated.  For example,
fortran
integer, parameter :: ik=selected_int_kind(20)
returns -1 on most systems, since it is outside the range of a 64-bit signed integer.  To specify a 64-bit integer without using an older notation, use something like
fortran
integer, parameter :: ik=selected_int_kind(15)
The value returned must be declared INTEGER, PARAMETER to be used in variable declarations.
An intrinsic module can be used to obtain the KIND parameters.
The KIND intrinsic can be used to return the KIND of a particular variable or literal.  This can also be used to set KIND parameters.
KIND(v)
For example
INTEGER, PARAMETER :: dp=KIND(1.0d0)
REAL(dp)           :: x
See the Intel documentation for examples for their compiler."
rc-learning-fork/content/courses/fortran-introduction/linkers_libraries.md,"Linkers and Libraries
When the executable is created any external libraries must also be linked.
The compiler will search a standard path for libraries.  On Unix this is typically /usr/lib, /usr/lib64, /usr/local/lib, /lib.
If you need libraries in other locations, you must give the compiler the path. -L followed by a path works, then each library must be named with the pattern libfoo.a or libfoo.so and be referenced -lfoo.
Example
gfortran –o mycode –L/usr/lib64/foo/lib mymain.o mysub.o -lfoo
A library ending in .a is static.  Its machine-language code will be physically incorporated into the executable.  If the library ends in .so it is dynamic.  It will be invoked by the executable at runtime.
Many libraries require include files, also called header files.  These must be incorporated at compile time.  As for libraries, there is a standard system search path and if the headers are not located in one of those directories, the user must provide the path to the compiler with the -I flag.
Example
gfortran –c –I/usr/lib64/foo/include mymain.f90
If the library, or your code, uses modules in addition to or in place of headers, the I flag is also used to specify their location.  We will learn about modules and how they interact with your build system later.
The current working directory is included in the library and header paths, but not its subdirectories.
Compiler Libraries
If the compiler is used to invoke the linker, as we have done for all our examples, it will automatically link several libraries, the most important of which for our purposes are the runtime libraries.  An executable must be able to start itself, request resources from the operating system, assign values to memory, and perform many other functions that can only be carried out when the executable is run.  The runtime libraries enable it to do this.  As long as all the program files are written in the same language and the corresponding compiler is used for linking, this will be invisible to the programmer.  Sometimes, however, we must link runtime libraries explicitly, such as when we are mixing languages (a main program in Fortran and some low-level routines in C, or a main program in C++ with subroutines from Fortran, for instance).  
Fortran compilers generally include nearly all the language features in their runtime libraries.  Input/output are implemented in the runtime libraries, for example.  This can result in errors such as from the Intel compiler, when it could not read from a file (which was deliberately empty in this illustration):
forrtl: severe (24): end-of-file during read, unit 10, file /home/mst3k/temp.dat
In this error, forrtl indicates it is a message from the Fortran runtime library.
Compiling and Linking Multiple Files with an IDE
Our discussion of building your code has assumed the use of a command line on Unix.  An IDE can simplify the process even on that platform.
We will use Geany for our example; more sophisticated IDEs have more capabilities, but Geany illustrates the basic functions.
We have two files in our project, example.f90 and adder.f90.  The main program is example.f90.  It needs adder.f90 to create the executable.  We must open the two files in Geany.  Then we must compile (not build) each one separately.  Once we have successfully compiled both files, we open a terminal window (cmd.exe on Windows).  We navigate to the folder where the files are located and type
gfortran -o example example.o adder.o
Notice that we name the executable the same as the main program, minus the file extension.  This follows the Geany convention for the executable.  It is not a requirement but if Geany is to execute it, that is the name for which it will look.
You can run the executable either from the command line (./example may be required for Linux) or through the Geany execute menu or gears icon.
If Geany is to run a multi-file executable then the main program file must be selected as the current file as well as match the name of the executable.
{{< figure src=""/courses/fortran-introduction/img/GeanyFiles.png"" width=500px caption=""Executing the example program"" >}}
This process becomes increasingly cumbersome as projects grow in number and complexity of files.  The most common way to manage projects is through the make utility, which we will examine next."
rc-learning-fork/content/courses/fortran-introduction/project1.md,"Write a program to compute the day of the week for any date of the Gregorian calendar. Here is the formula: 
W=(C+Y+L+M+D ) mod 7
Y is the last two digits of the actual year and D is the actual day. 
You need to obtain the value of C from the following rule for the years: 
* If year is in the 1400s, 1800s, 2200s, C=2 
* If year is in the 1500s, 1900s, 2300s, C=0
* If year is in the 1600s, 2000s, 2400s, C=5 
* If year is in the 1700s, 2100s, 2500s, C=4 
Months are numbered from 1 in the usual way, but (from January) M is 0, 3, 3, 6, 1, 4, 6, 2, 5, 0, 3, 5 
The only tricky part of this algorithm is L, the number of leap days that have occurred since the beginning of the century of the given date. 
To obtain this:
1. Integer divide the last two digits of the year by 4 to obtain the number of “ordinary” leap years in the century up to that year, not counting the century year itself if applicable. 
2. Obtain the remainder of the two digits and 4. If it is not a century year and the remainder is 0 the year is a leap year, otherwise it is not. If the year itself is a century year see Step 3. 
3. If the century (1400, 1500, etc.) was evenly divisible by 400 then the century year is a leap year, otherwise it is not. Thus 2000 was a leap year but 1900 was not. So add 1 for centuries divisible by 400 and 0 otherwise. 
4. If your date is January 1-February 29 of a leap year, subtract 1. 
Try to devise a method to obtain the last two digits on your own. Print the day of the week as a word (Monday, Tuesday, etc.). Remember that Sunday is the first day of the week and it will be counted as 0 in this algorithm. 
Test your program first with your own birthdate. Then test with the following dates: 
* Today’s date 
* December 25, 1642 (Note: this is Newton’s birthdate in the Julian calendar, but use it as a Gregorian date) 
* October 12, 1492 
* January 20, 2000 
* December 11, 2525
Try to write and test your own program before peeking at the sample solution.
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/day_of_week.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/update_old_code.md,"If your dissertation depends on it (and you are allowed to do it) it's worth the time unless the code is more than 50,000 to 100,000 lines or so.
Step 1: Replace all COMMON blocks with modules.  Initially these modules only need to declare the variables.
Step 2: Reorganize subprograms into modules.  This gives you a free interface, and checks agreement of type and number of arguments.
Step 3: Change variable names to something more meaningful as you are able.
Step 4: Globals are poison, and a major source of bugs. 
The author found a bug in a model that was due to a variable being global due to having been originally in COMMON, and ported to a MODULE. This caused the program to have a ""memory"" when it should not have had one, but since the users had never executed the model for different conditions in the same run, they had never noticed this error. 
Move variables out of the ""common"" modules into parameter lists as quickly as you can.
Step 5: Introduce types/classes as appropriate.
Testing
You will need a set of regression tests for this job.  A regression occurs when a change to a code creates an error in previously-tested scenarios.  Before starting to modify code, determine how you will test it as it stands.
You may find the original code was not as well tested as you expected (or hoped).  You may need to create a suite of tests.  Retain those and run them as you make modifications.
Programming Style
Style makes a big difference, especially in compiled languages that tend to be more verbose than interpreted languages.
Indentation is very important
Variable names are important–choose variable names that communicate some meaning to the reader.  Variable names are no longer limited to 8 characters.
We do not use green-and-white striped fanfold paper anymore to separate lines visually.
Insert blank lines between logical chunks of code
Example
Before
The obsolete DATA statement was used to initialize variables.  The old code below is written in Fortran 77, so still used fixed format, but some newer constructs such as END DO were available.
```fortran
subroutine biogeoclimate
include ""common.txt""
real tx(365),tn(365),ta(365),p(365)
real t1(12),t2(12),p1(12)
real littc1,littc2,littn1,littn2
real prcp_n
data prcp_n/0.00002/
c prcp_n: Nitrogen content for unit rianfall (tN/ha)
c
  rain_1=0.0
  rain_n=0.0
  do i=1,12
    tmp1=ynormal(0.0,1.0)
    tmp1=max(-1.0,min(tmp1,1.0))
    t1(i)=tmin(i)+tmp1tminv(i)
    t2(i)=tmax(i)+tmp1tmaxv(i)
    tmp2=ynormal(0.0,1.0)
    tmp2=max(-0.5,min(tmp2,0.5))
c   forest cover can increase rainfall by maximum 15%
    p1(i)=max(prec(i)+tmp2precv(i),0.0)
c         (1.0+lai0.02)
    rain_1=rain_1+p1(i)
c   write(,) tmp1, tmp2, tmin(i),tmax(i),prec(i)
    rain_n=rain_n+p1(i)prcp_n
  end do
call cov365(t1,tn)
  call cov365(t2,tx)
  call cov365a(p1,p)
do i=1,365
  ta(i)=0.5*(tn(i)+tx(i))
  end do
c
c Daily cycles of C, N, H2O
c
  rrr=0.0
  ypet=0.0
  yaet=0.0
  avail_n=0.0
  degd=0.0
  growdays=0.0
  drydays=0.0
  drydays1=0.0
  flooddays=0.0
c
  aoc0=aoc0+go_ao_c
  aon0=aon0+go_ao_n
```
After
```fortran
module Model
use Parameters
use Constants
use Soil
use Site
use Species
use Tree
use Random
use Climate
use Input
implicit none
   real, parameter                          :: grow_min=0.05
   real, parameter                          :: growth_thresh=0.05
   real                                     :: growth_min=0.01
contains
subroutine BioGeoClimate(site,year)
      integer,         intent(in)    :: year
      type(SiteData),  intent(inout) :: site
      integer                        :: gcm_year
      integer                        :: num_species
      real, dimension(NTEMPS)        :: tmin, tmax, prcp
      real, dimension(NTEMPS)        :: tmptmin, tmptmax, tmpprec
      real, dimension(days_per_year) :: daytemp, daytemp_min, daytemp_max
      real, dimension(days_per_year) :: daynums, dayprecip
      real                   :: litter_c_lev1,litter_c_lev2
      real                   :: litter_n_lev1,litter_n_lev2
      real                   :: rain,rain_n,freeze
      real                   :: temp_f,prcp_f
      real                   :: total_rsp,avail_n,n_avail,C_resp,pet,aet
      real                   :: growdays,drydays_upper,drydays_base
      real                   :: flooddays,degday
      real                   :: outwater
      real                   :: exrad,daylength,exradmx
      real                   :: pot_ev_day
      real                   :: act_ev_day
      real                   :: laiw0_ScaledByMax,laiw0_ScaledByMin
      real                   :: aow0_ScaledByMax,aow0_ScaledByMin
      real                   :: sbw0_ScaledByMax,sbw0_ScaledByMin
      real                  :: saw0_ScaledByFC,saw0_ScaledByWP
      real                   :: yxd3   !used but never set
      ! used to temporarily hold accumulated climate variables
      real                   :: tmpstep1,tmpstep2
      real                   :: tmp
      integer                :: i,j,k,m
      real, parameter        :: min_grow_temp =5.0
      real, parameter        :: max_dry_parm  =1.0001
      real, parameter        :: min_flood_parm=0.9999
  save

  num_species=size(site%species)
  rain  =0.0
  rain_n=0.0

 ! The user is expected to input decr_by values as positive.
 if ( linear_cc ) then
    if (year .ge.begin_change_year.AND. year .le.                         &
       (begin_change_year+duration_of_change)) then
            accumulated_tmin=accumulated_tmin+tmin_change
            accumulated_tmax=accumulated_tmax+tmax_change

        do m=1,12
           tmpstep1 =site%precip(m) +accumulated_precip(m)
           tmpstep2 = tmpstep1 *precip_change
           accumulated_precip(m) =accumulated_precip(m) + tmpstep2
        end do

    endif

 else if (use_gcm) then
    gcm_year=start_gcm+year-begin_change_year
    if (gcm_year.ge.start_gcm.and.gcm_year.le.end_gcm) then
       call read_gcm_climate(site%site_id,gcm_year,start_gcm,tmin,tmax,prcp)
       site%tmin=tmin
       site%tmax=tmax
       site%precip=prcp*mm_to_cm
     endif
 endif

end module
```
Changes


Converted to modules.


Eliminated COMMON, passing all variables (this not only controls the interface, but eventually eliminated a bug).


Renamed most variables to something more descriptive.


Added lots of whitespace for visual separation.


Aligned typographically


Note that considerable effort went into separating other parts of the code into the modules USEd in this subroutine."
rc-learning-fork/content/courses/fortran-introduction/project5.md,"Download the file vabirds.csv.
1. Create the derived type bird_data in a module bird_dat as illustrated in the example.  This is still a type; we do not need a class for this exercise. 
2. Add the following procedures to the constructor already written.
   * Write a stats procedure that takes only an instance of the type and returns the mean and standard deviation of the observations for that instance.
   * Write a minmax procedure that takes an instance of the type and the array of years and returns the maximum observed, the minimum observed, and the years for maximum and minimum.  You may use the maxval, minval, maxloc, and minloc intrinsics.
   * Write a main program that uses your module and also uses the sorters module that you can download (sorters.f90). This implements bubblesort.  Bubblesort is simple and slow but is more than sufficient for this exercise.  Note that the subprogram is destructive, i.e. it overwrites the array to be sorted, so make a copy if you don’t want that.
Remember to write an explicit interface for each subprogram in this “main” file.  Do not use CONTAINS. Read the file name from the command line.   First of all you will need to count the number of lines in the file.  Write a function count_lines that does this and returns the number.  It is up to you whether you pass it the number of header/footer lines.
Count_lines can check for the existence of the file and return 0 if it is not found.
Still in the read_data routine, using the number of items in the file, corrected for the header and the two footers, allocate an array of bird_data types.
Loop through this array calling your constructor for each species.
The read_data routine should return the array of years and the array of bird_data types.  
Request a species name from the user.  Find the species in your array of types 
and print its mean, standard deviation, and results from minmax. Print some appropriate message if the species is not found.  Compute an array of the means for all species.
Use the pbsort routine from sorters to sort this array.  This procedure also returns the permutation vector, which is an array of the indices of the original positions.
For example, if after the sort the permutation vector is (17,3,55,11,23, and so forth) that means that the element that was previously 17 is now the first in the new array, and so on.
Note that these sorters return in ascending order (smallest to largest).  From the sorted mean array and the permutation index, print the names of the 10 most common (by mean) species over the years of observations.
Hint: you can use a trick to reverse a dimension of an array in Fortran: 
fortran
R=A(ndim:1:-1)
Test the user input portion for 

TurkeyVulture

TuftedTitmouse 

ElegantTrogon
For this project you can require an exact match of the species name.  (Note that no spaces are allowed and words are separated by capitalization; we would have to do more sophisticated string handling if we were to allow spaces and variations in capitalization.)
In addition to the sorters.f90 module mentioned above, the sample solution uses the file_utils module that collects some useful file-related subprograms, including the count_lines function.
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/bird_dat.f90"" lang=""fortran"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/bird_obs.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/array_intrinsics.md,"Modern Fortran has many intrinsic functions that operate on arrays.
Array intrinsics can be classified as inquiry, construction and manipulation, and transformation/reduction.
In documentation, square brackets around an argument indicate that it is optional.  The argument dim refers to the dimension.  Many intrinsics can optionally work on the individual dimensions of an array.
This list is not exhaustive, and some optional arguments are omitted for some functions. For details about each intrinsic see a compiler reference, such as for gfortran.
Array Construction Intrinsics
These create new arrays from old. PACK and UNPACK can be used to ""flatten"" multidimensional arrays and to return a one-dimensional array to multidimensional rank.  This can be particularly useful for environments such as parallel programming with the Message Passing Interface.  
fortran
! Convert an array from one shape to another (total size must match)
! SHAPE must be a rank-one array whose elements are sizes in each dimension
RESHAPE(SOURCE,SHAPE[,PAD][,ORDER])
! Combine two arrays of same shape and size according to MASK
!   Take from ARR1 where MASK is .true., ARR2 where it is .false.
MERGE(ARR1,ARR2,MASK)
PACK(ARRAY,MASK[,VECTOR])
UNPACK(VECTOR,MASK,FIELD)
! Take ARR and make NCOPIES of it along dimension DIM. Given source has
!   rank n, result has rank n+1
SPREAD(SOURCE,DIM,NCOPIES)
Example
fortran
!Array and mask are of shape NxM
mask=A<0
merge(A,0,mask)
!B of shape MxN
B=reshape(A,(/M,N/))
! for C=1, D=[1,2]
print *, spread(C, 1, 2)            ! ""1 1""
print *, spread(D, 1, 2)            ! ""1 1 2 2""
Array Inquiry Intrinsics
fortran
! Allocated or not. Returns .true. or .false.
ALLOCATED(ARRAY)  
! Upper and lower bounds
LBOUND(ARRAY)
UBOUND(ARRAY)
! Returns a rank-one array containing the dimensions
SHAPE(ARRAY)      
! Returns the size (the total number of elements).  
!  If the optional argument dim is not present it returns the total number of 
!  elements; if dim is present it returns the number of elements on that 
!  dimension.
SIZE(ARRAY,[DIM]) 
! Returns the rank (extension, but widely supported)
RANK(ARRAY)
Array Transformation Intrinsics
fortran
! matrix must be square and rank 2
TRANSPOSE(MATRIX)
!both V1 and V2 must be rank 1
DOT_PRODUCT(V1,V2)
! A and B must conform, must return a rank-2 array even if it’s (1,1)
! Note: depending on compiler and version, might be slow
MATMUL(A,B)
! Returns the location of the first minimum it finds in array
MINLOC(ARRAY [,MASK]) 
! Along a particular dimension
MINLOC(ARRAY, DIM [,MASK])
! Like minloc for max
MAXLOC(ARRAY [,MASK])
The MINLOC and MAXLOc intrinsics return a rank-1 array of indices .
Array Reduction Intrinsics
fortran
! MIN/MAXVAL return the first-encountered min/max values, optionally along DIM
!  If DIM is absent, result is a scalar, otherwise an array of rank n-1
MINVAL(A [,DIM] [,MASK])
MAXVAL(A [,dim] [,MASK])
! For sum/product see example below
SUM(A [,DIM] [,MASK])
PRODUCT(A [,DIM] [,MASK])
Example
A has shape(4,5,6). Then
SUM(A,2) has shape (4,6) and elements SUM(A(i,:,j)).
Pay attention to how sum and product work when a dimension is specified.  It can be unintuitive.
{{< code file=""/courses/fortran-introduction/codes/arraysum.f90"" lang=""fortran"" >}}
Masking Array Intrinsics
These determine truth or falsity according to the specified mask.  ALL and ANY return a scalar logical value if dim is absent; otherwise they return an array of rank(mask)-1.
The COUNT intrinsic returns the number of .true. elements in mask, 
optionally along dim dimension.
fortran
ALL(MASK [,DIM])
ANY(MASK [,DIM])
COUNT(MASK)
Example
fortran
print *, count(A)
if all(A>0) then 
   A=B
endif
if any(A<0) then
   A=0.
endif"
rc-learning-fork/content/courses/fortran-introduction/building.md,"The ""traditional"" development environment for compiled languages was a text editor and a command line.  Many programmers continue to use these successfully, but modern tools can greatly improve programmer productivity.  Some such tools are especially recommended for the Windows operating system, since it does not support command-line usage as cleanly as Linux or macOS.
Compilers
Compilers are sophisticated software packages.  They must perform a complex analysis of the code, translate it to machine language, and invoke a linker to create an executable.  This ""big picture"" view and direct machine language output is what enables compiled programs to run generally with much higher performance than interpreted scripts.  Compilers also offer a large number of compile-time options that can significantly impact the performance and sometimes the results of the executable.  
Many compilers are available, but we will focus on those from three vendors.
Gnu Compiler Collection
The Gnu Compiler Collection is a well-established, free and open-source bundle. The base compiler is gcc for C.  Several add-on languages are supported, the most widely used of which are g++ (C++) and gfortran (Fortran).  
NVIDIA HPC SDK
The NVIDIA HPC SDK is another free (though not open-source) compiler suite for C/C++/Fortran.  Formerly the Portland Group compilers, it is a general-purpose package but is oriented toward extensions for programming NVIDIA GPUs.  For example, it provides Fortran bindings to CUDA.  These compilers are nvcc, nvc++, and nvfortran. 
Intel Compilers
The Intel compilers have a reputation for producing the fastest executables on Intel architectures.  Most high-performance computing sites provide the commercial Intel suite icc, icpc, and ifort.  Intel's Parallel Studio package also ships with high-performance Math Kernel Libraries (MKL), MPI (IntelMPI), and threading (tbb).  The Parallel Studio package is available on the UVA HPC system.
Intel has recently released the oneAPI Toolkits. They are free but not open source, and are supported only through a community forum.  In order to obtain the ""classic"" compilers described above, the HPC Toolkit must be installed.  The newer compilers provided in the Base Toolkit for Intel are icx, and icpx.  Both the classic and the new Fortran compilers ifort and ifx are in the HPC Toolkit.
Integrated Development Environments
An Integrated Development Environment (IDE) combines an editor and a way to compile and run programs in the environment.
A well-known IDE for Microsoft Windows is Visual Studio. This is available through the Microsoft Store; it is not free for individuals.
macOS uses Xcode as its native IDE. Xcode includes some compilers, particularly for Swift, but it can manage several other languages.  Available at the App Store and free.
A full-featured cross-platform IDE is [Eclipse] (http://www.eclipse.org/).  Free.
A lighter-weight IDE for Windows and Linux is [Code::Blocks] (http://www.codeblocks.org/).  Free.
Windows programmers using Intel's oneAPI distribution must also install Visual Studio.
An increasingly popular IDE is Visual Studio Code (VSCode) from Microsoft. It is also cross-platform, with versions available for Windows, macOS, and Linux.  It does not support C, C++, or Fortran by default; extensions must be installed to provide syntax highlighting and debugging for those languages.  C and C++ are installed with one extension that can be found at the top of the list.  To install a Fortran extension, open the extension panel if it is hidden, and type fortran in the search bar.  There are several options; the one simply called ""fortran"" is popular.  Also recommended are the breakpoint extension and fprettify.
In our examples, we will use a very lightweight IDE called Geany since it is free, easy to install and use, and  works on all three platforms.  It is more of a programmer's editor than a full-featured IDE, but it does include some build tools.
Building an Executable
Creating an executable is generally a multistep process.  Of course, the first step is the preparation of a source file.  Fortran has two conventions; the older is fixed format and conventionally those source files should end with .f.  The newer convention, which we will use for all our examples here, is free format and most compilers expect those files to end in .f90, even if the actual standard supported is 2003 or later.
From each source file, the compiler first produces an object file.  In Unix these end in .o, or .obj on Windows.
This is the compilation step.
Object files are binary (machine language) but cannot be executed.  They must be linked into an executable by a program called a linker (also called a loader).  The linker is normally invoked through the compiler.  The entire process of compiling and linking is called building the executable.
If not told otherwise a compiler will attempt to compile and link the source file(s) it is instructed to compile.  If more than one file is needed to create the executable, linking will not work until all object files are available, so the compiler must be told to skip that step.
For Unix compilers the -c option suppresses linking.  The compiler must then be run again to build the executable from the object files.
The linker option -o is used to name the binary something other than a.out.
Unix and macOS do not care about file extensions, but Windows will expect an executable to end in .exe.
Command Line
For full control and access to more compiler options, we can build from the command line.  For Linux and Mac this is a terminal application.  On Windows, use a command prompt for gcc.  The Intel oneAPI distribution ships with an integrated command prompt in its folder in the ""applications"" menu; this command prompt is aware of the location of the compiler executables and libraries.
Example
gfortran -c mycode.f90
gfortran -c mysub.f90
gfortran -o mycode mycode.o mysub.o
IDEs generally manage basic compiler options and usually name the executable based on the project name.  Our examples of command line usage will all assume a Unix operating system; there are some differences between Linux and macOS, with larger differences for Windows.  On macOS and especially Windows, using an IDE makes code management simpler.  Using Geany as our example, clicking the icon showing a pyramid pointing to a circle will compile the current file without attempting to invoke the linker.  The brick icon builds the current file, so it must be possible to create a standalone executable from a single file for that icon to work."
rc-learning-fork/content/courses/fortran-introduction/project4.md,"Download the file bodyfat.csv.  This is a dataset of body fat, age, height, and weight for a set of participants in a study. BMI categories are as follows:
{{< table >}}
|Severely underweight |  BMI < 16.0 |
|Underweight          | 16 <= BMI < 18.5 |
|Normal               | 18.5 <= BMI < 25 |
|Overweight           | 25 <= BMI < 30 |
|Obese Class I        | 30 <= BMI < 35 |
|Obese Class II       | 35 <= BMI < 40 |
|Obese Class III      | BMI > 40       |
{{< /table >}}
Write a bmi_calculator module containing functions/subroutines for the following:
1. Convert pounds to kilograms.  Use the actual conversion factor, not the approximate one.  Look it up on Google.
2. Convert feet/inches to meters.  Look up the conversion factor, do not guess at it. 
3. Compute BMI.
4. Determine where the user falls in the table supplied and return that information an appropriate form. 
Write a module stats that implements the following:
1. Mean of an array 
2. Standard deviation of an array 
3. Outlier rejection using Chauvenet’s criterion.  Pseudocode given further down.
Make as much use of Fortran intrinsics/array operations as you can.
Write a main program that implements the following:
1. Uses your modules
2. Reads the input file into appropriate allocatable arrays (use one-dimensional arrays for this project).  Don't assume you know the length of the file (but you can assume the number of header lines is fixed).
3. Pass appropriate arrays to a subroutine that computes an array of BMI data based on height and weight and returns the BMI array.
4. Rejects the outlier(s).  The function should return an array of logicals that you can apply to the original data using WHERE or similar.  Create new arrays with the outlier(s) deleted. 
Write a file that contains the corrected data for bodyfat and BMI.  Use Excel or whatever you normally use to plot BMI as a function of percentage body fat. 
Be sure to plot it as a scatter plot (points only, no connecting lines).  
Chauvenet’s criterion: It’s not the state of the art but works pretty well.
1. Compute the mean and standard deviations of the observations.
2. Compute the absolute values of the deviations, i.e. abs(A-mean(A))/std(A)
3. Use the tails devs=devs/sqrt(2.)
4. Compute the probabilities prob=erfc(devs) : erfc is an intrinsic in any fairly recent Fortran compiler.
5. The criterion is that we retain data with prob>=1./(2*N_obs) (number of observations).
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/stats.f90"" lang=""fortran"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/bmi_calculator.f90"" lang=""fortran"" >}}
{{< code-download file=""/courses/fortran-introduction/solns/bmi_data.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/fortran-introduction/setting_up.md,"Linux
For users of the University of Virginia's cluster, first load a compiler module.
module load gcc
This command brings a newer gcc, g++, and gfortran into the current environment. Now load Geany.
module load geany
geany&
For personal use, compilers, Geany, and the other cross-platform IDEs are available for all popular Linux distributions and can be installed through the distribution's package manager or by downloading from the vendor (e.g. the NVIDIA HPC SDK).  Most workstation users do not install environment modules, so the module command would not be required. 
On Mac and Windows, an IDE can be installed in the usual way for those platforms, either through the application ""store"" or starting at the package's home page.
Mac OS
GCC
Install Xcode from the App Store.  This will install gcc and g++.
To use Fortran, download a binary for your version of OS from
the binary site.  These compilers can be used with any IDE available for Macs.  Xcode does not support Fortran directly so another option is recommended, but it can be used for C and C++.
Intel oneAPI
Download the Mac version from Intel.
The NVIDIA HPC SDK is not available for Macs.
Geany can be installed from its homepage.  Other options, such as VSCode, can be installed similarly.
Windows
There are several compiler options for Windows.  Visual Studio supports C and C++ but not Fortran, and no Fortran is currently available for it.  However, there are several free options.
GCC
A popular vehicle for using the Gnu compilers on Windows is Cgwin.  Cygwin also provides a large number of other Unix tools.  The Gnu compiler suite is installed through the Cygwin installer.
Recently, Microsoft has released the Windows Subsystem for Linux (WSL).  This is not a virtual machine but is a type of Linux emulator.  It is a full-featured command-line-only Linux environment for Windows, but the X11 graphical user interface is not supported.
A drawback to both Cygwin and the WSL is portability of executables.  Cygwin executables must be able to find the Cygwin DLL and are therefore not standalone.
WSL executables only run on the WSL.  For standalone, native binaries a good choice is MingGW.  MinGW is derived from Cygwin.
MinGW provides a free distribution of gcc/g++/gfortran.  The standard MinGW distribution is updated fairly rarely and generates only 32-bit executables.  We will describe MinGW-w64, a fork of the original project.
{{< figure src=""/courses/fortran-introduction/img/MinGW1.png"" width=500px >}}
MinGW-w64 can be installed beginning from the MSYS2 project.  MSYS2 provides a significant subset of the Cygwin tools.  Download and install it.
{{< figure src=""/courses/fortran-introduction/img/MSYS2.png"" width=500px >}}
Once it has been installed, follow the instructions to open a command-line tool, update the distribution, then install the compilers and tools. For Fortran users, the mingw64 repository may be preferable to the ucrt64 repo. To find packages, visit their repository. 
A discussion of installing MinGW-64 compilers for use with VSCode has been posted by Microsoft here. To use mingw64 rather than ucrt64, simply substitute the text string. Fortran users should install both the C/C++ and Fortran extensions for VSCode.
Intel oneAPI
Download and install the basic toolkit and, for Fortran, the HPC toolkit.
NVIDIA HPC SDK
Download and install the package when it is available.
Environment Variables in Windows
To use any of these compilers through an IDE, they must be added to the Path environment variable.  You must use the path you chose for the installation.  The default is C:\msys64\mingw64\bin for the compilers.
Control Panel->System and Security->Advanced system settings->Environment Variables
{{< figure src=""/courses/fortran-introduction/img/WindowsEV.png"" width=412px >}}
Once you open Path, click New to add to the Path
{{< figure src=""/courses/fortran-introduction/img/WindowsPath.png"" width=500px >}}
To test that you have successfully updated your path, open a cmd window and type
gfortran
You should see an error
gfortran: fatal error: no input files
Compiling Your First Program
We will show Geany and VSCode on Windows.  Both look similar on the other platforms.  
Open Geany (or VSCode).  Type in the following
{{< code file=""courses/fortran-introduction/codes/hello.f90"" lang=no-highlight >}}
{{< figure src=""/courses/fortran-introduction/img/Geany1.png"" width=500px  >}}
Syntax coloring will not be enabled until the file is saved with a file extension that corresponds to the language.  Save this file as hello.f90.  The coloring will appear.
{{< figure src=""/courses/fortran-introduction/img/Geany2.png"" width=500px >}}
The appearance is similar in VSCode.
{{< figure src=""/courses/fortran-introduction/img/VSCode.png"" width=500px >}}
In Geany, click the Build icon (a brick wall).  A message confirming a successful compilation should be printed.
{{< figure src=""/courses/fortran-introduction/img/Geany3.png"" width=500px >}}
Now click the Execute button.  A new window will open and the message will be printed.
{{< figure src=""/courses/fortran-introduction/img/Geany4.png"" width=500px caption=""Executing the Hello World program"" >}}"
rc-learning-fork/content/courses/fortran-introduction/_index.md,"
I do not know what the scientific programming language of the year 2000 will look like but it will be called Fortran.

-- Apocryphal, sometimes attributed to John Backus (inventor of Fortran) or Seymour Cray (inventor of the supercomputer), circa 1985.

Please note that the official spelling is Fortran, not FORTRAN.
Most of the material will be oriented toward the Fortran 2003 standard, which is well-supported in recent compilers and includes a number of constructs for modern programming paradigms.
Fortran was introduced in October 1957.  It was developed by a team at IBM led by John Backus and was among the earliest of the high-level programming languages, which replaced the machine-oriented assembler programming model widely used at the time with more natural human-readable syntax.  Algol and COBOL followed in the next few years.
It is important to keep in mind that computers for which the first languages were written were extremely limited in memory, speed, and disk space.  Typical main memory in the 1960s was measured in kilobytes.  A supercomputer circa 1994 had 2GB of RAM as main memory.  Many of the obsolete constructs were developed to work around these limitations; for instance, COMMON allowed memory to be ""overbooked."" Given its decades of history, many old codes using such obsolete constructs still exist and are in use.
Fortran is still very widely used in many scientific and engineering disciplines. Mechanical engineering, chemistry and chemical engineering, and environmental sciences, particularly atmospheric sciences and oceanography, make significant use of Fortran.  Features of modern Fortran make it very well suited to numerically-intensive programming.  Due to both legacy codes and ongoing development, many scientific programmers must be able to use both C/C++ and Fortran.
{{< figure src=""/courses/fortran-introduction/img/Cray-1.jpg"" width=500px caption=""The first Cray-1 supercomputer had 1 80-MHz core and a maximum of about 8MB of main memory."" >}}
The best way to learn a programming language is to use it.  We strongly urge you to attempt the exercises and projects to practice your coding skills."
rc-learning-fork/content/courses/fortran-introduction/formatted_io.md,"List-directed IO is convenient.
But that frequently results in sprawling output that is difficult to read. Formatted output is frequently required for legibility.  
Formatted output in Fortran is similar to other languages (the general layout descends from Fortran, the oldest higher-level programming language).
Edit Descriptors
The edit descriptor modifies how to output the variables.  They are combined into forms like
RdF.w
where R is a repeat count, d is the descriptor, F is the total field width including space for +-, and if requested +-e and exponent, and w is the number of digits to the right of the decimal point. If you are willing to let the compiler calculate the number of characters to use, use Rd0.w. You may also omit the number of decimal places with Rd0 and the compiler will use its default for the type.
For floating-point numbers, Rd.0 will print the integer part. 
Strings take only RaF and do not usually require the F since the length will be known to the compiler.
Integers can be written as iF and any of the F spaces not needed will be blank filled, with the digits right justified.  When written as iF.m they will be printed with at least m digits and the rest of the field zero-filled on the left if all of F is not needed.
If the field width is specified and the requested literal does not fit, the compiler will output a string of asterisks, e.g. ********.  
If you allow the compiler to compute the total field width, note that it will not include spaces before or after the item.
Common Edit Descriptors
As usual, they are not case-sensitive.
fortran
I  !integer
F  !real (decimal output)
E  !real (exponential output)
ES !like E but use scientific notation.  
G  !general 
D  !double precision (prints D rather than E for exponent)
A  !character (does not require a field width in most cases)
X  !space
/  !write an EOL and go to the next line (record) within the format
:  !terminate the output if there are no more variables to write
The real descriptors F, E, G, and D all work for both single and double precision. G allows the compiler to choose whether to use decimal or exponential format.
The default exponential format writes in machine normalization, with the leading digit between 0 and 1. ES causes it to write with the leading digit between 1 and 9, which is what most humans can read most easily.  ES ignores p on output.
Modifiers
Some modifiers can change the appearance of the output.
fortran
p  !multiply by 10
kp !multiply by 10k
The p descriptor applies till the next scale factor is encountered.
Fill an integer field with zeros
fortran
I4.4
Format Strings
The format string is constructed as a list of how to output the variables.  Unlike some other languages, literal strings are never included, but must have their own edit descriptors.
The format string can be placed directly into the write statement or it can be in a separate format statement. In the write it is enclosed in parentheses and quotes.
For most purposes it is best to put the format string into the write statement.  The format statement is older and will be in old code, but it is usually harder to see what is happening.  It is useful for particularly long strings, however.
Examples
fortran
   write(*,'(i5,2x,i6)') i1,i2
   write(*,'(i5,a,i6)') i1,""     "",i2
   write(*,'(a,f0.6)') ""The result is  "",res
   write(*,'(a,i4,es15.7)') ""The answer"",i1,dpi
   write(*,'(2p,f8.2,0p,f8.2)') rpi, dpi
   write(*,'(a,f8.2,/,a,i6)') mess1,res,mess2,i1
   write(*,'(a)') ' '
   write(*,'(""first value "",f8.2,"", second value "",i6)') res
   write(*,'(a)') ' '
   write(*,'(""first value "",f8.2,:,"" second value "",i6)') res
A format string may be a variable.
fortran
character(len=32) :: formatstr
   code
   formt='(f8.3,es15.7)'
   write(*,formatstr) A, B
Repetition
Format strings can be repeated for multiple variables. If more than one descriptor is present, the format to be repeated should be enclosed in parentheses.
fortran
   write(*,'(2L)')is_zero,is_finite
   write(*,'(2f8.2)') z !complex
   write(*,'4(f0.6)') a(1,:)
   write(*,'(4(i2,3x,f8.3))') (j,b(j),j=1,4)
Especially when an array is allocatable, it may be awkward to specify the repeat count if it is unknown at compile time. List-directed I/O allows the compiler to choose to add end-of-line markers to line up columns, so the output can differ between different compilers and may not be what is desired for later processing. In principle a variable format string can be constructed with internal writes, but this can be complicated.  The Fortran 2008 standard introduced the * repetition count.  The compiler will repeat until it runs out of items. 
fortran
   do i=1,size(arr,1)
      write(*,'(*(f12.4))') arr(i,:)
   enddo
Format Statements
Format statements are abundant in older code, before the strings could be inserted into writes.
FORMAT is non-executable but can appear anywhere in the source.  It is the only non-executable statement that can do so.
It can still be useful for a particularly complex format (to keep the write statement short and readable) or for formats that are repeated in many write statements.
The second parameter to the write is then an integer statement label.  The label marks the format statement.
Example
fortran
    write(*,100)x,y,z
100 format(3e15.8)
Traditionally the format is placed immediately below the line which refers to it, or else all format statements are grouped together just before the end statement of their program unit.
Formatted Input
In Fortran it is best to avoid formatted input as much as possible, as it can lead to errors.  For historical reasons, if a format is specified to be real (floating point) but no decimal point is included in the data, the compiler inserts it based on the format.  For example, suppose we had data in the form
no-highlight
112 9876 12
with a format string of
fortran
read(infile,'(f8.4,f6.2,i4)'
We intended to read three values, two reals and an integer, but this format results in input values of
no-highlight
      112.987602      0.119999997               0
The errors in the real values are the consequence of converting from decimal to binary and back.
Note that the spaces between the values are ignored.
We get the expected result with read(infile,*):
no-highlight 
   112.000000       9876.00000              12
The compiler has more freedom so the conversion is also more accurate.
Unformatted input is permitted when casting from a character to a real, so there is no need for formatted input at all.
fortran
character(len=12) :: quantity_char
real              :: quantity
quantity_char=""100""
read(quantity_char,*) quantity
Fortran Non-Advancing IO
Unlike most languages, Fortran print and write by default add an end-of-line marker at the end of the output. If we’d like to suppress this so that we can write multiple groups of output on the same line, or we would like to write some output to the console and read something from the console, we can use non-advancing IO.
fortran
write(*,'(a)',advance='no') ""Enter input value:""
read(*,*) value
Non-advancing IO must be formatted
  * ‘yes’ for advance is valid also but is the default.
  * The argument to advance can be a character variable so that you can decide based on conditionals whether to advance.
  * If you do not want to advance, use advance='no'
Exercises

Examine this example code:

{{< spoiler text=""A variety of formatting examples"" >}}
{{< code file=""courses/fortran-introduction/codes/formats.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Make sure you understand all the formats. Correct unattractive outputs.  Experiment with changing the formats.

Write a program that computes pi using a trig identity such as pi=4*atan(1).
Use kind to switch between real and double precision
integer, parameter ::rk=kind(1.0)  or (1.0d0)


Using single precision, print pi in
E format
Scientific notation
Scientific notation with 8 decimal places
Repeat for double precision.



{{< spoiler text=""Solution with variable strings."" >}}
{{< code file=""courses/fortran-introduction/solns/print_pi.f90"" lang=""fortran"" >}}
{{< /spoiler >}}

In an “infinite” while loop:
 Request an integer from the user with non-advancing input/output, e.g.
""Please enter an integer:"" <then read integer>
If the integer is 1, print ""zebra"".  If it is 2, print ""kangaroo"".  If it is anything else other than zero, print ""not found"".  If it is 0, exit the loop. This requires only a simple change to a previous program.

{{< spoiler text=""Solution with variable strings."" >}}
{{< code file=""courses/fortran-introduction/solns/non_advance.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_jacobi_method.md,"There are many algorithms for solving the Laplace equation, but we will use one of the oldest, the Jacobi method. It is typically slow to converge to the solution, but it is easy to understand, easy to program, and easy to parallelize.
This method uses two arrays.  Omitting the derivation, given the current values of array $u$ we compute the new value with the equation
nohightlight
w[i,j] = u[i-1,j] + u[i+1,j] + u[i,j-1] + u[i,j+1])
You may notice that the ""updated"" value at a given point is just the average of its neighbors.  Boundary values must be provided for appropriate values of $i$ and $j$; in our previous study of halos, we used $i=0$, $i=nr+1$, $j=0$, and $j=nc+1$ for the boundary values, with the solution defined for $i=1..nr$, $j=1..nc$.
{{< figure src=""/courses/parallel-computing-introduction/img/grid.png"" caption=""Grid for the Jacobi method"" >}}
Before we begin, we must determine the tolerance $\epsilon$ for an acceptable solution. We make a sweep through the grid, computing the new values $w$ at each grid point.  We compare all these values to the old value at the same point. If 
$$ | w - u | < \epsilon $$
for all $w_{ij}$, we terminate the iterations.  Otherwise we set $u=w$ at all interior points and make another sweep through the grid.
In a Python-like pseudocode the algorithm is
```
Set a very large maximum number of iterations
Set boundary conditions
u[0,:]=topBC
u[nr+1,:]=bottomBC
u[:,0]=leftBC
u[:,nc+1]=rightBC
Make an initial guess for the solution. Zero is often reasonable.
u[1:nr,1:nc]=0.  
Set tolerance
eps=1.e-8
Loop over a maximum number of iterations
do while (iter<maxiter)
   do i=1,nr
      do j=1,nc
         w[i,j]=0.25*(u[i-1,j] + u[i+1,j] + u[i,j-1] + u[i,j+1])
      end
   end
   #Remember to compare only inner values
   if maximum(abs(w[1:nr,1:nc]-u[1:nr,1:nc]))<eps then
      break #exit iteration loop
   else
      u[1:nr,1:nc]=w[1:nr,1:nc]
   end
   #reapply boundary conditions if appropriate, should not be needed here
   #u[0,:]=topBC etc.
end
Output result to a file
```
This method sets a maximum number of iterations, to prevent an effectively infinite loop in case the method doesn't converge, which it should do in nearly all cases; we then break out of the loop once convergence is achieved.  Alternatively we could loop until convergence is achieved; in this case we would need to set u and w differently at the beginning and it would be prudent to keep a count of the number of iterations and check whether the maximum was exceeded."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_nonblocking_completion.md,"Waiting for Requests
To complete the communications we must wait for the requests to be fulfilled.  MPI provides procedures for this.
Wait
MPI_Wait blocks until a specific request has completed.
C++
c++
MPI_Request request;
MPI_Status status;
MPI_Wait(&request, &status)
(Be sure to pay attention to how request and status are declared; they must be called by reference in any case.)
Fortran
fortran
integer request, ierr
integer, dimension(MPI_STATUS_SIZE) :: status
! code
call MPI_Wait(request, status, ierr)
With the new mpi_f08 module this becomes
fortran
integer ierr
type(MPI_STATUS) :: status
type(MPI_Request):: request
! code
call MPI_Wait(request, status, ierr)
If the request is null or inactive, the procedure returns with an empty status field.
Python
In Python the methods for handling requests are invoked through the appropriate Request object.  See its documenation for more specifics.  Note that the completion methods in mpi4py that require a list of request objects will be class methods.
request.Wait(status)
The mpi4py Wait returns a literal (True) when it completes. 
Waitall
MPI_Waitall blocks until all requests have completed.
c++
int count;
MPI_Request requests[];
MPI_Status status_arr[];
//allocate arrays somewhere to size count or declare static size
MPI_Waitall(count,requests,status_arr)
Fortran
fortran
integer count, ierr
integer, dimension(:), allocatable :: requests
integer, dimension(MPI_STATUS_SIZE,*) :: status_arr
! code, allocate requests to size count somewhere
call MPI_Waitall(count,requests, status_arr, ierr)
Python
python
requests=[]  #fill list to size count
status_list=list[status]
MPI.Request.Waitall(requests,status_list)
The mpi4py Waitall returns a literal True when it completes.
Waitany
MPI_Waitany waits for any of an array/list of requests to complete. The index parameter returns the index of the request array that has completed. In C++ and Python it ranges from 0 to count-1.  In Fortran the range is 1 to count. 
C++
c++
MPI_Request requests[];
MPI_Status status_arr[];
int index;
MPI_Waitany(count, requests, &index, status_arr)
Fortran
fortran
integer count, index, ierr
integer, dimension(:), allocatable :: requests
integer, dimension(MPI_STATUS_SIZE,*) :: status_arr
! code, allocate requests to size count somewhere
call MPI_Waitany(count, requests, index, status_arry, ierr)
Python
python
requests=[]  #fill list to size count
status_list=list[status]
ind=MPI.Request.Waitany(requests,status_list)
Testing Requests
The Test family of procedures are similar to the Wait routines, but return a flag indicating whether the request has been completed.  Test routines return with a value whether the request has completed or not, so when used they are tyically contained within loops that are exited when the test is true.
Test
C++
c++
MPI_Request request;
MPI_Status status;
int flag;
MPI_Test(&request, &flag, MPI_Status &status)
Fortran
fortran
logical :: flag
integer :: request, status(mpi_status_size), ierror
!code
call MPI_Test(request, flag, status, ierror)
Python
python
flag=request.Test(status)
Testall
Testall tests for each request.
c++
int count, flag;
MPI_Request requests[];
MPI_Status status_arr[];
//allocate arrays somewhere to size count
MPI_Testall(&count,requests,&flag,status_arr)
Fortran
fortran
integer :: count, ierr
logical :: flag
integer, dimension(:), allocatable :: requests
integer, dimension(MPI_STATUS_SIZE,*) :: status_arr
! code, allocate requests to size count somewhere
call MPI_Testall(count, requests, flag, status_arr, ierr)
Python
python
requests=[]  #fill list to size count
status_list=list[status]
flag=MPI.Request.Testall(requests,status_list)
Testany
c++
int count, ind, flag;
MPI_Request requests[];
MPI_Status status_arr[];
//allocate arrays somewhere to size count
MPI_Testany(&count,requests,&ind,&flag,status_arr)
Fortran
fortran
integer :: count, ind, ierr
logical :: flag
integer, dimension(:), allocatable :: requests
integer, dimension(MPI_STATUS_SIZE,*) :: status_arr
! code, allocate requests to size count somewhere
call MPI_Testany(count, requests, ind, flag, status_arr, ierr)
Python
python
requests=[]  #fill list to size count
status_list=list[status]
ind,flag=MPI.Request.Testany(requests,status_list)"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_recvstatus.md,"The status variable contains information about the message.


MPI_SOURCE
    The source rank of the message. This is a field of the status structure.

C++
    status.MPI_SOURCE
Fortran
    status(MPI_SOURCE)
Python
    status.Get_Source()



MPI_TAG
    The tag. This is another field of the structure (or element of the array).

C++
    status.MPI_TAG
Fortran
    status(MPI_TAG)
Python
    status.Get_tag()



MPI_Get_count(MPI_Status* status, MPI_Datatype datatype, int* count)
    The length (item count) of the message.  

C++
    MPI_Get_count(&status,MPI_TYPE,&item_count);
Fortran
    MPI_Get_count(status,MPI_TYPE,item_count,ierr)
Python 
    status.Get_count(MPI.TYPE)



MPI_Error
    The error number.  Not often needed.

C++
    status.MPI_ERROR
Fortran
    status(MPI_ERROR)
Python
    status.Get_error()



Note that mpi4py implements some C/C++/Fortran procedures as methods in the status object.
The MPI_SOURCE and MPI_TAG items may be especially useful for the special dummy variables defined for source and tag.
If the programmer does not intend to examine the status field, the special value MPI_STATUS_IGNORE may be passed as the status parameter (C++/Fortran) or in Python, by omitting the argument, since it is optional. 
Special Source and Tag Variables
MPI defines special variables that can be used in MPI_Recv
no-highlight
MPI_ANY_SOURCE
MPI_ANY_TAG
Either or both can be used in a receive if the MPI_Recv can accept a message from any source and/or any tag.
Example Status Usage
Add a status query to the exchange code to find the source, tag, and any error.  Note that Python uses functions to retrieve all the members of the status structure, and must also initialize it with a constructor.
Although the status data structure can also return the item count, if the receiver does not know the number or size of the message, we recommend invoking MPI_PROBE, which allows the receiving process to determine the specifics of the message without actually receiving it. It can then set up an appropriate MPI_Recv.
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/send_recv_stat.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/send_recv_stat.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/send_recv_stat.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_ghost_zones.md,"This type of exchange is particularly common in boundary-value problems.  We do some kind of computation on the grid, but then the boundary conditions must be applied.  In an undivided domain, the boundaries are set by the model we are solving. When we divide the domains, suddenly we have converted one boundary into many boundaries; each subdomain is disconnected from the others and do not know what has been computed on them. Some information must be exchanged regularly at the boundaries between the subdomains. 
A very widely used method to accomplish this is to extend the subdomain with ""ghost"" or ""halo"" zones. These are ""fake"" zones that function as the new boundaries for the inner grids.
{{< figure src=""/courses/parallel-computing-introduction/img/ghost_zones.png"" caption=""Ghost or halo zones are added to communicate with the neighboring domain."" >}}
`"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_project_set3.md,"Project 7
Implement a serial Jacobi code to solve the two-dimensional Poisson equation with boundary conditions as described in our example, namely
nohighlight
u[0,:]=0.
u[nr+1,:]=100.
u[:,0]=100.
u[:,nc+1]=100.
Do not assume a square grid, but you may use one for testing, such as 500x500. Write your output to a file in your preferred format.
Suggestions.  
It is usually not necessary to check for convergence at every timestep. Make it  a variable and experiment with different values.  Examine whether it affects the time required for a run.
Python programmers: once your code is working, try to use NumPy array operations with appropriate ranges to replace the double for loop.  It should speed up the code substantially.
Fortran programmers: when writing double do loops over array indices, whenever possible arrange the loops in order of the indices from right to left, for cache efficiency. Correct loop ordering can be approximately a factor of 50 faster than incorrect for a double loop.
Use whatever plotting package you know to make a contour plot of the result. If you do not have a preference, you may use contour.py below. It reads files starting with a specified base name and followed by any number of digits, including none. It will assemble them into one image and show a contour plot; if given a command-line option of -f it will transpose each subimage for Fortran.
When plotting, the top of an array (row 0) is the bottom of the plot.
{{< spoiler text=""Python script to contour output"">}}
{{< code-download file=""/courses/parallel-computing-introduction/scripts/contour.py"" lang=""python"" >}}
{{< /spoiler >}}
Example solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/heatedplate.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/heatedplate.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/heatedplate.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 8
Using the halo exchange code we have already seen, plus at least one collective communication, to parallelize your heated-plate code. Do not declare a single lage global array; break up the domain as we have learned (row-wise for C++ and Python, column-wise for Fortran).  For output, have each rank write its set of rows or columns, labeling the file with its rank ID.  
Example syntax:
c++
  fname=argv[2]+to_string(rank);
Fortran
fortran
  write(fname,'(a,i4.4)') filename(1:len_trim(filename)),rank
Python
python
filename = filename + str( rank )
To plot the results you will have to stitch the files together appropriately. If you have no other preference you may use the contour.py code below. Add the -f command-line option for Fortran. The script must be given an argument that is the ""base"" of the filenames, and all the output files must be in the same folder and numbered appropriately.
{{< spoiler text=""Python script to merge output files and contour"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/scripts/contour.py"" lang=""python"" >}}
{{< /spoiler >}}
Feel free to use the example solutions provided as your basis, but we recommend that you attempt to write your own serial version first.
Hints
1. You will need to modify the stopping criterion for the update loop. There are several options to do this. One is to use the maximum number of iterations as the criterion, then break out of the loop when the maximum difference in all the ranks is less than the specified tolerance.
2. Physical boundary conditions generally must be reapplied at each iteration to the ""solution"" matrix u. A subprogram to do this might be a good idea.
3. You need to do a full swap of the ""current"" solution u and the ""updated"" solution w at each iteration, which is why the BCs must be reapplied.
4. Check that your program works for a single process.
Example solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpiheatedplate.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpiheatedplate.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpiheatedplate.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 9
Scaling Studies
We have discussed weak and strong scaling. If you have not already done so, add timing to your heated plate solution in the language of your choice. Use MPI_Wtime() since it will generate uniform timings regardless of your programming language.  It is sufficient to instrument the while loop since nearly all the time will be spent there.


The example solutions show how to do strong or weak scaling. You can do only one at a time.  Weak scaling will increase the size of the global domain in a way that will make it rectangular rather than square, but that doesn't matter for our study.


Start with strong scaling, dividing up the same amount of work over an increasing number of processes.  Choose a value for the number of rows or columns that will be evenly divisible by 8. Run your code for 1, 2, 4, and 8 processes.  Make a plot of time versus number of processes.  Compute the parallel efficiency for each run.


Repeat for weak scaling. 


Make sure you are using idle cores for this test, or your results will not reflect the time spent on your processes. If you have access to a high-performance computing cluster with a resource manager (queueing system), submit each test case as a separate job. Are the results what you expected?"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_halo_sendrecv.md,"From our examination of the illustrations for column-major and row-major languages, we conclude that we should split our two-dimensional grid by columns for Fortran and similar languages, and by rows for C++, Python, and others with that array orientation. 
Let us assume for both cases that we have nr rows and nc columns in our grid.  In a one-dimensional decomposition, we will split only one of those values among the processes.  Call the local number of rows and columns nrl and ncl. When boundary (""ghost"" or real) are added, each subdomain will contain nrl+2 and ncl+2 rows and columns. Our computed domains will extend from $1$ to nrl and 1 to ncl, with boundaries at $0$ and at $nrl+1$ and $ncl+1$.  Two exchanges will require two Sendrecv invocations.
C++ and Python
For these languages $nrl = nr \div nprocs$ and $ncl=nc$. We will send the first computed row up to row $nrl+1$ of the process at $rank-1$ and the last computed row down to row $0$ of the process at $rank+1$.  
C++ syntax:
```C++
MPI_Sendrecv(&w[1][1],nc, MPI_DOUBLE,up,tag,&w[nrl+1][1],
                      nc, MPI_DOUBLE,down,tag,MPI_COMM_WORLD,&status);
MPI_Sendrecv(&w[nrl][1],nc,MPI_DOUBLE,down,tag,&w[0][1],
                        nc,MPI_DOUBLE,up,tag,MPI_COMM_WORLD,&status);
``
Note that although the variablew` alone would be a pointer, a specific element is not (it dereferences the memory location) and so must be passed to MPI by reference in C++.
Python syntax:
python
comm.Sendrecv([w[1,1:nc+1],MPI.DOUBLE], up, tag, [w[nr+1,1:nc+1],MPI.DOUBLE], down, tag )
comm.Sendrecv([w[nr,1:nc+1],MPI.DOUBLE], down, tag, [w[0,1:nc+1],MPI.DOUBLE], up, tag )
Fortran
For Fortran $nrl=nr$ and $ncl= nc \div nprocs$. We send the first computed column left to column $ncl+1$ of $rank+1$ and the last computed column right to column $0$ of $rank-1$.
```fortran
call MPI_SENDRECV(w(1:nr,1),nr,MPI_DOUBLE_PRECISION,left,tag, w(1:nr,ncl+1),nr,&
                               MPI_DOUBLE_PRECISION,right,tag,                 &
                                                  MPI_COMM_WORLD,mpi_stat,ierr)
call MPI_SENDRECV(w(1:nr,ncl),nr,MPI_DOUBLE_PRECISION,right,tag, w(1:nr,0),nr, &
                                 MPI_DOUBLE_PRECISION,left,tag,                &
                                                  MPI_COMM_WORLD,mpi_stat,ierr)
```
MPI_PROC_NULL
Rank $0$ has no neighbor to the top or left, and rank $nprocs$ has no neighbor to the bottom or right. We might think that we would have to write a complicated set of conditionals to handle these situations, but this is a common scenario and MPI provides a built-in solution: MPI_PROC_NULL.
The special value MPI_PROC_NULL can be used in place of a source or destination and results in a ""no op,"" i.e. the send or receive is not attempted and the function returns immediately.
Exercise
Use the language of your choice.  Write a program that fills an array w with the uniform value of 50 in the inner grid (1 to nr, 1 to nc) where nr=nc=500.  Set the values of row 0 to 0 and the values of row nrl+1 and columns 0 and ncl+1 to 100.  Do not worry about whether values at corners match. 
Set up the transfer and run one exchange from u to w. Print some values to check your result.
Review strong and weak scaling.  Try both with different runs in your code. For strong scaling, make sure that the number of processes equally divides the number of rows or columns. 
Hint
Set up the neighbors for each rank (""up"" and ""down"" or ""left"" and ""right"") before doing any transfers.
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpi_halo_exchange.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpi_halo_exchange.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpi_halo_exchange.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_project_set2.md,"Project 4
A ""token ring"" is a circular messaging system.  Think of a relay, with a message or ""baton"" being passed from one runner to the next, but around a circle so that the last ""runner"" passes it back to the first. Write a program that implements this with MPI.  Hint: separate the ranks into root and everybody else. You may wish to use MPI_ANY_TAG and MPI_ANY_SOURCE.
Example Solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/ring.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/ring.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/ring.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 5
Write a program in which each process determines a unique partner to exchange messages.  One way to do this is to use
no-highlight
if rank < npes//2:
partner=npes//2 + rank
else
partner=rank-npes//2
where the // indicates integer division (no fractional part).
Each tasks sends its rank to its partner.  Each task receives the partner's rank.  Print the message received when done.  You may assume that the number of processes is even, but check that this is the case before proceeding.
Hints: do not overwrite the receiver's rank.  As always, Python programmers should take care that NumPy arrays are declared with the correct type.
Example Solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/send_recv_rank.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/send_recv_rank.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/send_recv_rank.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 6
A common pattern in parallel programming is the manager-worker structure. A ""manager"" process distributes work to ""worker"" processes.  Sometimes manager code is separated by if rank==0 (the manager should nearly always be rank 0) statements, while the other ranks execute the ""worker"" code. Sometimes the manager spawns distinct worker processes, but that requires using the more advanced MPI spawn capability.  In this project we will use a single code.  Usually the manager distributes work to the workers, which return results; the manager then hands more work to those processes that are ready, until all is completed. For this example the workers will do only one round of work.
Starting from the stub for your language, complete the send and receive calls.
Starting Codes
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/manager_worker_stub.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/manager_worker_stub.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/manager_worker.py"" lang=""python"" >}}
{{< /spoiler >}}
Example Solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/manager_worker.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/manager_worker.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/manager_worker.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_global1.md,"So far we have seen only examples without communication.  However, all practical MPI programs will make use of some form of interprocess communications.  The simplest are global, or collective communications in which every process in a communicator group participate.  Global communications can be one to many, many to one, or all to all.
In one-to-many collective communications, one process, generally called the root, sends a message to all other members of the communicator group. The buffer is read on the root and written to the recipients.
Broadcast
In a broadcast, the root process sends the same data to every other process.  It is not required, but it is usual to make process 0 the root, since that is the only one guaranteed to be present.  The buffer may be an array but all elements of the array are sent to every other process in the communicator group.
{{< figure src=""/courses/parallel-computing-introduction/img/broadcast.png"" caption=""Broadcast"" >}}
C++
The prototype is
c++
int MPI_Bcast (void *buffer, int ncount, MPI_Datatype datatype, int root, MPI_Comm communicator);
In this prototype, buffer is the variable holding the data, ncount is the number of items (not bytes) sent, MPI_Datatype is a struct defined in mpi.h, and MPI_Comm is also a struct defined in mpi.h.
{{< spoiler text=""C++ Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/bcast.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
Fortran
fortran
<type>   :: vals
integer  :: ncount, root, err
! more code
call MPI_Bcast(vals, ncount, MPI_TYPE, root, MPI_COMM_WORLD, err)
The argument vals can be of any primitive type that corresponds to a supported MPI_TYPE.
{{< spoiler text=""Fortran Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/bcast.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Python
python
comm.bcast(data, root=0)  #general object
comm.Bcast([sendvals,MPI.TYPE], root=0)  #NumPy array
The pickled version does not use MPI.TYPE because a pickled object is a binary stream and mpi4py handles the data description passed to MPI.  In the NumPy version we may specify the type, although it is optional because a Ndarray is aware of its type.  The array and type should be a list.
{{< spoiler text=""Python Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/bcast.py"" lang=""python"" >}}
{{< /spoiler >}}
Scatter
A scatter breaks up an array and sends one part to each process, with root keeping its own part.  The simplest function distributes the same ncount of the array to each process.  The sections are distributed from the first element in rank order.  If root=0 this means that it sends itself the first ncount elements and sends the next ncount elements to process 1, the ncount after that to process 2, and so forth.  If root is not zero, that process sends the first ncount to rank 0 and so on, sends the rank-appropriate section to itself, then sends to the next until all processes in the communicator group have received data. For a simple Scatter, the number of elements of the ""send buffer"" should be divisible by ncount.
{{< figure src=""/courses/parallel-computing-introduction/img/scatter.png"" caption=""Scatter"" >}}
In the  root process, the send buffer must contain all the data to be distributed, so it is larger than receive buffer by a factor of $ncount \times nprocs$.
C++
c
int MPI_Scatter(void *sendbuffer, int ncount, MPI_Datatype datatype, void *recvbuffer, int ncount, MPI_Datatype datatype, int root, MPI_Comm communicator);
{{< spoiler text=""C++ Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/scatter.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
Fortran
fortran
call MPI_Scatter(vals,ncount,MPI_TYPE,rvals,ncount,MPI_TYPE,root,MPI_COMM_WORLD,err)
{{< spoiler text=""Fortran Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/scatter.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Python
Both buffers should be Numpy arrays. The datatype is usually not required, and the root process is 0 by default, so that is an optional argument.
python
comm.Scatter([sendvals,MPI.DOUBLE],[recvals,MPI.DOUBLE,root=0)
{{< spoiler text=""Python Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/scatter.py"" lang=""python"" >}}
{{< /spoiler >}}
More General Scattering
The MPI_Scatter procedure allows only equal distribution of an entire array of values.  If more general distribution is needed, the MPI_Scatterv (vector scatter) allows considerable flexibility.  In this case, ncount becomes an integer array. Different ranks may receive different numbers of items.  An integer displacement vector is included, so that elements can be skipped.  Displacements are measured from the start of the array; each one is essentially the starting index of the block to be sent.  (Fortran programmers, pay attention to 1-based versus 0-based computations.)
In the example codes we will assume 8 processes; generally we should not hard-code a number of processes, but this simplifies the arithmetic.
C/C++
c
int MPI_Scatterv(void *sendbuf, int *sendcounts, int *displs, MPI_Datatype sendtype, void *recvbuf, int recvcounts, MPI_Datatype recvtype, int root, MPI_Comm comm);
{{< spoiler text=""C++ Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/scatterv.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
Fortran
fortran
call MPI_SCATTERV(sendbuf, sendcounts, displs, sendtype, recvbuf, recvcounts, recvtype, root, comm, ierr)
{{< spoiler text=""Fortran Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/scatterv.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Python
python
comm.Scatterv([sendbuf,sendcounts,displs,MPI.TYPE],recvbuf)
Unlike many other mpi4py procedures, the MPI.TYPE may often be required for correct data transmission.
{{< spoiler text=""Python Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/scatter.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_vector_type.md,"Let us examine a specific application of the MPI Vector type.  On each process, we will generate an identical matrix as the source, and another matrix of the same size, initialized to zeros, as the destination.  A process will send the column (for C and Python) or row (Fortran) corresponding to its rank, to the next-higher indexed column or row in the next-higher rank; except for the last rank which will send its colunn/row to the first (zeroth) column/row of rank 0, i.e. the exchanges are cyclic.
C/C++
C, C++, Python, and many other languages are row-major oriented.  That is, a two-dimensional array is mapped to linear memory by stacking one row after another, in order.  In this illustration, we are going to place the zeroth column into a buffer. 
{{< figure src=""/courses/parallel-computing-introduction/img/mpi_vector_type_C.png"" caption=""count=3, blocklength=1, stride=4"" >}}
We start the selection at u[0][0].  The count, the number of items, is the number of rows, while the stride is the number of columns.
Since we are receiving into a column, we can use the same type to receive. The count, blocklength, and stride would be the same, but the starting location of the buffer might be different. The important invocations will look like this:
C++
```c++
   //The length of the column is the number of rows
    int ncount=nr;
    //The number of items picked from each stride is 1
    int blocklength=1;
    //The length of the row is the number of columns
    int stride=nc;
MPI_Datatype cols;
MPI_Type_vector(ncount,blocklength,stride,MPI_DOUBLE,&cols);
MPI_Type_commit(&cols);

MPI_Irecv(&w[0][0], 1, cols, src, tag, MPI_COMM_WORLD, &requests[0]);
MPI_Isend(&u[0][0], 1, cols, dest, tag, MPI_COMM_WORLD, &requests[1]);

```
{{< spoiler text=""The full code in C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_vector_type.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
Python
As we have discussed, when sending MPI types we must extract the raw data from a NumPy array.
```python
The length of the column is the number of rows
ncount=nr
The number of items picked from each stride is 1
blocklength=1
The length of the row is the number of columns
stride=nc
cols = MPI.DOUBLE.Create_vector(ncount, blocklength, stride)
cols.Commit()
recv_request=comm.Irecv([np.frombuffer(w.data,np.double,offset=sendcolnp.dtype('double').itemsize),1,cols],src)
send_request=comm.Isend([np.frombuffer(u.data,np.double,offset=recvcolnp.dtype('double').itemsize),1,cols],dest
```
{{< spoiler text=""The full code in Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_vector_type.py"" lang=""python"" >}}
{{< /spoiler >}}
Fortran
Fortran is colum-major oriented.  Memory is laid out by columns. In this illustration, we are selecting the fourth row to place into the buffer.  
{{< figure src=""/courses/parallel-computing-introduction/img/mpi_vector_type_fortran.png"" caption=""count=4, blocklength=1, stride=3"" >}}
The buffer will start at u(3,1).  As for C++ and Python, we can use the same vector type for sending and receiving.
Our code snippet would look something like this.
```fortran
! The length of the row is the number of columns
   ncount=nc
   ! The number of items picked from each stride is 1
   blocklength=1
   ! The length of the column is the number of rows
   stride=nr
call MPI_Type_vector(ncount,blocklength,stride,MPI_DOUBLE_PRECISION,rows)
call MPI_TYPE_COMMIT(rows)
call MPI_Irecv(w(1,1),1,rows,src,tag,MPI_COMM_WORLD,mpi_requests(1))
   call MPI_Isend(u(3,1),1,rows,dest,tag,MPI_COMM_WORLD,mpi_requests(2))
```
{{< spoiler text=""The full code in Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_vector_type.f90"" lang=""fortran"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_types.md,"Modern programming languages provide data structures that may be called ""structs,"" or ""classes,"" or ""types.""  These data structures permit grouping of different quantities under a single variable name.
MPI also provides a general type that enables programmer-defined datatypes. Unlike arrays, which must be adjacent in memory, MPI derived datatypes may consist of elements in non-contiguous locations in memory.
Example: MPI_TYPE_VECTOR
While more general derived MPI datatypes are available, one of the most commonly used is the MPI_TYPE_VECTOR. This creates a group of elements of size blocklength separated by a constant interval, called the stride, in memory. Examples would be generating a type for columns in a row-major-oriented language, or rows in a column-major-oriented language.  
{{< figure src=""/courses/parallel-computing-introduction/img/mpi_vector_type.png"" caption=""Layout in memory for vector type. In this example, the blocklength is 4, the stride is 6, and the count is 3."" >}}
C++
c++
int ncount, blocklength, stride;
MPI_Datatype newtype;
// Note that oldtype is not passed by reference but newtype is
MPI_Type_vector(ncount, blocklength, stride, oldtype, &newtype);
Fortran
The ierror parameter is optional if the mpi_f08 module is used.
fortran
integer :: count, blocklength, stride
integer :: newtype
!code
call MPI_TYPE_VECTOR(ncount, blocklength, stride, oldtype, newtype [, ierror])
Fortran 2008
fortran
use mpi_f08
integer :: count, blocklength, stride
type(MPI_Datatype) :: newtype
!code
call MPI_Type_vector(count, blocklength, stride, oldtype, newtype)
For both C++ and Fortran, ncount, blocklength, and stride must be integers. The oldtype is a pre-existing type, usually a built-in MPI Type such as MPI_FLOAT or MPI_REAL. For C++ the new type would be declared as an MPI_Datatype, unless it corresponds to an existing built-in type.  For Fortran the types  would be an integer if not a built-in type, whereas for Fortran 2008 they are a type declared similarly to the C++ equivalent. The newtype is a name chosen by the programmer.
Python
python
newtype = oldtype.Create_vector(ncount,blocklength,stride)
Committing the Type
A derived type must be committed before it can be used.
c++
MPI_Type_commit(newtype)
Fortran
fortran
call MPI_TYPE_COMMIT(newtype[,ierr])
Python
python
newtype.Commit()
Using a Type
To use our newly committed type in an MPI communication function, we must pass it the starting position of the data to be placed into the type.  Notice that the item count is the number of instances of the type.  In our examples this will usually be 1.
C++
c++
//We need to pass the first element by reference because an array element
//is not a pointer
MPI_Isend(&u[0][i],1,newtype,dest,tag,MPI_COMM_WORLD,&mpi_requests[1]);
MPI_Irecv(&w[0][j],1,newtype,source,tag,MPI_COMM_WORLD,&mpi_requests[2]);
Fortran
Note that we do not use array slicing in this example. It is often best to avoid that, especially when using nonblocking communications, because a slice involves a copy.
fortran
! assuming mpi_f08 module
MPI_Isend(u(i,1),1,newtype,dest,tag,MPI_COMM_WORLD,mpi_requests(1))
MPI_Irecv(w(j,1),1,newtype,source,tag,MPI_COMM_WORLD,mpi_requests(2))
Python
Python NumPy arrays do not generally expose the underlying pointers to the values. The frombuffer method of NumPy will allow us to set the pointer by using the offset argument.
python
sendCol = 2
send_request=comm.Isend([np.frombuffer(a.data,intc,offset=sendCol*np.dtype('intc').itemsize),1,cols],dest)
Freeing Types
When we are done with a type, we should free it.
C++
c++
MPI_Type_free(newtype)
Fortran
fortran
call MPI_Type_free(newtype[,ierror])
Python
python
newtype.Free()"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_random_walk.md,"We would like to start with a simple example that has a simple work distribution.  To make it even easier, we will use no communications.
Two-Dimensional Lattice Random Walk
A particle is moving through a two-dimensional finite grid.  At each step, the ""walker"" can move left, right, up, or down, with equal probability.  We seek the distance from the origin after N steps.  
Theoretically, for N steps the distance is $\sqrt{N}$. We want to test this empirically, but one trial does not give us very good results.  We want to run a lot of trials and average the results.
Agglomeration and Mapping
This algorithm has the properties that:
  * Each trial is independent of the others
  * There is a fixed number of tasks
  * No communications are needed between tasks to obtain independent results.
Serial Code
Download the serial code for your language of choice.  Compile it (if appropriate), then run a test case.
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/random_walk.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/random_walk.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/random_walk.py"" lang=""python"" >}}
{{< /spoiler >}}
Using the mpi1 program in your language as an example, add the lines to run this code with MPI.  In your print statement make the first output value be the rank.
Run it
```bash
mpicxx -o randc mpirandom_walk.cxx
mpiexec -np 4 ./randc 1000000
0:1000000,1000,898.186
1:1000000,1000,189.589
2:1000000,1000,1235.87
3:1000000,1000,391.479
We have to compute the average manually
678.781
```
Try with -np 8
no-highlight
0:1000000,1000,1011.24
5:1000000,1000,1569.2
3:1000000,1000,1753.23
2:1000000,1000,967.042
1:1000000,1000,1212.17
6:1000000,1000,418.708
7:1000000,1000,881.862
4:1000000,1000,744.641
Why is the rank order jumbled?
MPI output is nondeterministic unless the programmer forces it to be ordered, using a barrier."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_buffers.md,"MPI documentation refers to ""send buffers"" and ""receive buffers."" These refer to  variables in the program whose contents are to be sent or received.  These variables must be set up by the programmer.  The send and receive buffers cannot be the same unless the special ""receive buffer"" MPI_IN_PLACE is specified.
When a buffer is specified, the MPI library will look at the starting point in memory (the pointer to the variable).  From other information in the command, it will compute the number of bytes to be sent or received.  It will then set up a separate location in memory; this is the actual buffer. Often the buffer is not the same size as the original data since it is just used for streaming within the network.  In any case, the application programmer need not be concerned about the details of the buffers and should just regard them as variables.  
For the send buffer, MPI will copy the sequence of bytes into the buffer and send them over the appropriate network interface to the receiver.  The receiver will acquire the stream of data into its receive buffer and copy them into the variable specified in the program. 
Buffer Datatypes
MPI supports most of the primitive datatypes available in the target programming language, as well as a few others.
In every language, it is imperative that the data types in the send and receive buffers match.  If they do not, the result can be anything from garbage to a segmentation violation.
C/C++
MPI supports most C/C++ datatypes as well as some extensions. The most commonly used are listed below.
{{< table >}}
|   C/C++ type   |  MPI_Datatype  |
|----------------|----------------|
|   int          |    MPI_INT     |
|   short        |    MPI_SHORT   |
|   long         |    MPI_LONG    |
|   long long    |    MPI_LONG_LONG_INT  |
|   unsigned int |    MPI_UNSIGNED    |
|   unsigned short |  MPI_UNSIGNED_SHORT  |
|   unsigned long |  MPI_UNSIGNED_LONG |
|   unsigned long long |  MPI_UNSIGNED_LONG_LONG |
|   float        |  MPI_FLOAT      |
|   double       |  MPI_DOUBLE     |
|   long double  |  MPI_LONG_DOUBLE     |
|   char         |  MPI_CHAR        |
|   wchar         |  MPI_WCHAR        |
{{< /table >}}
Specific to C:
{{< table >}}
|   C type       |  MPI_Datatype      |
|----------------|----------------|
|   bool         |  MPI_C_BOOL        |
|   complex         |  MPI_C_COMPLEX        |
|   double complex  |  MPI_C_DOUBLE_COMPLEX        |
{{< /table >}}
Specific to C++:
{{< table >}}
|   C++ type       |  MPI_Datatype      |
|----------------|----------------|
|   bool         |  MPI_CXX_BOOL        |
|   complex         |  MPI_CXX_COMPLEX        |
|   double complex  |  MPI_CXX_DOUBLE_COMPLEX        |
{{< /table >}}
Extensions
{{< table >}}
|   C/C++ type   |  MPI_Datatype  |
|----------------|----------------|
|   none         | MPI_BYTE         |
|   none         | MPI_PACKED       |
{{< /table >}}
Fortran
{{< table >}}
|   Fortran type |  MPI_Datatype      |
|----------------|--------------------|
|   integer      |    MPI_INTEGER     |
|   integer*8   |    MPI_INTEGER8    |
|   real         |    MPI_REAL        |
|   double precision    |    MPI_DOUBLE_PRECISION |
|   complex      |  MPI_COMPLEX       |
|   logical      |  MPI_LOGICAL       |
|   character    |  MPI_CHARACTER     |
|   none         |  MPI_BYTE          |
|   none         |  MPI_PACKED        |
{{< /table >}}
Most MPI distributions support the following types.  These are Fortran 77 style declarations; newer code should use KIND but care must be taken that the number of byes specified is correct.
{{< table >}}
|   Fortran type |  MPI_Datatype      |
|----------------|--------------------|
|   integer*16      |    MPI_INTEGER16     |
|   real*8      |    MPI_REAL8     |
|   real*16      |    MPI_REAL16     |
{{< /table >}}
Python
As we have mentioned, the basic MPI communication routines are in the Communicator class of the MPI subpackge of mpi4py.  Each communication subprogram has two forms, a lower-case version and another where the first letter of the method is upper case.  The lower-case version can be used to send or receive an object; mpi4py pickles it before communicating.  The argument of these routines is the sent object; the received object is the return value of the function.
The upper-case version works only with buffered objects, usually NumPy Ndarrays.  Communicating Ndarrays is faster and is recommended when possible. However, every buffer must be a Ndarray in this case, so even scalars must be placed into a one-element array. The upper-case buffered functions are more similar to the corresponding C/C++ functions.  For the buffered functions, it is very important that the types match, so use of dtype is recommended in declaring NumPy arrays.
The mpi4py package supports the C datatypes, in the format MPI.Dtype rather than MPI_Dtype, but they are seldom required as an argument to the MPI functions.  It is strongly recommended that the type of each NumPy array be explicitly declared with the dtype option, to ensure that the types match in both send and receive buffers.  
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi4py_ex.py"" lang=""python"" >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_memory_layouts.md,"Halo exchanges typically involve slices of arrays. In our current example, we are discussing exchanging parts of a two-dimensional array. We have learned how to communicate one-dimensional arrays; now we must consider communicating portions of higher-dimensional arrays.
As we learned, MPI does not construct buffers from variables; it reads a specified number of bytes starting at a specified location in memory.  We must therefore take into consideration how arrays are organized in memory.
In a given programming language, an array can be row major oriented or column major oriented.  In a row-major oriented language (most of them, including C/C++ and Python) a two-dimensional array is represented in memory in terms of its indices as
{{< diagram >}}
flowchart LR
   A[0,0] --- B[0,1]
   B --- C[0,2]
   C --- D[0,3]
   D --- E[1,0]
   E --- F[1,1]
   F --- G[1,2]
   G --- H[1,3]
   H --- I[2,0]
   I --- J[2,1]
   J --- K[2,2]
   K --- L[2,3]
   L --- M[3,0]
   M --- N[3,1]
   N --- O[3,2]
   O --- P[3,3]
{{< /diagram >}}
That is, the second index changes most rapidly as we traverse the array in memory.  
In C/C++ the rows may or may not be contiguous in memory, but the elements of each row will be contiguous.
In a column-major oriented language (Fortran and some others such as Matlab, Julia, and R) the layout is by column. These languages also count from 1, at least by default, as represented below.
{{< diagram >}}
flowchart LR
   A[1,1] --- B[2,1]
   B --- C[3,1]
   C --- D[4,1]
   D --- E[1,2]
   E --- F[2,2]
   F --- G[3,2]
   G --- H[4,2]
   H --- I[1,3]
   I --- J[2,3]
   J --- K[3,3]
   K --- L[4,3]
   L --- M[1,4]
   M --- N[2,4]
   N --- O[3,4]
   O --- P[4,4]
{{< /diagram >}}
In this case the first index varies most rapidly.  In Fortran all columns of a two-dimensional array (and analogously for higher-dimensional arrays) are guaranteed to be contiguous in memory.
The figure we examined previously illustrates an exchange of columns and would be used for the column-major languages. To visualize the exchange for row-major languages, we just rotate that figure 90 degrees.
{{< figure src=""/courses/parallel-computing-introduction/img/halo_exchange_row_order.png"" caption=""We will send edge rows for row-major languages."" >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/parallel_software_approaches.md,"Parallel programming has been studied by computer scientists for decades. Historically, several approaches have been developed.  They may be summarized as
- Extend existing compilers: translate sequential programs into parallel programs automatically.
- Extend existing languages: add parallel operations.
- Develop a parallel language layer as a library to be invoked by sequential languages.
- Develop new languages that incorporate parallelism from the ground up.
Extend Compilers
A parallelizing compiler should be able to detect parallelism in a sequential program and produce a parallel executable program. A historical example was 
High-Performance Fortran, an early effort to make an automatically parallelizing compiler. It never caught on, but some constructs and concepts were incorporated into the Fortran 90 and later standards.
More recently, some compilers can autogenerate parallel threads using their built-in libraries, such as OpenMP.  The ability of the compiler to auto-parallelize efficiently depends heavily on the program structure and the algorithms, but sometimes it is quite effective.
Extending existing compilers could leverage millions of lines of existing serial programs, thus saving time and labor.  No retraining of programmers is required. Sequential programming is easier than parallel programming so reducing the need for skilled parallel programmers would increase the number of programs that could run in parallel.
On the other hand, not all algorithms lend themselves to parallization at all; parallelism opportunities may be irretrievably lost when programs are written in sequential languages.  At least partly for this reason, the performance of parallelizing compilers on a broad range of applications still uneven, but recognizing and developing algorithms and codes that would be amenable to automatic parallelization requires much of the same skillset as writing the parallel portions.
Extend Existing Languages
Another option is to add functions to a sequential language that create and terminate processes, synchronize processes, and allow processes to communicate.
This has a number of advantages: it is the easiest, quickest, and least expensive
approach. It allows existing compiler technology to be leveraged. Minimal retraining of programmers is necessary.
Unfortunately, this can be very difficult to implement, thus compromising availability and uptake.  Work continues in this area with two examples being UPC++ (Unified Parallel C++) and Co-Array Fortran.  Both implement a parallel model called Partitioned Global Address Space (PGAS).  Arrays are distributed over processes in a manner that is transparent to the programmer.  UPC++ is an extended version of the language and must be installed separately, whereas Co-Array Fortran has been incorporated into the language standard from 2008 onward.
Add a Parallel Programming Layer
This is the most widely used approach, but it also requires the most programming knowledge and effort.  The implementation is in the form of libraries, which may be internal to the compiler (OpenMP) or external (MPI, pthreads). The libraries handle the core of the computation.  An upper-layer API (Application Programming Interface) is invoked by the programmer to create and synchronize processes or threads and manage their communications.  In some cases the programmer must also handle the partitioning of data among the processes.
Create a Parallel Language
Given the difficulties with extending existing compilers and languages, it might seem that the better approach is to develop a parallel language ""from scratch.""
Two examples are OCaml and Chapel.  Taking a bottom-up approach allows the programmer to communicate parallelism to compiler and increases the probability that the executable will achieve high performance.  However, it requires development of new compilers, with implementation difficulties similar to those for extending existing languages.  The new languages may not become widely adopted, especially since there is programmer resistance to learning new languages.  Finally, this could require rewriting millions of lines of code, which would be time-consuming and may not be particularly productive.
Standard Parallel Programming Libraries
SMP
SMP programs are implemented through libraries that communicate with the operating system to manage threads.
For a parallel program, an initial thread is created and subordinate threads
are generated as requested by the threading library.
Popular threading libraries are OpenMP and pthreads (POSIX Threads).  OpenMP is a standard
that is implemented within the compiler. Pthreads is independent
of the compiler but is written in C; usually a version built with the
system libraries is available and can be used with most compilers.
OpenACC, which is specific to GPUs, is similar to OpenMP in that it is
a compiler-provided library.
Since OpenMP is a standard and is provided by compiler vendors, it can
be used on any operating system for which the necessary compiler is installed; in particular it is available for Linux, Windows, and macOS.  The version supported
and the quality of the implementation will depend on the compiler vendor.
Pthreads originated on Unix and can be used on macOS, which is Unix-based, but it is not
native on Windows though a wrapper is available.
In shared-memory programming, the abstract ""process"" we have discussed
corresponds to a thread.  It is generally desirable that each thread be
run on its own core, or logical core in the case of hyperthreading hardware.
However, only a single copy of the executable is run.  The initial thread
manages the others.  In OpenMP, data distribution among the threads is handled by
the OpenMP library, though the programmer must specify certain attributes in
most cases.
DMP
In distributed-memory parallelism, each process corresponds to a separate
copy of the program's executable.  All processes are identical; different
behavior for communication purposes must be managed by conditionals in the
program.  Data decomposition is entirely the responsibility of the programmer.
By far the most widely used communication library for distributed-memory programming is MPI, the Message-Passing Interface.  When communication is required, one node sends a ""message"" to one or more other nodes.  The message consists of
the data to be exchanged, along with some information about source,
destination, and other identifiers.
GPU
GPU programming is in a category of its own. GPUs are highly optimized for data-parallel threaded coding, and current hardware designs do not have access to the host computer's memory, which means that data must be moved back and forth, much as for distributed computing, while computations are threaded internally.  Several libraries are in use for GPU programming, including CUDA for NVIDIA devices, OpenACC, and extensions to
OpenMP.  OpenCL is another popular library that aims to support a variety
of parallel architectures, including CPUs, GPUs, and FPGAs (field-programmable
gate arrays, customizable chips often used in embedded applications and artificial neural networks).
Software Taxonomy
Much as there was a taxonomy for hardware, there is a classification scheme for software.  The two main categories are as follows:


SPMD
    Single program multiple data.  This does not mean only one process is running, but that all the processes are the same but they are working on different data.  This is the majority of modern parallel programming, whether shared memory or distributed memory.


MPMD
    Multiple program multiple data.  This would represent different programs running on their own data.  The normal functioning of most computers is MPMD, with many programs running on different cores, and often sharing the cores, but in the parallel-programming context we would generally reserve this terminology for a set of programs that are in some way coordinated.  Often this occurs when a ""manager"" program spawns other programs, which communicate with the manager and possibly with each other.  Some gaming consoles operate on the MPMD model.

"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_sendrecv.md,"The pattern of sending and receiving we have just seen is so common that the MPI standard provides a built-in function to handle it, MPI_Sendrecv.  This function is guaranteed not to deadlock for an exchange between source and dest.  In general, sendcount and recvcount, and the sendtype and recvtype, should be the same.  Tags must also match appropriately.
Sendrecv
The syntax for MPI_Sendrecv is
c++
int MPI_Sendrecv(&sendbuf, sendcount, sendtype, dest, sendtag,
                 &recvbuf, recvcount, recvtype, source, recvtag, comm, &status)
fortran
call MPI_Send(sendbuf, sendcount, sendtype, dest, sendtag, 
              recvbuf, recvcount, recvtype, source, recvtag, comm, status, ierr)
python
comm.Sendrecv([sendbuf,sendtype], dest, sendtag=0, recvbuf=None, source=ANY_SOURCE, recvtag=ANY_TAG, status=None)
Python programmers should observe that the above syntax is taken from the mpi4py documentation, and values of variables are the defaults. As usual there is a lower-case form for pickled objects that does not use the list for the buffers.
Examples
We will rewrite the previous examples using MPI_Sendrecv. Each process will send its rank to its neighbor on the right and will receive that neighbor's rank.
{{< spoiler text=C++ >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_sendrecv.cxx"" lang=c++ >}}
{{< /spoiler >}}
{{< spoiler text=Fortran >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_sendrecv.f90"" lang=fortran >}}
{{< /spoiler >}}
{{< spoiler text=Python >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_sendrecv.py"" lang=python >}}
{{< /spoiler >}}
Exercise
Download the three codes for your language and try them."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_array_buffers.md,"So far in our examples we have only discussed sending scalar buffers.  In computing, a scalar is a variable that holds only one quantity. The exact meaning of array varies from one programming language to another, but in all cases it refers to a variable that represents several quantities, each of which can be individually accessed by some form of subscripting the array.
To understand array communications we must consider how MPI buffers are created. The first argument to a send or receive is a pointer to a location in memory.  In C++ this is explicit; the argument must be passed by reference if it isn't declared a pointer.  Fortran always passes by reference so nothing is required aside from the variable name. The mpi4py bindings arrange for the argument to be a pointer so as for Fortran, only the variable name is required.
For a scalar variable, the pointer is to the location in memory of that variable. For an array, the pointer is to the first element of the section of the array to be sent, which may be all or part of it. The item count is the number of items to be sent.  The MPI type specifies the number of bytes per item.  From this information the MPI library computes the total number of bytes to be put into or received from the buffer. The library reads that number of bytes, starting at the initial memory location.  It pays no attention to any indexing of that string of bytes. This is why it is extremely important that the send and receive buffers match up appropriately; in particular, if the send buffer is longer, in bytes, than the receive buffer, then invalid data can be written to the receiver, or the process may even terminate.
Consider an example where each rank computes an array u and sends it to its left into a receive buffer w.  We will show the syntax only for MPI_Sendrecv since how to break it into individual Send and Recv if desired should be obvious.
In the C++ example we create the arrays with the new operator.
c++
double* u=new double[nelem]{0};
double* w=new double[nelem]{0};
//Fill u with something
MPI_Sendrecv(u, nelem, MPI_DOUBLE,neighbor,sendtag,
             w, nelem, MPI_DOUBLE,neighbor,recvtag,MPI_COMM_WORLD,&status);
Normally in C++ an array variable will be declared a pointer and so it is not passed by reference to the MPI subprograms. 
Fortran and Python are straightforward.
fortran
! In the nonexecutable part we declare u and w. They can be allocatable or static.
! Fill in u with some values
call MPI_Sendrecv(u,nelems,MPI_DOUBLE_PRECISION,neighbor,sendtag,              &
                  w,nelems,MPI_DOUBLE_PRECISION,neighbor,recvtag,              &
                                                   MPI_COMM_WORLD,status,ierr)
Python
```
u=np.zeros(nelems)
w=np.zeros(nelems)
fill in u with some values
comm.Sendrecv([u,MPI.DOUBLE],neighbor,0,[w,MPI.DOUBLE],neighbor,0,MPI.Status())
```
Exercise
Use the above syntax for your language to write a complete program to implement the sending and receiving as specified above.  For u you should fill it with 
nohighlight
u[i]=20.+i*rank  C++ and Python
u(i)=20.+(i-1)*rank Fortran (just so the answer is the same)
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpi_send_array.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpi_send_array.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpi_send_array.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/parallel_hardware.md,"Our discussion so far has applied to an abstract computer with little 
reference to the physical machines.  We are running multiple ""processes,"" vaguely defined, on some unspecified computer architecture.  However, the hardware
affects what types of parallelism we can apply and vice versa.
Hardware Taxonomy
Much of the classification for parallel computing is based on Flynn's taxonomy. This was first developed in 1972 but much of it is still relevant.  It is based on how many instruction streams (programming code) and data streams the hardware can handle


SISD
    Single instruction single data.  Early computers had single processors that could run only one process at a time and could apply the instructions to only one data stream (thread) at a time.


SIMD
    Single instruction multiple data.  The processor applies a code instruction to multiple threads of data at a time.  Most modern graphical processing units (GPUs) are SIMD devices.


MISD
    Multiple instruction single data.  Not widely encountered outside special-purpose computers such as those used for the Space Shuttle.  In this category, multiple computers work on one set of data, such as sensor data.


MIMD
    Multiple instruction multiple data.  Nearly all modern hardware falls into this category.  This can apply either to multicore single computers or to distributed computing clusters.


High-Throughput Computing.
High-throughput computing (HTC) is also called ""embarrassingly parallel"" or ""pleasingly parallel"" computing  In this form of parallelism, independent processes with little or no need to communicate are run simultaneously.  This can involve data parallelism or task parallelism or both.  High-throughput workloads can
be run on any type of computer as long as multiple processes can be started 
on the system.  
Examples of HTC include


Image processing, where the analysis of each frame is completely independent of all other frames.  


Monte Carlo simulations.  Hundreds or thousands of models are run with randomized parameters.  


In principle, HTC is 100% efficient; in practice, some gathering and processing of results is necessary.  For instance, in the image-processing example the end product may be a movie, in which case the frames will have to be encoded into a new file.  
Shared-Memory Computing
Modern computers are essentially all multicore.  All cores share the same
main memory.  In most multicore systems, all cores have equal access to main
memory; hence these are called symmetric multiprocessing (SMP) systems. 
They may not share the same cache, however.  Some hardware utilizes a variant
called NUMA (non-uniform memory access), particularly servers with more than
one CPU socket.  Graphical processing units (GPUs) are essentially shared-memory processors as well.  Shared-memory parallel programs rely on internal
communication and coordination mechanisms, rather than external networks.  
{{< figure src=""/courses/parallel-computing-introduction/img/SMP.png"" caption=""Schematic of an SMP system."" >}}
Distributed-Memory Computing
Large programs must generally be run on a cluster consisting of multiple
compute nodes.  Each node is an independent computer connected to the others
through an external network.  Because they are independent, one node has no
direct access to the memory of any other node.  Data must be transferred over
the network from one node to another.  This is accomplished by a library 
that manages the communication, invoked through an interface to the 
program. 
{{< figure src=""/courses/parallel-computing-introduction/img/DMP.png"" caption=""Schematic of a DMP system."" >}}
Hybrid Parallelism
Hybrid parallel programming is a combination of shared and distributed memory programming.  Tasks and possibly some of the data are divided among the distributed executables that will run.  Each executable also uses a threading library
to split the work among the cores available on the node where it is running.
Most commonly, this involves a combination of MPI for larger-scale data
distribution and OpenMP at a smaller scale, often at the level of loops over
data arrays.
GPU Parallelism
GPUs are essentially SMP processors with a large number, hundreds or even thousands, of lightweight ""cores""; each core has a limited set of instructions it can carry out.  The GPU has its own onboard RAM memory but relatively little cache.  Efficient GPU programming keeps the cores busy with computations and reduces the number of times they must fetch data from or store data to their main memory.  General-purpose GPUs are functionally ""coprocessors"" and data must be moved to and from the host to the GPU.  This communication can be quite expensive
and should also be minimized for efficient utilization of the system."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_example_applications.md,"One of the most common categories of application that frequently uses parallel computing is the numerical solution of partial differential equations. A partial differential equation, or PDE, is an equation containing derivatives of a function of two or more variables.  This is in contrast to ordinary differential equations, which are equations containing derivatives of a function of one variable.
Some examples of phenomena that are modeled by PDEs include:
* Air flow over an aircraft wing
* Blood circulation in the human body
* Water circulation in an ocean
* Bridge deformations as its carries traffic
* Evolution of a thunderstorm
* Oscillations of a skyscraper hit by an earthquake
* Strength of a toy
Partial differential equations are solved numerically by several approaches.  Two particularly common ones are the finite element method (FEM) and the finite difference method. A simple conceptual explanation of the difference between these two methods is that finite elements approximates the solution and attempts to fit it to the conditions, whereas the finite-difference method approximates the equations and solves a discrete version of the original system.
We will focus on examples that use finite differencing, since this method is widely used in some areas and is a typical application for point-to-point messaging because it generally involves solving the equations on some form of grid, as we have been studying.
Linear Second-Order PDEs
Many physical phenomena can be modeled by linear second-order PDEs. Consider such an equation in the two variables $x$ and $y$.  The general form is
$$A \dfrac{\partial^{2}{u}}{\partial{x}^{2}} + 2B \dfrac{\partial^{2}{u}}{\partial{x}\partial{y}}+C\dfrac{\partial^{2}{u}}{\partial{y}^{2}} + D \dfrac{\partial{u}}{\partial{x}}+E\dfrac{\partial{u}}{\partial{y}}+Fu=G$$
where A, B, C, D E, F, and G are functions of $x$ and $y$ only. The behavior, and appropriate numerical methods, depends only on $B^2-AC$. There are three possibilities:
Elliptic Equations
In this case, $B^2-AC \lt 0$. This type of equation is often solved by some iterative method.  Elliptic equations occur widely in physical models.
Parabolic Equations
Here $B^2-AC = 0$. The most widely known example of a parabolic equation is the diffusion equation that describes the diffusion of heat or fluids across a gradient.
Hyperbolic Equations
For this category, $B^2-AC \gt 0$.  Hyperbolic equations generally represent wave phenomena."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_nonblocking_exchange.md,"Our first application of nonblocking point-to=point communications is to the famiamilar halo exchange problem.  We can modify the solutions to the Exercise, 
We can easily convert the Sendrecv invocations to two Irecv and two Isend calls.  With nonblocking sends and receives, we do not have to carefully manage the ""sweeps"" up and down or right to left; the MPI libary will handle that.  This avoids the serialization of the blocking exchanges; this was the main purpose for the introduction of nonblocking communications.  
As a general rule, the calls to Irecv should be posted first, followed by the Isend.  In addition, in many cases such as this, it is preferable to use Waitall so that the MPI library can handle the ordering of the completions.
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_nonblock_halo.cxx"" lang=c++ >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_nonblock_halo.f90"" lang=fortran >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi_nonblock_halo.py"" lang=python >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_halo_exchange.md,"In our example, suppose that we have a two-dimensional array A that must be computed over the entire (global) grid.  Each subgrid will compute a portion of A, but the boundaries must be exchanged. We match each ""ghost"" zone to a zone on the edge of the grid managed by the neighbor processes.
Our array will be 10x16 globally, and 10x4 on each of 4 ranks. Let us extend each grid so that the array indices are numbered 0 to 11 and 0 to 17.  (Note to Fortran programmers: C++ and Python always count from 0 whereas the default for Fortran is to start from 1, but Fortran can set the lower bound to any integer less than the upper bound.)  This numbering scheme isn't necessary and is not always possible, but it may make the example clearer and simplifies the coding.
More generally, consider a grid of size (0,M+1) by (0,N+1). We will consider the boundaries to be in the 0-indexed rows and columns, and the M+1 or N+1 rows or columns. (Corners may need to be set specially, depending on the problem, but we'll ignore that for now.)  
When we split the array into four parts we now have grids of size (0,11) by (0,5).  Only the columns need to exchange data, since the values of the tops and bottoms are set by the model we are computing.
{{< figure src=""/courses/parallel-computing-introduction/img/halo_exchange.png"" caption=""The halos overlap the edges of their neighbors."" >}}
Rank 0 will send its ""real"" values in Column 4 to the ""ghost"" values on Rank 1 in Column 0.  Rank 1 sends its computed value in Column 1 to Column 5 in Rank 0, and receives the value of Rank 2, Column 0 into its Column 4. Similarly for Ranks 2 and 3.  Note that the boundaries of Column 0, Rank 0 and Column 5, Rank 3, as well as Rows 0 and 11 on all ranks, are set by the conditions of the model and are not indicated in the diagram."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_build_run.md,"We are now ready to write our first MPI program.  
A Hello World example.
Download the appropriate code below for your choice of language.
C++
Each MPI program must include the mpi.h header file. If the MPI distribution was installed correctly, the mpicc or mpicxx or equivalent wrapper will know the appropriate path for the header and will also link to the correct library.
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi1.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
Fortran
All new Fortran programs should use the mpi module provided by the MPI software. if the MPI distribution was installed correctly, the mpif90 or equivalent will find the module and link to the correct library.
Any recent MPI will also provide an mpi_f08 module.  Its use is recommended, but we will wait till later to introduce it. This new module takes better advance of modern Fortran features such as types. In addition, the ubuiquitous ""ierror"" parameter at the end of most argument lists becomes an optional argument in the mpi_f08 subroutine definitions.  The compiler used must support at least the Fortran 2008 standard.
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi1.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Python
The mpi4py package consists of several objects.  Many codes will need only to import the MPI object.
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/mpi1.py"" lang=""python"" >}}
{{< /spoiler >}}
Build It
If using an HPC system log in to the appropriate frontend, such as login.hpc.virginia.edu.  If the system uses a software module system, run
bash
module load gcc openmpi
For Python add
bash
module load <python distribution>
This will also load the correct MPI libraries. You must have already installed mpi4py.  Activate the conda environment if appropriate.
Use mpiexec and –np only on the frontends!  Use for short tests only!
Compiling C 
bash
mpicc –o mpihello mpi1.c
Compiling C++
bash
mpicxx –o mpihello mpi1.cxx
Compiling Fortran
bash
mpif90 –o mpihello mpi1.f90
Execute it
C/C++/Fortran
bash
mpiexec –np 4 ./mpihello
Python
python
mpiexec –np 4 python mpi1.py
Submit It
For HPC users, rite a Slurm script to run your program.  Request 1 node and 10 cores on the standard partition.  The process manager will know how many cores were requested from Slurm.
bash
srun ./mpihello
Or
bash
srun python mpihello\.py
Using the Intel Compilers and MPI
Intel compilers, MPI, and math libraries (the MKL) are widely used for high-performance applications such as MPI codes, especially on Intel-architecture systems.  The appropriate MPI wrapper compilers are
```bash
C
mpiicc -o mpihello mpi1.c
C++
mpiicpc -o mpihello mpi1.cxx
Fortran
mpiifort -o mpihello mpi1.f90
```
Do not use mpicc, mpicxx, or mpif90 with Intel compilers.  Those are provided by Intel but use the gcc suite and can result in conflicts."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_memory_programming.md,"Programming in the distributed-memory model requires some low-level management of data distribution and communication.  The model is  nodes (computing systems) connected by an  interconnection network.  Each node consist of processors, memory, and network.  Some form of disk storage is usually available, though it is not always local to the node but could be shared among all the nodes.
{{< diagram >}}
flowchart LR
   subgraph Users
       A(User) --> F((Internet))
       B(User) --> F
       C(User) --> F
    end
    subgraph Cluster
       F --> G(Frontend)
       G --> H{{Interconnection Network}}
       H --> K(Node)
       H --> L(Node)
       H --> M(Node)
       H --> N(Node)
       H --> S[(Storage)]
    end
{{< /diagram >}}
In distributed-memory programming, each node runs some number of independent and identical copies of the executable. This means that if the program runs on $N$ nodes, with $M$ copies per node, the total number of processes is $N \times M$.
Since these processes do not have the ability to access any of another team member's physical memory, they must communicate using messages.  A message consists of a stream of bytes, with some form of address format to be sure the message is received at the right target at the right time. 
Communication by messages means that a byte stream is transmitted from Process $M_i$, the sender, to  Process $M_j$, the receiver. The message ""address"" could be formed from information including
- IP address and port pairs
- Logical task number
- Process ID
The software that implements message passing is usually implemented as a library.  Programmers use the application programming interface (API) to manage interprocess communication.  Most libraries will use internal networks among processes running on the same node, but must use the external interconnection network for internode communications.  Therefore, network bandwidth and especially latency are important for distributed parallel computing.
Partitioning
Partitioning refers to dividing computation and data into pieces or chunks. There are basically two ways to split up the workload.
Domain decomposition
Domain decomposition divides the data into chunks.  Typically these data are represent in some array-like form, though other data structures are possible.  The portion of the data known only to a particular process is often said to be local to that process.  Data that is known to all processes is global.  The programmer must then determine how to associate computations with the local data for each process.
Functional decomposition
In functional or task decomposition, the tasks performed by the computation is divided among the processes.
Partitioning Checklist
In distributed-memory programming, it is particularly important to maximize the computation to communication ratio, due to communication overhead.  The programmer should also minimize redundant computations and redundant data storage. As an example of avoiding redundant data, generally a program should not simply declare a large number of global arrays and then have each process determine which subset to use.  This usually wastes memory and may also make the program infeasible, since each process will consume a large amount of memory.  The programmer must determine how to distribute the data to each local array.
In addition, the quantity of work on each process should be roughly the same size (load balancing).  The number of tasks is generally an increasing function of problem size so tasks and/or their data should be divided equally among the processes.
Communication
The programmer must determine the values to be passed among the tasks.  Only the necessary data should be communicated.  The type of communication may be local, in which the task needs values from a small number of other processes, or global,  where a significant number of processes contribute data to perform a computation
The goal is to balance communication operations among tasks, and to keep communications as local as possible.
Agglomeration
We prefer to group tasks into larger tasks.  Here the goals are to improve performance, maintain scalability of the program, and simplify programming.
Due to overhead, it is better to send fewer, larger messages than more, smaller messages.  In MPI programming, in particular, the goal is often to create one agglomerated task per processor.
Mapping
Mapping is the process of assigning tasks to processes or threads. For threading (SMP), the mapping done by operating system. In a distributed memory system, the user chooses how many processes, how many nodes, and how many cores per node.  These choices can affect performance.
Mapping has the often conflicting goals of maximizing processor utilization and minimizing interprocess communication.  Optimal mapping is probably unsolvable in general, so the programmer must use heuristics and approximations.  Frequently, determining the best mapping requires experimentation (scaling studies)."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_heated_plate.md,"Suppose we have a very flat rectangular metal plate. We immerse one edge in an ice bath at 0C and the other three edges in steam at 100C.  The metal is a good conductor of heat.  Heat loss from the surfaces will be ignored.
Intuitively we believe that heat will diffuse through the plate, but eventually a steady state will be reached.  Mathematically, this is modeled by the heat diffusion equation, which is a time-dependent parabolic PDE. But with no change to the temperatures at the edges, eventually the transient flow of heat will settle into a steady state that depends on the temperature at the boundaries. We wish to predict the resulting temperature distribution.
We will skip to the equation that describes this steady state.  It is the Laplace equation
$$ \nabla^{2}{u} = 0 $$
Writing out the Laplacian operator $\nabla^2$ in two dimensions, we have
$$ \frac{\partial^{2}{u}}{\partial{x}^{2}} + \frac{\partial^{2}{u}}{\partial{y}^{2}} = 0 $$
Here $u$ represents the temperature.
To solve this equation numerically, we will first discretize $x$, $y$, and $u$ onto a two-dimensional grid such as we used to study halo exchanges. Grids are typically represented by some type of array so we will denote the elements by indices $i$ and $j$. A point on the plate and its temperature will be represented in C++ as
c++
x[i],y[i],u[i][j]
In Fortran as
fortran
x(i),y(i),u(i,j)
and in Python as
python
x[i],y[i],u[i,j]
The Laplace equation is an elliptical PDE.  The solution to elliptical PDEs is determined by the boundary conditions and the forcing function, if one is present (it is zero for the Laplace equation).  We expect the solution to look something like this.
{{< figure src=""/courses/parallel-computing-introduction/img/heated_plate_steady_state.png"" caption=""Steady state of a heated plate with time-independent boundary conditions."" >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_nearest_neighbor.md,"One of the most common patterns in MPI programming is nearest-neighbor exchanges.  In this case, a process sends to its neighbor and receives from its neighbor. The definition of ""neighbor"" may depend on the topology of the layout of ranks.  For a simple one-dimensional organization the neighbor to the left is rank- and the neighbor to the right is rank+1. 
{{< figure src=""/courses/parallel-computing-introduction/img/nearest_neighbor.png"" caption=""Schematic of nearest-neighbor exchange by rank"" >}}
Any blocking point-to-point communications can potentially deadlock, but we must be especially careful with nearest-neighbor communications.  Each process must be in an appropriate state when a message is sent to it.  How do we accomplish this?
As an example, let us consider an exchange in which each process sends its rank to its neighbor and receives the neighbor's rank.  Even-number processes send to their right and odd-numbered processes send to their left.
Deadlock
We might first think all processes should be in the Receive state, then Send the message.  
Examples
{{< spoiler text=C++ >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_deadlock.cxx"" lang=c++ >}}
{{< /spoiler >}}
{{< spoiler text=Fortran >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_deadlock.f90"" lang=fortran >}}
{{< /spoiler >}}
{{< spoiler text=Python >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_deadlock.py"" lang=python >}}
{{< /spoiler >}}
However, this pattern is guaranteed to deadlock because the MPI_Recv is blocking.  It will wait indefinitely for a message, so the process never has a chance to send anything.
Unsafe
Perhaps we can swap the MPI_Send and MPI_Recv. The Send will pack up the message into the buffer and return.  The process then proceeds to Receive, which accepts the message.
{{< spoiler text=C++ >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_unsafe.cxx"" lang=c++ >}}
{{< /spoiler >}}
{{< spoiler text=Fortran >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_unsafe.f90"" lang=fortran >}}
{{< /spoiler >}}
{{< spoiler text=Python >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_unsafe.py"" lang=python >}}
{{< /spoiler >}}
This pattern is unsafe because the buffer may not be able to contain all the messages to be sent. On most modern systems, however, it will very often work, especially with short messages. For that reason it is a good idea to use built-in MPI tools as much as possible, such as MPI_Sendrecv.
Safe
For a safe exchange, we split the ranks by even and odd.  One set sends first, while the other is waiting to receive; the reverse is true for the other set.  Note that we have restricted the example to run only on an even number of ranks to avoid the need for special handling of the rightmost process. In general we would have to add conditionals for that case, but we are keeping the example as simple as possible.
{{< spoiler text=C++ >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_safe.cxx"" lang=c++ >}}
{{< /spoiler >}}
{{< spoiler text=Fortran >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_safe.f90"" lang=fortran >}}
{{< /spoiler >}}
{{< spoiler text=Python >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/exchange_safe.py"" lang=python >}}
{{< /spoiler >}}
Exercise
Download the three codes for your language and try them.  The first example will have to be canceled manually."
rc-learning-fork/content/courses/parallel-computing-introduction/parallel_basics.md,"Most computer programs are serial (also called sequential).  One operation at a time is executed, one after the other in the sequence specified by the programmer.  The time for the execution of such a program is fixed for a given problem size.
Serial programs have at least two disadvantages.  First is the time to solution.
There is no opportunity to obtain a result more quickly.  Second may be a limit on the problem size that can be attacked.  A serial program and all its data must fit together in the memory assigned to it by the operating system.  Parallel programming seeks to eliminate one or both of these issues.
Parallel Computing refers to executing more than one process at a time to solve the same problem.
Processes may or may not need to communicate with one another while they are running.
The degree of communication required, the type of hardware available, and the organization of the code determines the type of parallelization chosen.  Parallel computing requires at minimum multiple cores on a computer, and may involve
multiple computers; in most cases it is carried out on large high-performance computing systems.
Why We Need Parallel Computing
Programming in parallel is more difficult than serial coding.  What makes it worth the effort?  To understand this, we must understand a few things about how computers work.
Roughly speaking, a conventional computer consists of a central processing unit (CPU) and memory for temporary working storage.  Usually there is also some form of permanent storage such as a disk drive.  Newer computers may utilize advanced permanent memory for both temporary and permanent storage, but the basic model is still accurate.  Working storage is hierarchical, with speed and price decreasing the farther from the CPU.  A very small amount of very fast memory called registers is part of the CPU.  Next is cache, usually in two to three levels, which is close to the CPU.  Main memory is usually RAM (random access memory); it is volatile, meaning that its contents disappear when power is removed, it is much larger than cache, and it is also much slower.  The CPU accesses RAM through an internal communication network.  The CPU also contains an input/output controller to manage devices such as hard drives.
RAM memory is many times slower than cache, up to 100 times, and hard drives are slower still, up to 10,000 times slower.  
The activities of the CPU's electronics are synchronized by a system clock; each instruction carried out by the CPU requires a certain number of clock cycles.  For example, some recent computer chips can do floating-point addition or multiplication in one or two clock cycles, whereas a division could take 20 or more cycles.  Since the speed of light ultimately limits signals, higher clock
speeds require denser electronic components.
For decades, improvements in computing performance were a result of ever-increasing clock speeds.  This followed two ""laws"" (both of which were actually observational):  Moore's law, which states that the density of the electronic components of the CPU would double every 18 to 24 months; and Dennard scaling, which posited that even as the components shrank, their power consumption per unit area remained constant.  
{{< figure src=""/courses/parallel-computing-introduction/img/Moores_Law.jpg"" caption=""Copyright 2015 A. Ostendorf and K. König, published by De Gruyter.  Licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License."" height=720 width=563 >}}
Moore's law is currently still approximately valid, though its end is in sight; but Dennard scaling broke down around 2005.  Further significant increases in clock speed would make it impossible to cool the chips adequately.  Manufacturers responded by keeping clock speeds nearly constant, but dividing their still-denser CPUs into separate execution units, usually called cores.  Because of this history there is still some inconsistency over what is called a ""CPU"" and what is called a ""core.""  Generally, cores are subunits of a CPU, but sometimes, especially in contexts such as the Slurm resource manager, a core is called a CPU.  Programmers should try not to become confused by this.
{{< figure src=""/courses/parallel-computing-introduction/img/Dennard_Scaling.png"" >}}
Most consumer computers have only one CPU, but many high-end servers have multiple CPUs, with each CPU representing several cores.  The CPU is physically plugged into a socket on the motherboard, so these CPU units may be referred to as ""sockets.""  Each CPU will have
its own pathway to memory, disk, and so forth.  
Computers can be connected to one another through some kind of network.  The best-known network is Ethernet. Networking allows multiple computers to be connected in physical proximity to one another, forming a cluster.  Each member computer in the cluster is usually
called a node.  Ethernet can be used for the interconnect, but Ethernet is a relatively slow network.  The time required for a message to be sent from one computer to another is the latency, whereas the amount of data that can be sent per unit time is the bandwidth.  Ethernet generally has fairly good bandwidth but quite high latency.  Since fast communication between nodes is important in a
high-performance cluster, interconnects with much lower latency and higher bandwidth are generally provided for these systems, usually along with Ethernet for routine communication.  The most widely used of these fast networks is InfiniBand, now owned by NVIDIA.  
{{< figure src=""/courses/parallel-computing-introduction/img/Frontera.jpg"" height=720 width=563 caption=""A portion of a large computing cluster."" >}}
Adapting to a Parallel Environment
Most programmers are taught only serial coding and are accustomed to laying out programs in a sequential manner.  Parallel computing brings new considerations that must be mastered.
Types of Parallelism
For both shared and distributed memory programming, we must determine how we will spread the work among different processes.  There are two main ways to do this.


Data parallelism
In this case, we divide the data into subparts.  Each process works on its assigned part individually, then if necessary the results are collected and the program goes to next phase.
Independent tasks apply the same operation to different elements of a data set.


Task parallelism
In task parallelism the processes perform multiple tasks at the same time on the same data. Independent tasks apply different operations to different data elements.


Example
The landscape service has several tasks to perform.   


Turn off the security system to access the client's garage.


Mow the lawn.


Edge the lawn.


Weed the garden.


Turn on water to the sprinklers, check the timer attached to the hose.


Turn the security system back on.


We can draw a graph to help us understand the sequence of events.
{{< diagram >}}
graph TD;
A(Turn off Security System) --> B(Edge Lawn)
A(Turn off Security System) --> C(Mow Lawn)
A(Turn off Security System) --> D(Weed Garden)
B(Edge Lawn) --> E(Set Sprinklers)
C(Mow Lawn) --> E(Set Sprinklers)
D(Weed Garden) --> E(Set Sprinklers)
E(Set Sprinklers) --> F(Turn on Security System)
{{< /diagram >}}
What can be done in parallel and what must be serial?
What portion is task parallelism?  Is there an opportunity for data parallelism? Assume sufficient staff and equipment are available."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_higher_dims.md,"We chose to divide our computational grid into rows for C++ and Python and columns for Fortran because of the memory layouts used by the three languages. But it could be that a one-dimensional decomposition would not be sufficient to allow us to complete the solution in a reasonable amount of time or memory utilization, and we will need a two-dimensional decomposition.
{{< figure src=""/courses/parallel-computing-introduction/img/two-d_decomp.png"" caption=""Two-dimensional halo decomposition. Physical boundaries are not shown."" >}}
Since columns in C++ and Python and rows in Fortran are not contiguous in memory, in order to create an MPI buffer we must skip through the elements and copy the appropriate ones into the buffer. 
C++
c++
  int nrows=nrl+2
  int ncols=ncl+2
  double *buf=new double[nrows];
  for (i = 0; i < nrows; i++ ) {
      [buf[i]=u[i%ncols][0];
  }
Python
python
nrows=nrl+2
ncols=ncl+2
buf=np.zeros(nrows)
for i in range(nrows):
    buf[i]=u[i%ncols,0]
Fortran
fortran
!declare buff allocatable(:)
    nrows=nrl+2
    ncols=ncl+2
    allocate(buf(0:ncols-1)
    do i=0,ncols-1
        buff(i)=u(0,mod(i,nrows))
    enddo"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_domain_decomp.md,"Parallel programs must break down something to distribute among the different processes or threads.  In our previous discussion we talked generally about task parallelism and data parallelism, but we have seen few concrete examples applied to programming.  One of the most common parallelisms is a form of data decomposition called domain decomposition.  This is typically used for data that can be represented on some form of grid.  We break the grid down into subgrids, and assign each subgrid to a process rank.  There is no hard and fast rule for numbering the subgrids and usually multiple options are possible.
For a concrete example, consider a two-dimension grid of points, with a function defined at each point.  Consider a simple case of four processes. Two obvious possible rank assignments are left-to-right or cyclic.
{{< figure src=""/courses/parallel-computing-introduction/img/core_numbering.png"" caption=""Possible rank assignment on a two-dimensional grid."" >}}
How to assign the numbers to the subdomains may depend on the problem. MPI provides some utilities that may help with this, but using them requires creating a new communicator group, which we have not discussed, so for now we will do our ordering manually.
Let us consider the ""brute force"" maximum problem of Project Set 1.  In that example, we randomly evaluated a function over a two-dimensional set of $x$ and $y$ values and used gathers to find an approximation to the value and location of the maximum.  Another approach is to divide up the grid and perform our random searches independently over each subgrid, then collect results from each subdomain and compare them to find the maximum and its location.  
A little thought and experimentation suggests that the simplest way to map ranks to subdomains is to divide the overall grid into $np$ regions, where $np$ is the number of processes, then label each domain by row and column number, starting the counts from $0$.  The number of processes must be a multiple of two integers.  In our example we will assume we will use a perfect square, so our grid is $\sqrt{np}$ on each side.  This is not required, but if a more general breakdown is desired, the program must be provided the number of rows and columns and the number of processes should be confirmed to be $n_{rows} \times n_{cols}$. Each process rank then computes its location in the grid according to
python
   col_num=rank%ncols
   row_num=rank//nrows
using Python syntax.  The local column number is the remainder of the rank divided by the number of columns, and the local row number is the integer division of the rank by the number of rows.  This layout corresponds to a rank assigment such as the following, for nine processes:
{{< figure src=""/courses/parallel-computing-introduction/img/domain_decomposition.png"" caption=""Domain decomposition for a typical two-dimensional distribution by rank."" >}}
With this decomposition, each rank evaluates the same number of random points, so this is an example of weak scaling.
Exercise
If you have not worked the example from Project Set 1, download the sample serial codes referenced there.
Implement the domain decomposition method to solve this problem.  The basic method does not take too many lines of code.  The example solutions also check that the number of processors is a perfect square, but you may assume that for a first pass.  For correct statistical properties you should also attempt to use different random seeds on different ranks.  Be sure not to end up with a seed of $0$ on rank $0$.
Hint: From $xlo$, $xhi$, and $ncols$, compute the increment for each block in $x$ values.  Repeat for $y$ values using $ylo$, $yhi$, and $nrows$.  Compute the location in rows and columns for each rank 
python
col_num=rank%ncols
row_num=rank//nrows
You can now compute the starting and ending $x$ and $y$ values for each rank.
Example Solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpifind_max_dd.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpifind_max_dd.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpifind_max_dd.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_reductions.md,"In a reduction, data are sent to a root process, which performs a specified binary operation on sequential pairs of data.
Ignoring details of implementation, which as described would be very inefficient, consider this example.  We want to compute the sum of all the local values of some variable over all ranks in the communicator group.  The root process, assumed to be rank 0 for our case, has a value in its buffer.  It receives another value from rank 1, adds it to the 0 value, and stores the result into a ""global"" buffer.  Next it receives the value from rank 2.  It adds this value to the contents of the global buffer.  The value from rank 3 is then received and added to the global buffer variable.  This continues until all ranks have sent their value to the root process.
The standard MPI Datatypes that we have already seen are used for the operations that return a single type of value.  If the buffer is an array, MPI_Reduce will perform the operation elementwise.
The built-in operations are the same for all languages.
{{< table >}}
|  operation  |  MPI_Op |
|-------------|---------|
|  bit-wise and  |  MPI_BAND |
|  bit-wise or  |  MPI_BOR |
|  bit-wise exclusive or  |  MPI_BXOR |
|  logical and  |  MPI_LAND |
|  logical or  |  MPI_LOR |
|  logical exclusive or  |  MPI_LXOR |
|  sum      |  MPI_SUM |
|  product  |  MPI_PROD |
|  maximum  |  MPI_MAX |
|  minimum  |  MPI_MIN |
|  maximum and its index   |  MPI_MAXLOC |
|  minimum and its index   |  MPI_MINLOC |
{{< /table >}}
The operations in Python mpi4py are the same, but with a period rather than an underscore, e.g. MPI.SUM.
MPI_MAXLOC and MPI_MINLOC require some special handling. They return both the value and the rank into the global variable.  The MPI_Datatypes for these two operations are particular to the language.  For more details and examples see the documentation at the MPI Forum.
C/C++
c
int MPI_Reduce(void *operand, void *result, int count, MPI_Datatype type, MPI_Op operator, int root, MPI_Comm comm);
{{< spoiler text=""C++ Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/reduce.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
The special predefined types used for MPI_MAXLOC and MPI_MINLOC in C/C++ assume a struct has been defined, with the first member's type as indicated and the second an int.
{{< table >}}
|  Types (value, index)  |  MPI_Datatype |
|-------------|---------|
| float, int  |  MPI_FLOAT_INT |
| double, int  |  MPI_DOUBLE_INT |
| long, int  |  MPI_LONG_INT |
| int, int  |  MPI_2INT |
| short, int  |  MPI_SHORT_INT |
| long double, int  |  MPI_LONG_DOUBLE_INT |
{{< /table >}}
Fortran
fortran
MPI_REDUCE(sendbuf, recvbuf, count, datatype, op, root, comm, ierr)
{{< spoiler text=""Fortran Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/reduce.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< table >}}
For MPI_MAXLOC and MPI_MINLOC, Fortran derived types are not accommodated at this time, so an array with two elements of the same type must be used. The index (the second element) can be coerced to an integer if necessary.
|  Types (value, index)  |  MPI_Datatype |
|-------------|---------|
| real  |  MPI_2REAL |
| double precision  |  MPI_2DOUBLE_PRECISION |
| integer  |  MPI_2INTEGER |
{{< /table >}}
Python
Note that the lower-case ‘reduce’ handles pickled objects; use the title-case ‘Reduce’ with NumPy arrays.
python
comm.Reduce(sendarr, recvarr, operation, root=0)
{{< spoiler text=""Python Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/reduce.py"" lang=""python"" >}}
{{< /spoiler >}}
The special datatypes for MAXLOC and MINLOC in mpi4py are the same as for C, but with the underscore replaced by a period as usual (MPI.FLOAT_INT).
Exercise
Return to the random-walk program with MPI.  Add an appropriate reduction and print only the overall result.  
{{< spoiler text=""Explanation"" >}}
The purpose of parallelizing the random-walk code was to run a large number of trials in order to obtain a more accurate estimate of the final distance.  Without the reduction, we would have collected the results and averaged them.  We can use the reduction to accomplish this automatically.
{{< /spoiler >}}
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpirandom_walk_red.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpirandom_walk_red.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpirandom_walk_red.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_project_set1.md,"Project 1
A very inefficient way to compute $\pi$ is to use a Monte Carlo method. In this we randomly throw “darts” at a circle of radius 1 within a square with sides of length 2. The ratio of the areas is thus $\pi \div 4$. If we compute the ratio of “hits” that land within the circle to the total number of throws, then our estimate of $\pi$ is $4 \times \mathrm{ratio}$. 
Write a program that computes pi by this method. Start with a serial code and test it for a relatively small number of “dart throws.” You will find it very inaccurate for a small number of throws but should be able to obtain an estimate at least close to 3.  Once you are satisfied that your program works in serial, parallelize it.  Distribute the throws over N processes and use a reduction to get the totals for your overall estimate of pi.  Test your parallel code with 8 processes and 1e9 throws.  Hint: C++ programmers should use long long for the number of throws and any corresponding loop counter.  Fortran programmers should use an 8-byte integer.  (See the C++ and Fortran random-walk examples.)  Python automatically handles large integers.  
Note that the formula is the same if one considers only the upper right quadrant of the figure.
{{< figure src=""/courses/parallel-computing-introduction/img/MCPi.png"" caption=""Monte Carlo computation of pi."" >}}
Fortran programmers: you may wish to extract the random module from mpirandomwalk.f90 into a separate file random.f90, since it is used frequently.  Remember that modules must be compiled before any program units that USE them.
Try to write the serial code before peeking at the examples.
Serial Codes
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/montecarlo_pi.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/montecarlo_pi.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/montecarlo_pi.py"" lang=""python"" >}}
{{< /spoiler >}}
Example Solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpimontecarlo_pi.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpimontecarlo_pi.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpimontecarlo_pi.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 2
The trapezoid rule is a simple method for numerical quadrature (integration).  Given a function $f(x)$, the integral from $x=a$ to $x=b$ is 
$$ I = \int^b_a f(x)dx $$
We can approximate the area under the curve as a trapezoid with area
$$ I_{trap} = \frac{1}{2}(b-a)(f(b)+f(a) $$
However, a single trapezoid isn't a very good approximation.  We can improve our result by dividing the area under the curve into many trapezoids:
$$ I_{trap} = \sum^n_{i=1} \frac{f(x_{i+1})+f(x_i)}{2} \Delta x_i $$
where
$$ \Delta x_i = x_{i+1}-x_i $$
The larger the number of trapezoids, the more accurate our approximation to the integral will be.  Typically $\Delta x_i$ is a constant usually represented by $h$, but that is not a requirement.
{{< figure src=""/courses/parallel-computing-introduction/img/trapezoid_rule.png"" caption=""Illustration of the trapezoid rule."" >}}
It should be clear that the trapezoid rule is very easy to parallelize.  The interval on the independent variable should be split up among the ranks.  Each rank then carries out the summation over its part.  The complete integral is the sume of all ranks' results.
Starting from the serial code in your language of choice, parallelize the trapezoid rule.  Only rank 0 should read the input data; then it should broadcast appropriately to the other processes.   Test for four processes.  Note that the integral of the sine from 0 to $\pi$ has an exact value of 2, making it a good test case.
C++ and Fortran programmers: if you are not familiar with passing subprogram names as dummy variables, refer to our courses for Fortran or C++.
Serial Codes
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/trap.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/trap.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/trap.py"" lang=""python"" >}}
{{< /spoiler >}}
Example Solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpitrap.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpitrap.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpitrap.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 3
We wish to find the maximum of a 3-d surface described by a relatively complicated function.  We can use a ""brute force"" method, which evaluates a large number of $x$ and $y$ values randomly distributed over the independent variables.  This method of optimization is easily parallelized and can often be faster than older methods even when those are parallelized.
The surface is defined by the following rules:
$$ \mu_1 = \sqrt{2}\ ,\ \mu_2 = \sqrt{\pi}\ ,\ \sigma_1 = 3.1\ ,\ \sigma_2 = 1.4 $$
$$ z_1 = 0.1 \sin(x) \sin(y) $$
$$ a = \frac{(x - \mu_1)^2}{2\sigma_1^2}\ ,\  b = \frac{(y - \mu_2)^2}{2\sigma_2^2} $$
$$ z_2 = \frac{e^{(a+b)}}{\sigma_1 \sigma_2 \sqrt{2\pi}} $$
$$ z = z_1 + z_2 $$
We consider the ranges
$$ -10\pi \le x \le 10\pi\ ,\ -10\pi \le y \le 10\pi $$
Generate a list of N random values for each of x and y over the above range. For testing you can use N=100000 for compiled languages and N=10000 for Python. Be sure to measure the time. Use appropriate MPI gathers to obtain not just the maximum value of $z$ from each process, but the corresponding $x$ and $y$ values.  Remember that gather collects items in strict rank order. Use one or more built-ins for your language (maxloc for Fortran, np.argmax for Python) or loops (C/C++) to find the $x$, $y$, and $z$ for the maximum.
{{< figure src=""/courses/parallel-computing-introduction/img/find_max_function.png"" caption=""Find the maximum of this jagged function."" >}}
Serial Codes
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/find_max.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/find_max.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/find_max.py"" lang=""python"" >}}
{{< /spoiler >}}
Example Solutions
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpifind_max.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpifind_max.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpifind_max.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_global2.md,"In many-to-one collective communications, all processes in the communicator group send a message to root. The buffer is read from the senders and written on the root.
Gather
A gather is the inverse of a scatter.  Each process sends ncount items to root, which assembles them in rank order.  The buffer in the root process must be large enough to accommodate all the data.  As for a scatter, the MPI_Datatype is specified twice but must be the same each time. Note that ncount is the number of items sent per process, not the total.
{{< figure src=""/courses/parallel-computing-introduction/img/gather.png"" caption=""Gather"" >}}
C++
The prototype is
c++
int MPI_Gather(void *sendbuffer,int ncount,MPI_Datatype datatype,void *recvbuffer,int ncount,MPI_Datatype datatype,int root,MPI_Comm communicator)
{{< spoiler text=""C++ Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/gather.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
Fortran
fortran
<type><dimension><size>   :: vars
<type><dimension><size>   :: all_vars
integer  :: ncount, root, err
! more code
call MPI_Gather(vars, ncount, MPI_TYPE, all_vars, ncount, MPI_TYPE, root, MPI_COMM_WORLD, err)
{{< spoiler text=""Fortran Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/gather.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Python
For this syntax, both buffers should be NumPy arrays.
python
comm.Gather([data,MPI.TYPE],[all_data,MPI.TYPE], root=0)
{{< spoiler text=""Python Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/gather.py"" lang=""python"" >}}
{{< /spoiler >}}
Gatherv
MPI_Gatherv is the inverse of MPI_Scatterv.  Chunks of data are distributed into a global array at the root process.  For C++ and Fortran, sendcount is an integer, but recvcounts and displs are integer arrays.
C/C++
c
int MPI_Gatherv(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int *recvcounts, int *displs, MPI_Datatype recvtype, int root, MPI_Comm comm);
{{< spoiler text=""C++ Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/gatherv.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
Fortran
fortran
call MPI_GATHERV(sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm, ierr)
{{< spoiler text=""Fortran Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/gatherv.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Python
```python
sendcounts and displ are integer ndarrays
comm.Gatherv([sendbuf,[sendcounts,displ,MPI.TYPE],recvbuf)
```
As for comm.Scatterv, the MPI.TYPE is generally required.
{{< spoiler text=""Python Example"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/gatherv.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_timing.md,"We should always benchmark our parallel programs to make sure we are utilizing resources effectively, especially if we are running on multi-user systems that may require justifications, proposals, or charges for time.  MPI provides procedures to time all or portions of a program that can be used regardless of how many processes are invoked.  These routines are portable and are the same for all languages that support MPI.
Wtime and Wtick
MPI_Wtime returns a double-precision floating-point number representing the time in seconds that has elapsed since some particular time in the past.  To time parts of the execution, a starting time and an ending time must be specified and the difference obtained.
Since MPI processes run independently, it does not make sense to compare a start time on one process with an end time on another process.  Only differences on the same process are significant.  For an overall time we can often use rank 0.
MPI_Wtick returns the resolution, in seconds, for each ""tick"" of the timer.  This may be of interest for profiling.
The syntax of both procedures is very simple.
C++
c++
double startTime=MPI_Wtime();
double tick=MPI_Wtick();
Fortran
Unlike most MPI routines, Wtime and Wtick are functions in Fortran.
```fortran
double precision :: start_time, tick
start_time=MPI_Wtime()
tick=MPI_Wtick()
```
Python
python
start_time=MPI.Wtime()
tick=MPI_Wtick()
Scaling and Efficiency Studies
In a previous chapter we learned about scaling and efficiency.  Let us apply this to our random_walk program. 
Copy the code from that chapter in your language of choice for this exercise.  Add appropriate timing procedures. It is only necessary to print the result from rank 0. Run the timed random-walk code over 1, 2, 4, and 8 processes for some number of ""steps"" that will run at least several seconds. Use any plotting package you know to plot speedup versus number of processes/cores.  Add a curve for perfect speedup.  Plot the parallel efficiency versus core number.  (Recall that the theoretical maximum efficiency is 1.)
The random-walk program is ""embarrassingly parallel"" since there is as yet no communication among processes.  We did not have to use MPI but could have run multiple tests and collected the results.  There is some small overhead in using MPI.
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpirandom_walk_timed.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpirandom_walk_timed.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/mpirandom_walk_timed.py"" lang=""python"" >}}
{{< /spoiler >}}
Strong Scaling
In strong scaling, a fixed amount of work is distributed over a varying number of processes. The timed random-walk code is written to implement weak scaling; the number of steps provided is the number each process will compute, so we must adjust the number of steps to keep the amount of work constant over different numbers of processes. The example solution used the Python version with $10^{7}$ steps for the serial code.  That was then divided by 2, 4, and 8 for subsequent runs.
{{< spoiler text=""Strong scaling example"" >}}
{{< figure src=""/courses/parallel-computing-introduction/img/MPI_strong_scaling.png"" caption=""Graphs of speedup and efficiency for a strong-scaling example"" >}}
{{< /spoiler >}}
Weak Scaling
Recall that weak scaling increases the amount of work as the process number increases.  Ideally, the quantity of work per process is the same across all processes.  For weak scaling, we must compute the serial time for the equivalent sized problem, so the serial code must be run for $8 \times 10^{7}$, $4 \times 10^{7}$, and $2 \times 10^{7}$ as well as for $10^{7}$ steps.  Plot the scaling compared to perfect scaling of $p$.  Compute and plot the parallel efficiency for these runs.  Was it similar to strong scaling?  Why might this be the case for this particular example?
{{< spoiler text=""Weak scaling example"" >}}
{{< figure src=""/courses/parallel-computing-introduction/img/MPI_weak_scaling.png"" caption=""Graphs of speedup and efficiency for a weak-scaling example"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_setup.md,"Using MPI requires access to a computer with at least one node with multiple cores.  The Message Passing Interface is a standard and there are multiple implementations of it, so a choice of distribution must be made.  Popular implementations include MPICH, OpenMPI, MVAPICH2, and IntelMPI.  MPICH, OpenMPI, and MVAPICH2 must be built for a system, so a compiler must be chosen as well.  IntelMPI is typically used with the Intel compiler and is provided by the vendor as part of their HPC Toolkit.  MVAPICH2 is a version of MPICH that is specialized for high-speed Infiniband networks on high-performance clusters, so would generally not be appropriate for installation on individual computers.
On a Remote Cluster
Refer to the instructions from your site, for example UVA Research Computing for our local environment.  Nearly always, you will be required to prepare your code and run it through a resource manager such as Slurm.  Most HPC sites use a modules system, so generally you will need to load modules for an MPI version and usually the corresponding compiler.  It is important to be sure that you use a version of MPI that can communicate correctly with your resource manager.
bash
module load gcc
module load openmpi
is an example setup for compiled-language users. 
For Python, the mpi4py package is most widely available. It is generally preferable, and may be required, that mpi4py be installed from the conda-forge repository.  On a cluster, mpi4py will need to link to a locally-built version of MPI that can communicate with the resource manager.  The conda-forge maintainers provide instructions for this here. In our example, we will use openmpi.  First we must load the modules for the compiler and MPI version:
bash
module load gcc openmpi
We must not install OpenMPI directly from conda-forge; rather we make use of the ""hooks"" they have provided.
bash
module list openmpi
In our example, the module list returns
bash
Currently Loaded Modules Matching: openmpi
  1) openmpi/4.1.4
Now we check that our version of OpenMPI is available
bash
conda search -f openmpi -c conda-forge
Most versions are there, so we can install the one we need
bash
conda install -c conda-forge ""openmpi=4.1.4=external_*""
Be sure to include the external_* string.  
After this completes, we can install mpi4py
bash
conda install -c conda-forge mpi4py
On a Local Computer
If you have access to a multicore computer, you can run MPI programs on it.
If using a compiled language, before you can build MPI codes you must install a compiler, and possibly some kind of IDE.  See our guides for C++ or Fortran.
For Python, on all operating systems install mpi4py. To install mpi4py you must have a working mpicc compiler.  If you use conda or mamaba from a distribution like miniforge, the required compiler will be installed as a dependency.  For pip installations you must provide your own compiler setup. 
The author of mpi4py recommends using pip even with a conda environment. This command will be similar on a local system to that used for installation on a multiuser system. 
no-highlight
python -m pip install mpi4py
This may avoid some issues that occasionally arise in prebuilt mpi4py packages. Be sure that an appropriate mpicc executable is in the path.  Alternatively, use the conda-forge channel (recommended in general for most scientific software).  Most of the time, if you are installing mpi4py from conda-forge, you can simply install the package.  MPICH is the default when installed as a prerequisite for conda-forge.
Linux
Parallel programs are most frequently used in a high-performance computing environment such as the clusters we have discussed, but many multicore workstations are available that can run Linux and many parallel programmers are familiar with this environment.  The simplest way to install MPI is to use a precompiled version for your distribution and compiler.  
GCC
The recommended MPI distribution is OpenMPI. Most distributions provide a package.
Intel oneAPI
Installing the HPC Toolkit will also install IntelMPI.
NVIDIA HPC SDK
The NVIDIA software ships with a precompiled version of OpenMPI.
The headers and libraries for MPI must match.  Using a header from one MPI and libraries from another, or using headers from a version from one compiler and libraries from a different compiler, usually results in some difficult-to-interpret bugs.  Moreover, the process manager must be compatible with the MPI used to compile the code.  Because of this, if more than one compiler and especially more than one MPI version is installed, the use of modules (environment modules or lmod) becomes particularly beneficial.  Both Intel and NVIDIA provide scripts for the environment modules package (lmod can also read these), with possibly some setup required.  If you plan to use mpi4py as well as compiled-language versions, creating a module for your Python distribution would also be advisable. Installation of a module system on an individual Linux system is straightforward for an administrator with some experience.
Mac OS
GCC
Installing homebrew is the simplest way to set up MPI on a Mac.  Install the gcc package followed by the open-mpi package.
Intel oneAPI
Install the HPC Toolkit for IntelMPI.
NVIDIA HPC SDK
The NVIDIA suite is not available for Mac OS.
Windows
GCC
The easiest way to use OpenMPI on Windows is through Cygwin.  In this case, the gcc compiler suite would first be installed, with g++ and/or gfortran added.  Then the openmpi package could also be installed through the cygwin package manager.
Intel oneAPI
Install the HPC Toolkit. 
NVIDIA HPC_SDK
Download the package when it is available.
MPI codes must generally be compiled and run through a command line on Windows.  Cygwin users can find a variety of tutorials online, for example here. 
The Intel oneAPI Basic Toolkit includes a customized command prompt in its folder in the Apps menu."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_programs.md,"MPI stands for  M essage  P assing  I nterface.  It is a standard established by a committee of users and vendors.  
Programming Languages
MPI is written in C and ships with bindings for Fortran.  Bindings have been written for many other languages, including Python and R. C++ programmers should use the C functions.  All of the C functions work the same for Fortran, with a slightly different syntax.  They are mostly the same for Python but the most widely used set of Python bindings, mpi4py, was modeled on the deprecated C++ bindings, as they are more ""Pythonic.""
Guides to the most-commonly used MPI routines for the three languages this course supports can be downloaded.
C/C++ 
Fortran 
Python
Processes and Messages
To MPI, a process is a copy of your program's executable.  MPI programs are run under the control of an executor or process manager.  The process manager starts the requested number of processes on a specified list of hosts, assigns an identifier to each process, then starts the processes. 
The most important point to understand about MPI is that each process runs independently of all the others. Each process has its own global variables, stack, heap, and program counter.  Any communications are through the MPI library. If one process is to carry out some instructions differently from the others, conditional statements must be inserted into the program to identify the process and isolate those instructions.
Processes send and receive messages from one another. A message is a stream of bytes containing the values of variables that one process needs to pass to or retrieve from another one. 
Usually when MPI is run the number of processes is determined and fixed for the lifetime of the program.  The MPI-3 standard can spawn new processes, but in a resource managed environment such as a high-performance cluster, the total number must still be requested in advance.
MPI distributions ship with a process manager called  mpiexec  or  mpirun. In some environments, such as many using Slurm, we use the Slurm process manager  srun.
When run outside a resource-managed system, we must specify the number of processes through a command-line option.  If more than one host is to be used, the name of a hostlist file must be provided, or only the local host will be utilized.  The options may vary depending on the distribution of MPI but will be similar to that below:
mpiexec –np 16 -hosts compute1,compute2  ./myprog
When running with srun under Slurm the executor does  not  require the -np flag; it computes the number of processes from the resource request.  It is also aware of the hosts assigned by the scheduler.
srun ./myprog
Message Envelopes
Just as a letter needs an envelope with an unambiguous address, a message needs to be uniquely identified. The message envelope provides that identification. It consists of several components. 
A communicator is an object that specifies a group of processes that will communicate with one another. The default communicator is
MPI_COMM_WORLD. It includes all processes.  The programmer can create new communicators, usually of subsets of the processes, but this is beyond our scope at this point.
In MPI the process ID is called the rank.  Rank is relative to the communicator, and is numbered from zero. 
A message is uniquely identified by its
- Source rank
- Destination rank
- Communicator
- Tag
The ""tag"" can often be set to an arbitrary value such as zero.  It is needed only in cases where there may be multiple messages from the same source to the same destination in a short time interval, or a more complete envelope is desired for some reason."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_standardsends.md,"The most widely used send is the ""standard"" form.  We will begin with the blocking version, since it is generally safer than the nonblocking form; it can be implemented intentionally or it can be used to develop a program and later replaced by nonblocking send/receive.
Send
The syntax for invocation for the standard MPI_Send is
c++
int MPI_Send(&sendbuf, count, datatype, dest, tag, comm)
fortran
call MPI_Send(sendbuf, count, datatype, dest, tag, comm, ierr)
```python
comm.Send([sendbuf,datatype],dest=rank,tag=0)
or for pickled objects
comm.send(sendbuf,dest=rank,tag=0)
```
Receive
The matching receive is called as follows:
c++
MPI_Status status;
int MPI_Recv(&recvbuf, count, datatype, source, tag, comm, &status)
fortran
integer, dimension(MPI_STATUS_SIZE) :: status
call MPI_Recv(recvbuf, count, datatype, source, int tag, comm, status, ierr)
```python
comm.Recv([data,MPI_Type],source=rank,tag=0,status=status)
or for pickled objects
data=comm.recv(source=rank,tag=0,status=status)
```
The status variable is optional for Python, but not for C++ or Fortran.
Example
Our first program will run on two processes and exchange some data between them.
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/send_recv.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/send_recv.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/codes/send_recv.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/parallel-computing-introduction/parallel_strategies.md,"Dependencies
A dependency occurs when the result of an operation relies on the result of a previous one.  Three types occur.
Data Dependency
A data dependency occurs when the ordering of calculations matters.  A ""true"" data dependency is called a flow dependency and it cannot be eliminated without reworking the code.
plaintext
X = 42
Y = X
Z = Y
Y cannot be computed until X has been set, and Z relies in turn on Y.
An anti-dependency occurs when a calculation uses a value that is later modified.
plaintext
X = 42
Y = 12
X = X+1
This type of dependency may be eliminated by introducing new variables:
plaintext
X = 42
X2= X
Y = X2+1
X = X+1
However, we now have a flow dependency because X2 must be set before Y can be computed.
The final category of data dependency is the output dependency.  This does not necessary refer to printing but to the ""output"" value of a variable.
plaintext
X = myfunc(Z)
X = X+1
As is the case for anti-dependencies, output dependencies can be corrected by renaming variables.
plaintext
X2 = myfunc(Z)
X  = X+1
Control Dependency
A control dependency occurs when whether a statement is executed depends on the result of a previous statement.
plaintext
if A>0 then
   X=sqrt(A)
end
Y=X+1
Managing Dependencies
Some dependencies are unavoidable; most programs will contain them.  However, sometimes a change of algorithm can reduce or eliminate some previously present.  For example, suppose we wish to solve the matrix equation
$$ Ax = b $$
where we will assume $A$ is a square matrix.  Many equations of this type that occur in scientific and engineering applications can be solved through an iterative method; this is generally faster and requires less memory than a direct method.  A popular iterative method is the Gauss-Seidel method.  We will not go into details of the derivation; the result is that we write the corrected version of the solution as
$$ x_{i}^{k+1} = \frac{1}{a_{ii}}( b_i - \sum_{j=1}^{i-1}a_{ij}^{k+1} + \sum_{j=i+1}^{n}a_{ij}^{k+1}), i=1,2,...,n $$
where $k$ represents the iteration number and the matrix is $n \times n$.
In pseudocode this is expressed for each iteration as
plaintext
for i in 1 to n
    t1 = 0
    for j in 1 to i-1
        t1 = t1+a[i,j]*x[j]
    end
    t2 = 0
    for j in i+1 to n
        t2 = t2 + a[i,j]*x[j]
    end
    x[i] = (b[i] - t1 - t2)/a[i,i]
This is repeated until some specified tolerance is achieved.  The solution vector x is continuously updated as we sweep through the rows.  Utilizing the new information increases the rate of convergence, but introduces a flow dependency.
An older method, called Jacobi iteration, is nearly identical but utilizes two variables for the solution, representing $x^{k}$ and $x^{k+1}$.
The pseudocode in this case is
plaintext
for i in 1 to n
    t1 = 0
    for j in 1 to i-1
        t1 = t1+a[i,j]*x_old[j]
    end
    t2 = 0
    for j in i+1 to n
        t2 = t2 + a[i,j]*x_old[j]
    end
    x[i] = (b[i] - t1 - t2)/a[i,i]
x_old=x
This method is slower in serial, sometimes considerably so, but is easy to parallelize.
Granularity
An important factor in parallelization strategies is the granularity of the work.
A coarse-grained distribution divides a large quantity of computation among the tasks, with relatively less communication or
synchronization required.  A fine-grained distribution assigns a smaller quantity of work to each task and does relatively more communication and synchronization.
The granularity determines whether special hardware might be required, and may drive a choice between shared or distributed memory programming.  Some programs contain both fine and coarse granularity and may be suited for a combination approach.  Often a program is
actually medium grained, intermediate between a truly fine-grained scenario and a coarse-grained approach.
Some algorithms are inherently fine-grained while others are coarse-grained.  In other situations, the programmer has control over the degree of granularity, through choices about data decomposition.
Example
A post-production company has several thousand frames of a movie to process.  Each frame consists of some number of pixels. 

Fine grained
    Divide each frame into subunits.  Process each subunit's pixels as a separate task.  Reconstruct the finished image.
Coarse grained
    Process each frame as a separate task.

Load Balancing
In parallel codes, the runtime is determined by the slowest process
The computational load should be distributed so that each process does approximately the same share of the work.
Example
With a fixed grid size a general-circulation model must do more computations over land than over ocean.  A possible solution is to use smaller grid sizes over land, so that the amount of work per grid zone is roughly constant.
Algorithm Selection
Some algorithms that are fast in serial cannot be parallelized efficiently or at all.
Conversely, some algorithms that are relatively slow in serial are easily parallelizable.
We have already seen one example of this when we looked at Gauss-Seidel iteration compared to Jacobi iteration.  In this case, the algorithm that is faster in
serial has a dependency which prevents parallelization.
But even some algorithms that can be parallelized in principle perform poorly;
the inherent granularity and how well the load can be balanced can play key
roles in the success of a parallelization project."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_basics.md,"We will start with the basic routines used to initialize MPI, obtain information about the process, and write simple examples.
Initialize MPI
MPI must be initialized before we can invoke any other routines.  This does not have to be the first executable statement in the program, but it must be performed before any other MPI routines can be called.  The initialization carries out the necessary system setup and establishes the default communicator.
In the current MPI standard, all C/C++ routines return an integer, the error code.  The Fortran bindings are mostly subroutines and include this return value as the last parameter.
C/C++
c
MPI_Init(&argc, &argv);
//more correct but rarely used
//int ierr=MPI_Init(&argc, &argv);
Fortran
fortran
integer ierr
!code
call MPI_Init(ierr)
Python
This tutorial applies only to the mpi4py package, which is the most popular MPI implementation for Python at this time.  This package consists of multiple subpackages, of which the most important is MPI.  Within the MPI subpackage are several classes.  Most of the basic functionality of MPI is implemented as methods in the Communicator class.
mpi4py calls Init internally when a Communicator object is instantiated. It is available but not required.
```python
from mpi4py import MPI
optional
MPI.Init()
More usually we just set up the default communicator
comm=MPI.COMM_WORLD
```
Determine The Number of Processes
C/C++ programmers should notice that parameters to the MPI routines must be called by reference, i.e. a pointer must be passed.  Fortran automatically calls by reference so no special treatment of parameters is required. 
The first argument is the communicator; the number of processes in that communicator group is returned as an integer in the second argument.
C
c
int nprocs;
MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
Fortran
fortran
integer ::  ierr, nprocs
!other statements
call MPI_Comm_size(MPI_COMM_WORLD, nprocs, ierr)
Python
python
nprocs=comm.Get_size()
Determine Process Rank
The rank is always relative to the communicator.  We are only considering the default MPI_COMM_WORLD in these examples.  Process rank is an integer in the range
0, 1, …,  (nprocs-1) returned through the second argument.
C
c
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
Fortran
fortran
integer :: rank
!other statements
call MPI_Comm_rank(MPI_COMM_WORLD,rank,ierr)
Python
python
rank=comm.Get_rank()
Shut Down MPI
C
c
MPI_Finalize();
Fortran
fortran
call MPI_Finalize(ierr)
Python
python
MPI.Finalize()
For mpi4py, Finalize must be invoked only if Init was explicitly called.
Finalize must be the last routine after all other MPI library calls.  It allows the system to free up MPI resources.  It does not have to be the last executable statement, but no more MPI routines may be invoked after it."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_sendmodes.md,"There are two basic types of messaging: blocking and nonblocking.  Blocking communications pause the processes until the message has been confirmed received.  Nonblocking communications initiate the message exchange, then continue without waiting for acknowledgement.  Nonblocking communications must be completed explicitly before the data can be used by the receiver.
Send
Within these two major categories, MPI defines four modes of sending data. 
Different routines correspond to these modes.
{{< table >}}
| Mode |  Blocking  |   Non-Blocking |  Pros  |   Cons  |
|------|------------|----------------|--------|---------|
| Synchronous | MPI_Ssend | MPI_Issend |  Safe  |  Introduces overhead (waiting) |
| Buffered | MPI_Bsend | MPI_Ibsend |  Predictable, fast | Requires more memory, copying takes time | 
| Ready | MPI_Rsend | MPI_Irsend | Fast  | Unpredictable, hard to debug |
| Standard | MPI_Send | MPI_Isend | Usually a good balance between speed and safety | Relies on quality of MPI implementation |
{{< /table >}}
All send routines have similar syntax
c++
int MPI_[SRI]send(const void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)
fortran
call MPI_[SRI]send(sendbuf, count, datatype, dest, tag, comm,ierr)
python
comm.[SRI][sS]end([sendbuf,datatype],dest=rank,tag=0)
Synchronous
The synchronous send waits until it receives an acknowledgement from the destination process before completing the send.  This is called a handshake.  In
our mail analogy, the synchronous send waits by its mailbox until it receives a signal from the recipient, and only then does it send the message on its way.
This waiting can slow down the process considerably, but it is safe since the sender knows the recipient is prepared to accept the message.
Buffered
We have discussed send and receive buffers, but in general the implementation and management of those buffers are left to the MPI library.  In a buffered send, the programmer must explicitly create a buffer and copy the message data into it. We may think of the sender making a copy of the information in the message, placing that copy into its ""mailbox,"" then returning to continue working.  The MPI library delivers the message to the recipient.  The sender may make changes to the original copy of the data.
To use a buffered send, the programmer must add an MPI routine
c++
MPI_Buffer_attach(buffer,size)
For C/C++, buffer and size must be passed by reference.  The size is the maximum.  To use a buffer of a different name or size, the first buffer must be explicitly detached:
c++
MPI_Buffer_detach(buffer,size)
Buffered sends do not incur the synchronization overhead of the synchronous send so are generally faster, but do require additional memory, and a copying of data.
Ready
In a ready send, the sender completes the send without checking whether the receiver is ready.  It may be analogized to the sender tossing the ""letter"" into a bin at the post office and expecting the receiver to find it.  The sender then returns to work.  If the receiver fails to fetch the message, the result is undefined, so a ready send could be unpredictable.
Standard
In a ""standard"" send, the sender posts the send and returns without waiting for a handshake.  In our analogy, the sender puts the ""letter"" into the mailbox and leaves. The exact behavior of the standard send is dependent upon the MPI implementation.  Blocking standard sends can deadlock relatively easily.
Because of its usually good balance between speed and safety, the standard send (blocking or nonblocking) is by far most commonly used, and we will focus on this approach.
Receive
The receive is either blocking or nonblocking.  All other modes depend on the send.
{{< table >}}
|  Blocking  |   Non-Blocking | 
|------------|----------------| 
|  MPI_Recv  |   MPI_Irecv    |
{{< /table >}}
The syntax of the blocking receive is
c++
int MPI_Recv(void *recvbuf, int count, MPI_Datatype datatype, int source, int tag,MPI_Comm comm, MPI_Status *status)
fortran
integer, dimension(MPI_STATUS_SIZE) :: status
call MPI_Recv(recvbuf, count, datatype, source, int tag, comm, status, ierr)
python
comm.Recv([data,MPI_Type],source=rank,tag=0,status=status)
In mpi4py the status argument is optional.
In C/C++, MPI_Status is a struct.  In Fortran MPI_Status is an array of integers of size MPI_STATUS_SIZE, which is set by the MPI library.  In Python, status is an object.  In all cases, it can be used to extract information about the exchange that can be useful for debugging. See the MPI documentation for details.
The syntax for MPI_Irecv is similar:
c++
int MPI_Irecv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Request * request)
We will discuss nonblocking point-to-point communications later, including the meaning of a ""request."""
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_nonblocking_comms.md,"So far we have only discussed blocking sends and receives. Effectively, the receive is the blocking operation in this case, since the operation causes the execution to halt on the receiving process until confirmation has been returned that the message was received.  This can result in a significant loss of performance, especially for codes that exchange messages in multiple directions.  For a blocking send, the buffer variable may be reused once the procedure has returned. A blocking receive returns once the bits have been copied into the buffer variable.
However, as we have seen, MPI also provides nonblocking point to point communications. Nonblocking communications initialize the messaging but do not wait for acknowledgements until explicitly requested.  This means that computations can continue while communications take place in the background.  The only requirement is that the message buffer must not change during the computations.  No modification to the buffer should occur until after the request has been completed with an MPI_Wait, MPI_Waitall, MPI_Waitany, MPI_Test, MPI_Testall, or MPI_Testany.
Not all MPI communication procedures have both a blocking and a nonblocking version. In particular, we have so far discussed only blocking collective communications.  Nonblocking collective procedures were introduced in the MPI3 standard.
One of the major advantages of nonblocking communications are that even with complex communication patterns, deadlock can be avoided. Another advantage is overlapping communications so that serialization does not occur. In our sendrecv example, for instance, all processes first send left and then right (or vice versa), causing serialization as they wait for each sweep to complete.
Nonblocking Receive
The nonblocking receive MPI_Irecv creates a receive request and returns it in a object of type MPI_Request. 
C++
c++
MPI_Request *request;
MPI_Irecv(buf, count, datatype, source, tag, comm, request);
In Fortran the MPI_Request is an integer defined in the mpi or mpi_f08 module.
Fortran
fortran
integer :: request
!executable code
call MPI_Irecv(buf, count, datatype, source, tag, comm, request, ierror)
The Python mpi4py implementation returns a Request object. As usual, the capitalized version is used with NumPy buffers whereas the uncapitalized version pickles or unpickles the object.
Python
python
request=comm.Irecv(buf,source,tag=0)
Nonblocking Send
A nonblocking Irecv can be used with any Send of any type.  However, we normally pair it with an MPI_Isend.
C++
c++
MPI_Request *request;
MPI_Isend(buf, count, datatype, dest, tag, comm, request)
Fortran
As for MPI_Irecv, request must be declared integer.
fortran
call MPI_Isend(buf, count, datatype, dest, tag, comm, request, ierror)
Python
python
req = comm.Isend(data, dest, tag=0)"
rc-learning-fork/content/courses/parallel-computing-introduction/_index.md,"This course is an introduction to parallel programming.
Proficiency in programming in C, C++, or Fortran is assumed.  Programming is Python using the mpi4py package is also supported.
Credits
This course material is an expanded version of material presented for several years at the High-Performance Computing Bootcamp at the University of Virginia.  The notes were developed by
* Andrew Grimshaw, UVA Department of Computer Science
* Aaron Bloomfield, UVA Department of Computer Science
* Katherine Holcomb, UVA Research Computing
* Some of the original slides were based on material in  Parallel Computing: Theory and Practice, by Michael J. Quinn."
rc-learning-fork/content/courses/parallel-computing-introduction/performance_analysis.md,"Speedup and Efficiency
The purpose of making the effort to convert our programs to parallelism is a shorter time to solution.
Thus we are interested in the speedup of a parallel code relative to running
the same algorithm in serial.  Speedup is a simple computation: 
$$ Speedup = \frac{sequential\ execution\ time}{parallel\ execution\ time} $$
Let us consider a problem of size $n$, where $n$ represents a measure of the
amount of work to be done.  For example, it could be the first dimension of a 
square matrix to be inverted, or the number of pixels to analyze in an image. 
Now assume we will run this on $p$ processes.  Let
$$ \sigma (n) $$
be the part of the problem that is inherently sequential.  Let 
$$ \phi (n) $$
be the part that is potentially parallelizable.  Any parallelization will
require some communication among the processes; the impact of this will be a 
function of both $n$ and $p$.  Let us represent this as 
$$ \kappa (n,p) $$
On a single processor, the total time will depend on
$$ \sigma (n) + \phi (n) $$
On $p$ processors, on the other hand, the time will depend on
$$ \sigma (n) + \phi (n)/p + \kappa (n,p) $$
Therefore we can express the speedup as
$$ \psi(n,p) = \frac{\sigma(n)+\phi(n)}{\sigma(n)+\phi(n)/p+\kappa(n,p)} $$
{{< figure src=""/courses/parallel-computing-introduction/img/parallel_speedup.png"" caption=""As the number of processes grows, overhead also grows and can become dominant."" >}}
This simple formula shows that we would achieve the maximum possible speedup if $\sigma(n)$ and $\kappa(n,p)$ are negligible compared to $\phi(n)$.  In that case we obtain the approximation
$$ \psi(n,p)\approx \frac{\phi(n)}{\phi(n)/p} = p $$
Thus the ideal is that speedup should be
linear in the number of processes.  In some circumstances, the actual speedup
for at least a limited number of processes can exceed this.  This is called
superlinear speedup.  One fairly common cause of this is cache efficiency.  When the problem is broken into smaller subdomains, more of the data may fit
into cache.  We have seen that cache is much faster than main memory, so 
considerable gains may be achieved with improved cache efficiency.  Some other
reasons include better performance of the algorithm over smaller data, or even
moving to an algorithm that is better but only in parallel, since speedup is computed relative to the best sequential algorithm. 
The efficiency is the ratio of the serial execution time to the parallel time multiplied by $p$, i.e. if $S$ is the speedup, 
$$ \mathrm{Efficiency} = \frac{S}{p} $$
Using the above formal notation we obtain
$$ \epsilon(n,p)=\frac{\sigma(n) + \phi(n)}{p\sigma(n) + \phi(n) + p\kappa(n,p)}$$
From this it should be apparent that 
$$ 0 < \epsilon(n,p) \le 1 $$
Amdahl's Law
Define
$$ f = \frac{\sigma(n)}{\sigma(n)+\phi(n)} $$
This is the fraction of the work that must be done sequentially.  Perfect
parallelization is effectively impossible, so $f$ will never be zero.
In the speedup formula, consider the limiting case for which $\kappa(n,p) \to 0$.
Thus
$$ \psi(n,p) \lt \frac{\sigma(n) + \phi(n)}{\sigma(n) + \phi(n)/p} $$
Rewriting in terms of $f$ gives
$$ \psi \le \frac{1}{f + (1-f)/p} $$
In the case that $p \to \infty$ this becomes
$$ \psi \lt \frac{1}{f} $$
Thus the irreducibly sequential portion of the code limits the theoretical speedup.  This is known as Amdahl's Law.
Example
Suppose 95% of a program’s execution time occurs inside a loop that can be executed in parallel, and this is the only parallelizable part of the code. What is the maximum speedup we should expect from a parallel version of the program executing on 8 CPUs?  What is the efficiency?  What is the maximum theoretical
speedup?
In this case
$$ f = 0.05 $$
So
$$ \psi \le \frac{1}{0.05+0.95/8} = 5.92 $$
$$ \epsilon = \frac{t_{seq}}{8t_{seq}/5.92} = 0.74 $$
$$ \psi_{inf} = \frac{1}{.05} = 20 $$
{{< figure src=""/courses/parallel-computing-introduction/img/strong_speedup.png"" caption=""Speedup is very sensitive to the fraction of the program that can be parallelized."" >}}
Scalability
The Amdahl effect is the observation that as the problem size, represented by
$n$, increases, the computation tends to dominate the communication and other
overhead.  This suggests that more work per process is better, up to various
system limits such as memory available per core.
The degree to which efficiency is maintained as the number of processes increases
is the scalability of the code.  Amdahl's Law demonstrates that a very high degree of parallelization is necessary in order for a code to scale to a large number of processes for a fixed amount of work, a phenomenon known as strong scaling.  
Weak Scaling
Strong scaling over a large number of processors can be difficult to achieve.  However, parallelization has other benefits.  Dividing work among more cores can reduce the memory footprint per core to the point that a problem that could not be solved in serial can be solved.  


Strong scaling: the same quantity of work is divided among an increasing number of processes.


Weak scaling: the amount of work per process is fixed, while the number of processes increases.


Amdahl's law shows that strong scaling will always be limited. We must achieve a parallel fraction of close to 99% in order for the program to scale to more than around 100 processes.  Many parallel programs are intended to solve large-scale problems, such as simulating atmospheric circulation or the dynamics of accretion disks around black holes, so will require hundreds or even thousands of processes to accomplish these goals; strong scaling is not feasible for these types of problem.  
The counterpart to Amdahl's Law for weak scaling is Gustafson's Law. Here we fix not the size of the problem but the execution time.  Using the same notation as before, with $f$ the fraction of the workload that must be executed sequentially, the sequential time that would be required for the problem size corresponding to $p$ processes will be 
$$ t_s = ft + (1-f)pt $$
where $t$ is the fixed execution time.
The speedup is $t_s/t$ which yields
$$ \psi = f + (1-f)p $$
For weak scaling there is no upper limit on the speedup, and as $f \to 0$, $S \to p$.  The ideal scaled speedup is thus linearly proportional to the number of processes.
The efficiency is 
$$ \epsilon = \frac{S}{p} = \frac{f}{p}+1-f $$
This expression ignores real-world complications such as communication and
other system overhead, but it provides a guide for understanding weak scaling.  In the case of $p \to \infty$, $\epsilon \to 1-f$.  Therefore, as for Amdahl's law, the sequential fraction will limit the speedup and efficiency possible.  However, for weak scaling, lesser parallelization can still produce acceptable efficiencies.
{{< figure src=""/courses/parallel-computing-introduction/img/weak_speedup.png"" caption=""Speedup for weak scaling."" >}}
Weak scaling allows a much larger workload to be run in the same time.  A significant portion of scientific and engineering problems require a large to very large workload for problems of interest to be solved.  For example, fluid dynamics is modeled better at higher numerical resolutions, but each computational cell adds to the problem size.  Many problems, such as weather models, could not be solved at all at the resolution of interest without weak scaling."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_global3.md,"In many-to-many collective communications, all processes in the communicator group send a message to others. 
Barrier
When MPI_Barrier is invoked, each process pauses until all processes in the communicator group have called this function.  The MPI_BARRIER is used to synchronize processes.  It should be used sparingly, since it ""serializes"" a parallel program. Most of the collective communication routines contain an implicit barrier so an explicit MPI_Barrier is not required.
C++
c++
MPI_Barrier(MPI_Comm comm)
Fortran
fortran
call MPI_Barrier(comm, ierr)
Python
python
comm.Barrier()
It is seldom needed in Python.  For examples in C++ and Fortran, please see 
{{< spoiler text=""scatter.cxx"" >}}
{{< code file=""/courses/parallel-computing-introduction/codes/scatter.cxx"" land=""cxx"" >}}
{{< /spoiler >}}
{{< spoiler text=""scatter.f90"" >}}
{{< code file=""/courses/parallel-computing-introduction/codes/scatter.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
In these examples, it is used in a loop to force the output from the processes to be separated distinctly and in rank order.  Upon entry into the loop, all processes synchronize before executing the loop body.  The process whose rank matches the loop variable writes its output, while the other processes skip back to the top of the loop.  However, they must wait there until the process doing the writing finishes and invokes MPI_Barrier.
Exercise
Write a program that generates an array of values from 1 to 10 only on the root process.  Broadcast the array to each process.  Print the array from each process.
Test it with four processes on the frontend or on your workstation.
{{< spoiler text=""C++"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/bcast_ex.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/bcast_ex.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/bcast_ex.py"" lang=""python"" >}}
{{< /spoiler >}}
Allreduce
As the examples in the previous chapter demonstrated, when MPI_Reduce is called, only the root process knows the result.  This can lead to bugs since it is fairly common that all processes should be aware of the global value.  The MPI_Allreduce procedure performs the reduction and distributes the result.  It should always be used rather than a Reduce followed by a Bcast, since the implementation should carry this out more efficiently than with an explicit broadcast.
The syntax for MPI_Allreduce is identical to that of MPI_Reduce but with the root number omitted.
c
int MPI_Allreduce(void *operand, void *result, int ncount, MPI_Datatype type, MPI_Op operator, MPI_Comm comm );
fortran
call MPI_ALLREDUCE(sendbuf, recvbuf, ncount, datatype, op, comm, ierr)
python
comm.Allreduce(sendarr, recvarr, operation)
Exercise
Modify the example reduction code in your language of choice to perform an Allreduce.
{{< spoiler text=""C++ Solution"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/allreduce.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran Solution"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/allreduce.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python Solution"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/allreduce.py"" lang=""python"" >}}
{{< /spoiler >}}
Allgather/Allgatherv
An allgather is the same as a gather, but each process sends into to the receive buffer of every other process.  As for Reduce/Allreduce, the syntax is essentially the same as for gather/gatherv, but without a root process specified.  The global result array must be created on each process for Allgather.
C++
c++
int MPI_Allgather(void *sendbuffer, int ncount, MPI_Datatype datatype, void *recvbuffer, int ncount, MPI_Datatype datatype, MPI_Comm communicator)
Fortran
fortran
<type><dimension><size>   :: vars
<type><dimension><size>   :: all_vars
integer  :: ncount, root, err
! more code
call MPI_ALLGATHER(vars, ncount, MPI_TYPE, all_vars, ncount, MPI_TYPE, MPI_COMM_WORLD, err)
Python
As before, for this syntax, both buffers should be NumPy arrays.
python
comm.Allgather([data,MPI.TYPE],[all_data,MPI.TYPE])
Exercise
Modify the example gather code in your language of choice to perform an Allgather.
{{< spoiler text=""C++ Solution"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/allgather.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}
{{< spoiler text=""Fortran Solution"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/allgather.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
{{< spoiler text=""Python Solution"" >}}
{{< code-download file=""/courses/parallel-computing-introduction/solns/allgather.py"" lang=""python"" >}}
{{< /spoiler >}}
Alltoall
In MPI_Alltoall, each process sends data to every other process.  Let us consider the simplest case, when each process sends one item to every other process. Suppose there are three processes and rank 0 has an array containing the values [0,1,2], rank 1 has [10,11,12], and rank 2 has [20,21,22].  Rank 0 keeps (or sends to itself) the 0 value, sends 1 to rank 1, and 2 to rank 2.  Rank 1 sends 10 to rank 0, keeps 11, and sends 12 to rank 2.  Rank 2 sends 20 to rank 0, 21 to rank 1, and keeps 22.
{{< figure src=""/courses/parallel-computing-introduction/img/alltoall.png"" caption=""Alltoall.  Note that as depicted, the values in the columns are transposed to values as rows."" >}}
C++
{{< spoiler text=""alltoall.cxx"" >}}
{{< code file=""/courses/parallel-computing-introduction/codes/alltoall.cxx"" land=""cxx"" >}}
{{< /spoiler >}}
Fortran
{{< spoiler text=""alltoall.f90"" >}}
{{< code file=""/courses/parallel-computing-introduction/codes/alltoall.f90"" lang=""fortran"" >}}
{{< /spoiler >}}
Python
{{< spoiler text=""alltoall.py"" >}}
{{< code file=""/courses/parallel-computing-introduction/codes/alltoall.py"" lang=""python"" >}}
{{< /spoiler >}}
Two more general forms of alltoall exist; MPI_Alltoallv, which is similar to MPI_Allgatherv in that varying data sizes and displacements are allowed; and MPI_Alltoallw, which is even more general in that the 'chunks' of data on the processes can be of different datatypes.  These procedures are beyond our scope but the interested reader can consult the documentation.
MPI_IN_PLACE
We often do not need one buffer once the message has been communicated, and allocating two buffers wastes memory and requires some amount of unneeded communication. MPI collective procedures allow the special buffer MPI_IN_PLACE. This special value can be used instead of the receive buffer in Scatter and Scatterv; in the other collective functions it takes the place of the send buffer.  The expected send and receive buffers must be the same size for this to be valid. As usual for mpi2py, the Python name of the variable is MPI.IN_PLACE.
Examples
```c++
MPI_Scatter(sendbuf, ncount, MPI_Datatype, MPI_IN_PLACE, ncount, MPI_Datatype, root, MPI_COMM_WORLD);
MPI_Reduce(MPI_IN_PLACE, recvbuf, ncount, MPI_Datatype, MPI_Op, root, MPI_COMM_WORLD);
```
```fortran
call MPI_Scatter(vals, ncount, MPI_TYPE, MPI_IN_PLACE, ncount, MPI_TYPE, root, MPI_COMM_WORLD)
call MPI_REDUCE(MPI_IN_PLACE, recvbuf, ncount, MPI_TYPE, MPI_Op, root, MPI_COMM_WORLD, ierr)
```
```python
comm.Scatter([sendvals,MPI.DOUBLE],MPI.IN_PLACE,root=0)
comm.Reduce(sendarr, MPI.IN_PLACE, operation, root=0)
```"
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_p2p1.md,"We have so far discussed global communications, in which all members of a communicator group take part.  Most MPI programs rely primarily on point-to-point communication, in which an individual process sends messages directly to one other individual processes.
Send and Receive
The sending process packs the outgoing data into the send buffer, which, as for global communications, is specified by a variable name.  It bundles the data into a message with an envelope consisting, at minimum, of the source rank, the destination rank, and the communicator.  Under certain circumstances, an additional identifier called a tag may be used to distinguish messages.  Most of the time the tag can be set to some arbitrary value such as $0$. The receiving rank extracts the data from the receive buffer.  We can think of the sender preparing a ""letter"" with an address, then placing it into an outgoing mailbox.  The MPI library is responsible for ""delivering"" the message to the intended recipient's ""mailbox.""  
The sending process is said to post a send when a communication is initiated.  The receiving processes posts the receive, which acknowledges it is prepared to accept the message.  If the message envelope from the sender matches the expectation of the receiver, the message will be delivered and the communication will complete.  As we can see, in point-to-point communications the state of the receiving process will determine completion of the message passing.  
A requirement of the MPI standard, that all implementations must honor, is that message order is preserved.  Messages must be received in the order in which they are sent.  
{{< figure src=""/courses/parallel-computing-introduction/img/message_passing.png"" caption=""Point to point communication."" >}}
The requirement of matching postings and messages lead to an important difference between global communications and point-to-point communications: the latter can deadlock.  A deadlock occurs when one process attempts to send a message to another process that is not ready to receive it.  The sender will wait indefinitely even though the receiver will never reach a state of readiness, causing the entire program to come to a halt.  A program that might deadlock is said to be unsafe.
Deadlock can also occur because the message is ""misaddressed"" so that the receive routine is not able to match it to anything it is expecting.  This most frequently occurs due to differing tags, so programmers must be careful when setting tags to values other than zero.
{{< figure src=""/courses/parallel-computing-introduction/img/deadlock.png"" caption=""Deadlock occurs when the destination process is not able to receive the message.""  >}}
It is important for MPI programmers to keep in mind that each process is running as an independent executable, each of which is generally called a task.  MPI library invocations handle communications among these tasks, but the tasks are otherwise uncoordinated.  It is the responsibility of the programmer to ensure that each rank is provided the information it requires, and to manage the communications.
Let us consider a two-task program that will exchange some data.  We might first think of a pattern such as the following:
{{< figure src=""/courses/parallel-computing-introduction/img/unsafe.png"" caption=""Potentially unsafe communication pattern."" >}}
Depending on the behavior of the sends and the particulars of the MPI implementation, this may work, but it is not guaranteed to work; if both processes wait in the send state, they will never post the required receive.  Thus this is generally unsafe.
Now consider another pattern:
{{< figure src=""/courses/parallel-computing-introduction/img/maylock.png"" caption=""Possible deadlock."" >}}
Both processes post a receive. What happens next depends on whether the receive will wait for the corresponding send (i.e. it will block) or whether it will continue on until it receives a signal to complete the receive (i.e. it is nonblocking).  If the receive blocks then this pattern is certain to deadlock.
Finally we look at the following:
{{< figure src=""/courses/parallel-computing-introduction/img/safe.png"" caption=""Successful exchange."" >}}
This should work regardless of the behavior of the sends and receives, so this pattern is safe."
rc-learning-fork/content/courses/parallel-computing-introduction/distributed_mpi_oned_division.md,"In the most common cases, we want a full exchange of data between every two communicating processes.  If we examine our schematic again
{{< figure src=""/courses/parallel-computing-introduction/img/nearest_neighbor.png"" caption=""Schematic of nearest-neighbor exchange"" >}}
we will observe that we can think of the blue arrows as a sweep from left to right, while the orange arrows represent a return sweep from right to left. This pattern is particularly common when we are computing on a distributed domain and must exchange information among the subdomains.
Domain Decomposition
We have discussed domain decomposition in a general case.  Let us now consider a more specific example.  We wish to do some computations on a grid. A grid divides some region into smaller segments often called zones. We will convert our problem from some kind of continuous equations into a discretized system that will be solved on the grid.
The illustration shows a small 10x16 grid that we will divide into subgrids, one for each rank. To keep our first example simple, we will do a one-dimensional decomposition in which we will split the grid into strips.
{{< figure src=""/courses/parallel-computing-introduction/img/halo_domain_decomp.png"" caption=""Dividing a grid into subgrids."" >}}"
rc-learning-fork/content/courses/fiji-image-processing/_index.md,ImageJ is a popular image-processing package written in Java. Fiji is a distribution of ImageJ which bundles many popular plugins.
rc-learning-fork/content/courses/julia/index.md,"Julia is a high-level programming language designed for high-performance numerical analysis and computational science. Distinctive aspects of Julia's design include a type system with parametric polymorphism and types in a fully dynamic programming language and multiple dispatch as its core programming paradigm. It allows concurrent, parallel and distributed computing, and direct calling of C and Fortran libraries without glue code. A just-in-time compiler that is referred to as ""just-ahead-of-time"" in the Julia community is used.
The code examples in this workshop are meant for running Julia on the Rivanna cluster. The Julia documentation page linked to below provides more detail on running these code examples. Attendees will be able to ask questions and receive feedback regarding workshop exercises during the Q&A session. Users will also have the opportunity to run MATLAB code on the Rivanna cluster if they have login accounts, otherwise they will be given training accounts on the cluster.
Julia Computing on Rivanna
Please see also the documentation for using Julia on Rivanna.
Recommendations
These notes are divided into four sections. The first section contains links to
documentation and tutorials for the Julia programming language. Besides browsing
through these links, I recommend the Julia Academy courses as well as listening
to Alan Edelman's talk ""The Power of Language"" (he is one of the authors of Julia).
The next section is on integrated development environments for writing Julia code.
Juno using the Atom editor is popular and the JuliaPro link includes Julia and Juno
in one download. More recently, Visual Studio Code (VScode) is gaining adherents
and I've included links to talks about it from the most recent Julia conference.
The third section contains links for using Julia in data science and computational
science. The ""Introduction to Computational Thinking"" is an ongoing course at MIT
that uses Julia. The last section includes links to the Julia package repository
and an extensive set of Julia code examples.
The Julia Language Tutorials


The Julia Programming Language (Official site) 
  https://julialang.org/


Julia 1.5 Documentation 
  https://docs.julialang.org/en/v1/


Julia Language Research and Development at MIT 
  https://julia.mit.edu/


Julia Academy Courses 
  https://juliaacademy.com/


From zero to Julia! 
  https://techytok.com/from-zero-to-julia/


The Power of Language 
  https://www.youtube.com/watch?v=nwdGsz4rc3Q


A Julia interpreter and debugger 
  https://julialang.org/blog/2019/03/debuggers/


Introduction to Julia’s Debugger 
  http://webpages.csus.edu/fitzgerald/julia-debugger-tutorial/


The Julia Language Slack 
  https://julialang.org/slack/


Julia Integrated Development Environments (IDE)


Juno: A Flexible IDE for Julia 
  https://junolab.org/


JuliaPro: 
  https://juliacomputing.com/products/juliapro


Julia for VSCode IDE 
  https://www.julia-vscode.org/


Debugging Julia in Juno 
  http://docs.junolab.org/latest/man/debugging/


JuliaCon 2020 | (Juno 1.0) VSCode for Julia 1.0 
  https://www.youtube.com/watch?v=rQ7D1lXt3GM&list=PLP8iPy9hna6Tl2UHTrm4jnIYrLkIcAROR&index=2


JuliaCon 2020 | Using VS Code for Julia development | David Anthoff 
  https://www.youtube.com/watch?v=IdhnP00Y1Ks&list=PLP8iPy9hna6Tl2UHTrm4jnIYrLkIcAROR&index=152


Visual Studio Code 
  https://code.visualstudio.com/


Julia and Data Science / Computational Science


Doing Scientific Machine Learning (SciML) With Julia 
  https://live.juliacon.org/talk/C9FGPP


SciML / DifferentialEquations.jl 
  https://github.com/SciML/DifferentialEquations.jl


Julia for Data science 
  https://ucidatascienceinitiative.github.io/IntroToJulia/


Tutorial to Learn Data Science with Julia 
  https://www.analyticsvidhya.com/blog/2017/10/comprehensive-tutorial-learn-data-science-julia-from-scratch/


Flux: The Julia Machine Learning Library 
  https://fluxml.ai/Flux.jl/stable/


Introduction to Computational Thinking 
  https://computationalthinking.mit.edu/Fall20/


Julia: A Fresh Approach to Numerical Computing 
  https://julialang.org/assets/research/julia-fresh-approach-BEKS.pdf


Quantitative Economics with Julia 
  https://julia.quantecon.org/index_toc.html


JuliaHPC Meeting 
  https://discourse.julialang.org/t/juliahpc-meeting/49093


JuliaCon 2020 | MPI.jl: Julia meets classic HPC | Simon Byrne 
  https://www.youtube.com/watch?v=pV-8YqfOxQE&list=PLP8iPy9hna6Tl2UHTrm4jnIYrLkIcAROR&index=88


Parallel Computing in Julia 
  https://docs.julialang.org/en/v1/manual/parallel-computing/


Julia Examples


JuliaHub: Package Repository 
  https://juliahub.com/ui/Home


Julia by Example 
  https://juliabyexample.helpmanual.io/

"
rc-learning-fork/content/courses/matlab-machine-learning/mathlab_ml.md,"Course Overview
Video: Machine Learning with Matlab
Course Example - Basketball Player Statistics


Getting Starting with Data
Exercise: Importing Data
Exercise: Using Logical Indexing
Exercise: Creating Categorical Data
Exercise: Grouping Data
Exercise: Merging and Visualizing Data
Exercise: Merging and Visualizing Data
Exercise: Normalizing Data
Exercise: Basketball Statistics Script


Finding Natural Patterns in the Data
Clustering Basketball Players
Video: Clustering Basketball Players
Low Dimensional Visualization



To effectively visualize the data containing more than three predictors, you can use the dimensionality reduction techniques such as multidimensional scaling and principal component analysis (PCA).


Exercise: Classical Multidimensional Scaling
Exercise: Nonclassical Multidimensional Scaling








Exercise: Basketball Players
k-Means Clustering

Video: What is k-Means Clustering






Exercise: k-Means Clustering
Exercise: Options for k-Means Clustering
Exercise: Basketball Players
Gaussian Mixture Models

Video: What are Gaussian Mixture Models


Exercise: GMM Clustering
Exercise: Basketball Players
Interpreting the Clusters






Exercise: parallelcoords
Exercise: cross-tabulation




Exercise: Silhouette Plots
Exercise: Basketball Players
Hierarchical Clustering

Video: Hierarchical Clustering



Exercise: Determine Hierarchical Structure
Exercise: Divide Hierarchical Tree into Clusters
Exercise: Basketball Players
Building Classification Models
Course Example: Heart Disease
Video: Heart Disease Classification
Preparing Data

Video: Training and Validation Data






Exercise: Making Training and Test Sets
Exercise: Heart Health
Fitting and Predicting









Exercise: Fitting and Predicting
Exercise: Using a Classification Variable
Evaluating the Classification



Exercise: Prediction and Resubstitution Loss




Exercise: Confusion and Cost Matrix
Exercise: Heart Health
Classification Methods
Course Example: Different Methods to Classify Heart Patients




Video: Classification Learner App
Exercise: Classification Learner App
Nearest Neighbor Classification

Video: What is k-NN?

Exercise: Using Nearest Neighbor Classification with Tables

Exercise: Heart Health
Classification Trees

Video: What is a Classification Tree?

Exercise: Using Classification Trees
Exercise: Heart Health – Numeric Data
Exercise: Heart Health – Numeric and Categorical Data
Naive Bayes Classification

Video: What is Naive Bayes?

Exercise: Using Naive Bayes Classification
Exercise: Heart Health – Numeric Data
Exercise: Heart Health – Numeric and Categorical Data
Discriminant Analysis

Video: What is Discriminant Analysis??


Exercise: Using Discriminant Analysis
Exercise: Heart Health
Support Vector Machines

Video: What are Support Vector Machines?

Exercise: Using Support Vector Machine Classification
Exercise: Concentric Data
Exercise: Heart Health – Numeric Data
Exercise: Heart Health – Numeric and Categorical Data
Multiclass Support Vector Machines





Exercise: Using Multiclass Support Vector Machine Classification
Exercise: Heart Health – Numeric Data
Exercise: Heart Health – Numeric and Categorical Data
Improving Predictive Models
Methods for Improving Predictive Models






Cross Validation


Video: What is Cross Validation?



Exercise: Cross Validation
Exercise: Heart Health
Reducing Predictors - Feature Transformation









Exercise: PCA
Exercise: Heart Health
Reducing Predictors - Feature Selection



Exercise: Built-in Feature Selection
Exercise: Heart Health – Built-in Feature Selection






Exercise: Sequential Feature Selection
Exercise: Heart Health – Sequential Feature Selection


Exercise: Creating Dummy Variables
Exercise: Heart Health – Feature Selection with Categorical Data
Ensemble Learning




Exercise: Creating Ensembles
Exercise: Using Templates
Exercise: Heart Health
Building Regression Models
Course Example - Fuel Economy




Linear Models

Video: What is Linear Regression?




Exercise: Fitting a Line
Exercise: Fitting a Polynomial
Exercise: Multivariable Linear Regression



Exercise: Multivariable Linear Regression with Numeric Arrays
Exercise: Fuel Economy
SVMS and Trees

Exercise: Using Tree and SVM Models
Exercise: Choosing a Regression Model
Exercise: Fuel Economy – Tree
Exercise: Fuel Economy – SVM
Gaussian Process Regression

Video: What is Gaussian Process Regression?


Exercise: Using GPR
Exercise: Using GPR with Outliers
Exercise: Fuel Economy - GPR
Regularized Linear Models














Exercise: Ridge Regression



Exercise: Lasso Regression
Exercise: Fuel Economy – Ridge Regression
Exercise: Fuel Economy – Lasso Regression
Stepwise Fitting




Exercise: Stepwise Feature Selection
Exercise: Fuel Economy
Creating Neural Networks
Overview of Neural Networks
Video: What are Neural Networks?
Self-Organizing Maps




Video: What are Self-Organizing Maps?
Video:  Interactively Creating SOMs



Exercise: Interactively Creating SOMs
Exercise: Using Commands to Create SOMs 
Feed-Forward Networks









Video: What are Feed-Forward Networks?
Video: Interactively Creating Feed-Forward Networks
Exercise: Interactively Creating Feed-Forward Classification Networks
Additional Resources
Videos: Introduction to Machine Learning


E-book: Introducing Machine Learning


E-book: Getting Started Machine Learning


E-book: Applying Unsupervised Learning


E-book: Applying Supervised Learning


Videos: Applied Machine Learning
Machine Learning Quick Start Guide
Machine Learning Workflow E-Book
Statistics and Machine Learning Toolbox
MathWorks Resources"
rc-learning-fork/content/courses/matlab-machine-learning/_index.md,"Overview
This short course focuses on data analytics and machine learning techniques in MATLAB® using functionality within Statistics and Machine Learning Toolbox™ and Neural Network Toolbox™. The course demonstrates the use of unsupervised learning to discover features in large data sets and supervised learning to build predictive models. Examples and exercises highlight techniques for visualization and evaluation of results. Topics include:

Organizing and preprocessing data
Clustering data
Creating classification and regression models
Interpreting and evaluating models
Simplifying data sets
Using ensembles to improve model performance

The material for this course uses selected topics from the online self-paced course from the MathWorks in the link below. Participants should have a Mathworks account in order to access the links in this document.
Matlab Academy: Machine Learning with Matlab"
rc-learning-fork/content/courses/python-introduction/project-set-3.md,"Functions, modules, NumPy, Matplotlib.
Remember that you need to add the import numpy (or commonly used import numpy as np) statement in your script before you can use the numpy package.  Similarly, we must import Matplotlib.
python
import numpy as np
import matplotlib.pyplot as plt
Project 10
Write a Python script that performs the following operations:
a) Create a numpy array, x, of values from -1.0 to 1.0 inclusive, with step sizes of 0.01.  Use numpy.pi (or np.pi) for pi. 
b) Create a numpy array, y, where y = sin(pix) + cos(3pix/2)
c) Determine and print the mean y value.
d) Determine and print the minimum value of y and the maximum value of y over the range of x.  Also print the corresponding x coordinates where y reaches a minimum and where y reaches a maximum. Hint: Look up the argmin and argmax functions.   Pay attention to the difference between index and value.
e) Go back to the first chapter and review how to make a plot of y versus x using 
Matplotlib.  Add code to plot y as a function of x.
f) Add a line to accept user input to specify the x start, x end, and stride values.  Your finished code should get these values from the user, print the values and x-coordinate of the max and min for y, and display a plot of y versus x. Upload the plot for input values of starting x=-2., ending x=2., stride=.01.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_3/numpy_basics.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 11
You may use your choice of lists or NumPy arrays for this project.
Download the file cpi.csv
This file is the value of $100 from a baseline in January 1913 (the oldest consistent data available) to January 2020, as computed by the US Bureau of Labor Statistics; this gives us the consumer price index. The ratio of any two years is an estimate of the change in cost of living from the one to the other. 
a) Write a function that takes two years and returns this ratio.  Note that the order will matter.
The derivative of the CPI is a crude estimate of the inflation rate.  Write a function that takes two arrays and returns another array using the formula infl_rate=(cpi[yr+1]-cpi[yr])/12
Note that the returned array will have one fewer element than the original.
b) Write a program that uses your two functions to
read cpi.csv by any means you choose.
Request two years and a price from the user.  Compute the corresponding price for the second year provided by the user. 
Plot the CPI and your approximation to the inflation rate.
  Plot inflation rate versus the midpoint of the years (so would start in 
  June 1913, i.e. 1913.5, and end June 2019, i.e. 2019.5).
c) In 1954 a color TV cost approximately $1295 in that year's dollars.  How much would that be in 2020 (as of January) dollars? 
d) Look at the Matplotlib documentation and find how to use subplots to plot CPI and inflation rate one above the other.
e) (More advanced) Read about exceptions in the Files chapter and add them to your code to test for the existence of the file before attempting to open it.  Add with/as to the reading of the file (you can surround a numpy call with this also).
f) (More advanced) Convert your file with the functions into a module.  Isolate the calls to input/output, including plotting, by using a main() function.  Add the if __name__==""__main__"" so that it will not be invoked if you import your module.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_3/inflation.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 12
Write a program that reads the file bodyfat.csv.  


Extract the body fat percentage, weight, and height data from each row (the first, third, and fourth columns).  We do not need the age data for the current project.


Use your BMI function as a ufunc to compute all the BMI values at once and return a new array.


In your bmistats.py file, add a main function that runs a test by computing the BMI for a set of heights and weights and returning the category corresponding to that BMI.  Compute the correct results and add code to compare them with your code's results.


Plot the BMI versus bodyfat as a scatterplot.

Add the if name==""main"" code so that main will be executed only if the file is the main module.  Run your tests.

The bodyfat.csv file contains an outlier, probably due to a typo. Add a function to your bmistats file to find outliers. Find the outlier in your list and remove it (don't forget to remove the corresponding bodyfat element).
You may use the np.percentile(a,m) function to compute the upper and lower quartiles using the IQR.  See this example.
Plot the corrected data.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_3/bmistats.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 13
Download cville_2017_april.csv, which contains April 2017 weather data for Charlottesville, VA.
- Read the file into appropriate NumPy arrays
- Make a line plot of average wind speed for each day
- Add main titles, and label axes to be more descriptive
- Play with the bottom axis (x-axis) to make sure all dates are visible
- Make a bar and line plot showing average wind speed (in bars) and max wind gust (as a line). Add legend to distinguish between the two.
- Make stacked bar chart of minimum and maximum temperatures for each day
- Make grouped bar chart of minimum and maximum temperatures for each day
- Plot the number of each weather 'condition'. Plot sunny days, partly cloudy days, and rain days. There are several ways to do this.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_3/cville_2017_apr.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/numpy_ndarrays.md,"NumPy is very powerful and our introduction will barely touch on its most important features.
Arrays
An array is an ordered data structure that contains elements all of the same type. NumPy introduces a new Python data type called the Ndarray (n-dimensional array).  Like all ordered structures in Python, each element is addressed by integer indices.  An array has one or more dimensions, which NumPy generally calls axes.  The bounds are the lowest and highest indexes.  The lower bound of an Ndarray is always 0. The rank is the number of dimensions.  Arrays should be regarded as fixed size once they are set up. NumPy arrays are mutable, so elements may be changed in place.
Unlike most mutable Python data types, arrays must be initialized before they can be addressed.  Several methods are available.

Convert from list 
A=numpy.array([x,y,z])
Size and initialize to all zeros.  The dimensions must be in a tuple.  There is no explicit maximum rank, but memory can be rapidly exhausted with large arrays.
A=numpy.zeros((10,10))
Size and initialize to arbitrary contents of memory 
A=numpy.empty(100) 
Size and initialize to ones 
A=numpy.ones((2,3,4))
Identity matrix (NxN but only N is declared)
A=numpy.eye(100) 
Initialize to all zeros, same shape as a pre-existing array 
B=numpy.zeros_like(A)
Initialize to random numbers in the half-open interval [0.0, 1.0)
R=numpy.random.random((4,5,7))

There are other functions that can be used to initialize but these are among the most common.
Array Attributes
A Ndarray is an object with attributes that can be accessed.
```no-highlight



A=np.random.random((10,10))
print(A.shape)
(10,10)
print(A.size)
100
print(A.ndim)
2
print(A.dtype)
dtype('float64')
``
Theshapeis a tuple that contains the extents of each of the dimensions.  Thesizeis the total number of elements in the array.  Thendim` is the number of dimensions (rank) of the array.



Ndarray Types
Python is typed by inference, meaning that it uses context to determine type, so normally we do not declare a variable's type, but Ndarrays may and sometimes must be declared with a type.  NumPy supports more types than base Python, including single-precision floating-point (float32). Unless otherwise specified, the type will be taken from the type of the initial elements if any are present.  If they are not, the default is float (double precision) but it can be modified with the dtype keyword argument to the constructor.
To save some typing, we'll use import numpy as np to import numpy and np to reference it.
python
import numpy as np
Z=np.zeros((3,4),dtype='complex')
M=np.array([True,True,False,False])
Be sure to note the differences between
python
A=np.zeros((10,10))
IM=np.zeros((10,10),dtype='int')
mask=np.zeros((10,10),dtype='bool')
We can explicitly declare multidimensional arrays as well.
python
C=np.array([[1,2,3], [4,5,6]], dtype=float)
print(C)
[[ 1.  2.  3.]
 [ 4.  5.  6.]]
However, this is not very practical for large arrays.  If we can declare an array as a linear sequence of numbers, we can use the built-in function arange.  The syntax is similar to range but it can take arguments of any numerical type, and it returns a Ndarray.
python
V=np.arange(10,30,5)  # start,end,stride 
print(V)
[10 15 20 25]
String Arrays
NumPy isn't really aimed at handling strings, but sometimes we need an array of strings.  In NumPy the 'str' type is more like a fixed-size array of characters.
The default length is one character.
```no-highlight



import numpy as np
Sa=np.zeros((4),dtype='str')
Sa
array(['', '', '', ''], dtype='<U1')
Sa[0]='hello'
Sa
array(['h', '', '', ''], dtype='<U1')
Sa2=np.array(['abc','def','ghi'])
Sa2[1]='define'
Sa2
array(['abc', 'def', 'ghi'], dtype='<U3')
```



Often, fixed-length strings are adequate, and they are faster than arbitrary-length strings, but if we really must create an array with variable-length strings we can use dtype='object'
```no-highlight



Sa3=np.array(['abc','def','ghi'],dtype='object')
Sa3[1]='define'
Sa3
array(['abc', 'define', 'ghi'], dtype=object)
```



Exercise
Use the zeros function for the following:

Initialize an array A1 of rank 1 with size 4 and type double.
Initialize an array IU of rank 1 with size 4 and type integer.
Initialize an array M1 of rank 1 with size 4 and type Boolean.
Print each of the arrays you just created.
Initialize a rank-3 array to arbitrary elements.

Print each array."
rc-learning-fork/content/courses/python-introduction/packages_environments.md,"Managing Packages with Conda and Mamba
Conda is a package manager from the developers of Anaconda.  It is free, but if you use a conda supplied by Anaconda, please pay attention to their licensing terms.  We recommend Miniforge, which provides its own build of conda, as well as an alternative called mamba.  Mamba is for most purposes a drop-in replacement for conda.  It is generally faster than conda.  Miniforge will draw packages from the conda-forge channel, which is a collection of community-built and maintained packages that are free for use.
On the UVA HPC system, please use conda rather than mamba to create environments.  Once an environment has been created, mamba can be used to install packages.  On a personal system, either conda or mamba may be used to create environments.
Conda or mamba can be used from a command line.  In Linux and Mac OS, the terminal can be used for this.  In Windows, use the Miniforge Prompt, which can be accessed through the Apps menu in the Miniforge folder, since it has the correct paths preset.  
To update a package, type
python
mamba update package
You can also install packages with the mamba or conda command line.
python
mamba install newpackage
Many more options are available.  
Environments
When you use conda or mamba, you always have an environment; the one with which you start is called base.  An environment is a ""bundle"" of a Python version, which need not be the same as your base, along with a set of packages installed against that version.  Only one environment can be active at a time (for Linux users, in a given shell) but environments can be activated and deactivated at will.  
A common use for conda/mamba environments is to create ""sandboxes"" to avoid duplication or to use different versions of Python.  For example, Xarray is a general-purpose package but it is particularly aimed at geophysical applications.  Suppose you wish to use it only for a particular application (a homework problem, for example) but do not wish to install it into your main environment.  To create a new environment for your Xarray project, start the terminal or Miniforge prompt.
From the command line, run
bash
conda create --name geodat --python=3.11
or
bash
mamba create --name geodat --python=3.11
Python must be explicitly included and its version must be specified.  To switch to the new environment, from the command line run 
bash
conda activate geodat
or
bash
mamba activate geodat
On the UVA HPC system, please use
bash
source activate geodat
Once activated, run
mamba install xarray
along with other packages you may wish to include.  If you wish to use Jupyterlab and/or Spyder, you must install those as well.
Conda's user guide describes conda's capabilities.
The Conda cheat sheet is handy.
Documentation for Mamba is here In the Mamba documentation, ""CLI"" stands for ""command-line interface"" and ""CI"" stands for ""continuous integration,"" which is an approach to software development.)
Keep in mind that each environment basically copies files into distinct folders, so creating many of them can consume disk storage space rapidly.
Pip
Pip is different package installer not connected directly to conda or mamba.  It can be used in any conda environments, but conda/mamba will not be aware of packages installed with pip.  However, not all packages or versions are available through conda so pip is useful.  It is command-line based. Miniforge provides a pip executable for cases where conda/mamba cannot be used or the user would prefer pip.  
For most packages it is sufficient to type
bash
pip install pkg
Another option is
bash
python -m pip install pkg
The latter form ensures that the pip is compatible with the Python in use.
As is the case for conda or mamba, the exact name of the package must be known.
Documentation for pip is at the official site."
rc-learning-fork/content/courses/python-introduction/conditionals.md,"Conditionals are how scripts choose different paths, depending on the circumstances.
Branching
When a program makes a decision, it is said to branch.  A program may be regarded as a tree with one root and many branches.  Executing the program causes the branches to be followed in a particular order.
Python If/Else
Most languages branch with some variant of 
if (something that evaluates to True or False) do something
else if (comparison) do some other things
else do some default things
Only the if is required; the other statements are optional.  The specific syntax in Python is
python
if some_condition:
    code block
elif another_condition:
    code block
else:
    code block
Observe the colons and the indentation.  To terminate the conditional, return to the same indentation level as the initial if.
If the ""code block"" consists of a single statement, it is permissible to write it on the same line as the if or elif
python
if x==0: z=0
Conditionals may be nested 
python
if some_condition:
    if another_condition:
        do_something
    else:
        do_something_else
elif condition2:
    if something:
        do_work
Some languages have a ""case"" or ""switch"" statement for a long sequence of options.  Python has no such construct so we use a series of elifs.
python
if cond1:
    block1
elif cond2:
    block2
elif cond3:
    block3
elif cond4:
    block4
else:
    final choice
The ""condition"" must be an expression that evaluates to True or False; that is, it must be a Boolean variable or expression.  Boolean expressions are formed from other types of variables with conditional operators.
Exercise
The Body Mass Index is a widely-used number to classify body shapes.  The formula in Imperial units (pounds, inches) is
python
BMI=weight*703.1/height**2
In metric units (kg, m) the formula is
python
BMI=weight/height**2
The categories are as follows:
{{< table >}}
|  Range   |  Category   |
|----------|-------------|
|Under 18.5| underweight |
|18.5 to 25| normal      |
|over 25 to 30| overweight |
|over 30 to 35| obese class I |
|over 35 to 40| obese class II |
|over 40 to 45| obese class III |
|over 45: obese | class IV (morbidly obese) |
{{< /table >}}
Using whichever unit system you prefer, write some code to assign the weight and height, compute the number, and determine its classification.  Assign your own weight and height.  Try a few others.  Use an online calculator to check your results.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/simple_bmi.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/conditionals_and_loops.md,"Now that we understand the basic data types, we can begin to study the fundamental programming constructs that all programs will use.  The two most important of these are conditionals and loops.  Almost every script we write will require one or both of these.
Code Blocks
Both loops and conditionals, as well as many other constructs we will encounter later, work with code blocks.  A code block is a group of statements that function logically as a single statement.  
Most of the time, a block is introduced by a colon (:) following a declaration or expression.  In Python these blocks are indicated by the indentation level. You may indent each block by however many spaces you wish (3 or 4 is usually recommended), but each block level must be indented by exactly the same number.  Do not use tabs. The block is terminated by returning to the previous indentation level; there are no special characters or statements to delineate a block.
Many editors, including Spyder, will automatically indent the next statement to the same level as the previous one.  You escape to an outer level with the backspace or some other key.  Spyder also provides Indent and Unindent options in its Edit menu.  These are extremely convenient for Python since you need only select lines, then click indent or unindent to create or move a code block level.
Examples
```python
if x==20:
    x=99
    if (x>=30):
        for i in range(x):
            j=i+x
print(j)
for i in range(1,101,10):
    x=float(i)**2
    print(i,x)
z=30.
```"
rc-learning-fork/content/courses/python-introduction/oop.md,"Object-oriented programming (OOP) is an approach to structuring a program.  The code is organized around coherent structures that contain both attributes (variables) and behaviors (procedures).  For example, consider a program for a human resources department.  They are concerned with employees.  An employee has several attributes, including such things as a name, an employee ID number, a salary, and possibly other information such as home address and start date.  If we did not have something like an object, we would have to represent a group of employees with a list or similar structure.  Each element of the list would have to contain all the information about one employee, so would have to be a list itself.  If we wanted to work with a particular employee's record, we would have to determine the index, then make sure to use it consistently across any other lists or arrays we might be using.  With the information about each employee bundled into an object, we can create a variable representing an employee, and load all the pertinent data into it.  
Moreover, employees have behaviors (or behaviors can be imposed on them).  They can be given salary raises (or perhaps cuts).  They can change their status.  They can be hired, fired, or they can quit.  We could write procedures to manage those behaviors, and include them in the object so that each employee would carry out the behaviors and the relevant attributes would be properly updated for the correct employee.  We could easily introduce new attributes or behaviors without needing to rewrite code to shuffle list elements.  We can more easily check our code dealing with an ""employee"" since everything relevant to the employee is encapsulated in the corresponding object.
Objects are another software paradigm that can be used to implement separation of concerns.
A longer discussion of OOP in Python is available here.
""The great thing about Object Oriented code is that it can make small, simple problems look like large, complex ones.""
-Anonymous"
rc-learning-fork/content/courses/python-introduction/dictionaries.md,"Dictionaries are mappings.  Elements are accessed by a key which may be of any immutable type. Tuples may be used as a key, but in that case no elements of the tuple may be mutable.  Keys must be unique (no duplication).
The key corresponds to a value.  The value may be of any type, including mutable types such as lists. The dictionary consists of all key-value pairs.  Dictionaries themselves are mutable and may be of any length up to the limits of the system.  Dictionaries can be nested, i.e. the value may itself be a dictionary.
Dictionaries are denoted by curly braces {}.  A key-value pair is separated by a colon :.
Creating Dictionaries

Declaring an empty dictionary
D={}
Note the similarity to an empty list, but we use curly braces.
Enumerating the key-value pairs:
D={""Alice"":2341, ""Beth"":9102, ""Cecil"":3258}
Another option to create a dictionary with initial entries takes the form of a list of tuples passed to the constructor function dict():
D=dict([(""Alice"",2341),(""Beth"",9102),(""Cecil"",3258)])


Adding an entry:
D[""Dan""]=5837
If the key \""Dan\"" is not present, it will be created and the specified value assigned.  If it is present already in the dictionary, the value will be replaced.

Dictionary Operations

Length of the dictionary:
len(D)
The length is the number of key-value pairs, i.e. the number of keys.
Delete the key-value entry:
del D[k]
Delete all entries:
D.clear()
Generate an iterator of keys:
keys=list(D.keys())
Omit the list constructor if all you need is an iterator.
for k in D.keys():
In a for loop the keys() can be omitted
for k in D:


Generate an iterator of values
D.values()
Analogous to enumerate for lists, we may use items to loop through a dictionary and obtain both the key and value:
for k,v in D.items():
Copy a dictionary:
D2=D1.copy()
Equating D2=D1 will not copy the dictionary, but as for other mutable types, will just make an alias.



Quiz
What is the difference between
python
data=[]
data[0]=12
data[1]=4
and 
python
data={}
data[0]=12
data[1]=4
Are both correct? Are either correct?
More Key Handling Methods
D[key] alone results in an error if the key is not in the dictionary.  If you must attempt a lookup and do not know whether the key is present, use get instead.  
python
D.get(key)
This returns None by default if the key is not found.  It can be set to return a value with an optional argument.
python
D.get(key,0)
The in operator may also be used 
python
if key in D
or
python
if key not in D
depending on what you want to do.
Exercises
Type into Spyder or JupyterLab and run
{{< code-download file=""/courses/python-introduction/exercises/dictionary_demo.py"" lang=""python"" >}}
Use the following lists to create a dictionary teams where the key is taken from the first list with the value from the second list.  Use your dictionary to print the team located in Chicago.  Note that there are two teams in Los Angeles, so you must make the values a list, most of which will have only one element. Hint: for neat printing, use the join function to create a string from a list.
{{< code-snippet  >}}
cities=[""Boston"",""Brooklyn"",""New York"",""Philadelphia"",""Toronto"",
        ""San Francisco"",""Los Angeles"",""Los Angeles"",""Phoenix"",
        ""Sacramento"",""Chicago"",""Cleveland"",""Detroit"",""Indiana"",
        ""Milwaukee"",""Dallas"",""Houston"",""Memphis"",""New Orleans"",
        ""San Antonio"",""Atlanta"",""Charlotte"",""Miami"",""Orlando"",
        ""Washington"",""Denver"",""Minnesota"",""Oklahoma City"",""Portland"" 
        ""Salt Lake City""]
mascots=[""Celtics"",""Nets"",""Knicks"",""76ers"",""Raptors"",""Golden State Warriors"",
         ""Clippers"",""Lakers"",""Suns"",""Kings"",""Bulls"",""Cavaliers"",""Pistons"",
         ""Pacers"",""Bucks"",""Mavericks"",""Rockets"",""Grizzlies"",""Hornets"",""Spurs"",
         ""Hawks"",""Bobcats"",""Heat"",""Magic"",""Wizards"",""Nuggets"",""Timberwolves"",
         ""Thunder"",""Trail Blazers"",""Jazz""]
{{< /code-snippet >}}
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/create_dictionary.py"" lang=""python"" >}}
{{< /spoiler >}}
Resources
Documentation is here."
rc-learning-fork/content/courses/python-introduction/scipy.md,"The SciPy library is part of the ""SciPy ecosystem"" that also includes NumPy, Sympy, and Pandas.  We will not discuss Sympy but it is a well-developed computer algebra system (CAS) that is also incorporated into several other packages such as SageMath.
Its homepage at www.scipy.org has details and documentation.  
Importing SciPy Packages
Importing from scipy alone brings only the base packages, which provide a fairly minimal set of tools.  Individual packages must be imported explicitly.
python
from scipy import linalg, optimize
For an imported package, type help(name) to see a summary of available functions.
python
help(linalg)
Exercises
Type in the following code to solve a linear system of equations.
python
import numpy as np
from scipy import linalg
A = np.array([[1, 2], [3, 4]])
b = np.array([[5], [6]])
c=linalg.solve(A, b)
print(c)
See examples of simple image processing here and here and here as well as many other sites.
Use the SciPy ndimage to tint a photograph.
1. Import numpy and matplotlab.pyplot. Import the scipy misc package
2. Extract the sample picture ""face"" (a raccoon).
3. Use matplotlab function imshow to look at the picture.  You may still need to use plt.show().
4. Use the NumPy array(img) function to get a numpy array. Specify dtype='float'.
4. Get the shape of the image array.
5. Tone down the blue by .9 and the red by .95.  The order of color channels in the image is RBG.
6. The multiplication will use floats which are not permitted for an image (the pixel values must be integers between 0 and 255).  Store the tinted array into a new array with dtype='int'.  You can do this in one step if you think about it.
7. Show the new image.  If you use plt.figure() and only one plt.show() you can view the images side by side.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/scipy_img.py"" lang=""python"" >}}
{{< /spoiler >}}
Resources
The SciPy reference guide is an invaluable resource for SciPy usage."
rc-learning-fork/content/courses/python-introduction/functions_and_modules.md,"Functions and modules are self-contained units of code.  They enable us to isolate chunks of code into small packages.  They are important components of software engineering.  One principle for designing programs is separation of concerns.  A concern, in this context, is a well-defined task or related set of tasks.  Separation of concerns makes the program easier to modify and maintain."
rc-learning-fork/content/courses/python-introduction/resources.md,"Resources
This concludes our overview of programming in Python.  The high popularity of Python has ensured that many online resources are available at all skill levels, from beginners to advanced programmers.  One sample listing is at Fullstack Python.  Many online resources emphasize Web programming, but some compilations more specific to sciences and data analytics include Python for Social Sciences, Python Programming for the Humanities, Python for Scientists and Engineers (this is a free online book), and others. A very good list, if you can get past the 1990's look of the site and the occasional dead link, is Python for Scientific Computing.  Note that you may see references to continuum.io; this was the original name of the company that now goes by the name of their product, Anaconda.
Other online tutorials include Tutorialspoint and DataCamp.
Many, many online resources refer to Anaconda, but Anaconda's licensing terms have changed, so be sure that you read and understand them before downloading and installing it. We have recommended Miniforge in this tutorial because it remains free for all users.
Happy programming!
UVA Specific Resources
Because you are a part of the UVA community, you have many places to go for help at your disposal. These are here for your benefit so please take advantage of them if you need assistance.
Research Computing - Research Computing is a support team at the University of Virginia whose mission is to empower researchers to achieve more through the use of cutting-edge computational resources. We strive to create innovative solutions for researchers who need help solving complex optimization, parallelization, workflow, and data analysis issues. Our strategy is to build and maintain the University's best computing platforms while educating the next generation of researchers on the power and utility of advanced computing.

User Support Form
Workshops & Courses Material
Office Hours
High Performance Computing

Research Data Services - Based in UVA's Brown Science & Engineering Library (Clark Hall), Research Data Services supports all phases of the research project through instructional workshops, research project consultations, or deeper collaborations.

Research Software Support
StatLab: Data Analytics Support
Workshop Series
Data Discovery
Research Data Management

Specific programs and packages documentation


Python Documentation - The Home of python's documentation, background info, events, etc.


Miniforge


Anaconda - View the Anaconda project's homepage.


Conda - A package manager for Python.


Pip - The python package installer. Pip Installs Packages

"
rc-learning-fork/content/courses/python-introduction/exceptions.md,"Many situations can result in an error.  

An attempt to read a file that isn't present.
An attempt to cast a string to a number when the string does not represent a number.
An index out of bounds.
A key requested is not in the dictionary.

All these conditions (and many more) are called exceptions.  If you do not handle them, the interpreter will, and it will just stop with some error message.  We can prepare for failure by catching exceptions ourselves.
Catching Exceptions
The interpreter throws an exception so we can catch it.  We can do this with a try except block.  
python
try:
    fin=open(""datafile.csv"",""r"")
except IOError:
    print(""Unable to open file."")
This stops on the named exception IOError.  Any other exception will not be caught. 
It is not necessary to specify exceptions if you are unsure of the correct name; the following will be executed for any exception.
python
try: 
    something
except: 
    print(""Error"")
More processing than we have shown is usually required, of course.  For example, if a file cannot be opened, we may choose to exit the script.
python
try: 
    fin=open(""datafile.csv"",""r"")
except IOError:
    print(""Unable to open file."")
    exit()
The interpreter provides a large number of built-in exceptions.  (See the documentation for a complete list.)  Some of the more commonly encountered are 
* EOFError
* IOError
* KeyboardInterrupt
* IndexError
* IKeyError
* NameError
* NotImplementedError
* TypeError
* ValueError
* ZeroDivisionError
You can provide more than one except for the same try
python
try:
   input=int(input(""Please enter an integer:""))
except EOFError:
   print(""You did not enter anything"")
except ValueError:
   print(""You did not enter an integer"")
If you want your script to perform some action upon passing the try, use else
python
try:
    fout=open(""outputfile.csv"",""w"")
except IOError:
    print(""Unable to open file"")
    exit()
else:
    print(""File successfully opened"")
If you have code that should be executed regardless of whether the try passes or fails, use finally:
python
try:
   fin=datafile.read()
except IOError:
   print(""Unable to read "",datafile)
finally:
   fin.close()
You cannot use an else in any try clause that contains a finally block.
With/As
In many case, especially those having to do with file operations, the with and as operators can replace the try/except/finally block. The following will close the file whether the operation was successful or not.
python
with open('myfile','w') as f:
    for line in f:
        print(line)
Exercise
Download the file numbers.csv. Write a program to read it and extract the columns as x,y.  It has a one-line header.  What problem did you encounter?  Use an exception to exclude the line with the error.  
Optionally, add exception handling to the file operations (opening, reading, etc.)
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/exceptions_example.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/project-set-2.md,"Input/output, strings, dictionaries, functions, modules.
A. Write a program that obtains the sum of the numbers from 1 to some specified positive (>0) integer N. Request the value of N as console input from the user. Your program should catch user inputs that cannot be converted to integers greater than 0.  Do not use the Gauss formula, do this via “brute force.”
Print the number, its sum as obtained from your work, and the correct answer from the Gauss formula sum(N)=N(N+1)/2.  Test your program with N=1, N=25, N=1000.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_2/sum_to_N.py"" lang=""python"" >}}
{{< /spoiler >}}
B. Convert your code to sum from 1 to N into a function.  Write a main program that requests a number M from the user, then prints a table of the integers from 1 to M and their corresponding sums.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_2/sums_table.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 7
Download the file us-state-capitals.csv.  Write a program that will read this file and create a dictionary with the state name as the key and the capital name as the value.  Using your dictionary, print the capitals of Arkansas, Virginia, and Wyoming.
Again using your dictionary, generate a list of all state capitals that begin with the letter 'A'.  Use the list to create a string consisting of these city names separated by a semicolon ;   Open a new file capitals-with-a.txt and write this string to it.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_2/state_capitals.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 8
Write a program to analyze a DNA sequence file.  The program should contain at minimum a function countBases which takes a sequence consisting of letters ATCG, with each letter the symbol for a base, and returns a dictionary where the key is the letter and the value is the number of times it appears in the sequence.
The program should contain another function printBaseComposition which takes the dictionary and prints a table of the proportions of each base, e.g.
-  A : 0.25
-  T : 0.25
-  C : 0.25
-  G : 0.25
Use the file HIV.txt to test your program.  You can look at the file in a text editor.  It consists of a label followed by a space followed by a sequence, for each sequence in the file.
Hints: read each sequence as a line.  Split the line on whitespace (rstrip first) and throw out the 0th element.
Copy the next element of the list into a string, and use substrings to extract each letter.  Build your dictionary as you step through the string.  Repeat for the next line until you have read all the lines.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_2/DNA.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 9
In the early 2000’s an “urban legend” circulated that one could read text in which all letters except the first and last were scrambled.  For example:

Aoccdrnig to rscheearch at an Elingsh uinervtisy, it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer is at the rghit pclae.

Write a program to scramble a passage input from a file.  Print the result to a file with the same name as the original but a suffix _scrambled added
(so if the original was Example.txt it will be Example_scrambled.txt).
Look at the scrambled file first—can you read it?
- First write a scramble_word function, then a scramble_line function, and finally a function that writes the lines to the new file.
- Your main() routine will request from the user the name of the input file, then call the printing function, which will call the scramble_line function that will in turn call the scramble_word function.
This is an example of how we divide up work into separate “chunks” or
“concerns.” Internal punctuation (apostrophes) can be scrambled, but leave any
punctuation such as periods, question marks, etc., at the end of a word in
place.
Look up the documentation for the random module to find some useful functions.  You can also import the string module to obtain a list of punctuation marks.

Hint: since you cannot overwrite strings you will need to convert them to a list and back again. Use Example.txt as your sample input.
FYI this is an “urban legend” because: firstly, no such research was ever conducted at any university, and secondly it is true only for very practiced readers of English and even then only for familiar words that are easy to recognize in context.

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_2/scramble_text.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/more_loops.md,"Leaving Early
To exit a loop prior to its normal termination, use the break statement.  
If break is executed, any else clause will not be executed.
Break can exit only from the loop level in which it occurs.  In Python this is the indentation level.
To skip the rest of the loop instructions and go to the next cycle, use continue. Similarly to break, it skips only the rest of the statements at its own level.
python
x=1.
while x>0:
    x+=1.
    if x>=1000.0: break 
    if x<100.0: continue
    x+=20.
    print(x)
Now we can better understand the purpose of the else clause.  One application could be to provide a warning for cases where a loop terminating normally may indicate a failure.  For example, if we were iterating on an algorithm that converges to an answer but may go astray, we can set a limit on the maximum number of iterations. In this example, optimize is a function that we invoke.  It may be in another package or it may be something we wrote.  The variable f stands for the function we are optimizing.  Therefore this code fragment is incomplete and cannot be run as is; it is an example of how else works.
python
max_iter=1000000000
tol=1.e-14
iter=0
while iter<max_iter:
    x_new=optimize(f,x_old)
    iter+-1
    if abs(x_new-x_old)<tol:
        break
    x_old=x_new
else:
    print(""Warning: max_iter exceeded, solution may not be valid."")
Repeat/Until
A while loop is equivalent to
python
while True:
    if not condition:
        break 
    codeblock
The while always tests at the top of the loop.  If we wish to test elsewhere we can use break to accomplish this.  If we test at the bottom of the loop, the pattern is often called repeat/until.
python
while True:
   codeblock
   if condition: break
Example:
python
x=1.
while True:
    z=x-1
    x+=1
    if x>10: break
Nested Loops
We can write loops within loops.  The outer loop variable remains fixed while the inner one goes through its iterator; then the outer one takes the next value and the entire inner loop is repeated.
python
for i in range(5):
    for j in range(10):
        print(i,j)
Reinitializing
In nested loops, if we need to recompute something we often need to reinitialize a variable.  Examine the difference between
{{< code-snippet >}}
s=0.
for i in range(10):
    for j in range(15):
        s=s+i+j
print(s)
{{< /code-snippet >}}
and 
{{< code-snippet >}}
for i in range(10):
    s=0 
    for j in range(10):
        s=s+i+j
print(s)
{{< /code-snippet >}}"
rc-learning-fork/content/courses/python-introduction/ides.md,"Our first example of an Integrated Development Environment, or IDE, is Spyder.  Spyder is aimed specifically at Python.  It is well suited to developing longer, more modular programs.  
The authors generally recommend the standalone version referenced earlier.  If using this application, always start it from your operating system's launcher.  Remember that each environment must include the ipython package for it to be recognized by this application.
In an HPC system, or if not using the standalone version, you must activate the environment and spyder must be installed. Go to the Miniforge Prompt (Windows) or a terminal (Linux or MacOS) and type
bash
spyder
It may take a while to open (watch the lower left of the Navigator).  Once it starts, you will see a layout with an editor pane on the left, an explorer pane at the top right, and an iPython console on the lower right.  This arrangement can be customized but we will use the default for our examples, but with a different theme. 
The first time you open Spyder it will offer a tour of its features. We recommend you go through it.  You can change the appearance, layout, and other options from Tools->Preferences.  Spyder's full documentation is here.
Type code into the editor.  The explorer window can show files, variable values, and other useful information.  The iPython console is a frontend to the Python interpreter itself.  It is comparable to a cell in JupyterLab.
{{< figure src=""/courses/python-introduction/imgs/Spyder.png"" caption=""Default layout for Spyder with a dark-on-light theme."" >}}
Spyder automatically inserts a few lines at the top of the editor pane for each new script.  The first line, starting with the hash mark (#) is a comment and is ignored by the interpreter. After that are some lines that start and end with triple double quotes ("""" """" """").  This is a special kind of string called a documentation string or docstring.  We will discuss comments and docstrings later. Start any of your scripting lines below the docstring.
If you are using Python 2.7, add the line from future import print_function immediately after the triple-quoted section at the top of the editor pane.
Type
python
print(""Hello World"")
into the editor pane. Anything entered into the editor pane is a script and you must run it as a whole in order for the command to be carried out. Click the green arrow in the top ribbon to run the current script.
You can also type commands directly into the iPython console, located by default in the lower right pane. Just as with JupyterLab, if you type an expression its value will be printed.
Type into the console:
python
In  [1]: x=5
In  [2]: y=7
In  [3]: x+y
Out [3]: 12
This is not the case for expressions typed into the editor pane.
python
x=5
y=7
x+y
We will see nothing if we run this as a script.  You must add a print command to see any output as well as running the script.  Expression evaluation is only available directly from an interpreter console.
In the iPython console we can also use up-down arrow keys to scroll through our commands, and right-left arrows to edit them.
Example
We can see some of these features in action by creating a simple plot. After the triple double quotes in an ""untitled"" editor tab, type
python
import matplotlib.pylab as plt
First we see a yellow triangle, indicating a syntax problem -- in this case, plt is imported but not used.  We ignore this warning since we will be using it.  As we type
python
x=plt.
we see the editor show us our choices from the pylab package. We can select one or keep typing.  We type
python
x=plt.linsp
to narrow it down further.  That leads us to
python
x=plt.linspace
The editor then pops up a box with the arguments required by linspace.  Finally we type inside the parentheses
python
-1.*plt.pi,plt.pi,100
for a final result of
python
x=plt.linspace(-1.*plt.pi,plt.pi,100,endpoint=True)
(Note that we are using a different choice from the ""pyplot"" package in this example. As we will learn, most packages contain many functions, and the user must learn to go through the documentation.)
After this we type
python
y=plt.sin(x)
plt.plot(x,y)
You must save a file before you can run it.  Go the File menu, Save As, and name it sine.py  Be sure to save it to the project folder you created earlier, which will require navigating there in the file-browser window.  
When we run this code, we see the plot appear in the Plots tab of the upper-right pane.  We can right-click on the image to bring up a menu that allows us to save the plot.
{{< figure src=""/courses/python-introduction/imgs/SpyderSine.png"" caption=""A simple plot in Spyder."" >}}
The Variable Explorer
The Variable Explorer allows us to show the values of variables in our programs.  It is particularly helpful for looking at a group of values (an array). We can examine a variable more closely by double-clicking its name to bring up a new window. In this window you can change the number of decimal places printed by clicking Format and typing in an expression such as %.3f for three decimal places.  The Variable Explorer also includes icons for saving, refreshing, or importing data to our workspace.
To clear all values in the workspace, type at the iPython console
python
%reset
The same effect can be achieved through the trash-can icon in the Variable Explorer.
Now re-run your sine-plotting code and observe how the variables acquire values.  Change sin to cos and rerun your script.
Environments in Spyder
If you are running Spyder from within an environment, it should automatically set that interpreter as its ""internal"" interpreter.  However, if you have installed a standalone spyder, it will generally be necessary to set the Python interpreter to one of your environments, even if it a base environment, since the standalone bundle includes only the Python packages required for Spyder itself. 
To activate a conda/mamba environment in Spyder, go to the Tools->Preferences->Python Interpreter menu.  Select the ""Use the following interpreter"" button, then click the down arrow for the dropdown.  It will see your environments as long as they are in a standard location and include the necessary support packages.  Choose an environment by highlighting and clicking the path to the interpeter located in that environment; this is equivalent to activating it.
{{< figure src=""/courses/python-introduction/imgs/Spyder_envs.png"" width=500px >}}
Other IDEs
Other popular IDEs for Python include the ""fremium"" PyCharm.  The ""Community"" (free) edition is adequate for most casual programmer's needs.  Some organizations, such as the UVA Computer Science Department, hold licenses that students or employees can access.
VSCode is another IDE that is very popular.  It is multilanguage and integrates source control with git.  Full support for most languages requires the use of an extension.  Extensions are installed from the Extensions menu.  Click on the stacked-boxes icon, or View->Extensions from the top bar.  We recommend the Python extension supported by Microsoft.  It will install several Python packages for syntax checking and debugging.
The VSCode interface has some similarities to the Spyder editor pane's interface.
{{< figure src=""/courses/python-introduction/imgs/VSCode_python.png"" width=500px >}}
Environments are selected in VSCode in much the same way as in Spyder.  Open the Command Palette with Ctrl-Shift-P.  If an appropriate extension is installed, a Python: Select Interpreter dropdown will be available. 
{{< figure src=""/courses/python-introduction/imgs/VSCode_envs.png"" width=500px >}}
See their documentation for more details."
rc-learning-fork/content/courses/python-introduction/compound_types.md,"﻿---
title: Compound Types
toc: false
type: docs
draft: false
weight: 30
date: ""2020-11-17T00:00:00""
menu:
    python-introduction:

The variables we have seen so far represent one single item each.  If x is a floating-point number, it takes on one value at a time.  Compound types are those for which a single variable represents many elements.  In Python, compound types can be sequences, which can be ordered by integer indexes.  We will focus on sequences here, since they are very widely used.  A few compound types are unordered, such as dictionaries and sets; we will study those later.
Mutability
In Python types are categorized as mutable or immutable.  Immutable types cannot be changed in place, but they can be overwritten.
python
x=1.0
x=2.0
All the types we have seen so far are immutable.  Compound types may be mutable; their elements can be changed in place.  Compound types may also be immutable.  Strings are a compound type but they are immutable.
python
S1=""Hello world""
S1[0:5]=""Goodbye"" #illegal
S1=""Goodbye cruel world."" #legal"
rc-learning-fork/content/courses/python-introduction/setting_up_environment.md,"In this course we will use the free Miniforge distribution of Python and Python packages.  Miniforge is available for Windows, MacOS, and Linux.  Download the latest release version for your operating system from the above page, and follow the instructions to install the base.  Mac users: ""Apple Silicon"" refers to the M chips on newer Macs.  Please be sure to install the standard Miniforge3 and not Miniforge-pypy.
There are two somewhat incompatible versions of Python; version 2.7 is obsolete but still sometimes used.  Version 3 is the supported version.  We will use Python 3, preferably version 3.9 or above.  If you find that you need to use Python 2.7, Miniforge makes it easy to run both versions side-by-side through environments.  At the time of this writing, Miniforge3 installs Python 3.10.4 by default with Release 23.0-0, but will later switch to Python 3.12.0. 
When installing, we strongly recommend staying with the defaults unless you are experienced and very sure you want to make changes. In particular, it is best to install it ""Only for You.""  
On Linux, the installation is a shell script that you must run with
bash
bash ~/Downloads/Miniforge3-Linux-x86_64.sh
Be aware that it will alter your .bashrc or other shell resource file unless you request that it not do so. 
Multi-user Linux systems such as UVA HPC may have Miniforge or other Python distributions preinstalled.  See the instructions for your local system, such as ours. Typically, on a system such as UVA's HPC resource, you must load a module before you can use the distribution. 
On Windows, Miniforge will install a miniforge prompt which will appear in your Apps menu under the Miniforge folder.  It can be used much like a terminal on Linux or Mac.  It will be aware of python, pip, mamba, and conda commands, but unless you add those to your personal PATH environment variable, which is usually not recommended, other CMD or PowerShell windows will not find those commands.
The default installation of Miniforge creates a base environment consisting of a very minimal number of packages.  It uses a package manager to install further packages.  You can choose conda or mamba to install packages.  In general, we recommend mamba as it is usually much faster than conda.
If you wish to install some of the more popular packages into your base environnment, open a terminal (Linux and MacOS) or a Miniforge prompt (Windows).  A good ""starter pack"" would be NumPy, Matplotlib, Pandas, Seaborn, Jupyterlab, and Spyder.  We would also like to upgrade our Python from the 3.10.4 in our current release version. In this example, our python version distributed with miniforge3 is 3.11.  At your prompt type, all on one line,
bash
(base)$mamba install python=3.11 numpy pandas scipy mkl seaborn numba jupyterlab ipython ipywidgets
This will install many more packages than just those requested, since each will have packages on which they depend. 
You can choose to keep your base minimal and set up at least one new environment that contains more packages.  This is highly advisable in general, and will avoide potential library conflicts as your system is updated. For detailed instructions, see the instructions, but to summarize, decide on a name for your environment, let's call it work, then type
bash
conda create -n work python=3.11
conda activate work
mamba install numpy pandas scipy mkl seaborn numba jupyterlab ipython ipywidgets
When done, deactivate the environment
bash
conda deactivate
In order to work with Python scripts, you will need a development environment. We will emphasize Jupyterlab as well as integrated development environments, particularly Spyder.  Miniforge environments do not include either by default, so they must be installed.
bash
conda activate yourenv
mamba install jupyterlab
In an environment such as an HPC cluster, you may need to install spyder into each environment much like jupyter-lab
bash
conda activate yourenv
mamba install spyder
The applications may be started directly from a Miniforge shell
bash
jupyter-lab
This will open a tab on your default browser.
or
bash
spyder
Spyder and JupyterLab Apps
If using a system you control, you may wish to install Jupyterlab and Spyder as standalone applications.  You may need administrator privileges for these installations.
Using a standalone application can save some space and may be more convenient and less prone to conflicts.
For Jupyterlab, install the Jupyterlab Desktop.  The installation method depends on your operating system but should be similar to other software.  
Download the appropriate installer from their site  and follow the instructions to install.  It will insert a launcher into your desktop environment from which it can be started.  This could be the Applications menu for Windows, or the Launcher for Mac OS or Linux.
To use the Jupyterlab Desktop, the jupyterlab package must be installed in the environment.  The nbconvert package is recommended as well.
The Spyder standalone application can be downloaded and installed from its Website. It will attempt to autodetect your operating system and offer the correct version for download, but other versions can be chosen. The Spyder standalone application requires that at least the ipython package must be installed into an environment.  Installing the jupyterlab and nbconvert packages should be adequate to use both applications."
rc-learning-fork/content/courses/python-introduction/polymorphism.md,"Polymorphism means literally ""having many forms.""  In computer science, it is when the same interface can be used for different types.  In most cases the interface is a function name.  Many of the built-in Python functions are polymorphic; the len function can be applied to lists, dictionaries, and strings. Polymorphic functions are often said to be overloaded.  The function's signature is the unique description of the number and type of the arguments, and if relevant, the class to which it belongs.  The signature is the means by which the interpreter determines which version of the function to apply. This is called overload resolution.
Method Overriding
A particular type of polymorphism is subtype polymorphism.  We can define a function in a subclass that has the same name as in the base class, but which applies to instances of the subclass.   This is overriding the method.  
Example
{{< code-download file=""/courses/python-introduction/scripts/zoo.py"" lang=""python"" >}}
Notice that in this code the base class throws a NotImplementedError if it is invoked with an instance of Animal.  In our code, the Animal class is not intended to be used to create instances, and we do not have a base implementation of speak, so we make sure to warn the user of our class.
Since Python is mostly dynamically typed, the correct polymorphic instance is determined  at runtime.
Python uses “duck typing” (if it walks like a duck and quacks like a duck...);  the type is determined dynamically from context.  If your usage does not agree with what the interpreter thinks it should be, it will throw a type exception.
These built-in functions specify whether a given object is an instance of a particular class, or is a subclass of another specified class. All return Booleans.
In Jupyter or Spyder, enter the classes shown above, then in the interpreter window or cell type these lines.  (Recall that >>> is the interpreter prompt; you may see something different.)
```python



print(issubclass(Animal, Canine))
print(issubclass(Canine, Animal))
print(issubclass(Animal, Feline))
print(issubclass(Feline, Canine))
print(issubclass(Feline, Feline))
print(isinstance(zoo[0], Animal))
print(isinstance(zoo[0], Canine))
print(isinstance(zoo[0], Feline))
```



Operator Overloading
Operators are themselves functions and can be overloaded.  In Python the arithmetic operators are already overloaded, since floats and integers are different within the computer.  The addition operator + is also overloaded for other purposes, such as to concatenate strings or lists.
python
print(1+2)
print(1.+2.)
print(""1""+""2"")
We can overload operators in our classes so that we can, say, add two instances of our class.  Of course ""adding"" the instances should make sense. Python defines a number of special methods which are distinguished by having a double underscore before and after the name; for this reason they are sometimes called ""dunders"" or they may be called ""magic methods.""  We have already encountered the __init__ dunder but there are many others.
Example
We would like to create a Point class to define points in a three-dimensional Euclidean space.  Points are added by adding corresponding components; i.e.
$$
p1=(1,2,3),\  p2=(7,8,9),\ p1+p2=(8,10,12)
$$
To implement addition we will use the __add__ dunder.  This dunder will take self as its first argument, another instance of Point as its second, and it must return another instance of the class Point, so we invoke the constructor in the dunder.
{{< code-download file=""/courses/python-introduction/scripts/points.py"" lang=""python"" >}}
We could similarly define subtraction with __sub__.
Exercise
Implement subtraction for points using the rule $x_1-x_2$, $y_1-y_2$, $z_1-z_2).
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/points.py"" lang=""python"" >}}
{{< /spoiler >}}
We'd like to be able to print a Point object in the standard mathematical format $(x,y,z)$.  To allow print to handle our class we overload the __str__ dunder.  The __str__ dunder is used by str, print, and format.  
{{< code-download file=""/courses/python-introduction/scripts/points_printer.py"" lang=""python"" >}}
There are many other dunders we can use in our classes.  Multiplication and division don't make sense for points, but they can be defined with __mul__ and __truediv__ respectively.
If we define the comparison dunders then we can invoke sort on our class instances.
A list of the most widely used magic methods is here.
Resources
A longer discussion of OOP in Python is available here."
rc-learning-fork/content/courses/python-introduction/strings.md,"The string type is widely used in Python.  A string consists of a sequence of characters, even if the sequence consists of a solitary character; Python does not have a distinct ""character"" type.  The string is a compound type and immutable.  The representation of a single character internally in the computer as a sequence of bits is called the encoding.  Individual characters are represented either by the ASCII standard (1 byte per character) or Unicode (2-4 bytes per character).  Strings that are to be treated as Unicode are type unicode rather than string, but otherwise behave similarly.  The default encoding may depend on the operating system but in newer Python versions is usually a standard called utf-8.  UTF-8 can represent over one hundred thousand characters and can embed different scripts within the same text file."
rc-learning-fork/content/courses/python-introduction/numpy_working_with_arrays.md,"Numpy arrays are easy to use and have many properties and built-in functions that make them a good choice for many applications, especially but not exclusively working with numerical data. 
Array Elements and Slices
Each element can be addressed by its index or indices.  As for other ordered sequences, numbering starts from zero.  Indices are enclosed in square brackets.  The dimensions are usually enclosed in a single set of square brackets.
python
A[i,j,k]
Negative indices count from the last element.
python
V[-1]   #last element
A[0,-2] #first in column 0, next to last in column 1.
Slicing Arrays
Subarrays, or slices, are indicated by the colon-separated range operator we have seen several times now.   As usual for Python, the upper bound must be one greater than the intended upper bound, i.e. it is non-inclusive.  We can omit lower and upper bounds if they are the edges; [lb:] extends from lb to the last element, while [:ub] extends from the beginning to ub-1.  The entire axis is expressed as [:].
python
import numpy
A=numpy.zeros((100,100))
B=A[0:11,:]
C=A[S1:E1,S2:E2]
D=A[:,1]  #second column
Pay close attention to negative indices, especially when they are part of a slice specification.
python
V=np.array([0,1,2,3,4])
u=np.arange(25).reshape(5,5)
V[-1]
 4
V[:-1] #note element versus slice
 [0,1,2,3]  
u[:-1,:]
 [[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]
 [15 16 17 18 19]]
More Advanced Indexing
A Ndarray can take as its index an integer or Boolean array.
python
A=np.array([-2.,3.,-8.,-11.,12.,12.,45.,19.])
I=np.array([2,4,5])
valid=A>0
print(valid)
[False  True False False  True  True  True  True]
print(A[I])
[-8.,12.,12.]
print(A[valid])
[  3.  12.  12.  45.  19.]
Axes
NumPy refers to dimensions as axes.  Many NumPy functions take an optional axis argument.  As always, we number axes from zero.
python
a=np.arange(12).reshape(3,4)
print(a)
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]]
print(a.sum())
 66
print(a.sum(0))  # across rows for each column
 [12 15 18 21]
print(a.sum(1))  # across columns for each row
 [ 6 22 38]
In sum and similar methods, such as prod (product), the axis argument is the one to be summed out, that is, eliminated.  In the above examples, the first sum uses all elements, resulting in a single value.  In the examples with an optional axis argument, we reduce the rank by one and the shape of the result removes the summed axis. Starting from a rank-2, $3 \times 4$ array, the sum over axis 0 produces a 4-element rank-1 array.  The sum over axis 1 results in a 3-element rank-1 array.
Array Operations
NumPy provides many functions, including a large selection of mathematical functions, that accept array arguments as well as scalars.  They operate on the arrays elementwise.  The arithmetic operators are also defined to work elementwise on arrays; this is called operator overloading since these operators can work either on ordinary scalar numerical variables or on Ndarrays.
python
T=numpy.ones(4)
Z=3.0*T+numpy.ones_like(T)
I=numpy.array([1,0,0,0])
A=math.pi*I
B=numpy.sin(A)
C=A/B  #remember: elementwise
Array operations and NumPy built-in functions can be many times faster than loops, so they should be utilized whenever possible.
Exercise
Generate an NxM two-dimensional array A with the smallest element 1 and the largest one N*M*incr.  Make sure your values are inclusive and the size of the array is correct.  Use reshape to convert from the one-d arange to the NxM array. N, M, and incr should be variables.
Create another array B the same shape, such that it is 0.25*A+.01.
Create a third array C that is the elementwise sum of A and B.
Create a fourth array D that sums C over the second axis.  Print the shape, size, and the values of D.
Print the first N-2 rows and M-3 columns of B. Be sure B is large enough. Use the shape of B to check that this is true.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/numpy_arrays.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/jupyter.md,"We will begin with JupyterLab. If using the Jupyterlab Desktop, start it from the application launcher of your operating system.  If using it directly from the terminal or miniforge prompt, type
bash
jupyter-lab
For users of the University of Virginia's HPC systems, Jupyterlab can be started through its interactive app on the Open OnDemand interactive login.  Your request will be submitted to the appropriate partition and you can work on a compute node with a full Jupyterlab graphical interface.
If started from a prompt, Jupyterlab will open in your default browser.
{{< figure src=""/courses/python-introduction/imgs/Jupyterlab_start.png"" width=500px >}}
When JupyterLab starts up, it shows tiles corresponding to the available kernels.  Jupyterlab runs code under the control of a kernel. 
you will see a list of your files on the left and three icons to select the mode.  JupyterLab incorporates a Jupyter notebook server as well as a plain Python or iPython console and a simple text editor.  We want to start a Jupyter notebook so click on the Python3 kernel tile.
The Jupyterlab Desktop will start with a splash screen without tiles.
{{< figure src=""/courses/python-introduction/imgs/Jupyterlab-Desktop_start.png"" width=500px >}}
Choose ""Start a New Session"" to choose a kernel.
A tab will open with a text entry area.
{{< figure src=""/courses/python-introduction/imgs/JupyterLabInput.png"" >}}
Your notebook is untitled.  Open the File menu and click Rename.  Name your notebook first_script.ipynb, then click the Rename button.
{{< figure src=""/courses/python-introduction/imgs/JupyterLabRename.png"" caption=""Rename and save your notebook."" >}}
Cells
The blank line with the blinking cursor is a cell.  You can type code into the cell.  After the In[] prompt type print(""Hello"")
To run the cell click the arrowhead in the ribbon at the top of the tab, or type the shift+enter keys together.
Your First Program
If you are using Python 2.7 please begin all your programs with the line
from __future__ import print_function. The symbols surrounding future are double underscores.
Type the following lines into a cell.
python
Numerals=list(range(1,11))
print(Numerals[3])
print(Numerals[:3])
print(Numerals[3:])
print(Numerals[4:6])
Run this cell.  Is the result what you expected?
Now add lines
python
Numerals.extend(list(range(11,21)))
Numerals[3]=11
del Numerals[4]
print(Numerals)
len(Numerals)
Output
When you are working directly at the interpreter, you can type a variable and it will print the value exactly.  This is called expression evaluation.  Using the print function observes any formatting.  
In a new cell type
python
greeting=""Hello World""
hello=greeting[0:5]
greeting2=hello+"" there""
output=greeting2+""\n""*2
The symbol \n stands for ""new line.""  Run this cell.  In another new cell type
python
output
Run this cell, then in another cell, enter and run the line
python
print(output)
Plotting in Jupyterlab
Type
python
import matplotlib.pylab as plt
Run the cell. In a new cell type
python
x=plt.arange(-1.,1.1,.1)
y=1./plt.sqrt(x**2+1)
Execute the cell.  In the next cell type
python
plt.plot(x,y)
Text Editor
JupyterLab includes a simple text editor you can use to create files.  In the upper left of your JupyterLab tab, click + to start the launcher. Choose the text file tile. Type
python
def hello():
    print(""Hello"")
    return None
Be sure to indent lines exactly as shown. 
From the File menu choose Rename Text. Name the file hello_func.py.  By default, files will be saved into the folder in which JupyterLab is working. The default is your ""User"" directory.  
Saving the file with the .py extension should automatically enable syntax coloring.  Syntax coloring marks different constructs with different font colors. It is very helpful and all modern technical editors provide it for most programming languages.
After saving the file, return to your Jupyter notebook page and type
python
import hello_func
Run the cell, then in a new cell type and run
python
hello_func.hello()
Exporting your Notebook
Exporting to a Script
You can export embedded text in your notebook into a script.  First make sure your notebook has a name.  If you have not named your current notebook yet, call it first_script.ipynb.  From the Notebook menu find Save and Export Notebook As->Executable Script.  JupyterLab will default to the Downloads folder; move it to a location of your choice.  You can make a new directory for your Python scripts if you wish.
Exporting to Other Formats
If you have installed miniforge on your local computer, other export options are available to you.  PDF, HTML, and Markdown are popular formats.  These are not all available for Open OnDemand users due to the need for certain translation software, but exporting can be done from the command line.  See the documentation for more information.  On your personal system, you will need to install the nbconvert package along with jupyterlab.
Managing Environments
If started from a prompt within an environment, browser-based Jupyterlab should use the active environment by default.  
If started from a Jupyterlab Desktop application in a different environment, you can restart the session in a different environment. Find the ""switch"" icon next to the name of your current environment at the top right. Click on it, and a list will be presented.  Select a different location to restart the session.
{{< figure src=""/courses/python-introduction/imgs/Jupyterlab-Desktop_switch_env.png"" width=500px >}}
You can also create a new environment within the Jupyterlab Desktop.  From the current environment at the top right of the window, click the ""menu"" icon.  
{{< figure src=""/courses/python-introduction/imgs/Jupyterlab-Desktop_manage_envs.png"" width=500px >}}
This will bring up a list of the available environments, with options to add another or to create a new one.
{{< figure src=""/courses/python-introduction/imgs/Jupyterlab-Desktop_envs.png"" width=500px >}}
For more information about Jupyterlab Desktop, see their User Guide.
Kernels
Jupyter can use multiple kernels, though each notebook must be associated with a kernel while it is running.  To create a kernel, first install the ipykernel package into any environment you wish to set up. 
Activate your environment, then type
```bash
(myenv)$mamba install ipykernel
or conda install
```
Now, at the terminal or Miniforge prompt, run
bash
python -m ipykernel install --name python-3.12 --display-name ""Python 3.12"" --user
You can choose any name and display names, but generally the name should not contain spaces.  The display name can be any valid string and will be the identifier of the kernel shown on the startup page in Web-based Jupyter. 
{{< figure src=""/courses/python-introduction/imgs/Jupyterlab_new_kernel.png"" width=500px >}}
You can change kernels used by a notebook from the Jupyterlab ""Switch Kernel"" menu, located in the upper right of the tab ribbon. Click the name of the currently active kernel. A dropdown will appear and you can change the kernel.  The apperance is similar in Jupyterlab Desktop, the main difference between the two being that the Jupyterlab Desktop has the ""Switch Environment"" menu above the tab ribbon, whereas directly Web-based Jupyterlab does not.
{{< figure src=""/courses/python-introduction/imgs/Jupyterlab-Desktop_select_kernel.png"" width=500px >}}
Resources
Several tutorials are available for Jupyter and JupyterLab online.  One good one is here.
The official JupyterLab documentation is here"
rc-learning-fork/content/courses/python-introduction/input_output.md,"Programs are not very useful if they cannot communicate their results.  They are also not terribly useful if they cannot change their controlling parameters to compute different results.  
We have already been using the print function to display results.  This is an example of console input/output.  The console is text-based and displays on the screen.  It is capable of both input and output.  So far we have been allowing the interpreter to arrange the appearance of the output, but we can control that with formatting. 
Much of the time our programs will be working with files for both input and output.  Files are saved to storage devices such as disk drives or SSDs.  Files must be located by the program, made available for reading and/or writing (opened), and eventually closed.
Exceptions occur due to error conditions anywhere in the code.  They have wide applicability but are frequently encountered when dealing with input/output and files, so we will look at them in this section."
rc-learning-fork/content/courses/python-introduction/pandas_readwrite.md,"Pandas provides multiple functions to read files in several formats.
CSV
Pandas easily reads files in CSV (comma separated values) format.  The separator does not have to be a comma, but anything else must be specified through the sep keyword argument.
Suppose we have a file weather.txt containing weather data over a year for one site.  It is in comma-separated form with exactly one line of column headers.
wdata=pd.read_csv('weather.txt')
The read_csv function stores the column headers as the column names and the rest of the data as the columns in a dataframe object.  To see the data we would type 
wdata
The read_csv method has many optional arguments.  Some important ones are

sep= 
     Column separator character.  Default is comma
header= 
     Number of header lines.  Default is 1. 
     Specify header=None if no header 
names=
     List of column names if no header. 
     Should be provided if header=None.  If there is a header, can be used to 
     rename the columns, but then header=0 should be given.
skiprows=
     Number of rows to skip before importing the data.
usecols= 
     List of columns to import, if not all are to be read.

There are many other options; see the documentation.
Excel
The read_excel method can read files stored in Excel format (.xls, .xlsx, and similar).
my_data=pd.read_excel('weather.xlsx')
It has fewer options because an Excel file includes more information about formatting.  Commonly-used ones include

header= 
     Number of header lines.  Default is 1. 
     Specify header=None if no header 
names=
     List of column names if no header. 
     Should be provided if header=None.  If there is a header, can be used to 
     rename the columns, but then header=0 should be given.
usecols= 
     List of columns to import, if not all are to be read
sheet_name=
     Can specify a string for a sheet name, an integer for the sheet number, counting from 0. A lists of strings or integers will request multiple sheets. Specify None for all available worksheets.  Default is 0 (first sheet only).

Writing CSV and Excel Files
A dataframe can be written to a CSV file with to_csv
python
outfile='new_data.csv'
my_new_df.to_csv(outfile)
If the Dataframe is to be only one Excel worksheet, a similar method to_excel can be used.
python
my_new_df.to_excel(""newfile.xlsx"")
However, if more than one worksheet is needed, an ExcelWriter object must be created.  The following example is from the documentation.
python
df1 = pd.DataFrame([[""AAA"", ""BBB""]], columns=[""Spam"", ""Egg""])  
df2 = pd.DataFrame([[""ABC"", ""XYZ""]], columns=[""Foo"", ""Bar""])  
with pd.ExcelWriter(""path_to_file.xlsx"") as writer:
    df1.to_excel(writer, sheet_name=""Sheet1"")  
    df2.to_excel(writer, sheet_name=""Sheet2"")
HDF5 and NetCDF
HDF (Hierarchical Data Format) and NetCDF are self-describing, cross-platform, binary data formats that are widely used in a number of scientific disciplines, particularly earth sciences.
Pandas can also directly import HDF5 files, using Pytables.
python
my_data=pd.read_hdf(""weather.hdf"")
Pandas dataframes are strictly two-dimensional objects.  The Xarray package provides an extension of the Pandas dataframe to more than two dimensions. 
Since Pandas 0.20, Xarray is the recommended package to manage higher-dimensional data, replacing the Pandas Panel data structure.
Xarray can read NetCDF files directly if the netCDF4-Python is installed.  It can also read HDF5 files via h5netcdf.  
python
import xarray as xr
data=xr.open_dataset(""mydata.hdf"", engine=""h5netcdf"")
ncdata=xr.open_dataset(""myfile.nc"")
HDF and NetCDF data is often in multiple files.  Xarray can merge them.
python
alldata=xr.open_mfdataset(""climate/*nc"",parallel=True)
To write a NetCDF file:
python
ds.to_netcdf(""newdata.nc"")
Exercise
Download the weather.csv file.
Read the data into a dataframe.  Summarize the data.  Look at the first 20 lines.  Print the columns.  Change the names of ""Data.Temperature.Avg Temp"",
""Data.Temperature.Max Temp"" and ""Data.Temperature.Min Temp"" to ""Data.AvgTemp"", ""Data.MaxTemp"", and ""Data.MinTemp"" respectively.  Print the mean of the average temperature.
{{< spoiler text=""Example Solution, zipped Jupyter notebook"" >}}
pandas_weather_ex1.zip
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/files_rw.md,"Most of the time we read or write from a file rather than from the console.  File input/output can be complicated in Python and we will later see some built-in means of reading particular types of files, particularly comma-separated-values (CSV) text files.
Reading from a File
The standard read commands in Python read only strings.  Even if the input values are intended to be numbers, they are read in as strings.  Any conversions must be done by the programmer.

Read the entire file identified by f into one large string:
f.read()
Read a single line (including the newline character) from the file f
f.readline()
Read all the lines into a list, one line per list element.  Newlines are retained at the end of each list element. 
f.readlines()

We have several options to handle the strings when reading directly from a file.  One option is to use read, then split all the lines on the \n character:
python
fin=open(""filename"",""r"")
the_file=fin.read()
file_list=the_file.split(""\r\n"")
We can now process through the list using a loop.  Note that we use \r\n to make sure we accommodate Windows, macOS, and Linux operating systems.  
If we have numeric data, one way to obtain a list of floats for each line of the file is to employ map with readlines:
python
fin=open(""filename"",""r"")
lines=[line.strip('\r\n') for line in fin.readlines()]
all_data=[]
for line in lines:
    #do whatever with each line, for example
    data=list(map(float,line.split(',')))  #will expect CSV file
    #do something with the data list, for example
    all_data.append(data)
Yet another option is to use a Python idiom in which we iterate through the file using the file identifier.  Suppose we have a file with comma-separated values.  We can read the data with
python
fin=open(""filename"")
for line in fin:
    data=line.rstrip(""\r\n"").split("","")
In the examples above, we have not done anything to store the values of the data, so each time through the data variable will be overwritten and the previous values lost.  How we handle this will depend on the file and what should be done to its contents.  One common approach is to declare lists to hold the values in advance, then append as we go.
python
fin=open(""filename"")
x=[]
y=[]
fin.readline()
for line in fin:
    data=line.rstrip(""\r\n"").split("","")
    x.append(float(data[0]))
    y.append(float(data[2]))
When we have completed this loop x will contain the values from the first column and y will contain values from the third column.  In this example we are not interested in the other columns.  We also convert on the fly to float.  This example assumes that our data file has one line of text header, which we use readline to skip.  Since we are not interested in the contents of the header line, we discard it and only use readline to move past it.
Exercises
Read the following file and store the values into two lists x and y.  You may choose any name for the file but make the file extension be .csv.  Print the values for the 4th row after reading in the data.
{{< code-snippet >}}
x,y
11.3,14.6
9.2,7.56
10.9,8.1
4.8,12.8
15.7,9.9
{{< /code-snippet >}}
Make sure there is no blank line at the beginning of the data file.
{{< code-download file=""/courses/python-introduction/scripts/read_file_ex.py"" lang=""python"" >}}
Use readlines rather than a loop to read the data file.  Eliminate the header appropriately.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/readlines_demo.py"" lang=""python"" >}}
{{< /spoiler >}}
We will not go into more detail about reading files since we will later cover packages that offer more convenient ways to read the most common formats.  
Writing Files
To write a file it must have been opened appropriately.  We can use print with the addition of a file= argument.
python
f=open(""myfile"",""w"")
print(""Format string {:f} {:f}"".format(x,y),file=f)
Corresponding to read there is a write.  It writes one string to the specified file 
python
fout=open(""outfile"",""w"")
fout.write(s)
The string can contain newline markers \n but write will not insert them.  They must be positioned explicitly.
Corresponding to readlines there is a writelines:
python
fout=open(""outfile"",""w"")
fout.writelines(seq)
Here seq is a sequence, usually a list of strings.  The writelines will not add any end-of-line markers so as with write, they must be appended to every string in the sequence where a linebreak is desired.
Closing Files
When you have completed all operations on a file you should close it.
python
fin.close()
fout.close()
Exercise
Open a file 
no-highlight
data.txt
in write mode.  Write to the file three columns of numbers separated by commas.  These columns should be
python
n n**2 n**3
for n going from 1 to 20.
Read back the file.  Store each variable into a list.  Use these values to compute and print 
$$ a+bn+cn^2+dn^3 $$
for 
python
a=1.; b=2.4; c=5.8; d=0.7
Print the results for each n you have.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/polynomial.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/more_about_functions.md,"The scope of a variable is the range over which it has a defined value.  In Python, the scope is the code block in which the variable is first referenced.  Therefore a calling program may have a variable x, a function may also have a variable x, and if x is not an argument to the function then it will be distinct in the two units.
Variables and function names defined at the outermost indentation level are global to the functions in the same file; that is, they are in scope in the entire file.
When working in the interpreter (including running codes through Spyder repeatedly in the same kernel), the interpreter stores all variables with global scope into a workspace.  It may be desirable to reset these variables for new runs.  In an iPython console you can type
%reset
to clear the workspace.  In JupyterLab you can also go to the URL bar and add ?reset at the end, viz
http://localhost:8888/lab?reset
Exercise
Experiment with the following code (leave in or comment out x=100)
python
def set_x(x):
    print(x)
    x=100
    while x>0:
        x+=1
        if x>10000: break
        if x<100: continue
        x+=20
    return x
x=1
z=set_x(x)
print(x)
print(z)
Anonymous Functions
In Python we can use a lambda expression to evaluate a function without giving it an explicit definition.  These are often called ""anonymous functions"" since they do not require naming.  Lambdas must be expressible as a single expression; no statements are allowed.  A tuple of the variable names follows the lambda keyword, then a colon, and finally the expression of the variables.
python
fsquared=lambda x:x**2
fsquared(4)
import math
fhypot=lambda x,y:math.sqrt(x**2+y**2)
fhypot(3,7)
One of the most common applications of lambda expressions is to the built-in functions map and filter, with reduce built in to Python 2.7 and available through functools in Python 3.

map 
first argument is a function, second argument is a sequence
applies the function to each element of the sequence and returns a new list (Python 2.7) or iterator (Python 3)
L1=map(square,S)          #Python 2.7
L1=list(map(square,S))    #Python 3
reduce 
first argument must be a function that takes two inputs and returns a single output of the same type
total=reduce(sum,s)   #Python 2.7
import functools; total=functools.reduce(sum,s) #Python 3
filter
first argument is a function that returns True or False.  
applies the function to each element of the sequence and returns a new sequence (Python 2.7) or iterator (Python 3)
L2=filter(func,s)  #Python 2.7
L2=list(filter(func,s)) #Python 3

Exercise
Print the results of the following code.  Try to express map and filter with an equivalent list comprehension.  For the equivalent to the reduction, try the sum built-in function.  Note that not all reductions will have an equivalent built-in, but when available it may be faster than the corresponding reduce construct.
python
import functools
V=[-1,0,1,2,3,4,5]
L=list(map(lambda x:x**2, V))
R=functools.reduce(lambda x,y:x+y, V)
F=list(filter(lambda x:x>0, V))
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/functionals.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/pandas_detailed_example.md,"We are now ready to pull together what we have learned about Pandas and work through a detailed example.
It will be based on the American baseball player Mike Trout's statistics, through 2019.
To follow along, download the data file.
Start by reading it into a Pandas Dataframe.
```python
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
read data from the csv file into a Dataframe object called 'df'
df = pd.read_csv(""MikeTroutData.csv"")
extract columns into variables.
year = df['Year']
hits = df['H']
at_bats = df['AB']
home_runs = df['HR']
salary = df['Salary']
```
Make a simple bar plot showing hits on the Y axis, year on the X axis. If we are willing to accept all the defaults, all that is necessary is to invoke the Matplotlib bar method directly on a subset of the Dataframe.
python
df.plot.bar(x=""Year"",y=""H"")
{{< figure src=""/courses/python-introduction/imgs/pandas_barchart.png"" >}}
We can also use Matplotlib directly with our extracted variables.  The figure() method advances to a new plot.
python
plt.figure()
plt.bar(year, hits)
{{< figure src=""/courses/python-introduction/imgs/barplot1.png"" >}}
Let's add some labels to make this more readable.
python
plt.xlabel('Year')
plt.ylabel('# of Hits')
plt.suptitle('Mike Trout Hits per year')
plt.bar(year, hits)
{{< figure src=""/courses/python-introduction/imgs/barplot2.png"" >}}
Turn it into a horizontal barchart and change the color of the bars.
python
plt.figure()
plt.xlabel('# of Hits')
plt.ylabel('Year')
plt.suptitle('Mike Trout Hits per year')
plt.barh(year, hits, color='red')
{{< figure src=""/courses/python-introduction/imgs/barplot3.png"" >}}
Make a line plot using the .plot() function instead of a barchart.
python
plt.figure()
plt.xlabel('Year')
plt.ylabel('# of Hits')
plt.grid()
plt.plot(year, hits)
{{< figure src=""/courses/python-introduction/imgs/lineplot1.png"" >}}
We can superimpose a line and a bar plot. We will represent 'At Bats' by a red line and 'Hits' by blue bars. 
python
plt.figure()
plt.xlabel('Year')
plt.ylabel('# of Hits')
plt.plot(year, at_bats, color='red')
plt.bar(year, hits)
{{< figure src=""/courses/python-introduction/imgs/barline1.png"" >}}
The y-label we used before is no longer appropriate, so let us add a legend.
plt.figure()
plt.xlabel('Year')
plt.plot(year, at_bats, color='red', label='At Bats')
plt.bar(year, hits, label='Hits')
plt.legend()
{{< figure src=""/courses/python-introduction/imgs/plotwithlegend1.png"" >}}
Without an intervening figure() method, plots will be stacked.  We can utilize that to stack the bars.  We are also rotating the x-axis tick marks and labels 45 degrees.
```
plt.figure()
plt.xlabel('Year')
plt.bar(year, hits, label='Hits')
plt.bar(year, home_runs, label='Home Runs')
plt.legend()
plt.xlabel('Year')
plt.xticks(rotation=45)
plt.xticks(year)                #shows all years in label
```
{{< figure src=""/courses/python-introduction/imgs/stackedbar1.png"" >}}
To make a grouped bar chart, do the same as a stacked bar and move the position of one of the bars as shown below. Notice that for the second bar(), the first argument is 'year+.2'. This shifts the position on the x-axis .2 units to the right of the default starting point.
```python
plt.xlabel('Year')
plt.xticks(rotation=45)
plt.xticks(year)                #shows all years in label
plt.bar(year, hits, width=.2, label='Hits')
plt.bar(year+.2, home_runs, width=.2, label='Home Runs')
plt.legend()
```
{{< figure src=""/courses/python-introduction/imgs/groupedbar1.png"" >}}
Suppose you are interested in exactly how many hits each bar represents. We can iterate over each bar to label it with the corresponding number.
```
plt.xlabel('Year')
plt.xticks(rotation=45)
plt.xticks(year)                #shows all years in label
plt.ylabel('# of Hits')
plt.suptitle('Mike Trout Hits per year')
for bar in plt.bar(year, hits):
    plt.text(bar.get_x() + .4,              #x position of label
             bar.get_height() - 20,         #y position of label
             bar.get_height(),              #actual value of label
             ha='center',
             va='bottom')
```
{{< figure src=""/courses/python-introduction/imgs/barwithlabels.png"" >}}
Let's plot how much Mike Trout is paid per home run. 
```python
cost_per_home_run = salary/home_runs
plt.xlabel('Year')
plt.xticks(rotation=45)
plt.xticks(year)
change Y Axis to show dollar amount
fig, ax = plt.subplots()
formatter = ticker.FormatStrFormatter('$%1.0f')
ax.yaxis.set_major_formatter(formatter)
plt.ylabel('Price')
plt.suptitle('Mike Trout Yearly Cost Per Home Run')
plt.bar(year, cost_per_home_run)
```
{{< figure src=""/courses/python-introduction/imgs/Formatter.png"" >}}
Many plotting options can be applied directly to the Dataframe object, without the need to extract the variables. See the documentation for the Pandas plot method.
Exercise
Download the data file and work through the example.
Resources
Many Pandas tutorials are available online. A good starting point is here.
A repository of pandas practice projects is located here"
rc-learning-fork/content/courses/python-introduction/classes.md,"A class is a generalized, programmer-defined data structure that can contain multiple variables under one name, as well as functions that work on the data represented by those variables. These programmer-defined types are the foundation of object-oriented programming.  Python is an object-oriented language even though most programmers mostly use classes written by others, so it is useful to understand the basics.  
This ""object"" is a concept; the class is its most common representation as code.  Strictly speaking, an object must contain both data (variables) and procedures (functions) and it must be possible to declare variables of its type.  Declaring a variable of a class is called instantiation and each variable is called an instance of the object/class.  This is a key difference between a class and module; as we have seen, a module also contains variables and functions, but no variables can be declared of its type.
Variables and functions defined in a class are collectively called members of the class.  The variables are usually called attributes while the functions are called methods.  The methods describe the interface to the class.  Attributes and methods are accessed through the instances of the class.  As long as the interface is not changed, we can make whatever changes to the class that may be required, and other code that uses the class will not be affected. 
Classes may have two special methods called constructors and destructors.  Constructors are called when a new instance is created and perform whatever setup work is needed.  If the language supports a destructor, it removes an instance and releases the memory it occupies.
Classes in Python
We have already been using classes in our Python program, since all data types (including the apparently ""primitive"" types) are actually classes. In Python syntax, an instance is separated from its methods by a period.  So when we invoke
python
L=[1,2,3,4]
we have invoked a constructor (in an unusual format) for the list class.  When we subsequently type
python
L.append(5)
we are calling the append method on the instance L of the class list to add an element at the end of the list represented by L.
Syntax
Classes are always defined in a module.  Frequently only the class is in the module, though this is not required.  The keyword is class.  The class body, including its docstring, is indented starting one level from the margin.  Classes without methods are allowed in Python, but normally the class will at least contain a constructor.  Instances are defined with
python
new_inst=MyClass()
Calling the class by name in the form of a function automatically invokes the constructor.  If there is no constructor, this creates an empty instance.
As for modules, any variables defined outside of and above the methods are global to the class. All methods can see them.
Example:  
{{< code-download file=""/courses/python-introduction/scripts/mymodule.py"" lang=""python"" >}}
The first line defines the class name.  The next line is the docstring.  After that we define and initialize a variable that is global to the class.  The first method is the constructor.  The ultimate class constructor is always named __init__ (two underscores).  The first argument to init, and to all methods in Python, is the instance variable.  The next two arguments to the constructor are actually passed from the caller.  They are used to initialize the two attributes x and y.  Notice that self.x and x are completely different variables; similarly for self.y and y.
Next is a method that acts only upon the instance.  Note that it does not return the instance.  Instance variables are never returned from a method of their own class.  Traditionally None is returned (explicitly or by default).  Finally, we have a function that performs a computation using a class attribute, and returns the result to the caller.  The global variable i is referenced with the class name, not with self, because it is a class variable and not an attribute of an instance.
Self
The first argument to all class methods must be the instance variable, which is a placeholder for the instance on which the method was invoked. This variable is always called self.  Unlike some other object-oriented languages such as C++, the instance variable must always be explicitly present in the argument list. It is not used when we invoke the method, but is understood.
python
thing=MyClass(x,y)
thing.reset(w,z)
Constructor
The constructor __init__ is invoked when a new instance is declared.  It need not take any arguments from the caller at all, though it will always include self in its parameter list.  Any attributes to be declared should be set to some default value in the constructor.
Exercise
Type in the example class.  Save it into a file called mymodule.py.  Run the file or cell.  In your interpreter window or a new cell, type
python
from mymodule import MyClass
mc=MyClass(12,14)
x=12
print(x,mc.x)
mc.reset(19,11)
print(x,mc.x)
a=mc.addit(13)
print(a)
In the interpreter you can create new attributes dynamically; they will not, however, be preserved in your class module.
python
x=MyClass(11.,13.)
x.counter=1
print(x.counter,x.i,x.reset(12,15))
Class Methods
Class methods access the class variables rather than the instance variables.  This may seem obscure, but class methods are fairly widely used to create alternate constructors.  For example, suppose we wish to load values into an instance, but in some cases we want to read those from a file.  We are now faced with something of a chicken-or-egg situation; we need the constructor to create the instance, but if we don't already have the values we can't call the constructor.  We do not wish to write a function outside the class since we want to keep everything encapsulated.  A class method solves this problem.  
To mark a class method we use @classmethod ahead of the def statement.  Statements beginning with the @ symbol are called decorators and have other uses.
Here is a code snippet for our example:
```python
class pData:
    def init(self,x,y,z,t):
        set_values
@classmethod
def fromfile(cls,filename):
    read_values_from_filename
    return cls(x,y,z,t)

aDataPoint=pData.fromfile(inputfile)
```
The alternate constructor must return the new instance in order to invoke the constructor.
Pickling
Objects may have a more complicated state than the simple variables we have encountered so far.  Saving it correctly could be tedious and error-prone.  Python provides an operation called ""pickling"" that coverts the state of an instance into a linear stream of bytes that can be stored or read back.
python
import pickle
aclass=MyClass(11)
f=open(""myresults.dat"",""w"")
pickle.dump(aclass,f)
To restore, use 
python
aclass=pickle.load(f)
Dill
The pickle module is built into Python but it is limited.  It cannot handle several built-in types.  The dill package overcomes these limitations.  For consistency it is even possible to import it as the pickle namespace.
python
import dill as pickle
Dill is not included in the base Python distribution but can be installed through conda or mamba."
rc-learning-fork/content/courses/python-introduction/function_arguments.md,"Variable Passing
Python passes variables in a manner called assignment. This means that if an argument is mutable and it is changed in the function, it will change in the caller.  If it is immutable it will not be changed even if the function changes its value; a local copy will be made within the function block.  If a function changes one of its parameters that is called a side effect.
Exercise
python
def side_effect(L,x):
    L.append(x)
    return None
L=[1,2,3,4]
side_effect(L,11)
print(L)
print(side_effect(L,99))
What is printed in the last line and why?  What is L now?
Optional and Keyword Arguments
Arguments whose values are determined from their ordering in the parameter list are called positional variables.  Python supports optional and keyword arguments as well.  Optional arguments are assigned a default value in the parameter list of the function definition.  If an optional argument is not present in the argument list passed by the caller, it takes on its default value; otherwise it is positional.
python
def func(x,y=0,w=3):
    return x+y-w
We can call this function with
python
c=func(x)
c=func(x,yy)
c=func(x,yy,zz)
In this example the optional arguments are passed by position as usual.  Optional arguments may also be passed by keyword, not position.  Any optional/keyword arguments must follow any and all positional arguments in the list; except for that restriction their order does not matter and some may be omitted.
python
z=func(x,w=6,y=2)
val=func(x,w=9)
Default values are set only once, when the function is first encountered by the interpreter.  Only immutable types should be used as default values.
Args and Kwargs
What if we wanted to pass a number of arguments that might vary, depending on how we wish to use the function?  Python has a built-in mechanism to handle this.  We use args for a varying number of positional (non-keyword) arguments.  If they should be keyword arguments, we use kwargs.
python
def myvarfunc(arg1, arg2, *args, **kwargs):
Example
{{< code-download file=""/courses/python-introduction/scripts/args_kwargs.py"" lang=""python"" >}}
In this context, the asterisk (*) is called the unpacking operator. Python reads the args into a tuple and any kwargs into a dictionary.
Variable-length arguments and keyword arguments are particularly common in packages.  Even if you never write a code that uses them, you will almost certainly use them in code. For example, we started our Python adventure with the plot function from Matplotlib.  We specified two positional arguments
python
plt.plot(x,y)
The plot function actually works through variable-length arguments and keyword arguments. Its true interface looks like
python
matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)"
rc-learning-fork/content/courses/python-introduction/dictionaries_and_sets.md,"So far the compound types we have studied have been ordered.  We access elements by integer indices numbered from 0 to N-1, where N is the total number of elements.  Dictionaries and sets, in contrast, are unordered.  Elements must be accessed by a key for a dictionary.  Set elements are not individually addressable, but it is possible to iterate over a set."
rc-learning-fork/content/courses/python-introduction/modules.md,"Modules are fundamental to the programming model of Python.  Modules are programming units that (should) consist of related variables and functions that form a coherent block of data+procedures (functions).  They are an extension of the concept of packaging code that we have already studied with functions.
In Python every file you write is a module.  The name of the module is the file name without the extension.  Every Python file must end in .py regardless of your operating system, so if you write a file mycode.py the corresponding module is mycode.  If you are using Jupyter you will need to export your code into a script in order to create a new module (Notebook tab -> Export to -> Executable Script).
Modules that are not run as the main program must be imported for its contents to be accessible.  When a module is imported, it is compiled to a special representation called bytecode.  A new file of the same base name with the suffix .pyc will be created.  
Many modules and packages (collections of modules) are available through a base Python installation.  Conda-forge provides thousands more.  We have already seen a handful of these built-in modules.  
python
import math 
import matplotlib.pyplot
When we import a module we bring in its namespace.  A namespace is an environment that holds a group of identifiers, such as variable and function names.  In Python the namespaces take the name of the module in which they are defined.  Namespaces can be renamed when the module is imported, but not afterward.
Importing Modules
With a simple import statement we must refer to the module's components with its native namespace.
```python
import math 
import os 
import numpy 
z=math.sqrt(x)
home_dir=os.getenv(""HOME"")
A=numpy.zeros(200)
```
We can select only certain components with from
python
from modulename import func1, func2
Now only func1 and func2 can be used, and we do _not* precede their names with the native namespace.
python
z=func1(x,y)
w=a+b*func2(z)
To import all symbols without a prefix use
python
from modulename import *
This statement imports all names that do not begin with a single underscore (_) and makes them accessible without being preceded by the module name.
We can also rename individual symbols
python
from math import sqrt as squareroot
w=squareroot(10.)
One of the most commonly used versions of import changes the name of the namespace, typically to something simpler.
```python
import numpy as np
import pandas as pd
z=np.zeros(200)
data=pd.read_csv(""my_data.txt"")
```
Main Modules
When you run a script directly through the interpreter, such as by using the Run arrow in Spyder, it is in the ""main"" namespace.  Your module can also be imported into the interpreter or into another module.  It will still execute everything in the module, including requests for input and the like, unless you use the special variables __name__ and __main__ (two underscores on each side).  If you use __main__ you can place all code you want to execute only when run directly after a conditional.  
python
if __name__==""__main__"":
    do_work
It is customary to include code for the main namespace into a function named main().  
```python
def main():
    do_work
if name==""main:""
    main()
```
Example
{{< code-download file=""/courses/python-introduction/scripts/rooter.py"" lang=""python"" >}}
Exercise
Type in the example.  Save it into a file called rooter.py.  Type in and save a file testmain.py
python
import rooter
sqrtrt=rooter.MySqrt(11.0)
print(sqrtrt)
First run rooter.py as a standalone script, then run testmain.py.  What's the difference?"
rc-learning-fork/content/courses/python-introduction/numpy_matplotlib_scipy.md,"NumPy, Matplotlib, and SciPy are packages that are fundamental to many applications of Python, particularly in numerically-oriented programming.  They are not part of the set of base Python packages, can be easily added to a Python environment.  
The NumPy package adds many features important or useful to scientific and numeric computing.  These include

True multidimensional arrays
Linear algebra functions
Fast Fourier Transform (FFT) Functions
Random number generators
Tools for integrating Fortran, C, and C++ libraries.

Matplotlib is a Python package that can be used to produce high-quality plots similar to those of MATLABTM.  Its homepage and documentation can be found at matplotlib.org.  A full complement of plot types is available, including

line plots
scatter plots
histograms
bar charts
pie charts
contour plots

NumPy and SciPy (Scientific Python) are closely linked and frequently are used together.  Both provide a large selection of built-in functions.
SciPy builds on NumPy to provide a set of modules and packages that add functions for data analysis and numerical computations.  These include

special functions
optimizations
linear algebra
quadrature (numerical integration)
interpolation
signal processing
basic statistics
"
rc-learning-fork/content/courses/python-introduction/testing.md,"Suppose you have some data in a file and you wish to plot it. The file is 
{{< file-download file=""/courses/python-introduction/data/VaGoldfinch.txt"" text=""VaGoldfinch.txt"" >}}
Please download this file and follow along in Jupyter or another environment of your choice.
The data in this file is the number count of Eastern Goldfinches observed at the Christmas Bird Count.  No year information is attached, but we know from other sources that the time span is 1966-2015.
An easy way to read it is to use NumPy's loadtxt:
python
import numpy as np
import matplotlib.pyplot as plt
input_file=""VaGoldfinch.txt""
obs=np.loadtxt(input_file,delimiter=',')
We need to set up the independent variable so we create an array of the years:
python
years=np.arange(1966,2015)
plt.plot(years,obs)
We run this and…it doesn't work. The size of the two arrays isn't the same. What did we 
forget? We forgot the Python rule about the upper bound of arange. We fix it with:
python
years=np.arange(1966,2016)
Now it works. However, the data are noisy and we'd like to smooth it. We'll use a very simple algorithm; each point will be replaced by the average of it and its two neighbors. We add a loop to do this computation:
python
smoothed_obs[:]=obs
for i in range(len(obs)):
    smoothed_obs[i]=(obs[i+1]+obs[i]+obs[i-1])/3.
Another failure; an index exception. We forgot that Python always starts indices at 0, so that the range gives us integers from 0 to 49; but when i=49, i+1 is 50, which is out of bounds for our array. We can correct this error easily:
python
for i in range(len(obs)-1):
    smoothed_obs[i]=(obs[i+1]+obs[i]+obs[i-1])/3.
Now plot the smoothed data versus year.
This runs but the plot looks strange; the first value seems far too large. Now we need to start looking at the values so we can figure out what the program thinks it is supposed to be doing. This is an important aspect of debugging! Just as you may not see typos in an essay you write no matter how many times you proofread it, you may have a difficult time finding your own bugs just by reading your code. So we add a print statement
python
for i in range(len(obs)-1):
    print(i,obs[i-1],obs[i],obs[i+1])
    smoothed_obs[i]=(obs[i+1]+obs[i]+obs[i-1])/3
If using Jupyter, remember to reinitialize smoothed_obs before each change to the for loop.
This prints a lot of numbers, so we must scroll back to find the first few values. We look and find the first value is 614, far too large compared to what it should be. But why? What happened? Looking back to the last numbers in the printout, we realize that we used the last value in an average for the first. Why did that happen? Looking more closely at our loop, we realize that when i=0, i-1 is -1. That is a legal index in Python, but it's not what we want, it's the shortcut for the last element in the list. Now we change our loop to
python
for i in range(1,len(obs)-1):
    smoothed_obs[i]=(obs[i+1]+obs[i]+obs[i-1])/3.
This time the plot looks correct. Neither the zeroth element nor the last element is changed at all, which is acceptable for our purposes here. If we had wished to provide some special handling for elements 0 and len(obs), we would have had to add those outside our loop."
rc-learning-fork/content/courses/python-introduction/the_basics.md,"Python is an interpreted language.  That means that each line is processed sequentially, starting from the top of a file.  Programs for interpreted languages are often called scripts.  We will start our scripting by learning about variables, operators, expressions, and statements."
rc-learning-fork/content/courses/python-introduction/sets.md,"Sets are another unordered type.  No element of a set may be duplicated.  The set is mutable but all elements must be immutable.
Create the set with the set() function.  Empty parentheses will not work because those are used for an empty tuple (which will thus remain empty).
A frequent application of sets is to eliminate duplicates.
python
L=[0,0,1,4,8,8,10]
M=list(set(L))
print(M)
Since sets are unordered, the order of L is not guaranteed to be preserved.
Set Operations

Add an element to set s
s.add(item)
Extend with a sequence 
s.update(t)
Remove an item (will fail silently if the item isn't present)
s.discard(item)
Remove with an exception if the item isn't is_present 
s.remove(item)

The in operator works even though, strictly speaking, sets are not sequences.
python
item in s
Sets attempt to reproduce most of the properties of mathematical sets.  

Test for subset (a set is a subset of itself):
s2.issubset(s1) or s2<=s1
Test for superset (similarly, a set is its own superset):
s1.issuperset(s2) or s2>=s1
Intersection:
s1.intersection(s2) or s1&s2
Union -- the | symbol is a pipe:
s1.union(s2) or s1|s2
Symmetric difference (elements in one or the other but not both):
s1.symmetric_difference(s2) or s1^s2
Set difference (elements in s1 but not in s2):
s1.difference(s2) or s1-s2

Exercise
Type at the interpreter 
python
s=set()
s.update(""California"")
print(s)
What happened?  Lesson: be careful with strings since they are sequences.
python
states={""Alabama"",""Arkansas"",""California"",""California""}
Initialization with curly braces has been valid since Python 2.6.  Since there are no key-value pairs it will not be construed as a dictionary.
python
print(states)
states2=set()
states2.add(""California"")
states2.add(""Colorado"")
states2.add(""Oregon"")
states-states2
states&states2
states^states2
states|states2"
rc-learning-fork/content/courses/python-introduction/operators.md,"Operators are defined on types and return a new value, usually of the same but sometimes of a different type.  The most familiar are the arithmetic operators + - * / (addition, subtraction, multiplication, division).  Python also provides an exponentiation operator **, e.g. a**b.  Python also accepts pow(a,b) for exponentiation.
The equal sign (=) is an assignment operator in Python.  It does not indicate equality; rather it assigns a value to a variable.  Since variables represent locations in memory, the assignment tells the interpreter to place the value into that memory location.  This means that a mathematically nonsensical expression like
python
x=x+1
is perfectly correct Python.  The interpreter fetches the value of x from memory, adds one to it, and stores the result back into the same memory location.
Python supports add/assign and similar operators.  Thus
python
x+=1
is the same thing as
python
x=x+1
Experienced programmers tend to use operator/assignment (+=, -=, *=, /=) but it is not required.
Integer Operators
The arithmetic operators are defined on all numerical types.  Two additional operators are generally or always used to return integers.

Integer division // returns the integer part of the division.  For example, 7//2=3. 
The mod or modulo operator % returns the remainder of the division, so 7%2=1.

These operators are also defined on floating-point numbers, but the results may not be what you expect!  Also avoid using % with negative numbers, for the same reason.
Special note for Python 2.7 users: In Python 2.7, 2/3=0.  This is because each operand is an integer, so the operator returns an integer.  However, in Version 3 and up, 2/3=0.6666666666666666 (this value is the result of an expression evaluation).  Python 2.7 users who want the newer behavior should begin each file with
python
from __future__ import division
If you'd like the new print function as well, modify this line to
python
from __future__ import division, print_function
Exercises
Type or paste into your choice of Spyder's interpreter pane or a JupyterLab cell the following assignments.
{{< code-snippet >}}
x=17.
Xs=11.
num_1=10
num_2=14
{{< /code-snippet >}}
Type the following lines one at a time and examine the results. In JupyterLab each line must be in its own cell. In the Spyder interpreter pane it will evaluate each line when you go to the next one.
python
x
Xs/x
Xs//x
Xs/x+x
Xs/(x+x)
x/num_1
num_1/num_2
num_2/num_1
Use the same method of typing one line at a time (using a separate cell for each in Jupyter) to study the outcome of the following operations:
python
4+2*3
(4+2)*3
20/4*5
20/(4*5)
.1+.2
5//2
5//-2
11//3
11.4//3.5
11%3
11.4%3.5 #?
11.4-(11.4//3.5)*3.5
Boolean Operators
Boolean operators operate on Boolean expressions.  

Negation
not someVar
AND 
someBool and anotherBool
OR
someBool or anotherBool
Warning: in Python or is nonexclusive.  The expression is True if either or both are True.
This is different from the usual meaning of or in natural languages. ""You can have ice cream or cake"" usually implies ""but not both.""  But in Python,
if ice_cream_OK or cake_OK:
    is true if both are True.  



Comparison Operators
Comparison operators operate on other types but return Boolean values.  They are also called conditional operators or relational operators.
Comparison operators represent relationships between two or more variables.  They can be defined on any type, but arithmetic and string comparisons are the most common.
Arithmetic Comparison Operators

Equal, not equal. Note that equality is a double equals sign.
==,  !=
Less than, greater than, less than or equal to, greater than or equal to
<,  >,  <=,  >=

Chaining
In Python we can write an expression like 
python
0<a<=1
just as in the analogous mathematical expression.  An and operator is always assumed.  This chain is equivalent to
python
0<a and a<=1"
rc-learning-fork/content/courses/python-introduction/matplotlib.md,"Most of our sample scripts in this section are taken directly from the Matplotlib gallery.
One-Dimensional Plots
A simple example:
python
import numpy as np
import matplotlib.pyplot as plt
x=np.linspace(-4.,4.,401)
y=1./(np.pi*(1.+x**2))
plt.plot(x,y)
plt.show()
This results in
{{< figure src=""/courses/python-introduction/imgs/SimplePlot.png"" caption=""A simple Matplotlib plot."" >}}
Let us write a more sophisticated example.  This is a scatter plot with points randomly placed according to a normal distribution.
{{< code-download file=""/courses/python-introduction/scripts/scatter_plot.py"" lang=""python"" >}}
{{< figure src=""/courses/python-introduction/imgs/ScatterPlot.png"" caption=""Scatter plot."" >}}
We can place more sophisticated labeling or multiple plots on a graph with subplot
{{< code-download file=""/courses/python-introduction/scripts/subplots.py"" lang=""python"" >}}
{{< figure src=""/courses/python-introduction/imgs/SubPlot.png"" caption=""Scatter plot."" >}}
Many other options are available for annotations, legends, and so forth.
More advanced plots are provided.  The following demonstrates streamlines for vector fields, such as fluid flows.
{{< code-download file=""/courses/python-introduction/scripts/streamlines.py"" lang=""python"" >}}
{{< figure src=""/courses/python-introduction/imgs/StreamPlots.png"" caption=""Streamlines plot."" >}}
Matplotlib can also make histograms, pie charts, and so forth.  These are commonly used with Pandas, and Pandas can access them directly, as we will see.
Higher-Dimensional Plots
For higher-dimensional plots we can use contour, contourf, surface, and others.
Contour plot example:
{{< code-download file=""/courses/python-introduction/scripts/contour.py"" lang=""python"" >}}
The meshgrid function takes two rank-1 arrays and returns two rank-2 arrays, with each point labeled with both x and y values.  Notice how NumPy array operations are used to compute the function values from the meshgrid arrays.
Surface plots require the mplot3d package and some additional commands to set views and sometimes lighting.  
{{< code-download file=""/courses/python-introduction/scripts/surface.py"" lang=""python"" >}}
Recent versions of Matplotlib can apply style sheets to change the overall appearance of plots.  For example, NumPy has modified its default style, but the older one (shown in some of our illustrations) is available as ""classic.""  Matplotlib can also be styled to imitate the R package ggplot.  See the gallery
for the possibilities.
Exercise


Type into your choice of Spyder's interpreter pane or a JupyterLab cell the example plotting codes we have seen so far.  These were all taken from the Matplotlib gallery.


In the contour plot example, change contour to contourf and observe the difference.


Seaborn
Seaborn is a package built upon Matplotlib that is targeted to statistical graphics.  Seaborn can be used alone if its defaults are satisfactory, or plots can be enhanced with direct calls to Matplotlib functions.
Seaborn 0.9 or later is needed for the ""relationship"" plot example below. This example uses a built-in demo dataset.
{{< code-download file=""/courses/python-introduction/scripts/seaborn_demo.py"" lang=""python"" >}}
{{< figure src=""/courses/python-introduction/imgs/SeabornDemo1.png"" caption=""Seaborn line plots with error bounds."" >}}
Many other statistical plots are available including boxplots, violin plots, distribution plots, and so forth.  The next example is a heatmap.
{{< code-download file=""/courses/python-introduction/scripts/seaborn_demo2.py"" lang=""python"" >}}
{{< figure src=""/courses/python-introduction/imgs/SeabornDemo2.png"" caption=""Seaborn heatmap."" >}}
The call to sns.set() imposes the default Seaborn theme to all Matplotlib plots as well as those using Seaborn.  Seaborn provides a number of methods to modify the appearance of its plots as well as Matplotlib plots created while the settings are in scope.  For many examples see their tutorial on styling plots.
Seaborn's documentation can be found here"
rc-learning-fork/content/courses/python-introduction/loop_alternatives.md,"Loops in Python are very slow.  Nested loops are especially slow.  Some alternatives are available in the standard set of packages that are usually faster..
List Comprehensions
A list comprehension collapses a loop over a list and, optionally, an if clause.
python
squares=[x**2 for x in range(10)]
This is equivalent to
python
squares=[]
for x in range(10):
    squares.append(x**2)
With an optional conditional it becomes
python
positives=[math.sqrt(x) for x in range(-10,11) if x>0]
This is equivalent to
python
for x in range(-10,11):
    if x>0:
        positives.append(math.sqrt(x))
List comprehensions may be nested.
{{< code-snippet >}}
list_2d = [[i+j for j in range(1,6)] for i in range(10,16)]
{{< /code-snippet >}}
Observe the ordering in the previous example.  The inner loop is first.
Exercise
Use math.sin and list comprehensions to generate a list of the sines of the numbers from -1.0 to 1.0 inclusive with an increment of 0.1.  Print the result as a table of x, sin(x) using a for loop.
{{}}
python
import math
xs=[0.1*float(x) for x in range(-10,11)]
sines=[math.sin(x) for x in range(-10,11)]
for i,x in enumerate(xs):
    print(x, sines[i])
{{< /spoiler >}}
Functionals
A functional in this context is a function that takes a function as its argument.  Python has functionals that can take the place of loops.

 Map/Reduce/Filter 
The map functional applies a function individually to each element of an iterable and returns an iterator (in Python 3).  Since we frequently want to do something with the result we can cast it to a list.
python
float_vals=list(map(float,range(20)))
print(float_vals)
Map is said to broadcast the function across the iterable.
The reduce function takes a binary (two arguments) function and applies it pairwise to each element, working its way down the iterable, to produce a single result.  It was removed from core Python in Python 3 but can be imported from the functools module.
python
from functools import reduce
def adder(x,y):
   return x+y
sum_it=reduce(adder,range(20))
print(sum_it)
The filter functional takes a function that returns a Boolean (True or False) and applies it elementwise to the iterable, returning an iterator.
python
def is_even(n):
   return n%2==0
evens=list(filter(is_even,range(21)))
print(evens)
Map, filter, and reduce are frequently used with lambda functions.
NumPy
Another way to avoid for loops is to use NumPy.  The best speedups are usually achieved when it is possible to use NumPy built-in ""vectorized"" functions.  For more details, see our workshop on High-Performance Python."
rc-learning-fork/content/courses/python-introduction/tuples.md,"﻿---
title: Tuples
toc: true
type: docs
draft: false
weight: 33
date: ""2020-11-17T00:00:00""
menu:
    python-introduction:
        parent: Compound Types

A tuple is a Python ordered sequence object that is similar to a list but is immutable.  Tuples are indicated by parentheses (round brackets). Like all ordered sequences, we can refer to individual elements with [n] and slices with [lb:ub].  As for all other ordered sequences, numbering of elements starts at 0 and the upper bound of a range is excluded.

Creation
T=tuple((1,2,3))
Length
len(T)
Concatenation (as for strings, must assign to a new variable)
T3=T+T2
Membership
3 in T
Iteration
for i in T: print(i)

Although the tuple is immutable, any mutable elements can be changed.
python
myList=list()
t=(myList,myList)
myList.append(1)
print(t)
myList.append(2)
print(t)
Since they are immutable, tuples have fewer defined operations than lists.  They can be indexed and sliced like lists.
python
T=(1,2,3)
T2=T[1:]
One important set of operations on tuples is packing and unpacking.  If the context is unambiguous, the parentheses are not required.
python
T=1,2,3
print(type(T))
x,y,z=T
print(x,y,z)
Occasionally we need a tuple with one element.  This is not the same thing as a single variable so we must distinguish them.  A tuple with one element must be declared like (E,) -- the comma is required.
Lists or Tuples?
A tuple should be used whenever the structure should not be dynamically changed or resized. 
Tuples are preferred over lists for returning multiple values from functions. 
Tuples are often used for heterogeneous data, i.e. elements of different types.  List elements are typically homogeneous (all the same type) though this is not a requirement.
Resources
Lists, tuples, and the range iterator are described here."
rc-learning-fork/content/courses/python-introduction/lists.md,"﻿---
title: Lists
toc: true
type: docs
draft: false
weight: 32
date: ""2020-11-17T00:00:00""
menu:
    python-introduction:
        parent: Compound Types

Lists are one of the most important data types in Python.  They are flexible and easy to use. Lists are sequences of objects.  Each element of the list can be of any type, including another list.
Lists have all the common properties of sequences: they are ordered which means that each element can be referenced by an integer index.  As for all ordered types in Python, the indices start at 0.  
Lists are dynamically sized and they are mutable.  They must be declared in some manner before they can be used.

Empty list
L=[]
List with specified elements (the values of the elements must have been assigned)
L=[L1,L2,L3,L4,L5]
List with N items, each having value V (both N and V must have been assigned and N must be an integer)
L=[V]*N
Return a new list from a built-in function
L=list(range(12))
    This returns a list of integers 0,1,..11

List Elements
To access a particular element by its index, we enclose the index value in square brackets.  If a list L has been defined, we can access its third element with
python
print(L[2])
Lists are mutable so individual elements can be changed.
python
L[2]=42
Exercise
In your choice of JupyterLab or Spyder, type and run
python
myL=[1,2,3,5,6,7]
myL[1]=4
print(myL)
Sublists are often called slices.
python
subL=myL[1:3]
Always remember that the upper bound is excluded, so this slice is elements 1 and 2, which are the second and third elements.
This extracts elements from the third to the last.
python
subL=myL[2:]
This extracts the elements from the beginning to the third element (index number 2).
python
subL=myL[:3]
An increment can also be specified:
python
subL=myL[1:7:2]
This extracts elements 1, 3, and 5.
Changing Lists

Initialize an empty list
L=[]
Append an element to a list
L1.append(""Graham"")
Extend a list with another list
L1.extend([""Michael"",""Terry""])

It is important to understand the difference between appending and extending. Appending adds the argument as the new last element exactly as it appears. It takes any type.  Extending requires a list as its argument.  It joins the two lists sequentially.  It is equivalent to
  * L=[1,2,3]+[4,5,6]

Insert an element
L.insert(i,item)
This inserts item before element i. To add an item at the beginning of the list, use
L.insert(0,item)



Shortening lists:

Delete an element by its index
del L[i]
Delete the first occurrence of a particular element
L.remove(item)
The item must match exactly or an error occurs.
Remove and return an element
item=L.pop(<i>)
The angle brackets indicate an optional argument and are not typed.  If the argument is absent the last element is returned.  If it is present that value is returned.
lastVal=L.pop()
A_val=L.pop(2)


Keep in mind that pop shortens the list.

Much more can be done with lists.

Length (number of elements)
lenL=len(L)
Sum of values (if all elements are the same type and sum is defined for them)
sumL=sum(L)
Maximum or minimum value of the items (if they are the same type and max and min are defined)
max(L) min(L)
Membership test (returns Boolean)
item in list
Index of first time item occurs
myIndex=L.index(item)
Number of times item occurs
numItem=L.count(item)
Sort a list (when possible) in place (overwrites the original)
L.sort()
Return a sorted list to a new list
Lsorted=sorted(L)
Reverse the list in place (overwrites)
L.reverse()
There is no direct function to reverse and return into another list, so we use this handy trick
Lreversed=L[::-1]
In Python 3, reversed(L) returns an iterator and not a list, but you may use the list constructor to convert it.
Lreversed=list(reversed(L))

Copying lists:
python
A=[1,2,3,4]
B=A
print(A)
B[2]=9
print(A)
B is just an alias (a ""nickname"") for A.  If B changes so does A.  This is true for all mutable types. Slicing notation creates a view that can make a copy if the entire list is included.
python
C=A[:]
C[1]=11
print(A)
print(C)
An alternative is to explicitly use the list constructor function:
python
D=list(A)
Exercise
Type
python
numList=list(range(10))
Print the length of the list.
Change the fourth element to 11.
Extend the list with L=[20,30,40].
Print the index of the item that has the value 9.
Remove that item from the list.
Print the current length of the list.
Compute the average of the list elements.
Sort the list and then reverse the sorted version."
rc-learning-fork/content/courses/python-introduction/string_basics.md,"String literals are indicated by double quotes ""a"".  Unlike some other languages, Python is not too picky about single or double quotes to indicate strings, but double quotes are usually preferred for multi-character strings. If a string contains an apostrophe or its own quotes, the surrounding quotes must be of the other type.
python
s1=""This is a string.""
s2=""It's time to go.""
s3='The man said, ""Time to go.""'
The length of a string can be dynamically determined when the script is run, but once set, it is fixed because strings are immutable. The string variable can be overwritten, however.
python
Line_1=""The first line of a file\n""
The \n symbol represents a new line and is treated as a single character.  The length of the above string is 25; spaces and the newline count.
If a string literal is surrounded by triple double quotes """"""s"""""" it is verbatim, including newlines typed.
python
s=""""""This string is a
     multiline quote.""""""
If evaluated as an expression, the string will show the newline.  If the print function is used, it will print exactly as typed.
String Comparisons
String comparisons use the same familiar symbols as arithmetic comparisons, but with lexical ordering.  This can result in some surprises if the strings represent numbers.  Never forget that strings are a completely different type from the numbers they may seem to represent!  Equality also requires exact equality, including spaces, matching cases, etc. 

Equality 
==
Lexically greater than or lexically greater than or equal
\> \>=
Lexically less than or lexically less than or equal 
< <=

Example
python
s1=""This is a string.""
s2=""That is a string.""
s3=""This is a string""  #no period
print(s1==s3)
print(s1<=s2)
Exercise
python
number_1=""10""
number_2=""2""
print(number_1 < number_2)
String Operators and Functions
Python supplies many string operators and functions.  Among the most commonly used are

Concatenation
s1 + s2
Number of characters
len(string)
Type conversion from numerical type to string
str(f)
Type conversion from string to numerical type.  This must be possible according to the interpreter's rules for the numbers.  In particular, the string ""3."" does not represent an integer.
float(s)
Raw string: no characters are taken to be special characters.  Sometimes particularly useful on Windows. Either r or R can be used.
r'This is a string \ with no special characters \n'

Exercise
Type in the following code.  What causes the difference?
python
s1=""Today \n is a new day.""
s2=r""Today \n is a new day.""
print(s1)
print(s2)
Define variables x=21.0, n=30, s=""My new string.""
Convert n into a float and store the results into a new variable y.
Set a variable the_answer containing the literal string ""42."" (be sure to include the period). Type
python
z=int(the_answer)
What happened? Try
python
z=float(the_answer)
Substrings
Although a particular string variable is immutable, it is possible to extract substrings from it.
python
sub_string=string[0:3]
In this context the colon (:) is again representing the range operator. Recall that for all ordered types, Python counts from zero.  So the first character is numbered 0, the second is 1, and so forth.  As we have seen before, the upper bound is always exclusive in Python. Thus the variable sub_string consists of characters 0, 1, and 2.
Since strings are immutable we cannot assign values to a substring; that is, they cannot appear on the left-hand sign of an assignment = statement.
Exercise
Type into the Spyder interpreter pane or a JupyterLab notebook.  Remember that in Jupyter each evaluation expression should be run in its own cell.
python
title=""This is a string.""
subtitle=""This is another string.""
len(title)
title+"":""+subtitle
newtitle=title+"" : ""+subtitle
len(newtitle)
newtitle[2:4]=""at""  #Error-why?
x=19.58
print(""The value of x is {:f}"".format(x))"
rc-learning-fork/content/courses/python-introduction/data_hiding_inheritance.md,"In object-oriented languages that are stricter than Python, class data may be public or private.  Public members are directly accessible from an instance.  Private attributes can only be accessed through methods; the data is said to be encapsulated within the object.  Private methods can only be accessed by other methods.  This is to prevent outside code from changing the attributes, or the results of methods, without a ""message"" to the class instance being sent.  
Data Hiding in Python
Python does not enforce this but does have a mechanism for ""hiding"" some members.
All symbols beginning, but not ending, in two underscores are not accessible through an instance.  They can only be utilized through a method.  Symbols beginning with a single underscore are understood to be part of the implementation and not the interface.  Outside code must not rely on them and should rarely to never use them directly.
Example: 
{{< code-download file=""/courses/python-introduction/scripts/mymodule_private.py"" lang=""python"" >}}
This makes a difference. 
python
ac=MyClass(19.,20.)
ac.addit(30.)
ac._i
ac.__redo()
The last line will result in an AttributeError: MyClass instance has no attribute '__redo'.
However, it is not absolute:
python
ac._MyClass__redo()   #impolite!
print(ac.x,ac.y)
Accessors and Mutators
To handle ""private"" or even just ""implementation"" variables we use methods.  Accessors (""getters"") get the value and return it.  Mutators (""setters"") change the value.
{{< code-download file=""/courses/python-introduction/scripts/privateclass.py"" lang=""python"" >}}
Inheritance: New Classes from Old
One of the cornerstones of Object-Oriented Programming is inheritance.  New
classes can be derived from existing classes; the lower-level class is called the base class.  The derived class inherits the attributes and methods from its parent. The new class is said to be a subclass of its parent.
Inheritance facilitates code reuse and extension of code functionality while minimizing the modifications required to the original code.  The new class may add members, both attributes and methods, along with the inherited ones.  It can also override the methods of the base class, adapting them to the requirements of the new class. No changes to the base class are required.
The relationship between the base class and the derived class can be understood as ""Child IS_A Parent.""
Examples
no-highlight
A Sparrow IS_A Bird
An Employee IS_A Person
Let us consider a more detailed example. An important object in a forest model is a tree.  An individual tree will have particular attributes depending on the species, and they may have additional behaviors in some cases, but they will have a number of attributes in common.  We can define a class Species which might contain attributes such as
 - genus
 - species
 - wood_density
 - max_life_expectancy
 - max_height
The methods would include behaviors such as
 - sprout
 - grow
 - die
A Tree would add members specific to an individual tree, such as
 - diameter
 - height
 - branch
In Python, we indicate the parent with parentheses
python
class Child(Parent):
Example
Suppose we wish to develop code for an employee class when we already have a Person class.  Person defines general data such as name and address. The additional attributes for Employee will be the employee's salary and ID number.
{{< code-download file=""/courses/python-introduction/scripts/person_module.py"" lang=""python"" >}}"
rc-learning-fork/content/courses/python-introduction/pandas.md,"Pandas is a Python data analysis library.  It was developed to bring a portion of the statistical capabilities of R into Python.  Pandas accomplishes this by introducing the Series and DataFrame objects to represent data, and incorporating Matplotlib and many features of NumPy into these objects to simplify data representation, analysis, and plotting.  Pandas works with the statsmodel and scikit-learn packages for data modeling.  Pandas supports data alignment, missing data, pivoting, grouping, merging and joining of datasets, and many other features for data analysis.
Note that we will adhere to a convention of
python
import pandas as pd
This naming scheme is not required, but it is very common, much like np for NumPy."
rc-learning-fork/content/courses/python-introduction/pandas_filtering.md,"We often need to extract data based on specified criteria, and to regroup and reorganize it.  Pandas provides a wealth of methods for these operations.
Missing Data
We frequently must filter or account for missing data.
Pandas can easily handle this task.  Conventionally, we use the IEEE Not A Number (nan/NaN) indicator for missing data, since it is easy to check for its presence.  We can also use None; Pandas treats None and NAN as essentially the same thing.  In documentation, missing data is usually referenced as NA.
To check for missing data in our student_grades dataframe we can use isna() and notna(), both of which are Boolean functions so will return True or False. 
{{< code-snippet >}}
dates=[""2000-01-01 00:00:00"",""2000-01-02 00:00:00"",
       ""2000-01-03 00:00:00"",""2000-01-04 00:00:00"",
       ""2000-01-05 00:00:00"",""2000-01-06 00:00:00""]
weather=pd.DataFrame({""Minimum Temp"":[-5.87,np.nan,-4.58,-6.40,-5.50,-3.29],
                      ""Maximum Temp"":[8.79,np.nan,5.10,2.68,6.18,4.50],
                      ""Cloud Cover"":[3,5,3,2,3,5]},
                      index=dates)
{{< /code-snippet >}}
```python



weather.isna()
                     Minimum Temp  Maximum Temp  Cloud Cover
2000-01-01 00:00:00         False         False        False
2000-01-02 00:00:00          True          True        False
2000-01-03 00:00:00         False         False        False
2000-01-04 00:00:00         False         False        False
2000-01-05 00:00:00         False         False        False
2000-01-06 00:00:00         False         False        False
Most built-in numerical methods omit rows with missing data.python
weather[""Minimum Temp""].mean()
-5.128
```



We can fill missing values with a quantity.
```python



new_weather=weather.fillna(-999.)
new_weather
                     Minimum Temp  Maximum Temp  Cloud Cover
2000-01-01 00:00:00         -5.87          8.79            3
2000-01-02 00:00:00       -999.00       -999.00            5
2000-01-03 00:00:00         -4.58          5.10            3
2000-01-04 00:00:00         -6.40          2.68            2
2000-01-05 00:00:00         -5.50          6.18            3
2000-01-06 00:00:00         -3.29          4.50            5
```



In many cases we just wish to remove rows with missing data.
```python



corrected_weather=weather.dropna()
corrected_weather
                     Minimum Temp  Maximum Temp  Cloud Cover
2000-01-01 00:00:00         -5.87          8.79            3
2000-01-03 00:00:00         -4.58          5.10            3
2000-01-04 00:00:00         -6.40          2.68            2
2000-01-05 00:00:00         -5.50          6.18            3
2000-01-06 00:00:00         -3.29          4.50            5
```



We can find the number of elements that are not NA in a dataframe with count
```python



weather.count()
Minimum Temp    5
Maximum Temp    5
Cloud Cover     6
dtype: int64
```



Searching DataFrames
Conditionals
We can search for values in columns that satisfy specified conditions.
Conditionals in indexes are accepted much as for NumPy arrays.  The operators are <,<=,>,>=,==, and != as for Numpy.
freezing_days=weather[weather.Tmax<=0.]
Conditionals may be compounded
cold_days=weather[(weather.Tmax<=6.) & (weather.Tmin>-5.)]
The ampersand & indicates and. Use the pipe symbol | for or.  The parentheses are generally required when creating a compound conditional.
Conditional searches return a new DataFrame.  Returning to the grade book example, we can select students by various criteria
python
second_years = grade_book.loc[grade_book['Year'] == 2]
top_students=grade_book.loc[grade_book['Grade']>=90]
With no row specification, the loc is optional in the above code.
Where
The where method returns a dataframe with NaN values for rows where the conditional is not satisfied, and the original values where it is true.
```python



grade_record.where(grade_record[""Test4""]>85)
                Test1  Test2  Test3  Test4
Jim Dandy         NaN    NaN    NaN    NaN
Betty Boop       91.7   89.8   92.4   87.2
Minnie Moocher    NaN    NaN    NaN    NaN
Joe Friday        NaN    NaN    NaN    NaN
Teddy Salad      98.5   96.3   96.8   93.9
If we want to remove the NaNs we can add `dropna`python
grade_record.where(grade_record[""Test4""]>85).dropna()
             Test1  Test2  Test3  Test4
Betty Boop    91.7   89.8   92.4   87.2
Teddy Salad   98.5   96.3   96.8   93.9
```



Exercise
Return to the weather_data.ipynb notebook from the previous section. Loop through the dataframe and use some things you have learned about strings to find the first line for the station location of Richmond, Virginia.  Get the corresponding station code.
Use this to extract the data for Richmond, Virginia into a new dataframe.
Print the overall mean temperature and the means of the minimum and maximum to two decimal places. 
(If using f strings, don't forget the rule about writing quotes inside the f-string.)
Print the number of days with a minimum below freezing. Hint: ""False"" is 0 and True is 1.
{{< spoiler text=""Example solution, zipped Jupyter notebook"" >}}
pandas_weather_ex2.zip
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/pandas_dataframes.md,"Selecting and Manipulating Data
Many of the operations we perform with data involve working with parts of it.  Pandas has many powerful ways to extract and reorganize data.
Accessing Rows and Columns
Series
Series can be sliced
select=randns[1:41]
This slices both the index and the data.  To extract values with specific row numbers, use the iloc method (note the square brackets).
some_vals=randns[1:41]
one_val=randns.iloc[11]
other_vals=randns.iloc[3:9]
Pandas Dataframe rows are ordered, so you can call rows by index, just as for lists or NumPy Ndarrays.
python
grade_book[2:4]   #returns third and fourth rows
Remember that the upper bound is not included, as is usual for Python ranges.
You can use .iloc to achieve the same result. The iloc method selects rows based on their integer indexes.
python
grade_book.iloc[[2,3]]
The .loc method is more flexible. It allows you to access row indexes based on the value of a column. loc is label-based rather than index-based.  It is used to extract a block of rows and columns by row identifier and column name, or by a Boolean.
first_student= grade_book.loc[0,'Name']
first_student_data = grade_book.loc[0]
The first_student variable picks out the content of row 0, column Name, which is a string in this case.  The first_student_data object is a new Series containing the information about the student in the first row. 
Note that when the index is an integer, iloc[0] and loc[0] are equivalent.  However, the index need not be the default integers and loc can use a more general type.
We can extract multiple specified columns into a new Dataframe by providing a list of columns.
python
grades_only=grade_book.loc[:,[""Name"",""Grade""]]
Observe that label slicing is inclusive.
Specifying Rows and Columns
By default, Pandas row indexes are integers starting at 0.
```python
weather=pd.DataFrame({""Date"":[""2000-01-01 00:00:00"",""2000-01-02 00:00:00"",
                              ""2000-01-03 00:00:00"",""2000-01-04 00:00:00"",
                              ""2000-01-05 00:00:00"",""2000-01-06 00:00:00""],
                      ""Minimum Temp"":[-5.87,-3.82,-4.58,-6.40,-5.50,-3.29],
                      ""Maximum Temp"":[8.79,4.78,5.10,2.68,6.18,4.50],
                      ""Cloud Cover"":[3,5,3,2,3,5]})
print(weather)
print(weather.index)
for s in weather.index:
    print(weather.loc[s])
no-highlight
                  Date  Minimum Temp  Maximum Temp  Cloud Cover
0  2000-01-01 00:00:00         -5.87          8.79            3
1  2000-01-02 00:00:00         -3.82          4.78            5
2  2000-01-03 00:00:00         -4.58          5.10            3
3  2000-01-04 00:00:00         -6.40          2.68            2
4  2000-01-05 00:00:00         -5.50          6.18            3
5  2000-01-06 00:00:00         -3.29          4.50            5
RangeIndex(start=0, stop=6, step=1)
Date            2000-01-01 00:00:00
Minimum Temp                  -5.87
Maximum Temp                   8.79
Cloud Cover                       3
Name: 0, dtype: object
Rest of loop output omitted
We would probably prefer to access the data by date, rather than trying to determine which rows to use.  Pandas has a built-in [date generator](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html):python
date_ranges=pd.date_range(""2000-01-01"",periods=6)
date_ranges
DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',
               '2000-01-05', '2000-01-06'],
              dtype='datetime64[ns]', freq='D')
``
The default is an interval (freq`) of one day. Start and end dates can be specified.  Multiple date formats are accepted. For this example, however, we will make a list and use it as the index.  We can then select items by dates.
Accessing and Renaming the Column Names
If we'd like to save some typing, we can rename columns to make them conform to Python variable-naming rules.  Then we can treat the column name as an attribute.
python
weather_df.columns=[""Tmin"",""Tmax"",""Cloud Cover""]
Order matters, and each column must be included even if we do not wish to rename it.  In order to rename only certain columns, we can use the rename method, which uses a dictionary format.
python
weather_df.rename(columns={'Minimum Temp':'Tmin','Maximum Temp':'Tmax'},inplace=True)
Be careful with the period (""dot"") notation for column names, since if one happens to coincide with a built-in Pandas attribute or method, the method will be assumed, which may result in unpredictable or incorrect behavior. 
Without an assignment, the columns attribute holds the names of the columns.
{{< code file=""/courses/python-introduction/scripts/weather_df.py"" lang=""python"" >}}
{{< code file=""/courses/python-introduction/scripts/weather_df.out"" lang=""no-highlight"" >}}
This range syntax for the row range is not inclusive, as is usual for Python.
Extracting Row Indices
The index attribute contains the index values
python
weather_df.index
To rename the indexes we use rename much as for the column names.
python
weather_df.rename(index={'2000-01-01 00:00:00':'2000-01-01 00:00:10'},inplace=True
We can obtain the equivalent NumPy values for the indices.  We can also convert the index object into a list.
python
inds_array=weather_df.index.values
inds_list=weather_df.index.tolist()
Exercise
The Seaborn package includes some sample datasets.  We will look at the ""iris"" dataset.
python
import seaborn as sn
iris=sn.load_dataset('iris')
Describe the dataset. Print the column names. Iterate through the indexes and print the corresponding value of the species name for indexes 0 to 30 inclusive.  Print the mean petal length.  Print the series data for row 90.  Make a new dataframe that contains only the petal length, petal width, and species. Summarize the new dataframe.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/iris.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/unit_test.md,"A function, module, or class is a unit of code.  Testing each unit is thus called unit testing.  It is not always possible to completely isolate each function and test it, but every function should be tested in some way as it is written.  Do not wait until you have written dozens or hundreds of lines of code to start testing.
Unit Testing Frameworks
We have constructed our own attempt at testing our unit, the function DoW.  More formalized unit-testing frameworks exist for Python scripts.  The built-in standard package is PyUnit (formerly called, and still imported as, unittest).  Nose2 can be installed to add extensions and plugins to PyUnit.  PyUnit may be somewhat cumbersome for a beginner, since it involves subclassing and writing methods.  A fairly simple, and very popular, framework is PyTest.  It should be installed by default with Spyder or the VSCode Python extension, or it can be explicitly installed if needed.
Other, more sophisticated or special-purpose, frameworks exist, but we will focus on PyTest for our example.
PyTest
PyTest is easy to install and simple to use.  To implement our tests, we write functions that begin with test_ and contain at least an assert statement.
Example
This is a very simple example from the PyTest documentation
{< code-download file=""/courses/python-introduction/scripts/test_example.py"" lang=""python"" >}}
PyTest in Spyder or other IDEs, or Command-Line
This is easy to run in Spyder.  In the iPython console window, type
python
!pytest test_example.py
Depending on your working directory in Spyder, you may need to provide a path, such as
python
!pytest /home/myid/Python/scripts/test_example.py
This test is intentionally written to fail.  You should see a message.
The pytest executable can also be run from a command line, such as a Linux or macOS terminal, or the Miniforge prompt.
no-highlight
pytest /Users/myid/Python/scripts/test_example.py
Pytest allows, but does not require, the programmer to set up  multiple testing functions in a class. Another example from their documentation:
{< code-download file=""/courses/python-introduction/scripts/test_class.py"" lang=""python"" >}}
The name of the class must begin with Test.  The tests are run by specifying the module name at the command line.
python
pytest test_class.py
We may need to run the same test with multiple values for the input.  Pytest provides the parameterize capability.  We invoke it with a decorator:
python
@pytest.mark.parametrize(""test_input,expected"",[(""3+5"", 8),(""2+4"", 6),(""6*9"", 42)])
def test_eval(test_input, expected):
    assert eval(test_input) == expected
Pytest has a number of other capabilities, including fixtures, which are functions that can be used to generate data for input into test functions.  This is beyond our scope, however.
PyTest with Jupyter Notebooks
There are a few options for using PyTest with Jupyter notebooks.  A popular package is testbook.  It can be installed from conda-forge in Miniforge, or with pip. Testbook is not run within a notebook, but loads the notebook into a Python script and runs the tests.
Example
Our notebook testnb.ipynb contains the following two cells
```python
def func(x):
    return x + 1
def func2(x,y):
    return x + y
``
We have two functions we wish to test.  Testbook will require us to use the PyTest _fixtures_ feature mentioned above, so that we can access both functions through thetb` object set up by PyTest. 
{< code file=""/courses/python-introduction/scripts/testnb.py"" lang=""python"" >}}
We would run this as for other PyTest files with
python
pytest testnb.py
at a command line, or
python
!pytest testnb.py
within an iPython interpreter.
Exercise
We went to all the trouble to write the day-of-the-week function, but the datetime module has it built in.
```python
import datetime
days = [""Monday"", ""Tuesday"", ""Wednesday"", ""Thursday"", ""Friday"", ""Saturday"", ""Sunday""]
day_number = datetime.date(year=2016, month=5, day=30).weekday()
print(days[day_number])
```
Use this to write a unit test for the DoW function. You will need to parameterize the test. See the documentation for a more detailed explanation of parameterization.
{{< spoiler text=""Example solution"" >}}
{< code-download file=""/courses/python-introduction/exercises/test_dow.py"" lang=""python"" >}}
{{< /spoiler >}}
Test-Driven Development
In test-driven development, the programmer develops unit tests for each function first, before writing the code.  This forces the developer to think in terms of small, easily-tested units, Code is then written to pass each test in turn.  Once all tests pass and the unit is complete, it can be combined with others into larger units and a complete program.
Even if full unit testing is not practical, especially in the case of non-professional programmers, the fundamental principle of testing code continuously is easy to apply.  Tests should also be automated in some manner, even if a more formal framework is not used.  Do not wait to start testing until you have written hundreds of lines of code.  
In addition to their other benefits, unit tests can help to detect and correct regressions.  A regression occurs when a change to the code introduces new bugs.  Regression testing should also be part of the development process.  When changes are made, the entire code should be retested, not just the new units."
rc-learning-fork/content/courses/python-introduction/testing_debugging.md,"One of the most important skills we must acquire to be a competent programmer is correcting mistakes, or debugging. You will probably find that you spend more time debugging than writing the code in the first place. And in order to debug, you must test that your program is working correctly. Testing must be considered from the beginning, before you even start writing your own programs, and should be systematic."
rc-learning-fork/content/courses/python-introduction/functions.md,"Functions break down the programming into a series of well-defined tasks.  This makes code easier to read, debug, and maintain.  Functions also reduce ""cut and paste"" errors.  If a code carries out the same operations on different data more than once, those sections should be converted to a function.
Functions have some similarities to mathematical functions, but also some differences.  A function has a name, arguments, a body of code implementing the algorithm for evaluating the function, and a return value.  The name of the function is chosen by the programmer and must obey the same rules as variable naming in Python. 
Defining  Functions
Functions (usually) take arguments.  In computer-science language the variable that represents an argument in the function definition is often called a parameter or a dummy variable. In strict usage the sequence of variables may be called the argument list in the caller and the parameter list in the function definition; however, these terms tend to be used interchangeably.  Arguments are said to be passed from the caller to the function.  
The function returns a value, which replaces the function's name after it has been executed.  In Python functions must always return a value; if the programmer does not specify one, Python returns the special value None.  A function may return exactly one item, but that quantity may be a compound data structure such as a list or tuple, so it is possible to return multiple values from a function.
Functions must be called by name or they do nothing.  Their code is static until it is invoked by a caller.  The interpreter must have encountered the function definition before it can be called; interpreters cannot move upward in a file.  Invoking a function causes its name to take on the return value, but in Python functions are not equivalent to variables -- that value will not be stored anywhere.  It must be explicitly assigned, used in an expression, or printed for the result to be captured.
Best practice is to place all the functions you will use at the top of the file, right after the main docstring.  Functions may have a docstring themselves.  This is particularly important if the function is lengthy or complicated.
Python Syntax
The keyword is def (define) followed by the name of the function.  The function name must be followed by parentheses whether any arguments are passed or not.  The def keyword begins a code block, so a colon is required.  All statements in the body of the function must be indented.  The docstring must immediately follow the def line and be indented one level.  Recall that docstrings are enclosed in triple double quotes (""""""string"""""").  Values are returned with the return statement.  The return causes an immediate exit from the function; no further statements will be executed.
Examples 
{{< code-snippet >}}
def sum_vals(x,y,z):
    """"""Computes the sum of its input values""""""
    return x+y+z
def make_list(x,y,z):
    """"""Makes a new list""""""
    new_list=[x,y,z]
    return new_list
def sum_diff(x,y):
    """"""Returns the sum and difference of two values""""""
    return x+y,x-y
{{< /code-snippet >}}
Notice the use of tuple packing in the third example.  An equally valid, but not commonly used, last line could be return (x+y,x-y).
Exercise
If you have not already done so, type in the three functions from the example.  Run the cell or script.  Then type into the interpreter
python
sum_vals(1,2,3)
sum_vals(1.2,3.4,4.5)
sum_vals(""1"",""2"",""3"")
make_list(1.,11.,3.)
make_list([1.,11.],3.,4.)
sum_diff(3,4)
s_3=sum_vals(8,8,10)
s_3
If you want to use these functions in a Python script, it is often convenient to store their returned values in a variable and then do something with that variable, e.g. print its value. 
For example:
```
result=sum_values(1,2,3)
````
Invoking Functions
The names of the variables in a function's parameter list are called dummies because they are placeholders.  The function can be called with any variable names in the caller.
python
xx=1.; yy=2.; zz=3.
sum_vals(xx,yy,zz)
sum_vals(zz,xx,yy)
sum_diff(xx,yy,zz)
sum_diff(zz,xx,yy)
make_list(xx,zz,yy)
make_list(yy,xx,zz)
The arguments in the lists in both caller and callee must agree in number and in type.  There is a one-to-one correspondence between positional arguments in the caller and the callee so there must be exactly one in the caller for each in the callee.  In Python there is more freedom in the types but whatever happens to the variable in the function must make sense for the type that is passed to it.  For example, the + operator in sum_vals is defined for integers, floats, and strings (as well as for some other types) so it is valid to pass those types to the function, but they cannot be mixed.  An illegal operation on the type actually passed will cause the interpreter to stop with an exception.  
Exercise
Convert your program from an earlier exercise to use a function to compute the BMI.
Remember the formula: 
BMI = (Weight in Kilograms / (Height in Meters x Height in Meters))
Write another function that takes a BMI value and returns the category (Underweight, Normal, Overweight, Obese I-III) as an integer. Use a data structure to convert the integer to a message.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/user_bmi_function.py"" lang=""python"" >}}
{{< /spoiler >}}
Early Returns
The return statement exits immediately with no more statements being executed.  A function may contain multiple return statements, but only the first encountered will be executed.  In conjunction with conditionals, the function can thus return early.
python
def sum_vals(x,y,z):
    """"""A stupid function.""""""
    if (x==0 and y==0 and z==0):
        return -1
    return x+y+z
In this example, an else clause is not required with the if because there is no subsequent use of the results.  Also note that the conditional as it as written affects the types that will be valid within the function."
rc-learning-fork/content/courses/python-introduction/expressions_statements.md,"Expressions
Expressions are combinations of variables, literals, operators on variables, invocations of functions, and so on.  Given values for each variable that appears, it must be possible for the interpreter to evaluate the expression to yield an unambiguous result.
The interpreter has a strict set of rules it follows to arrive at a unique evaluation.  It does not care what the programmer actually intended so you must adapt to it.  If it cannot arrive at an unambiguous result it will reject your code.
Examples:
python
a*b+c/d
math.pi*sin(x)
1./(x+y)
Operator Precedence
Among the rules for expression evaluation is operator precedence.  If you write
python
x*z-w/y+v
the interpreter must follow an order of operations to evaluate the expression.  Python, as well as most other programming languages, carries out the operations from left to right by the priority assigned to each operator.  In Python the ranking for arithmetic operators is, from first to last, **, then (* /) have equal rank, followed by (+ -) also with equal rank.  So in the expression above, the interpreter first evaluates x*z, then w/y, then adds those two results together, and finally adds v.  If you want a different grouping you must use parentheses. For example, you may want to add y and v before dividing.
python
x*z-w/(y+v)
A popular mnemonic for the order of operations is PEMDAS (Parentheses, Exponentiation, Multiplication/Division, Addition/Subtraction).
The interpreter will never be confused about the order in which it will evaluate an expression, but humans can often become confused.  It is better to include more parentheses than needed than to have too few, in order to keep your meaning clear both to the interpreter and your reader.
The Boolean operators have their own precedence rules. Highest to lowest are not, then and, then or.
All comparison operators have the same precedence relative to each other.  All comparison operators outrank all Boolean operators.
Exercise
Examine the results of the following:
{{< code-snippet >}}
a=11.; b=9.; c=45.; n=3
print(a > b)
print(a < b and c==n)
print(a < b or c==n)
print(a > b or c==n and a < b)
print((a > b or c==n) and a < b)
is_greater=a > b
print(is_greater,type(is_greater))
{{< /code-snippet >}}
Statements
A statement is one complete ""sentence"" of the language.  It contains one complete instruction.  Examples:
python
B=A
C=0.25*math.pi*d**2
Unlike some other languages, Python statements do not require a semicolon at the end of the line, and the standard programming style does not use one.  Semicolons may be used to separate multiple statements on one line.
The backslash character \ is the line-continuation marker.  A comma that separates elements can also mark a continuation as long as it is the last character on the line.
A statement that does nothing (a no-op) is the single word
python
pass
Examples
python
x=x+1
x+=1
(x,y,z)=myfunc(a)
f=open(""myfile.txt"",""w"")
x=0; y=1; z=2
A=[1,2,3,
   4, 5, 6]
Comments
Comments are statements or partial statements inserted for the benefit of human readers of the program.  Comments are ignored by the interpreter.  In Python ordinary comments begin with a hash mark (or octothorp) #.  All symbols from the hash mark to the end of the line are ignored.
Examples:
```python
The following line of code computes a number
z=a*b+c

f=open(""input.dat"",""r"")  #open file for reading
```
Docstrings
A special type of string literal is surrounded by triple double quotes """"""a"""""". When placed at the top of a unit of code, immediately after the declaration of the unit name if present, and indented to the correct level, the interpreter recognizes these as a special type of comment called a docstring (documentation string).  Spyder automatically puts a mostly-empty docstring at the top of each new file.  Docstrings are used to summarize the purpose and usage of the code that follows.
python
""""""
   Program: My Program to compute a value
   Author:  A. Programmer
""""""
It is a good practice to get into the habit of including docstrings in your code.  When properly done, they can not only provide useful information, but also certain automated tools included with Python can extract them and generate information for a built-in help system."
rc-learning-fork/content/courses/python-introduction/sequences.md,"﻿---
title: Sequences
toc: true
type: docs
draft: false
weight: 30
date: ""2020-11-17T00:00:00""
menu:
    python-introduction:
        parent: Compound Types

A sequence in Python is an ordered group of values that can be represented by a single variable. We can address individual elements or subgroups of a sequence with square brackets and possibly a range of indices.  Square brackets hold the index or range. 
python
A[0]
A[2:5]
A[i]
A[1:j+k+1]
Python is zero based (the first element is numbered 0) and the upper bound of any range is always noninclusive.  Python defines several intrinsic sequences: strings, Unicode strings, lists, tuples, and a few others that we will not cover.
An iterator is a data type that can be traversed in order.  Sequences either are directly iterators or can be readily converted to iterators.
Sequence Operators

Belonging
in operator.  x in S returns True or False if x is or is not an element of the sequence S.
Identity
is operator.  S1 is S2 returns True or False if S1 and S2 are exactly the same or different.  ""Exactly the same"" is quite rigid in Python so check documentation for the behavior of is with different objects.
in and is can be negated. S1 is not S2; A not in B.
Range extraction
S[il:ul+1] starts at il and goes to ul.  The colon is here called a range operator.
Starting from the end 
S[-N] is the N-1 element.  Thus S[-1] is the last element, S[-2] the next to last, and so forth. 
Concatenation (Joining)
S1+E1
Repetition
S1*N replicates the sequence S1 N times.  

Exercise
Examine the results of the following:
python
A=[1.,2,3.,4.,5,6]
1 in A
1. in A
9 in A
9 not in A
Remember that 1 and 1. are different types. 
python
A[3]
B=A[:]
C=A
B is A
C is A
B is not A"
rc-learning-fork/content/courses/python-introduction/for_loops.md,"One of the most fundamental processes in a computer program is to repeat statements many (perhaps many, many, many) times.  Computers never run out of patience.
Like most languages, Python has two major types of loops.  
For loops execute for a fixed number of iterations.  It is possible to exit early, but this must be explicitly added to the code. 
While loops do not start with a predetermined number of iterations.  They terminate when some condition becomes False.
For Loops in Python
python
for item in iterator:
    block1
else:
    block2
The else clause is optional and not frequently used; it is executed if the loop completes all the steps. Colons introduce a code block and are required as indicated. The code blocks must be indented.  The item is a variable which successively takes on the values in the iterator.  
Iterables and Iterators
An iterable is any data structure that can step through items one at a time.  An iterator is the structure by which an iterable is traversed. Some data structures, such as range, are direct iterators, whereas others, such as strings, are iterables with the ability to be iterated in a for loop.
Range
The range iterator is used to step over a sequence of numbers and is very frequently used with for loops.  It takes up to three arguments.

range(10)  : 0,1,2,3,4,5,6,7,8,9
range(1,10) : 1,2,3,4,5,6,7,8,9
range(0,10,2) : 0,2,4,6,8
range(10,0,-2) : 10,8,6,4,2 (note that zero is not included)

The interval is often called a stride.  If it is present the lower bound must also be present even if it is the default, 0.  Otherwise the lower bound may be omitted.  If the stride is omitted it is 1.  The last value is never reached.
In Python 3 the range function returns an iterator object and is not directly accessible.  To see the values or assign them to a variable, convert it to a list
python
print(list(range(10)))
Exercise
Execute the following for loop:
python
for i in range(10):
    print(i)
Modify this loop to print the values of i for 
python
range(10)
range(1,10)
range(0,10,2)
range(1,0,-2)
Modify your loop to print the first N integers.  Be sure that N is set to a value before you try to run the loop.
Write a loop that will sum the first N integers.  Hint: you will need a variable called an accumulator whose value is assigned outside the loop to 0.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/sum_to_N.py"" lang=""python"" >}}
{{< /spoiler >}}
Other Iterables
Any ordered sequence can be iterated.  Common types used with for loops are lists and strings.
python
cast=['John','Michael','Terry','Graham','Eric']
for player in cast:
    print(""One of the Pythons:"",player)
Another example:
python
for ch in ""HelloEverybody"":
    print(ch)
Enumerate
Sometimes we need both the item and its index.  We can use the enumerate function for this purpose.  Enumerate returns an iterator each element of which is a tuple consisting of a count, by default starting at 0, and the corresponding value of its argument.
python
velocity=[-11.,-3.,-1.,1.,2.3,4.]
for i,v in enumerate(velocity):
    print(i,v)
Zip
The zip() function is also handy, especially but not limited to, for loops.  Zip accepts multiple iterables and returns a new iterable whose elements are tuples of corresponding items from its arguments.  
python
x = [20.,9.,6.,5.,6.,8.3]
velocity=[-11.,-3.,-1.,1.,2.3,4.]
for t in zip(x,velocity):
    print(t)
When using zip there is a risk that the iterables will not be the same length. The default is for the function to stop when it reaches the end of its shortest argument, but this often masks a bug.  Python 3.10 introduced the strict parameter to zip.  If the strict=True option is added, it will throw an error if the lengths do not match.  
```python
Must be 3.10 or greater
x = [20.,9.,6.,5.,6.,8.3,12.3]
velocity=[-11.,-3.,-1.,1.,2.3,4.]
for t in zip(x,velocity,strict=True):
    print(t)
```"
rc-learning-fork/content/courses/python-introduction/while_loops.md,"A while loop uses a conditional to determine when to exit.  The loop must be coded to ensure that the conditional will become False at some point, or it will never terminate.
Python syntax
python
while conditional:
    block1
else:  #optional
    block2
As for the for loop, colons and indentations are required.  The optional else clause is executed if and only if the conditional becomes False, which for a while loop is normal termination.
{{< code-snippet >}}
x=-20
y=-10
while x<0 and y<0:
    x=10-y
    y=y+1
    z=0
print(x,y,z)
{{< /code-snippet >}}
Exercise
Modify each for loop in the previous section's exercise to use a while loop instead.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/sum_to_N_while.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/advanced_strings.md,"One of Python's strong points is its ability to do many things well, so both numerical and textual analysis can be done with the same language.  We will look at some of the many ways we can manipulate strings in Python.  It is important to distinguish between string functions, which take a string as an argument, e.g. sfunc(mystring), and string methods that follow the string variable, e.g. mystr.amethod().  Both perform operations on the string.  The classes chapter will make clear why the syntax differs.
The result of all the built-in methods and functions must be stored into a new string variable; they do not modify the original string.  For example,
python
str_list=str.split(',')
Categorization
Several methods are available to determine whether a string represents letters or may be text.  They apply to the entire string. They return a Boolean value.

Is alphabetic, is a number, is alphanumeric (combination of numbers and other characters).  Whitespace is none of these.
isalpha, isdigit, isalnum
mystr.isalpha()


Is uppercase, is lowercase, is ""title case"" (first letter of each word capitalized, all others lower case)
isupper, islower, istitle
mystr.isupper()



Manipulating Case

Switch to all uppercase
upper
mystr.upper()


Switch to all lowercase
lower
mystr.lower()


Capitalize first letter
capitalize
mystr.capitalize()


Convert to title case 
title
mystr.title()


Swap cases 
swapcase
mystr.swapcase()



Searching and Tests

Find a character or substring.  Returns location of the first occurrence only.
find
returns -1 if it does not find the substring 
mystr.find(s)


rfind(s)
searches right to left


Find the index of the beginning (left to right) of a substring.  Throws an exception (error) if the substring is not found.
index
mystr.index(s)


rindex(s)
searches right to left


Count the number of occurrences of substring s.  It is case-sensitive.
count
mystr.count(s)


Determine whether a string ends with a particular substring
endswith
mystr.endswith(s)


Determines whether a string starts with a particular substring
startswith
mystr.startswith(s)



Modifying and Filling

Remove characters from the beginning and end (if no arguments, i.e. the parentheses are left empty, remove spaces and tabs). The angle brackets indicate an option that can be omitted and are not typed out.
mystr.strip(<chars>)
mystr.rstrip(<chars>), string.lstrip(<chars>)
The default is whitespace (spaces and tabs)


Replace substring a with b.
mystr.replace(a,b)
Expand tabs.
The default is 8 spaces per tab.  If a different number is required, pass it in the parentheses.
mystr.expandtabs()  #8 spaces 
mystr.expandtabs(4) #4 spaces 
Justify in a field of width n spaces. Returns original string if the field width is too short.
mystr.rjust(n) # right justify
mystr.ljust(n) # left justify
Center in a field of n spaces. Returns original string if the field width is too short.
mystr.center(n)
Fill spaces with zeros in field of width n (mainly used with numbers).
mystr.zfill(n)

Splitting and Joining

Split on string s.  Is mostly used to split on a character or sometimes a very short string.  Splits on whitespace (spaces and tabs) when the delimiter isn't specified.  Returns a list with the delimiter removed, and each separated string an element of the list.
split(<s>)
mystr.split()
mystr.split(',')


Split on newlines.  Returns a list of the lines, with newline characters stripped.
mystr.splitlines()
Join a list of strings with a string (usually a single character) into a single string.  This is the inverse of split.  The syntax is peculiar, for Python.
<s>.join(list)
joins a list of strings with no spaces or other characters between them.
"""".join(strlist)
joins a list with commas between
"","".join(strlist)



String Module
All of the string operators and methods are available in the base Python installation.  However, there is a package string which contains some useful string literals.
python
import string 
string.ascii_letters
string.ascii_lowercase 
string.ascii_uppercase 
string.digits 
string.hexdigits 
string.octdigits 
string.punctuation   #(depends on the locale)
string.printable 
string.whitespace    #space, tab, linefeed, return, formfeed, and vertical tab.
Exercise
Assign the string ""1,ninety,23.8,4,two"" to a variable. Split the string on the commas. Go through the resulting list and find the items that consist of alphabetical characters, collecting them into another list.  Join them into a new string using - as the delimiter.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/string_functions.py"" lang=""python"" >}}
{{< /spoiler >}}
Resources
The official documentation for the string type is here.  A more complete discussion of built-ins for strings is here, including optional arguments for some of the methods described above."
rc-learning-fork/content/courses/python-introduction/files.md,"Files are the main ingredients of our programs.  Our scripts are files; we may have input files; we will usually want some kind of output file.  We can manipulate files in Python without having to go through the operating system's user interface.
Files, Folders, and Paths
Three major operating systems are in use today; Windows, macOS, and Linux. Each one does things a little differently.
Paths and Platforms
The location of a file is specified by its path.  The exact format of the path varies somewhat by operating system.  Python uses forward slashes to separate folders, even on Windows where the backslash (\) is ""native.""
Python tends to be rooted in the Linux operating system so some of the vocabulary comes from there.  ""Folders"" in Windows and macOS are called directories in Linux.  The full path to a file is the tree of all folders/directories that must be traversed to reach it. 
```python
Windows
filename=""C:/Users/You/Desktop/Python Programs/myscript.py""
MacOS
filename=""/Users/You/Desktop/Python Programs/myscript.py""
Linux
filename=""/home/you/python_programs/myscript.py""
```
We can use the os module to make our scripts a little more platform-independent.  The os module provides access to some basic operating-system functionality, particularly those related to files, in an interface that is uniform across different systems. 
In this example, the expanduser function will get the home directory of the user running the script.  For this we must use the path submodule of os.
python
import os
home_dir=os.path.expanduser('~')
The tilde ~ stands for the home directory in any operating system.
We can use path.join to concatenate directories and paths into complete file names. On Windows, path.join understands both forward and backward slashes.
```python
Windows
dir=""Desktop\Python Programs""
MacOS
dir=""Desktop/Python Programs""
Linux
dir=""python_programs""
All
script=""myscript.py""
filename=os.path.join(home_dir,dir,script)
f=open(filename)
``
Theos` module can perform basic file and directory manipulations in a way that is appropriate for each operating system.  
Changing and Creating Directories
When we run a script, the path from which it is run is the current working directory.  Note that JupyterLab and IDEs may set the current working directory their own way.  We can get it through the os module and we can change it.
python
mycwd=os.getcwd()
newpath=""/home/you/some_dir""
os.chdir(newpath)
To create a new directory, use mkdir
python
os.mkdir(new_dir) #relative to CWD
os.mkdir(fullpath)   #full path
Listing Files
We can list the files in a directory with listdir from os.  It returns a list.
python
files=os.listdir() #current working directory
input_directory=os.path.listdir(input_path) #using a full path
We can check whether an item is a directory with os.path.isdir(), which can also check whether it exists.  Similar for os.path.isfile().
```python
Starting from CWD
for file in os.listdir():
    if os.path.isdir(file):
        print(""{} is a directory"".format(file))
    elif os.path.isfile(file):
        print(""{} is a file"".format(file))
    else:
        print(""{} is neither a file nor a directory"".format(file))
```
If we do not need a list of the files but only an iterator, we can use scandir.  Scandir returns an object, not a string, so we must extract the parts we need.  The advantage to scandir is that it can be faster if we need to test any attributes of the file.  The with statement is intended to handle any exceptions.
{{< code file=""/courses/python-introduction/scripts/scandir.py"" lang=""python"" >}}
Copying and Moving Files
Another module, shutil, allows us to move and copy files and perform other basic operations on them.
python
import shutil
shutil.copy(source,destination)
shutil.move(source,destination)
In the above, both source and destination should be either strings or should be paths created by some function such as os.path.join.
For more details about the os module, see its documentation.
Opening a File
Before anything can be done with a file we must open it.  This attaches the file name to the program through some form of file descriptor, which is like an identifier for the file.  Once opened we do not refer to the file by its name anymore, but only via the ID.
We open a file and associate an identifier with it by means of the open statement.
python
fp=open(""filename"")
fin=open(""filename"",""r"")
fout=open(""filename"",""w"")
The default is read-only.  Adding the r makes this explicit.  To open for writing only, add the w.  This overwrites the file if it already exists.  To append to an existing file, use a in place of w.  Both w and a will create the file if it does not exist.  To open for both reading and writing, use r+; in this case the file must exist.  To open it for reading and writing even if it does not exist, use w+.
Closing Files
When you have completed all operations on a file you should close it.
python
fin.close()
fout.close()
Files will be automatically closed when your script terminates, but best practice is to close each file yourself as soon as you are done with it.  You must close a file if you intend to open it later in a different mode.  You cannot reopen a file using an active file descriptor.  You must first close it.
Exercise
All three major operating systems have a Documents directory by default in their desktop environments.  Write a script to 
 1. change the working directory to the Documents directory.  Use a general way to construct the path.
 2. List all the files in the directory.  Use a string function to check whether any end in "".txt.""  OK to use listdir.
 3. Make a new folder ""MyTestFolder"" in the Documents directory.  Check first whether it already exists.
 4. Open a file ""new_file.txt"" for writing.
 5. Use the following line to write a little text into it.  Replace ""f"" with your choice of file identifier.
    f.write(""Here are some words for this file.\n"")
 6. Close the file.
 7. Move the file into ""MyTestFolder.""
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/file_fiddling.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/pandas_plotting.md,"Pandas relies on Matplotlib for plotting.  We can combine them to work seamlessly with data.  To use any Pandas plotting methods, matplotlib.pyplot must be imported. 
Graphs and Charts
Many basic charts can be created through the plot method for Dataframes. 
python
import matplotlib.pyplot as plt
import pandas as pd
df=pd.read_csv(""some_file.csv"")
df.plot()  #plot all columns (must be numbers)
df[x='Column2',y='Column4'].plot() #choose columns
Example from the Python documentation:
{{< code-download file=""/courses/python-introduction/scripts/pandas_plot.py"" lang=""python"" >}}
Other types of charts and graphs are available and can be accessed through the kind keyword to plot, or by df.plot.kind()

df.plot.bar()
    Vertical bar chart
df.plot.barh()
    Horizontal bar chart
df.plot.box()
    Box plot
`df.plot.pie()
    Pie chart
df.plot.scatter()
    Scatter plot

python
df[x='A',y='B'].plot.scatter()
Other keywords to plot and its kinds control the appearance of the output.
In addition, there are separate methods df.hist() and df.boxplot() that have their own sets of arguments.
If multiple columns of a dataframe are compatible numerically, they can be specified and the plot method will create a superimposed chart with a legend.  Remember that the index is not a column of the dataframe, so it can be a date.
This example is a modification of another one from Pandas documentation.
{{< code-download file=""/courses/python-introduction/scripts/multiplot.py"" lang=""python"" >}}
Bar charts are similarly simple to create, with nice default labeling.
{{< code-download file=""/courses/python-introduction/scripts/barchart.py"" lang=""python"" >}}
Tables
A Dataframe is similar to a table, but printing it directly does not always produce good-looking output.  For Jupyter, Pandas has a ""Styler"" that can make a pretty table from a Dataframe.  It uses Web standards and the output is HTML.  For printing to a plot or paper, the table function of Matplotlib can be used within Pandas.
Example
This example is based on Pandas documentation, with some modifications, for the HTML output, with the table version based on online sources.
{{< code-download file=""/courses/python-introduction/scripts/plot_table.py"" lang=""python"" >}}
To see the ""pretty"" version, paste the text into a Jupyter notebook. If using the ""table"" version, place that into a separate cell.
Documentation
The Pandas visualization documentation is very thorough and shows a wide range of examples.
Exercise
Return to the bodyfat.csv file from a previous exercise.
Use Pandas to read the data into a Dataframe.  Use your BMI function from a
previous exercise to compute BMI for each row.  Add a new column for BMI.  Plot BMI versus body fat percentage.  Look up the documentation for pandas.plot.scatter for this plot.  Does one value seem out of place?
One way to remove outliers is to compute the 25% and 75% quantiles, take the difference QIF=quantile(75)-quantile(25), then use 1.5QIF as the threshold, i.e. anything less than quantile(25)-1.5QIF or quantile(75)+1.5*QIF is rejected.
Figure out a way to set the BMI values that are outside the cutoffs to np.nan using the .loc method.  Redo the scatter plot.  Pandas automatically removes missing data.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/bmi_pandas.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/project-set-5.md,"Classes.
Project 19
Download the file vabirds.csv. 
a) Use Pandas to read the file and capture the header information. Use the header to create of list of years.  Convert to a numpy array of floats.
b) Write a class Bird that contains the species name as a string and a numpy array for the observations.  Write a constructor that loads the values into an instance.  
c) Create an empty list.  Go through the dataframe and create a Bird instance from each row.  Append to your list of instances.
d) Add one or two methods to your Bird class that compute the maximum, minimum, mean, and median numbers of birds observed.  For the maximum and minimum also obtain the year.  (Hint: look up argmax and argmin for numpy).  You may wish to add an attribute years as well.  
e) Read the name of a bird from the command line or from user input (your choice).  Print a summary of the statistics to the console and produce a plot of observations versus years.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_5/birds.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 20
Write a program that reads a file with the following format:each line consists of an actor’s name followed by a semicolon, followed by a (partial) list of movies in which the actor has appeared.  Movie titles are separated by commas.  


You can handle this file by reading each line, then splitting on the semicolon so that you carve off the performer’s name.  Append that to an actorslist.  The rest of the line is a comma-separated string.  Don’t forget to strip the end-of-line marker.   Take the movies string and split on commas to create a list. 


Append (not extend) this list to a movies list.  This gives you a two-dimensional list (each element is itself a list).  Use your two lists to print the information in a nicer format.
Each line should be printed as has appeared in the following movies:   You should use your two lists to construct the above string.  Use join to rejoin the movies list into a string.
Use either a format string or concatenation to create the message string.

Read the name of the input file from the command line.  Use movies.txt as your example input file.
Modify your code to define a class Actor whose attributes are
name
filmography 


Write a constructor that stores these attributes as members of the instance.  The filmography will just be the movie list for this project.
Write a printme method that uses code you wrote previously to print an instance in the format specified.
That is, it will use self.name and self.filmography in the formatting. 
Keep your Actor class definition in its own file.  (The example solution is all one file for convenience in downloading.  Split off the class.)
Modify your code so that instead of storing actor and movielist separately, you will create instances of your Actor class.  Specifically, your actors list will now be a list of instances of your Actor class.  
As you read the file you will create a new instance using a line like
      actors.append(Actor(something,something))
After creating your list of instances, use your printme method to reproduce the output.
In addition to a constructor and a printme method, your Actor class should contain a method to return the actor name (a “getter”) and another to return the filmography as a string.  You can use these in your printme method.  
Write a separate program with a main() that reads the movies.txtfile and constructs the list of Actor instances.  
Use the list of Actor instances to create a dictionary in which the movie titles are the keys and the value corresponding to each key is a set of the cast member names.  

You will need to process through your actors list, checking whether each movie title is already in the dictionary.
If it is not, first create an empty set, then immediately update the set with the actor name (use the “getter”).  If the movie title is already a key, update the set of the castlist.

Write code to request from the user a movie title.  Use that movie title to print the castlist of the movie.   You can convert a set to a list with list(movie_dict[key]) and then use join to print the castlist neatly.

Be sure to use appropriate functions rather than monolithic code throughout this project.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_5/movies.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 21


Write a Fraction class that implements a representation of a fraction, where each instance consists of a numerator and a denominator. Overload addition, subtraction, and multiplication for this class.  Write a dunder to format each fraction in the form
no-highlight
5/7
For your first attempt it is not necessary to reduce the fraction, i.e. it is acceptable to have fractions like 6/8. Be sure to check for division by zero in your __truediv__ method.


Add a reduce method that finds the least common multiple to obtain the lowest common denominator and reduce the fraction.


Use NaN to represent division by zero and isnan to check for it.


{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_5/fractions.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/pandas_intro.md,"Pandas introduces new data structures, the most important of which are the Series and the DataFrame.
Series
The Series data structure consists of an index plus data.  It is similar to a dictionary with the differences that 

the size is fixed
requesting a non-existent index results in a Key Error (no dynamic creation)

Series objects are one-dimensional and can contain any type.  The indices are treated like row labels.
This simple example loads a Series with normally-distributed random numbers, then prints some of them, prints the basic statistics, and creates a line plot.
{{< code-download file=""/courses/python-introduction/scripts/pandas_series.py"" lang=""python"" >}}
Series are conceptually like a single column of a spreadsheet, with any headers omitted.  Values are similar to a NumPy Ndarray and most NumPy methods can be applied to the Series data, provided they are defined on the data type.  Missing data, represented by np.nan by default, will be omitted from the data used by these methods.
However, if the values are needed as an actual Ndarray they must be converted.
vals=randns.to_numpy()
We can load a Series with a dictionary.
scores=pd.Series({'Cougars':11,'Bears':9,'Cubs':8,'Tigers':6})
We can still slice it
scores[1:3]
We can still use iloc to extract by row number.  We can also use loc to extract by the row name.
scores.loc['Cubs']
Remember to print if using Spyder, or to run in the interpreter pane.
DataFrames
The most important data structure in Pandas is the DataFrame.  It can be conceptualized as a representation of a spreadsheet.  DataFrames are two-dimensional.  Each column has a name, which can be read from the headers of a spreadsheet, rows are numbered, and datatypes may be different in different columns.  Alternatively, a DataFrame may be regarded as a dictionary with values that can be lists, Ndarrays, dictionaries, or Series.
The DataFrame is a mutable type.
We can create a DataFrame by passing a dictionary. Consider a simple grade-book example.
{{< code-snippet >}}
grade_book=pd.DataFrame({""Name"":[""Jim Dandy"",""Betty Boop"",""Minnie Moocher"",
                                 ""Joe Friday"",""Teddy Salad""],
                         ""Year"":[2,4,1,2,3],""Grade"":[85.4,91.7,73.2,82.3,98.5]})
{{< /code-snippet >}}
python
print(grade_book)
The result of printing the DataFrame should look like this:
{{< table >}}
| | Name           | Year | Grade |
|-|--------------- | ---- | ----- |
|0| Jim Dandy      | 2    | 85.4  |
|1| Betty Boop     | 4    | 91.7  |
|2| Minnie Moocher | 1    | 73.2  |
|3| Joe Friday     | 2    | 82.3  |
|4| Teddy Salad    | 3    | 98.5  |
{{< /table >}}
Now we can apply methods to the grade_book DataFrame.
python
grade_book.describe() #Summarizes
grade_book.head()     #print first few lines
grade_book.tail()     #print last lines
The head and tail methods are more useful for longer datasets. We can provide them parameters to print a specified number of rows other than the default 5.
python
grade_book.head(2)
grade_book.tail(1)
Accessing and Modifying Data
We can access individual columns by name.  If the name of the column is a valid Python variable name then we may use it as an attribute; otherwise we must refer to it as we would to a dictionary key.
python
grade_book.Name
grade_book['Name']
grade_book.Grade.mean()
An individual column is of type Series.
Columns can be deleted. This does not change the original dataframe; it returns a new dataframe.  To overwrite the dataframe, add an option inplace=True.
python
grades_only=grade_book.drop(columns='Year')
A new column can be appended (the number of rows must be the same)
python
grade_book[""Letter Grade""]=[""B"",""A"",""C"",""B"",""A""]
Extract values into an Ndarray
python
grades=grade_book[""Grade""].values
To add a row, we should use concat.  The number of columns must match.
python
new_row=pd.DataFrame([[""Dinsdale Piranha"",1,75.5]],columns=[""Name"",""Year"",""Grade""])
grade_book=pd.concat([grade_book,new_row],axis=0)
To delete a row
python
grade_book.drop([len(grade_book)-1])
This drops the last row.
Plots
We can directly apply basic Matplotlib commands to DataFrame columns
python
grade_book.Grade.hist()
grade_book.Grade.plot()
Exercise
Set up a dataframe with the following ""weather"" data (it is synthetic):
no-highlight
Date, Minimum Temp, Maximum Temp
""2000-01-01 00:00:00"",-5.87,8.79
""2000-01-02 00:00:00"",-3.82,4.78
""2000-01-03 00:00:00"",-4.58,5.10
""2000-01-04 00:00:00"",-6.40,2.68
""2000-01-05 00:00:00"",-5.50,6.18
""2000-01-06 00:00:00"",-3.29,4.50
Run describe.  Print the mean values.  Extract the minimum temperature and the maximum temperature into Ndarrays.  Plot the data using Pandas.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/fake_weather_data.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/project-set-1.md,"Basic operations, lists, conditionals, loops.
Project 1
Write a program that:

Creates a list of temperatures [0, 10, 20, 30, 40, 50].
Prints the number of items in the list.
Prints the index of temperature 30.
Adds another temperature 60 at the end.
Loops through the list of temperatures and converts them from Celsius to
Fahrenheit, printing each value in degrees C and F.

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_1/temperature_list.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 2
Write a program that:

Creates the temperature list 0 through 60 by 10 using a loop rather than by
typing in all the values.
As each degree C value is added, converts it to F and adds the F value to a list
for the Fahrenheit equivalents.
Makes another loop which prints out C and F similarly to Project 1, i.e. both
on the same line, but does it by indexing into the two lists.

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_1/temperature_loop.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 3
Write a program that:

Generates a list of temperatures from -40 to 100 inclusive by increments of 5 degrees Celsius. 
Creates another list of the corresponding Fahrenheit temperatures. 
Creates a list of temperatures in degrees Fahrenheit which are greater than zero but for which the corresponding temperature in Celsius is less than zero. Print the elements of this last list.

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_1/temperature_cond_loop.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 4
The Collatz conjecture is a fun little exercise in number theory.

Given a positive integer, if it is odd multiply it by 3 and add 1. If it is even divide by 2.
Repeat this procedure until the result is 1.

The Collatz conjecture is that the sequence will always reach 1. No exceptions have been found...yet.  The number of steps required to reach 1 is called the stopping time.
A. Write a program that will find and print the stopping time for the first N positive integers. Count the starting number itself as one of the steps. Print a table of N and stopping time.
Test your program for N=30 and N=50.
B. Modify your program to print the starting number, its stopping time, and the maximum value of the sequence of numbers. Hint: If you use a list you will be able to use the len() and max() intrinsic (built-in) functions. Confirm that you get the same stopping numbers as before. Note: the example solution uses some capabilities of printing in Python 3 that we have not yet encountered, in order to make the output neater and easier to read.  Do not worry about aligning your results at this point.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_1/collatz.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 5
The algorithm for converting an integer N in base 10 to another base is as follows:

Find the remainder of N divided by the target base.
Divide N by the target base using integer division. If the result is greater than zero, replace the previous value of N by the result of the integer division. Store the remainder previously obtained as the new leftmost digit for the number in the target base. Repeat until the result of the integer division is 0.

A. Write a program to convert the first 51 integers, starting at 0 and ending at 50, to octal (base 8). Print a table of the decimal number and its octal equivalent.
Hint: construct a list of digits as you work through the integer divisions. The elements of the list should be strings so you’ll need to convert from integer to string. To change from a list of individual strings to a single string for printing, use the join function as follows:
 """". join(digits)
That is two (regular, not “smart”) double quotes with nothing between them, followed by a period, followed by join and in parentheses, the name of the list you have created.
B. Modify your program to handle bases up to 16 (hexadecimal). Use the letters of the alphabet to represent digits 10, 11, 12, ... as A, B, C, ... Hint: the char() built-in converts from an integer to its representation in the ASCII collating sequence. Note that A is number 65, i.e. chr(65)=""A"". The rest of the alphabet follows in numerical sequence to 96, then the lower-case letters begin at 97. Please use upper case letters.
The only widely used base greater than 10 is hexadecimal (base 16). Print a table of 0 to 32 as hexadecimal numbers.  Play with formatting to produce a nice table (use spaces, dashes, and the like).  The solution demonstrates controlling the appearance of printed output with format strings.  Take a look back when you have studied formatted output.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_1/base_convert.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/assertions.md,"We finally obtained a correct version of the dow.py day-of-the-week calculator, but we arrived by a rather cumbersome process of manually adding code.  We determined several test cases that revealed bugs in our code, but we added each one individually.  Why not automate this testing?
We can write what is sometimes called a test harness for the DoW function.  We will attempt to cover easier cases and the ""corner cases"" mentioned earlier.  It is particularly important to test leap years and non-leap years, since that is a major potential source of errors.
We can recast our tests for dow.py in a somewhat ad hoc manner.
{{< spoiler text=""Setting up tests for our DoW function."" >}}
{{< code-download file=""/courses/python-introduction/scripts/dow_tests.py"" lang=""python"" >}}
{{< /spoiler >}}
We can easily add individual days to the list of scattered test cases, and could repeat the while block to test additional individual years.
Assert
The assert keyword, as its name implies, tests whether a conditional is True.  If it is, execution continues.  If it returns False, the statement throws an AssertionError and execution ceases.  
The syntax is assert \, \.  The message is printed if the assertion fails.  
python
doW=DoW(30,5,2016)
assert doW==""Monday"", f""Should be Monday, got {doW}""
NumPy adds some additional array assertion functions, several of which are particularly useful for floating-point numbers, which cannot be reliably compared for strict equality.
The assert statement is intended for debugging only; use standard try and except, or other validation methods such as if statements, for checking production cases such as file errors, type errors, erroneous input to functions, and so forth.  This is just good practice in general, but it is also because the Python interpreter can be told to suppress assert statements.  One way to do so is to run from the command line with the -O (optimize) option.
Example
Run the following code from the command line (or use !python in an iPython console, or open a terminal in JupyterLab), first with python assert_it.py and then with python -O assert_it.py.  What should you do instead of using assert in this code?
{{< code-download file=""/courses/python-introduction/scripts/assert_it.py"" lang=""python"" >}}
Exercise
Starting from the dow_buggy.py script, add assertions to test for bugs.
{{< spoiler text=""Setting up tests for our DoW function."" >}}
{< code-download file=""/courses/python-introduction/scripts/dow_assert.py"" lang=""python"" >}}
{{< /spoiler >}}
Work through the bugs as you did before.  Hint: you may need to play with working through next, step in, continue, going in and out of debugging mode, etc.  Pay attention to the value of test_id."
rc-learning-fork/content/courses/python-introduction/debugging.md,"Let's try a more challenging debugging example. Download the file dow_buggy.py and the document Day_of_the_Week.pdf. The document describes an algorithm to find the day of the week for any date in the Gregorian calendar between the years 1400 and 2599. The algorithm is straightforward but has many steps, and also requires that we remind ourselves how to obtain a remainder from a division. In Python we use the modulo operator, represented by %; thus 7%3 is 1. Our code hard-codes in days of the week rather than obtaining input from the user, so to change the date you will have to edit the source file. 
Our first date is 30 May 2016. The code computes this to be a Thursday when it should have been a Monday. We don't know where to start, so we'll start at the beginning.
Spyder, or another IDE such as VSCode, may be a better choice than Jupyter for this project, since its built-in debugger and Variable Explorer view can be quite useful.  If you wish to use JupyterLab, install the ipykernel or xeus-python kernel, according to the documentation.  If using ipykernel, make sure it is at least Version 6.0. 
Debuggers work via breakpoints.  We set a breakpoint on specific lines.
Most debuggers use instructions like Next, Step In, Step Return or Step_Out, and Continue.  Continue means move on to the next breakpoint.  Next tells the debugger to execute the next line, but not to go line by line through a function.  Step or Step In enters the function and waits for a Next to be issued. Step Out/Return executes the rest of the function and returns.
In Spyder, click the blue ""Play"" icon to start debugging.  Set a breakpoint by clicking on the line number.  Note that some Spyder versions on some platforms, such as 5.1.5 on Linux, may not advance properly.  Upgrading (or, if necessary, downgrading) should solve that problem.  Stop debugging with the blue square icon.  The blue double arrow is ""Continue,"" the arc over a dot is ""Next,"" the down arrow over a dot is ""Step In,"" and the up arrow over a dot is ""Step Return.""
{{< figure src=""/courses/python-introduction/imgs/Spyder_debug.png"" >}}
In a debugger-enabled JupyterLab, first paste the function into a cell, and the main body into another cell.  At the right of the top ribbon, click on the ""bug"" icon next to the kernel name so that it turns orange. Also expand the ""bug"" icon on the right-hand sidebar to open views of the equivalent of the Variable Explorer.  Then breakpoints may be set by clicking on a line.  In JupyterLab with an appropriate kernel, the functions to move through the code are the icons above the ""Callstack"" pane on the right-hand ""debug"" sidebar.  Hovering over the icons shows which is which. They are very similar to the corresponding Spyder icons.
{{< figure src=""/courses/python-introduction/imgs/Jupyter_debug.png"" >}}
We will set a breakpoint at the line
python
year=2016
Run to the breakpoint, then step into the function.  The variables passed into DoW will appear in the Variable Explorer or Variables pane.  Press Next and watch the value of the variables after each line.  This should show that the variable D has the correct value, but M has the value 4. We are off by one on the month. That's because we number the months 1 to 12, but the corresponding indices of our lookup table run 0 to 11, so we must make a change to the indexing for the month. We change
python
M = months[month]
to
python
M = months[month-1]
Rerun with that correction. Now M is correct. We remove the breakpoint (or Step Out and Continue) and rerun the entire program. This time we get the right answer.  
Are we then done? We have only tested the code for one date. We must test more 
thoroughly, and we must especially test “corner cases,” situations where some unusual conditions may apply. For this code, we should test at least one date in each month, and we should test dates in years that are and are not leap years, taking special note of the rule for leap years in centuries. Century years must be divisible by 400, not 4, to be a leap year, so this is an example of a corner case.
Add the following code at the bottom of your script:
python
print(""\n\nTesting first of each month"")
day = 1
month = 1
while month < 13:
    print(""For {:} 1 the day of the week is {:}"".format(month_names[month-1],DoW(day,month,year)))
    month += 1
What do you get when you run it? Now we have to check with an independent calendar whether our results are correct. We are still in 2016 so we can use a calendar of that year.  In JupyterLab, disable debugging to see the output.
We find that our code says that January 1, 2016 was a Saturday, when it was actually a Friday. We are also off by one day for February 1, but March, April, and May are correct. The year 2016 was a leap year.  When we think about it, we realize that only days after February 29, 2016 will be affected by the leap year. We forgot to implement that part of the algorithm. Add the following code to DoW to fix this bug, right before it determines the value of C:
python
leap_year = (century_leap_year) or (year%4==0 and year%100 > 0)
if leap_year and month<3:
    L -= 1
Now it looks correct for the entire year…or does it? September 1, 2016 is reported to be a Friday rather than a Thursday. Another off-by-one error, it appears. Since it's dependent on the month, we need to look at the month computation. We double-check the values in our lookup table months and discover a typo; we have a 6 rather than a 5, in the ninth position. Correcting that makes our days correct.
We need to test other days, however. Try at least the following:
no-highlight
February 14, 2000
February 14, 1900
July 4, 1971
July 4, 1776
It's easy to find day of the week calculators online, but test against two of them to make sure all the methods agree. You can try your own birthdate as well.
{{< spoiler text=""Corrected dow.py"" >}}
{{< code-download file=""/courses/python-introduction/exercises/dow.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/pandas_grouping.md,"Sorting DataFrames
We can sort by column, single or multiple
```python
grade_book.sort_values(by=""Grade"",inplace=True) #ascending order
grade_book.sort_values(by=""Grade"",inplace=True,ascending=False) #descending
sorted_grades=grade_book.sort_values(by=[""Grade"",""Year""])
```
To sort by row, we must specify the row index, and all values across the rows must be the same type.  We then sort with axis=1.
{{< code-snippet >}}
inds=[""Jim Dandy"",""Betty Boop"",""Minnie Moocher"",""Joe Friday"",""Teddy Salad""]
grades={""Test1"":[85.4,91.7,73.2,82.3,98.5],
        ""Test2"":[88.1,89.8,75.9,84.0,96.3],
        ""Test3"":[83.7,92.4,70.1,88.2,96.8],
        ""Test4"":[84.1,87.2,69.3,81.7,93.9]}
{{< /code-snippet >}}
grade_record=pd.DataFrame(grades,columns=[""Test1"",""Test2"",""Test3"",""Test4""],index=inds)
```python



grade_record.sort_values(by='Jim Dandy',axis=1)
                Test3  Test4  Test1  Test2
Jim Dandy        83.7   84.1   85.4   88.1
Betty Boop       92.4   87.2   91.7   89.8
Minnie Moocher   70.1   69.3   73.2   75.9
Joe Friday       88.2   81.7   82.3   84.0
Teddy Salad      96.8   93.9   98.5   96.3
```



When True, the inplace parameter causes the sort to overwrite the dataframe. Otherwise a new dataframe will be returned.
Grouping
Groups can be created with groupby
clouds=weather.groupby(""Cloud Cover"")
This creates a ""GroupBy"" object.  We can print the first item in each group
print(clouds.first())
             Tmin  Tmax
Cloud Cover            
2           -6.40  2.68
3           -5.87  8.79
5           -3.82  4.78
We can get the group members with get_group
print(clouds.get_group(3))
                     Tmin  Tmax  Cloud Cover
2000-01-01 00:00:00 -5.87  8.79            3
2000-01-03 00:00:00 -4.58  5.10            3
2000-01-05 00:00:00 -5.50  6.18            3
The mean of a group is easy to compute
print(clouds.Tmax.mean())
Cloud Cover
2    2.68
3    6.69
5    4.64
Name: Tmax, dtype: float64
We can extract values corresponding to a quantity.  For our grade-book example, we can create a new dataframe for the students who received an ""A""
grade_book[""Letter Grade""]=[""B"",""A"",""C"",""B"",""A""]
top_students=grade_book[grade_book[""Letter Grade""]==""A""]
top_students
Pivot Tables
We can create pivot tables to reorganize the data.  Continuing with the grade book, we can make the names into the index and organize by grades.
(For this example the result will have quite a bit of missing data, but a more complete grade book could have more scores.)
student_grades=grade_book.pivot(index=""Name"",columns=""Letter Grade"",values=""Grade"")
student_grades
Exercise
Continuing with the weather_data notebook, group the data by state and print the first value for each state.
If you look closely, you will note there are some errors.  There are ""states"" DE and VA, which are the abbreviations for those states.  Correct those errors and obtain a new grouping by state.  Get the mean temperature, minimum temperature, and maximum temperature per state, using the round method to round to 2 digits. Determine the Python type of each of these results.  Look up how to concatenate them into a new dataframe.  Print this dataframe.  Sort the dataframe by mean average temperature, in descending order.
{{< spoiler text=""Example solution, zipped Jupyter notebook"" >}}
pandas_weather_ex3.zip
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/variables.md,"Variables in a computer program are not quite like mathematical variables.  They are placeholders for locations in memory.  Memory values consists of a sequence of binary digits (bits) that can be 0 or 1, so all numbers are represented internally in base 2.  Eight bits is a byte, another frequently used unit in computing.  Memory is organized into chunks called words; most modern computers use 64-bit (8 byte) words.   
Names of variables are chosen by the programmer.  Python is case-sensitive, so myVariable is not the same as Myvariable which in turn is not the same as MyVariable.  With some exceptions, however, the programmer should avoid assigning names that differ only by case since human readers can overlook such differences.
Variable names must use only letters of the Latin alphabet, digits, or underscores.  A variable name must begin with a letter or underscore.  Special symbols other than the underscore are not allowed.  By convention, variable names beginning with underscores are regarded as ""special"" and are reserved for certain circumstances, such as internal variables in classes.  Names surrounded by double underscores (sometimes called ""dunders"") generally are used by internal Python operations. 
Python Types
Variables always have a type even if it is not explicitly declared.  The primitive types correspond more or less to the types handled directly by the hardware, specifically integers, floating-point numbers, and characters.  Many languages define a number of other simple types including Booleans, strings, complex numbers, and so forth.  Python defines several primitive types and has some built-in compound types.
Integers
Integers are whole numbers and are not written with a decimal point.  In Python an integer can be of any size but numbers larger than what can be represented in the hardware will be handled by software and can be slow. On modern systems the hardware size is 64 bits.  Only signed integers are supported (so any integer may be positive or negative).
Floating-Point Numbers
Floating-point numbers have a decimal point, i.e. a fractional part. 
A standard called IEEE 754 defines the way these numbers are represented as base-2 numbers with a specific, and finite, number of bits for each value.
They are represented internally by a variant of scientific notation, to base 2, with one bit for the sign and the rest for the significand and the exponent. In most languages there are two types of floating-point numbers, single precision and double precision.  Single precision numbers are 32 bits long in total.  Double precision numbers occupy 64 bits.  Most of the time a Python floating-point variable is double precision.  Only in a few packages, mainly NumPy, is a single-precision floating-point number available.
A double-precision floating-point number has an exponent range, in base 10, of approximately 10-308 to 10308 and a decimal precision of about 15-16 digits.  The first thing to note is that this is finite.  The number of mathematical real numbers is infinite, and they must all be represented by the finite number of floating-point values.  It should be obvious, then, that infinitely many real numbers will map to the same floating-point number.  It also follows that only integers or terminating rational numbers can be exactly represented at all.  All other numbers are approximated.  Some numbers that are terminating in base 10 are not terminating in base 2 and vice versa.  Floating-point numbers do not obey all the rules of mathematical real numbers; in particular, they are commutative (so $a+b = b+a$) but are not necessarily associative (i.e. $a + (b+c)$ may not equal $(a + b) + c$) and they are generally not distributive (so $a \times (b-c)$ is not necessarily equal to $a \times b-a \times c$).  For most work these properties of floating-point numbers do not matter, but for some types of scientific programs this behavior can be important.
The floating-point standard also defines special values INF (and -INF) and NAN.  INF or -INF means the absolute value of the number is too large to be represented.  NAN stands for ""Not a Number"" and is returned when a program attempts to perform a mathematically illegal operation, such as dividing by zero.  It is also frequently used in some Python packages to represent missing data.
Complex
Python supports complex numbers.  A complex number consists of two double-precision real numbers and is expressed in the form
python
R+I*1J
or
python
R+I*1j
The imaginary part is denoted by the letter ""J"" (not ""i"" as in most mathematics) and it is not case-sensitive in this context.  The numerical value of the imaginary part must immediately precede it with no multiplication symbol.  If the imaginary part is a variable, as in the examples, the digit 1 must be present.  This is so the interpreter knows that the J indicates the square root of -1, and is not a variable.
Boolean
Boolean variables indicate truth value.  Booleans have only two possible values, True or False (note the capitalization).  Internally Booleans are integers, but this is (usually) not important to the programmer.  Use Booleans when a variable naturally represents a value that can be expressed as true/false or yes/no.
Choosing Good Variable Names
Variable names can improve or reduce the readability of a script or code.  Interpreters do not have problems keeping track of variables or following the logical flow of a code, but human readers do often have these problems.  Thinking of the script as text will help guide choices for variable and other names.  A variable name should be descriptive of what the variable represents without being too long. It is acceptable and common to include two words, but they should be separated either capitalizing some or all letters beginning a word or by underscores.  Some choices may be better for different types; for instance, the name of a Boolean may be a phrase that can be answered ""yes"" or ""no.""
isValid=True
is_valid=False
The first form in the example above is called camel case, which uses capitalization to separate parts of a long variable name. It gets its name from a fanciful comparison of the ups and downs of the case changes to a multi-humped camel.  The other convention uses underscores for purposes of clarity.  Camel case and underscores may be used together; choose whichever seems clearer given the context, though there tends to be a preference in Python for underscores for ordinary variables.  However, it is conventional to use camel case for certain advanced constructs such as classes.
Literals
Literals are specific values, as distinct from variables.  Literals also have a type, which is determined by their format.  Examples:

3 (integer)
3.2 (floating point)
1.234e-25 (floating point, exponential notation)
""This is a string"" (Python string)
True (Python Boolean)
1.0+2J (Python complex)

Type Conversions
Type conversions, also known as casts, are used when it is necessary to change a variable's type from one to another.  Python assigns types to variables by inference; that is, it determines from context what the type should be.  If the programmer needs it to be something different, an explicit cast can be used.
python
n=int(f)
f=float(n)
Often an arithmetic expression contains variables of more than one type. If no explicit casts are specified, the interpreter promotes each variable to the highest-rank type in the expression.  The hierarchy from top to bottom is complex, float (double), then integer.
In Python it is easy to convert a string to the number it represents, and vice versa.  No special ""internal buffers"" are required.  It is, however, important that the string represent a valid number, and it must match the type to which it is being cast.
```python
x=float(""11.3"")
ind=int(""11"")
y=int(""11."") #wrong, an error will result
amount=str(x+.4)
```"
rc-learning-fork/content/courses/python-introduction/numpy_funcs.md,"NumPy provides many built-in functions for array manipulations, mathematical/statistical calculations, and reading files.
Reading and Writing Files
NumPy includes several functions that can simplify reading and writing files.  For files with a simple spreadsheet-like structure, loadtxt works well.  The first argument can be either a file name or a file handle.
python
x,y=np.loadtxt(f, delimiter=',', usecols=(0, 2), unpack=True)
v=np.loadtxt(f,delimiter=',',usecols=(1,)) #usecols needs tuple
W=np.loadtxt(fin,skiprows=2)
If unpack is not specified, loadtxt returns the values into a rank-2 array; otherwise it returns the values into a tuple of rank-1 arrays.  Columns may optionally be selected with usecols.  Header rows can be ignored with skiprows.  Other options are available.  The loadtxt function assumes the delimiter is whitespace, so if it is another character it must be specified through the delimiter argument.
More generally the genfromtxt function can be applied.  The loadtxt function does not handle files with missing or irregular data, but genfrom txt can to an extent.
python
data = np.genfromtxt(infile, delimiter=';', comments='#', skip_footer=1)
For a simple data dump and restore we can use the save and load commands.  The counterpart to loadtxt is savetxt for saving a rank-1 or rank-2 array.
python
np.savetxt(outfile,A,delimiter=',')
The save function will dump an array in binary format to a .npy file. The first argument is a ""file-like"" object, such as a filename, or a file handle that has been opened.
```python
np.save(f,A)
or
np.savez(f,A)  #compresses, saves as .npz
```
Its counterpart is load
python
A=np.load(f)
The load function can read a compressed file generated by savez.
Some Frequently Used NumPy Functions
{{< table >}}
| Array Manipulation | Mathematical Operations    |
| ------------------ | -------------------------- |
| arange             | abs, cos, sin, tan         |
| array              | average, mean, median, std |
| argmin, argmax     | min, max                   |
| all, any, where    | ceil, floor                |
| compress           | dot, matmul                |
| copy               | sum, product               |
| ones, zeros, empty | min, max                   |
| reduce             | nan, isnan                 |
| repeat, reshape    | inf, isinf                 |
| rollaxis, swapaxis | linspace                   |
| transpose          | lstsq                      |
{{< /table >}}
This is just a sample; the full reference can be examined at the manual.
Random Values
Some NumPy functionality is implemented through subpackages.  One of the more widely used subpackage is the random module.  Base Python has a random module, but just as the Python math module cannot operate on Ndarrays, neither can the base random return arrays of numbers.  
There are now two sets of random functions.  The ""legacy"" functions are in the RandomState class.
The random_sample function generates uniformly-distributed pseudorandom numbers on the interval [0,1).  
python
import numpy as np
x=np.random.random_sample()  #a single value
y=np.random.random_sample(10) #a one-d array of 10
z=np.random.random_sample(4,5) #a two-d array of shape 4x5
w=np.random.rand(4,5)  #rand is a wrapper around random_sample
Other functions include
python
np.random.randint(1,11) #a random integer between [1,11) 11 not included
np.random.randint(1,11,size=10) #one-d array of random numbers
np.random.random_integers(1,10,size=10) #like above but inclusive of upper
np.random.choice([2,4,6,8]) #random selection from the sequence
deck=list(range(1,53))
np.random.shuffle(deck) #overwrites its argument
np.random.randn(4,5) #4x5 array of normally-distributed random numbers.
The newer class is the Generators class.  The names of the methods are generally the same.  To invoke Generator functions start off by calling the constructor.  In this example, PCG64 is the random-number generator algorithm.
python
from numpy.random import Generator, PCG64
rng = Generator(PCG64())
rng.standard_normal()
Ufuncs
Functions that accept both arrays and scalars are called ufuncs for ""universal functions"".  The mathematical, statistical, and random functions we have discussed are examples of built-in ufuncs.  You can write your own ufuncs easily.  These functions are subject to some restrictions:

The function must not change its input parameters.  
The function cannot print anything, or do any other form of IO
The function cannot exit (stop the run)
The function must return a single value (no tuples or other compound type)

Functions that adhere to these rules are said to be pure.  The prohibition on printing does complicate debugging somewhat.  Your functions must be thoroughly debugged for scalar inputs before testing with arrays. 
```python
import numpy as np 
def F2C(T):
    return 5.*(T-32.)/9.
TempsF=np.arange(0.,213.,2.)
TempsC=F2C(TempsF)
print(TempsC)
print(F2C(50.))
```
Optimization
Python can be quite slow, as is the case for most interpreted languages. Loops are generally the slowest constructs.  NumPy array functions are highly optimized and often can eliminate loops.
Example: 
The built-in sum over an array or array slice can replace the corresponding loops, and is much faster.
python
s=0
for e in V:
    s+=s+e
Use instead
python
s=V.sum()
Exercise
Download the bodyfat.csv file.  The weight is the third column and the height is the fourth column (in units of pounds and inches).  Write a program that contains a ufunc for converting pounds to kg and another to convert inches to meters.  Write a ufunc to compute BMI from metric height and weight.  Read the bodyfat.csv file and use the ufuncs appropriately to create a new array of the BMI values.  Look up the NumPy functions to print the mean and standard deviation of the values as well as the maximum and minimum values.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/bmi_code.py"" lang=""python"" >}}
{{< /spoiler >}}
Resources
Essential documentation for NumPy is at its home site.
The documentation includes a beginner's tutorial."
rc-learning-fork/content/courses/python-introduction/project-set-4.md,"Pandas
These are in generally in order of difficulty, from easiest to most difficult. We have not covered all of these items in our notes. There are many good resources online, e.g. you can Google: ""python pandas create new dataframe"".
Project 14
Create a new dataframe using the following table:
{{< table >}}
|  Coach    |    School |  Email  |  Career Wins  | National Championships |
|-----------|-----------|---------|---------------|------------------------|
|Tony Bennett| Virginia | tbennett@virginia.edu | 346  | 1  |
|Roy Williams | North Carolina | rwilliams@unc.edu | 871 | 3 |
|Mike Krzyzewski | Duke | coachk@duke.edu | 1132 | 5 |
|Tom Izzo | Michigan State | tizzo@msu.edu | 606 | 2 |
|Jim Boeheim | Syracuse | jboeheim@syracuse.edu | 944 | 1 |
{{< /table >}}

Begin by importing pandas
Create a new column that is the ratio of career wins to national championships.
Rename your columns with a variable name. This is not required but often makes your code more readable. 
Print a list of all columns.
split a subset of your dataframe based on some condition. Do it at least twice. Once using .loc and once using .iloc.
use the groupby() method to group your dataframe in some way that makes sense for your data

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_4/basketball_data.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 15
Take the following code block:
```
import pandas
df1 = pd.DataFrame({'ID #': [1,2,3,4,5,6,7,8,9,10],
                    'Names': ['John','Dave','Mary','Sarah','Mohammed','Rohan','Prisha','Vijay','Ananya','Raj'],
                    'Country': ['USA','USA','USA','UK','India','India','UK','India','UK','India']})
df2 = pd.DataFrame({'ID #': [1,2,3,4,5,6,7,8,9,10],
                    'Salary': [50000, 60000, 65000, 53000, 59000, 74000, 86000, 41000, 94000, 66000],
                    'Age': [24, 46, 51, 29, 33, 38, 70, 46, 49, 35]})
```

Join the two dataframes together using the merge function
How many people come from each country? (Hint: Don't just count them. Which function allows you to see that easily?)
Reshape the data and create a pivot table view of people by country using the pivot_table function. Also include the name, age, and salary in the results

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_4/pivot_table_example.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 16
Download the file pandas_demo.ipynb and the data files eighthr.data and eightr.names. If you are using Windows, check that it does not append "".txt"" to the data files.  You may need to open File Explorer, go to View, and check ""File name extensions.""  Open the notebook in JupyterLab or Jupyter.  Go through the exercises in each cell.
{{< spoiler text=""Example solution, zipped Jupyter notebook"" >}}
pandas_demo.zip
{{< /spoiler >}}
Project 17
Download the file cigarette-smoking-behaviour-2018-census.csv, which is about cigarette smoking in New Zealand.
- Read the file into a pandas dataframe
- Make a bar plot in Matplotlib of types of cigarette smokers ('Regular Smoker', 'Ex-Smoker', etc.) and their count
- Because we have a total number of respondents, let's make a new column that is a ratio of # of each category / total number of respondents
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_4/cigarette_smoking_nz.py"" lang=""python"" >}}
{{< /spoiler >}}
Project 18
Redo Project 14 using Pandas.
Download cville_2017_april.xlsx, which contains April 2017 weather data for Charlottesville, VA.
- Read the file into a pandas dataframe
- Make a line plot of average wind speed for each day
- Add main titles, and label axes to be more descriptive
- Play with the bottom axis (x-axis) to make sure all dates are visible
- Make a bar and line plot showing average wind speed (in bars) and max wind gust (as a line). Add legend to distinguish between the two.
- Make stacked bar chart of minimum and maximum temperatures for each day
- Make grouped bar chart of minimum and maximum temperatures for each day
- Plot the number of each weather 'condition'. Plot sunny days, partly cloudy days, and rain days. There are several ways to do this.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/solns/proj_set_4/cville_2017_apr_pandas.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/io.md,"Console Input
The console is a text interface for input to and output from the computer.  In Spyder, the iPython window itself can serve as a console.  In JupyterLab the output console is indicated by lines marked with Out [] whereas a reference to an input console will open a textbox. 
To read input from the console we use the input() function (Python 3+).  In Python 2.7 the equivalent is raw_input().  Any string within the parentheses is optional and, if present, it will be printed to prompt the user.  The input from the user is captured as a string and returned; it must be stored for any subsequent use.
The input function returns only a string.  If you need to use the values as any other type, you must perform the conversion yourself.
Example:
python
weight=input(""Enter your weight in pounds:"")
print(type(weight))
weight=float(input(""Enter your weight in pounds:""))
print(type(weight))
Console Output
We have already been using the print function.  Let us now examine it in more detail.
The print function

always inserts a space between its arguments
always adds a newline character unless the end argument is added.
print(var,end="""")

Messages can be added in between variables.
python
h=1.45;
w=62.
print(""Your BMI is"",w/ht**2)
Exercise
Use Spyder or another IDE to write a complete program to compute BMI from weight and height input from a user.  First request the user's choice of units.  We have not spent much time with strings yet so you may use a digit to indicate the user's choice, but remember it will still be a string on input. Then request weight and height.  You will need to convert these from strings. Look up the correct conversion factors for Imperial to metric units. Compute the BMI. 
The categories are 
{{< table >}}
|   BMI     |   Category |
|-----------|------------|
| less than 18.5    |  Underweight |
| 18.5 to 25.0      |  Normal      |
| 25.0 to 30.0      |  Overweight  |
| 30.0 to 35.0      |  Obese Class I |
| 35.0 to 40.0      |  Obese Class II |
| more than 40.0      |  Obese Class III | 
{{< /table >}}
Print the user's BMI value and category.
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/python-introduction/exercises/user_input_bmi.py"" lang=""python"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/python-introduction/_index.md,"Python is one of the most popular programming languages, widely used in areas as diverse as data science and Web programming.  This course is an introduction to programming as well as to the Python language.
Learning to program requires practice and patience.  You cannot learn by watching videos or even reading notes with examples.  You should attempt all the examples throughout this course.  Resist the urge to peek at the solutions as long as you can.  Work through at least a few of the Projects for each section.
When the course is being offered live the project sample solutions may be unavailable: try them yourself anyway, and return when the solutions are posted again.
Once you have mastered programming in Python, you may find that your scripts are slow.  We offer a course in High-Performance Python to help you learn to optimize your code."
rc-learning-fork/content/courses/python-introduction/formatted_io.md,"So far we have only considered list-directed input/output.  We leave it to the interpreter to format.  However, for output we often want or need more control over the appearance.  
Format Codes
We define format codes to describe how we wish the output to appear when printed.  A format code contains a letter for the type and, for numbers, one or more numerals to indicate the length to be printed.
Floating Point
Formats for floating-point numbers are indicated by a pattern:
python
F.wf
F.wg
F.we
F is the field width (total number of columns occupied), w is the number of digits to the right of the decimal point; these must be substituted by numbers. The letters f, g, and e are called format codes and indicate floating-point (decimal point) format, general format (interpreter decides f or e), and exponential notation.  If we do not specify w for numbers, the default is 6 decimal digits printed.  If we specify the number of digits the interpreter will round the result; if unspecified it will truncate.
Examples:

Use exactly 15 spaces with 8 decimal places 
15.8f
Let the interpreter decide the total field width but use 8 decimal places 
.8f
Print a float as an integer (this will truncate, not round, the decimal places)
0f
Print in scientific notation with three decimal places
.3e

Strings
Strings are specified with the letter s. They do not take a field width but can be adjusted or zero padded by using more advanced operations on the string to be printed.
Integers
Integers are specified by d (this stands for decimal, as in base-10 number) with an optional field width.  If the field width is wider than the integer, it will be padded with blanks by default.  If it is narrower it will be ignored.  If omitted the interpreter will use the width of the integer.  
To pad with zeros, write the zero in front of the width specifier.
Examples:
python
5d
05d
Formatters
To construct our formatted output we use format strings, also called formatters.  We insert curly braces as placeholders for variables in the finished string.  Format codes go inside the braces following a colon.
python
print(""The value of pi is approximately {:.6f}"".format(math.pi))
print(""Pi to {:d} digits is {:.12f}"".format(n,math.pi)
We have left the space to the left of the colon blank, but it represents the order of the arguments to format.  Thus the second example is equivalent to
python
print(""Pi to {0:d} digits is {1:.12f}"".format(n,math.pi))
This means we can rearrange the output if we wish.
python
print(""Pi is {1:.12f} to {0:d} digits"".format(n,math.pi))
Empty curly braces result in default formatting and ordering.  In this situation one could use list-directed formatting as well, but using a formatter enables more control over layout and spacing.
python
print(""I have {} books to read by {}"".format(12,""tomorrow""))
Values can be aligned in a field with < (left align), > (right align), or ^ (center align). These symbols must be followed by a field width.
python
print(""The number {:^8} is {:>15.4f}"".format(""pi"",math.pi))
Exercise
Type at the interpreter 
python
import math
Practice printing math.pi
- Print without any formatting
- Print using the default f format
- Print to 5 decimal places 
- Print the integer part 
- For at least one of the above, add a message
- Print the number of digits to at least 6 spaces and pad with zeros
- Print the number with a message that specifies the number of digits, where the number of digits is also to be printed.
Even more sophisticated formatting is possible.  In the above examples, when printing pi the value of n had to be 12 or the output would be inconsistent.  In newer Python versions we can make the width a variable.

print(""Pi to {} digits is .{dec}f}"".format(math.pi,dec=n))

Formatting with f-Strings
For Python 3.6 and up the formatted string literal or ""f-string"" was introduced.  These can be used to create formatted output easily.
Example
python
n=12
print(f""Pi to {n} places is {math.pi:.{n}f}"")
The f-string evaluates the contents of the curly braces when the program is run.  That means that expressions can be used inside the curly braces.
python
print(f""Pi to {n//3} places is {math.pi:.{n//3}f}"")
In this example, the integer division is required because the result of the expression must be an integer.
If quotation marks are needed inside an f-string (such as for a dictionary key) they must be single quotes.  In addition, the backslash \ cannot be a character within an f-string, so if a character such as the newline \n is needed, it must be assigned to a variable.
Resources
Full documentation is here.  Details on format strings is here.
A good reference for f-strings is here."
rc-learning-fork/content/courses/cpp-introduction/conditionals.md,"A conditional is a programming construct that implements decisions. 
* If the weather is good then we will go for a walk, else we will stay inside and watch TV.
* If it is cold enough to snow I will wear my heavy coat, else if it is warmer and just rains I will wear my rain jacket.
The expression following each if or else must be true or false, i.e. a logical expression (in Fortran terminology).
If - Else If - Else
The else if and else are optional. The parentheses around the Boolean expression are required.
c++
   if ( boolean1 ) {
      code for boolean1 true
   } else if ( boolean2 ) {
      more code for boolean2 true but boolean1 false
   } else {
      yet more code for both boolean1 and boolean2 false
   }
Here the curly braces denote a code block.  The else if and else can be on separate lines, but the layout illustrated is fairly conventional.
Only one branch will be executed.  Once any Boolean is determined to be true, the corresponding code will be executed and then the flow will proceed beyond the if block.
Exercise
Experiment with various truth values for bool1 and bool2.
{{< code-download file=""/courses/cpp-introduction/codes/if_demo.cxx"" lang=""c++"" >}}
The ? Operator
C and C++ support a very succinct operator for cases in which the purpose of the if block is to assign a variable.
no-highlight
expr1 ? expr2 : expr3
where expr1 must evaluate to a Boolean.  The expressions expr2 and expr3 should evaluate to the same type.
The operation should be read as ""IF expr1 THEN expr2 ELSE expr3"".  It returns the value of the selected expression.
c++
    float v = (y>=0.0) ? sqrt(y) : 0.0;
This is equivalent to
c++
   float v;
   if (y>=0.0) {
       v=sqrt(y);
   } else {
       v=0.0;
   }
Switch
Many ""else ifs"" can become confusing.  The switch construct can simplify the statements, under the right conditions.
""Expression"" must be an integer type or one that can be treated as an integer, e.g. char (but not string). 
The options must be literals or declared const.  The break statements are optional but cause the flow of control to jump out of the switch if one is executed.  Otherwise it will continue to the next case.
c++
switch (expression) {
    case const value0:
        code;
        break;  //optional
    case const value1:
        code;
        break;  //optional
    case const value2:
        code;
        break;  //optional
    case const value3:
        code;
        break;
    default :   //optional
       code;
}
default is for the action, if any, to be taken if the expression does not evaluate to any of the options available.
Example:
{{< code file=""/courses/cpp-introduction/codes/switch.cxx"" lang=""c++"" >}}
Exercise:
This is the National Football Conference standings in late 2020:
   The Packers only need a win over the Chicago Bears to secure the No. 1 seed in the NFC. A loss or tie by the Seattle Seahawks would also give Green Bay the top spot.  If the Packers lose, though, the Seahawks or New Orleans Saints could claim the top spot. The Saints would secure the No. 1 seed with a Packers loss, a win and a Seattle win. Seattle can get it with a win, a Green Bay loss and a New Orleans loss or tie.
Write a program to determine whether the Packers will win the No. 1 seed.  Given the following conditions (not what happened), who won?
Packers lose to Bears

Seattle wins

The Saints tie 
Hint: code can often be simplified with the introduction of logical variables which are sometimes also called flags.

Hint: if a variable is already a logical it is not necessary to test it against .true. or .false.

Hint: a loss or tie is not a win.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/nfc.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/project7.md,"
Write a Fraction class that implements a representation of a fraction, where each instance consists of a numerator and a denominator. Overload addition, subtraction, and multiplication for this class.  Write a method to format each fraction in the form
no-highlight
5/7
Optionally overload assignment (=).

For your first attempt it is not necessary to reduce the fraction, i.e. it is acceptable to have fractions like 6/8. Be sure to check for division by zero in your division procedure.


Set fractions with a denominator of 0 to 0/0 (this is arbitrary).


Write a driver to test all your procedures.


Add a reduce method that finds the least common multiple to obtain the lowest common denominator and reduce the fraction.


Extra: Look up how to overload << so that you can write
c++
cout<<f<<""\n"";
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/fractions.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/make.md,"make is a tool to manage builds, especially with multiple files.
It has a rigid and peculiar syntax.
It will look for a makefile first, followed by Makefile (on case-sensitive systems).
The makefile defines one or more targets .  The target is the product of one or more rules .
The target is defined with a colon following its name.  If there are dependencies those follow the colon.
Dependencies are other files that are required to create the current target.
Targets and Rules
Example:
myexec:main.o module.o
<tab>g++ -o myexecmain.o module.o
The tab is required in the rule.  Don’t ask why.
Macros (automatic targets) for rules:
$@ the file name of the current target
$< the name of the first prerequisite
Variables and Comments
We can define variables in makefiles:
CC =gcc
CXX=g++
We then refer to them as $(CC),$(CXX), etc.
Common variables: F90, CC, CXX, FFLAGS, F90FLAGS, CFLAGS, CXXFLAGS, CPPFLAGS (for the preprocessor), LDFLAGS.
The continuation marker \ (backslash) can be used across multiple lines. It must be the last character on the line; do not add spaces after it.
Comments are indicated by the hash mark #.  Anything beyond it will be ignored except for a continuation marker, which will extend the comment.
Suffix Rules
If all .cxx (or .cc or whatever) files are to be compiled the same way, we can write a suffix rule to handle them.
It uses a phony target called .SUFFIXES.
.SUFFIXES: .cxx .o
    $(CXX) -c $(CXXFLAGS) –c $<
Pattern Rules
This is an extension by Gnu make (gmake), but nearly every make is gmake now.
It is similar to suffix rules.
The pattern for creating the .o:
%.o: %.cxx
    $(CXX) $(CXXFLAGS) -c $<
Example:
{{< code-download file=""/courses/cpp-introduction/codes/Makefile"" lang=""make"" >}}
In this example, notice that the suffix rule applies the global compiler flags and explicitly includes the -c option.  If a particular file does not fit this pattern, a rule can be written for it and it will override the suffix rule.  The link rule includes the loader flags and the -o flag.  The compilation suffix rule uses the special symbol for the prequisite; the link rule applies to the current target.
The example also demonstrates switching back and forth between ""debug"" and ""optimized"" versions.  The ""debug"" version would be created this time.  The -g flag is required for debugging.  The -C flag is a very useful flag specific to Fortran that enables bounds checking for arrays.  Both these flags, but especially -C, will create a slower, sometimes much slower, executable, so when debugging is completed, always recompile with the optimization flag or flags enabled, of which -O is the minimum and will activate the compiler's default level.  You must always make clean anytime you change compiler options.
For further reading about make, see the gmake documentation.
Makemake
Makemake is a Perl script first developed by Michael Wester soon after the introduction of Fortran 90, in order to construct correct makefiles for modern Fortran code.  The version supplied here has been extended to work for C and C++ codes as well.  It is freely licensed but if you use it, please do not remove the credits at the top.
makemake
This version works reasonably well for Fortran, C, and C++.  It will generate stubs for all languages. You may remove any you are not using.  Also note that the output is a skeleton Makefile.  You must at minimum name your executable, and you must fill in any other options and flags you wish to use.  The makemake script blindly adds any files ending in the specified suffixes it finds in the current working directory whether they are independently compilable or not, so keep your files organized, and be sure to edit your Makefile if you have files you need but cannot be compiled individually.
Several other build tools, some called makemake, are available and may be newer and better supported.  See here for example.  That script also produces files for CMake, a popular build system, especially for Windows.
Building with an IDE and a Makefile
Several IDEs will manage multiple files as a ""project"" and will generate a Makefile automatically.  They do not always pick up dependencies correctly, however, so the programmer may need to write a custom Makefile.  A script like one of the makemake examples can help.
We will use the NetCDF library as an example.  This is a popular library for self-describing data files.  The example code is taken from their standard examples.  The file are simple_xy_wr.cpp.
On our test Linux system, the library is not installed in a standard location, so we must add flags for the headers and library paths.  Our example assumes the programmer added the environment variable $NETCDF_ROOT to the shell.
First we run makemake to obtain a skeleton Makefile.
{{< code file=""/courses/cpp-introduction/netcdf_example/Makefile.sample"" lang='make' >}}
We edit it to add the addition information required and to remove unneeded lines.
{{< code-download file=""/courses/cpp-introduction/netcdf_example/Makefile"" lang=""make"" >}}
Make with MinGW/MSYS2 on Windows
The MinGW64/MSYS2 system provides two versions of make.  In newer releases, on newer Windows, either should work.  If you do not wish to add an additional path to your PATH environment variable use mingw32-make.  You can change the Geany build commands through its build tools menu.  The mingw32-make tool may not support as many features as the full-fledged Gnu make provided by MSYS2. You can use Gnu make by adding the folder C:\msys64\usr\bin to your PATH variable.  This would not require changing the build tool on Geany.  To build a make project with Geany, be sure the main program tab is selected, then from the Build menu select Make.
Exercise 1:
If you have not already done so, download or copy the example.cxx and its required adder.cxx and header adder.h.  Place them into a separate folder.  Run makemake.  Edit the Makefile appropriately.  Build the project using Geany or your choice of IDE.
Exercise 2:
If you are working on a system with NetCDF available, download the two files and the completed Makefile into their own folder.  Open Geany and browse to the location of the files.  Open the two source files.  Either select Make from the Build menu, or from the dropdown arrow next to the brick icon choose Make All.
Build the code using the Makefile.  Execute the result."
rc-learning-fork/content/courses/cpp-introduction/class_structure.md,"There is more to object-oriented programming and classes than attributes and methods.  The basic principles are encapsulation, abstraction, inheritance, and polymorphism.  When we learned to write a C++ class, we were learning to deal with abstraction; how to express in code the abstract data types.  Now we will deal with encapsulation.
Data Hiding
In object-oriented programming, encapsulation may refer to bundling of variables and procedures (attributes and methods) into a cohesive unit, but also to data hiding, in which internal variables are kept internal to the objects and must be accessed through methods.
This is to prevent outside units from accessing members in an uncontrolled way.
Making a member public “exposes” it and allows anything that uses the class direct access to the member.
Typically, attributes are private and methods are public.
Methods can be written to obtain or change the value of an attribute.  These are often called accessors and mutators, or more informally ""getters"" and ""setters.""
(C++ also defines a category called protected, which we will discuss in more detail later.)
public
  * Accessible directly through an instance (myobj.var)
private
  * Accessible only within the class or to ""friend"" classes.  Must be communicated outside the class through an accessor and changed through a mutator.  This is the default for a C++ class member.
protected
  * Private within the class, accessible to ""friend"" classes and to descendant classes.
The default in C++ classes is private so we must explicitly declare members public if we wish them to be exposed.  Typically all or most attributes are kept private, whereas the methods must be public.  The default access class applies until an access label is encountered; subsequently the new access applies.
c++
class MyClass {
   double var1;
   double var2;
   public:
      int var3;
      MyClass();
      set_var1(double);
      set_var2(double);
};
The attributes var1 and var2 are private by default.
Example:
Download and run the following code.  The exact error will depend on the compiler, but it should detect an illegal attempt to access a private variable directly.
{{< code-download file=""/courses/cpp-introduction/codes/illegal_access.cxx"" lang=""c++"" >}}
Benefits of Data Hiding
We may ask what we gain from data hiding.  Consider an instance foo of a class FooBar containing an attribute bar.  If it is public we can change it with
c++
foo.bar=101;
However, what if 101 is an invalid value for this variable?  No checking will occur and we may have introduced a bug that could be very difficult to find.  If we force the program to use a mutator
c++
foo.set_bar(101);
then the set_bar method can perform checks on the input, take any other appropriate actions when bar is set or reset, and so forth.  
Once it is declared private, then we must also have an accessor if we need to know its value.
Structs and Classes
We did not go into details in our discussion of structs, but in C++ structs may contain methods.  The salient difference between a struct and a class is the default access control.  Struct members are public by default, whereas class members are private by default.
If structs and classes are so similar, why have both?  Structs were inherited from C, with methods added as an extension, whereas classes are specific to C++.  The choice of a struct or a class is up to the programmer's personal style or coding standards established for a project; there are no hard and fast rules.
However, many programmers use structs when it is natural for all members to be public, especially if the struct consists mostly of methods, or for when the data type represents POD or ""plain old data.""  The definition of POD is somewhat fluid but often refers to basic types such as are available in C.  This would limit it to primitive types such as numbers as well as char (but not string). This can be useful for interfacing with C libraries.
Friends
We have stated that private and protected members are accessible to ""friends"" without explaining what those are.  Functions and classes may be ""friends"" of a class.  Friends have access to all members of their buddies.
A friend may be a function or a class.
Friend Functions
A friend function is a function that is not part of the class.  It must be declared friend.
{{< code file=""/courses/cpp-introduction/codes/testfriendfunc.cxx"" lang=""c++"" >}}
Here color is a function that we might use elsewhere; it does not belong in the class, but we would like the class to be able to invoke it directly with its private attributes.
Friend Classes
Classes may also be friends with other classes.
{{< code file=""/courses/cpp-introduction/codes/testfriend.cxx"" lang=""c++"" >}}
Here we have an empty class declaration (a ""prototype"" of sorts) because ClassB contains a method that uses an instance of the class so the compiler needs to be aware of its existence.  We must define ClassA after ClassB, however, because ClassB is a friend so ClassA must know its definition.  
Classes may use instance of other classes.  This is called ""composition"" and we will discuss it in more detail along with inheritance.
""Friendship"" is not mutual unless explicitly declared; in the above example ClassB is a friend of ClassA, but ClassA is not a friend of ClassB.  
Exercises

Correct illegal_access.cxx by adding an accessor and mutator for the private variable.

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/access_demo.cxx"" lang=""c++"" >}}
{{< /spoiler >}}

Modify your Employee class to use appropriate access levels for attributes and methods.  Add any ""setters"" and ""getters"" that you may need.  Place the interface into a .h file and the implementation into a corresponding .cxx file.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/class_struct_example/Employee.h"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/class_struct_example/Employee.cxx"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/class_struct_example/employees.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/project3.md,"Using the cpi.csv data, write a program that will read from the command line the name of a file. Read this file into your program.  Request a year on the command line.  Optionally request from the user on the command line an amount. Check that you have enough command line input. Stop with a message if you don’t have enough command line values.
Once the data have been read, check that the requested year is in range.
The ratio of any two years is an estimate of the change in the cost of living.  Compute the change in the cost of living from the year you specify to 2020. Print it out neatly with some informative text.  Print the result to 2 decimal places.
In 1954 a color television cost $1295. From your result how much would that be in 2020 dollars?  
A rough estimate of the year-over-year inflation rate can be obtained from
c++
inflation[i]=(cpi[i+1]-cpi[i])/12.
Compute the inflation rate for the data and print with the corresponding year to
 a comma-separated file.  Use any plotting package with which you are familiar (Excel, Matlab, Python, R, etc.) to plot the data.
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/inflation.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/structs.md,"Even the standard templated types such as vectors are not sufficiently flexible for many applications; while they can be set up for many underlying types, all elements must consist of a single type.  Frequently we wish to collect different types together in some form of cohesive data structure.  To address this, a variety of avenues are available for programmers to define their own types.
For example, consider a program to update employee information.  We can define several variables relevant for an employee; for example we might use salary, name of manager, name of department, employee ID number, and so forth.  Each of these is potentially a different type.  Salary would be floating point, the names would be strings, and the ID number would generally be an integer.  We have more than one employee to handle, so we must use some form of list or array.  In most languages we cannot define a single array to accommodate all these fields.
This leads to the need for a way to keep all the information about one employee coordinated.
If we were restricted to generally-available types in C++, we would have to declare separate vectors for each field of interest.  When processing data, we would have to take pains to ensure that the index of one vector was correct for another vector.  Suppose we wanted to find all employees making more than a certain amount.  We would have to search the ""salary"" vector for the elements that met the criterion, while storing the indices into some other vector, so that we would have a vector whose contents were the indices for another vector.  This is very prone to errors. 
c++
std::vector<int>         employee_ID;
std::vector<std::string> employee_name;
std::vector<std::string> employee_manager;
std::vector<std::string> employee_dept;
std::vector<float>       employee_salary;
We need a different type of data structure.  Programmer-defined datatypes allow the programmer to define a new type containing the representations of a group of related data items.
For example, some languages define dataframes, which are essentially representations of spreadsheets with each column defined as something like an array. This would be an example of a defined datatype, since it must be described relative to basic types available in the language.  This is perhaps easier in languages that use inferred typing, where the interpreter or compiler makes its best guess as to the type of data, as opposed to statically typed languages like Fortran or C++.  But conceptually it is a good example of a programmer-defined datatype.
In C++ abstract user-defined types are called structs.
The syntax is extremely simple (ptype stands for a base type):
no-highlight
struct MyType{
  <ptype> var1;
  <ptype> var2;
};
Be sure to remember the semicolon after the closing curly brace.
We can apply this to our employee example.  The longer name for the fields is not necessary or helpful since we declare everything to be pertinent to an ""employee.""
c++
struct Employee {
   int     ID;
   string  name, manager, department;
   float   salary;
};
Each variable belonging to a struct is called a member.
It is customary for the name of a struct (or class) to be capitalized, or to use ""camel case.""
Note that a struct is a scoping unit.
Declaring Types and Accessing Fields
We declare variables much as for base types.  Such variables are often called instances of that struct.
c++
Employee  fred, joe, sally;
In C the struct keyword must be repeated before the struct name, but that is not required in C++ as long as there is no ambiguity.
Variables of a struct may also be declared when the struct is defined.
c++
struct Employee {
   int     ID;
   string  name, manager, department;
   float   salary;
} moe, curly, larry;
To access the members of the struct use the name of the struct, a decimal point as a separator, and the name of the member.
c++
   fred.name=""Frederick Jones"";
   fred.ID=1234;
   fred.department=""Accounting"";
   fred.salary=75200.00;
Initializing Variables
In C, struct members could not be initialized at declaration.  In C++11 and above, they can be.
c++
struct GridPoint {
   int x=0., y=0., z=0.;
}
They may also be initialized when an instance is declared using curly braces, in order of declaration in the struct.
c++
GridPoint p={1.,1.,1.}
Structs in Structs
Struct members may be instances of other structs.  The definition of the member struct must have been visible to the compiler before it is included in another struct.
```c++
struct Address {
string streetAddress;
string city, state;
int zipCode;
};
struct Employee {
string name, department;
int ID;
float salary;
Address homeAddress;
};
```
Pointers and the Arrow Operator
As for other types, variables can be declared pointer to struct:
c++
Employee *jane;
This is particularly common when passing struct (and class) instances to functions, to avoid a possibly expensive copy.
When using a pointer, the . operator is replaced with the arrow operator
c++
jane->name=""Jane Smith""
Arrays and Vectors of Structs
Arrays may be declared of struct types.
This example assumes a C++11 compiler for the initialization.
{{< code file=""/courses/cpp-introduction/codes/employees.cxx"" lang=""c++"" >}}
Notice in the vector example, we create an element with push_back by invoking the default constructor.  In C++ a struct has many of the same properties as a class, but we will defer discussing this until we cover classes.
Exercise
Copy the vabirds.csv file. Each line of this file consists of a string followed by a number of integers.  In the header (one line) the string is the word ""Species"" with the integers a sequence of years.  In subsequent lines, the string is the common name of the bird, and the integers are observations for the years indicated in the header.  Write a program that will
1. create a struct birdDat with members ""species"" and a vector for the observations.
2. create a vector of the struct.
3. Read the file name from the command line.
4. read the file.  Store the years from the header in a vector.  Load the data for each line into an element of the birdDat struct vector.  
5. Print the observations for AmericanCrow.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/read_bird_data.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/loops.md,"Much computing is repetitive work.  Evaluate an expression many times with different values.  Read lines of a file.  Update a large number of variables.  To accomplish this, we use loops.
Loops may invoke other loops as part of their bodies.  These are generally called nested loops.  Nested loops can result in many millions of executions of statements, so some care may be required in their construction to avoid unnecessary repetitions.
For Loops
The for loop executes a fixed number of iterations unless explicitly terminated.
It is most generally expressed as
c++
for ( initializer; stop condition; increment ) {
    code;
}
Starting from the initializer statement, the ""stop condition"" is evaluated.  If it is false the flow continues to the body of the loop.  If it is true, the loop is exited without executing any of the statements in the body.
One of the most common forms employs integer loop variables.
c++
for (int i=l;i<=u;i+=s) {
  code;
}
* i: Loop variable
* l: Lower bound
* u: Upper bound
* s: Stride, or increment.  Usually we use i++ for a stride of 1.
     * s can be negative, in which case l must be greater than u. For an increment of -1 use i--.
Do not change the loop variable within a loop.  That is, it should not appear on the left-hand side of an assignment.
There is a subtle difference between
c++
int i;
for (i=0;i<10;i++) {
    std::cout<<i<<""\n"";
}
versus
c++
for (int i=0;i<10;i++) {
    std::cout<<i<<""\n"";
}
In the first case, i is in scope within and beyond the loop.  In the second case, i is in scope only within the loop.  Try the example:
{{< code-download file=""/courses/cpp-introduction/codes/loop_scope.cxx"" lang=""c++"" >}}
Uncomment the line to print j after the second loop and try it again.  What happened?  To the compiler, the variable j literally does not exist outside the loop.
We will discuss scope in more detail later.
Range-Based For Loops
The C++11 standard has introduced a new version of the for loop that may be familiar to programmers of Python and similar languages.  This loop steps through an iterator.  An iterator is a sequence that can be traversed in a unique order.  The only iterator we have encountered so far is the string.  For example, we can loop over a string and extract each character:
{{< code file=""/courses/cpp-introduction/codes/iter_for.cxx"" lang=""c++"" >}}
While Loops
Whereas for loops execute a particular number of iterations, while loops iterate based on the truth or falsity of an expression.  The while continues as long as the expression is true and terminates when it becomes false. It is up to the programmer to be sure to add statements to ensure that the expression eventually evaluates to false so the loop will end.
c++
while (boolean expression) {
   statement;
   statement;
   statement somewhere to check expression;
}
Example:
{{< code-download file=""/courses/cpp-introduction/codes/while_demo.cxx"" lang=""c++"" >}}
Do While
The standard for and while loops test at the top.  That is, upon entry to the loop, the termination condition is evaluated.  If it is false, the statements of the loop are executed.  Otherwise the loop is exited.
With the ability to break out of the loop at will, we can change this pattern.
C++ provides a do while construct for this.
c++
do {
   statement;
   statement;
   while (boolean expression);
}
A standard while loop may not be entered if the condition is initially false, whereas a do-while will always be executed at least once.
{{< code-download file=""/courses/cpp-introduction/codes/do_while.cxx"" lang=""c++"" >}}
Exiting Early and Skipping Statements
The break statement leaves the loop immediately.
A break is able to break out of only the loop level in which it appears.  It cannot break from an inner loop all the way out of a nested set of loops.  This is a case where goto is better than the alternatives.
The continue statement skips the rest of loop and goes to the next iteration. Like break, it applies only to the loop level in which it is located.
c++
x=1.;
while (x>0.0) {
    x+=1.;
    if (x>=10000.0) break;
    if (x<100.0) continue;
    x+=20.0;
}
Exercises

Loop from 0 to 20 by increments of 2.  Make sure that 20 is included.  Print the loop variable at each iteration.
Start a variable n at 1.  As long as n is less than 121, do the following:
If n is even, add 3
If n is odd, add 5
Print n for each iteration.  Why do you get the last value?


Set a real value x=0. Loop from 1 to N inclusive by 1.
If the loop variable is less than M, add 11.0 to x.
If x > w and x < z, skip the iteration.
If x > 100., exit the loop.
Print the final value of x.
Experiment with different values for the variables.  Start with N=50, M=25, w=9., z=13.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/loops.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/c_arrays.md,"One of the most common data structures, especially for scientific and numerical programming, is a group of variables all of the same type.  This is frequently called an array.
Terminology
A scalar is a single item (real/float, integer, character/string, complex, etc.)
An array contains data of the same type with each scalar element addressed by indexing into the array.
An array has one or more dimensions .  The bounds are the lowest and highest indexes.  The rank is the number of dimensions.
A C-style array is nothing more than a block of memory that can be interpreted as an array; it is not a defined data type.  Other options are available in class libraries.
Arrays must be declared by type and either by size or by some indication of the number of dimensions.
c++
float a[100];
int M[10][10];
If a variable is used, it must be a const
c++
const int N=10;
float z[N];
The starting index is always 0, so for a 100-element array the items are numbered 0 to 99.
Orientation
Elements of an array are arranged linearly in memory no matter how many dimensions you declare.  If you declare a 3x2 array the order in memory is
no-highlight
(1,1), (1,2), (2,1), (2,2), (3,1), (3,2)
“Orientation” refers to how the array is stored in memory , not to any mathematical properties.
C++ and most other languages are row-major oriented.  Some (Fortran,Matlab, R) are column-major oriented.
Loop indices should reflect this whenever possible (when you need loops).
Move left to right.
A(i,j,k)loop order isdo for i/for j/for k
Initializing Arrays in C++
Arrays can be initialized when created using a constructor:
c++
   float A[3]={10.,20.,30.}
Curly braces are required.
Example:
{{< code-download file=""/courses/cpp-introduction/codes/simple_array.cxx"" lang=""c++"" >}}
Elements not explicitly initialized will be set to 0.
Try it: In the above program, change the initialization to
c++
float A[n]={};
Allow it to print the output.  Then try
c++
float A[n]={10.,20.,30.};
(i.e. setting only three out of five) with no other changes to the program.
WARNING
C++ happily lets you “walk off” your array.
Most commonly this occurs when you attempt to access an element outside of the declared size.  For instance, in C++ an index of -1 will never be legal, but for ordinary C-style arrays it will not check if your program attempts to access a value like A[-1].  It is also commmon, especially in loops, to try to access an element beyond the last one.  Do not forget that array indices are 0-based, so the last element of an array of size N will be N-1.
An error of this type usually results in a segmentation violation, or sometimes garbage results.
Example: in your previous code change
c++
cout<< A[i]<<"""";
To
c++
cout<< A[i+1]<<"" "";
Multidimensional Arrays in C++
Multidimensional arrays are just ""arrays of arrays"" in C++.
They are declared with multiple brackets:
c++
float A[2][5];
Elements are referenced like
c++
A[0][2]
A[i][j]
Small arrays can be initialized as follows:
A={{1.,2.,3.,4.,5.},{6.,7.,8.,9.,10.}};
Exercise
Write a program to:
* Declare an integer array of 10 elements
In your program:
  * Print the size of the array
  * Change the fourth element to 11
Declare a real array of rank 2 (two dimensions) and allocate it with new.  Allocate the array and set each element to the sum of its row and column indices.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/c_arrays.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/error_handling.md,"Errors in code can be the result of a programmer's mistake (i.e. a bug) or they can occur at runtime due to invalid input, system errors, and so forth.  For debugging, C++ uses the assert statement.  For runtime errors, we can try something and handle the result if a problem occurs.
Exceptions
Runtime errors may include
* An attempt to read a file that isn't present 
* A floating-point exception due to invalid input to a function
* A divide-by-zero floating-point exception
and so forth.
All these conditions (and many more) are called exceptions.  If the programmer does not handle them, execution will stop  
Catching Exceptions
The ""dangerous"" section of code is enclosed in a try block.  If an error occurs within it, an exception will be thrown so we can catch it. 
c++
try {
  if (y != 0) {
    float z=x/y;
  } else {
    throw 10;
  }
}
catch (int e) {
    cout << ""An exception occurred, error ""<<e<<""\n"";
}
Exceptions can be stacked
```c+++
try {
   //code, throw things
}
catch (int e) { cout << ""Integer exception \n""; }
catch (char c){ cout << ""Character exception \n"";}
}
```
Generic Exceptions
If we do not know what type of exception might occur, we can replace the parameters to catch with an ellipsis ...
c++
try {
    if (y != 0) {
        float z=x/y;
    }
    else {
        throw ""Exception"";
    }
}
catch (...) {
   cout << ""An exception occurred\n"";
}
Standard Exceptions
The C++ standard includes a base class for exceptions.  The exception header must be included. The programmer can derive custom exceptions, or use the built-in set provided by this class.  Some of the more commonly-seen ones are:
{{< table >}}
|   Exception     |    Cause    |
|-----------------|-------------|
| std::bad_alloc  | Can't allocate memory | 
| std::invalid_argument  | Invalid argument to function |
| std::length_error      | String too long |
| std::out_of_range      | Can be thrown by at operator (vector etc.) |
| std::overflow_error    | Numerical overflow error |
| std::underflow_error    | Numerical underflow error |
{{< /table >}}
See the documentation for more information.
c++
try {
   cout << ""Last item ""<<v.at(6)<<endl;
}
   catch (const out_of_range &e) {
   cout << ""Out of range \n"";
}
Exercise
Assemble the example snippets into a working program.
{{< spoiler text=""Example"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/exceptions.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
Assert
The assert statement is used to check for conditions that should not happen at all.  It is primarily used for debugging or quality assurance, not routine error handling.  The assertions in a program are typically disabled for production versions.
```c++
include 

assert(conditional);
```
Example
c++
assert(x != 0);
To disable assert statements, use the NDEBUG flag.
```c++
include 
define NDEBUG
```
A common practice is to define NDEBUG for debugging, then comment it out and recompile for the production executable.
Static Assertions (C++11)
The assert statement is executed at runtime.  C++11 introduces the static_assert statement, which is evaluated at compile time.
c++
static_assert (const_bool);
One example of how static_assert may be helpful is checking for the size of certain types.  For instance, the C++ standard does not require a specific size for int, just a minimum.  The exact size may be platform-dependent.  The statement
c++
sizeof(type) * CHAR_BIT
evaluates to the number of bits in type on the compiling platform.  If, for instance, we need to be sure that int is 32 bits, we can use static_assert
c++
static_assert(sizeof(int) * CHAR_BIT == 32);
The CHAR_BIT macro is in the C limits.h header.
Exercise
Run the following code as is on your platform. Change the number of bits to make the assertion pass or fail.
{{< spoiler text=""Example"" >}}
{{< code-download file=""/courses/cpp-introduction/codes/static_assert.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/project2.md,"Some languages, such as Python, can split a string on a specified delimiter character and return a list of strings. The delimiter is dropped.  Write a program that can accomplish this.  
This should be a function, but we have not yet covered subprograms, so you may write a monolithic program.  If you wish to look ahead, write this as a function.
Hint: a vector would be a good data structure. It is close to a ""list"" in Python.
Print the result in Python format, 
python
[sub1,sub2,sub3]
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/splitter.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/subprogram_args.md,"Passing scalar arguments to a function is straightforward.  It is a little more complicated when pointers or compound types are passed.
Passing by Value and Reference
Internally, the contents of the variables in the argument list must be passed to the function so it can evaluate them.  C++ uses two methods, pass by value and pass by reference.  Pass by value is the default for most arguments; a copy is made of the contents of the variable.  The variable in the caller is unaffected by what happens to it inside the function.
Pass by reference uses the operator & to pass a reference to the memory location of the variable.  If a referenced variable is changed in the function, its contents outside will also be changed.
{{< code-download file=""/courses/cpp-introduction/codes/subroutine.cxx"" lang=""c++"" >}}
Changing the value of an argument within a function is often called a side effect of the function.
C++ can also pass by pointer, which is very similar to passing by reference but has a few subtle distinctions.  A pointer is an actual variable whose value is the memory address to which it points; a reference is an object that holds the memory address of the variable it references.  A pointer can have the value NULL; a reference cannot.  Arithmetic can be carried out with pointers but not with references.
{{< code-download file=""/courses/cpp-introduction/codes/pointer_pass.cxx"" lang=""c++"" >}}
Passing Arrays to Subprograms
One-dimensional C-style arrays may be passed as pointers or with empty square brackets [].  The size must be passed as well.
c++
float mean(float A[],int n);
//code
myMean=mean(A,n);
This is equivalent to
c++
float mean(float *A,int n);
C-style arrays are always passed by pointer.
Containers such as vectors may be passed either by copying or by reference.
The choice depends on whether it is important to avoid changing the variable and how much speed matters.  Copying, especially of a large compound type, can be quite slow.
Example
```c++
include 
using namespace std;
float mean(float A[],int n) {
   float sum=0;
   for (int i=0;i<n;++i){
      sum+=A[i];
   }
   return sum/(float)n;
}
float mean2d(float *A,int n,int m) {
   float sum=0;
   for (int i=0;i<n;++i) {
      for (int j=0;j<m;++j) {
         sum+=A[i][j];
      }
   }
   return sum/(float)(nm);
}
int main(int argc, char argv) {
   int n=6, m=4;
   float *A=new float[n];
   float B=new float*[n];
   for (int i=0;i<n;++i) {
      B[i]=new float[m];
   }
for (int i=0;i<n;++i) {
      A[i]=i+1;
      for (int j=0;j<m;++j) {
         B[i][j]=i+j+2;
      }
   }
float mymean=mean(A,n);
   cout<<mymean<<""\n"";
   float mymean2d=mean2d(B,n,m);
   return 0;
}
``
In this example, deleting the memory allocated forAandB` is not necessary because it occurs in the main program, and all memory will be released when the program exits.  Best practice would generally be to delete the memory explicitly, however.
Local Arrays
Arrays that are local to a subprogram may be sized using an integer passed to the subprogram.
Local array memory must be released in the subprogram or a memory leak will result.
Wrong:
```c++
float newmean(float A[],int n){
   float *B=new float[n];
   float sum=0;
for (int i=0;i<n;++i){
      B[i]=A[i]+2;
      sum+=B[i];
   }
   return sum/(float)n;
}
Right:c++
float newmean(float A[],int n){
   float *B=new float[n];
   float sum=0;
for (int i=0;i<n;++i){
      B[i]=A[i]+2;
      sum+=B[i];
   }
   delete[] B;
   return sum/(float)n;
}
```
Passing Function Names
The name of a subprogram can be passed to another subprogram.  One method is to pass a function pointer to the subprogram.  A newer method is to use the standard functional templates.  
Example
a numerical-integration subroutine needs the function to be integrated.
```c++
include 
//Function pointer
//float trap(float a, float b, float h, int n, float (*function)(float)) {
//Templated function object
float trap(float a, float b, float h, int n, function f) {
```
where f is a function.
{{< spoiler text=""Full example of passing a subprogram as a dummy variable"" >}}
{{< code-download file=""/courses/cpp-introduction/codes/trap.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/project6.md,"Modify your bird_data struct from Project 5 to make it a class.  
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/birdclass/birdData.h"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/birdclass/birdData.cxx"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/birdclass/birdstats.cxx"" lang=""cxx"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/polymorphism.md,"Polymorphism means ""many shapes.""  In C++ it refers to the ability to define functions with the same name but different arguments, or in different classes; the latter case amounts to at minimum a different type for the hidden instance variable among the arguments.  There are two types of polymorphism: compile time and runtime.
For a compiler, binding is the process of associating the calls to a function name with the actual definition of the function.  Compile-time polymorphism means that this binding takes place at compile time and does not change when the program is run.  Hence it is often called static binding or early binding.  Runtime binding is also possible; in this case the compiler's runtime library is responsible for associating the invocation with the function definition.  This is called dynamic binding or late binding. 
Compile-Time Polymorphism
Overloading and Templating
Compile-time polymorphism is not limited to user-defined classes.  Functions may be overloaded by defining different functions with the same name but different return types and/or different argument lists, in number and/or type of arguments.  The compiler internally generates a ""mangled"" function name that creates a unique function for each case.
{{< code file=""/courses/cpp-introduction/codes/overload.cxx"" lang=""c++"" >}}
Templating is a form of overloading.  We can convert our sum function into a template, and it will work for any type for which the + operator is defined.
{{< code file=""/courses/cpp-introduction/codes/sum_template.cxx"" lang=""c++"" >}}
The class variable name (typename may also be used) is arbitrary, but T is customary.
Many built-in C++ libraries use templates for type declarations.
c++
vector<float> v;
Overriding Class Methods
Even more generally, inherited class methods can be overridden by derived classes, modifying them to be more appropriate to the derived class. 
{{< code file=""/courses/cpp-introduction/codes/override.cxx"" lang=""c++"" >}}
Runtime Polymorphism
Runtime polymorphism, or late binding, is achieved in C++ through virtual functions.  The virtual function is declared in the base class and is referenced through pointers or references in the derived classes.
{{< code file=""/courses/cpp-introduction/codes/virtual.cxx"" lang=""c++"" >}}
In this example we attach instances of each type to a reference to the appropriate type.
Among other things, virtual functions enable behaviors to occur when an instance of a class is passed (as a pointer or reference) to a function.  The function can bind the class method dynamically when it is invoked.
{{< code file=""/courses/cpp-introduction/codes/late_binding.cxx"" lang=""c++"" >}}
Without virtual each derived class would need its own version of printme.
Note that dynamic/late binding can be slow, so use it judiciously for codes where performance is important.
Operator Overloading
Just as named functions can be overloaded, so can operators.  This allows the programmer to define operators on user-defined types (structs and classes), which can result in much more compact and intuitive code, as long as the operators chosen are appropriate and make sense in the context in which they are used.  For example, a programmer can define
```c++
//Partial implementation
class Point {
   public:
      float x,y;
      Point(float x, float y);
      Point operator+(const Point &another);
};
Point Point::operator+(const Point &another);
    return Point(x+another.x,y+another.y);
}
In this example,c++
Point A(x1,y1), B(x2,y2); 
Point C(0.,0.);
C=A+B;
```
makes intuitive sense.  We would not want to overload multiplication with this definition.
Most of the standard C++ operators, including several we have not discussed, can be overloaded. Exceptions are . (member selection), .* (pointer to member selection), :: (scope resolution), and :? (conditional).  No preprocessor arguments (#) may be redefined.
In general, operator redefinitions do not have to be class members, but typically they are when it is possible.  If they are not members, they must be global (not recommended) or a friend function (better).  Some operators, such as << for printing, cannot be members because they must take the instance as an argument.
Assignment has a few extra requirements.  It must be a member function of a struct or class.  The argument is the right side of the equality and must be declared const.
```c++
class Point {
   public:
      float x,y;
      Point(float x, float y);
      Point operator+(const Point another);
      Point& operator=(const Point&);
};
Point& Point::operator=(const Point& rhs) {
    x=rhs.x;
    y=rhs.y;
    return this;
}
There are some additional complications for overloading assignment that we will not consider here; the major subtleties are copying versus assignment.  Copying creates a _new_ instance to hold the data, whereas assignment assigns values to an instance that already exists. C++ also allows self-assignment (f1=f1) and our simple-minded assignment operator above may fail in this case if memory must be allocated.  We can improve our example by checking for self-assignment 
by adding at the top of the function the linesc++
if (this == &str)
return this;
```
See here for details.
For a good discussion of operator overloading, see here or here.  
Overloaded operators other than assignment (=) can be inherited by derived classes."
rc-learning-fork/content/courses/cpp-introduction/scope.md,"Variable Scope
Scope is the range over which a particular variable is defined and can have a value.  In C++, scope is defined by code blocks, which are marked by curly braces {}.  Function parameter lists are also scoping units, as are certain statements (for, while, if, switch).
In the case of functions, the calling unit may have a variable named x, and a function may also have a variable named x, and if x is not an argument to the function then it will be distinct from the x in the calling unit.
```c++
x=20.;
float z=myfunc(x);
etc.
float myfunc(float y) {
float x=10.;  //can declare locals anywhere
return x*y;
}
```
A variable that is only in scope in a particular block is said to be local to that unit.  Variables that are visible from more than one block are global to those blocks.
Global variables are ""in scope"" within the file where they are declared.  To make them visible in other files, they should be placed within a header and declared with the extern keyword.
c++
extern float myglobal;
As a general rule, globals over multiple files should be avoided.  Defining them in a class would be preferable.  C++20 and up will add modules, which would be the most appropriate structure for this type of use.  Modules are familiar to Python and Fortran programmers, to name two languages that support them; they are a programming construct that isolates a unit of code, separating it into a separately compiled bundle, from which other program units can import its variables and functions.
Within a scoping unit, if a local variable is defined with the same name as a global variable, the local variable takes precedence.  The global variable can be accessed with the scope resolution operator ::.
{{< code-download file=""/courses/cpp-introduction/codes/scope.cxx"" lang=""c++"" >}}
Beware of changes in behavior in for loops.  Prior to the C++98 standard, declaring a loop variable within the parentheses left it in scope.  So code such as the following was legal:
```c++
for (int i=0; i<10; i++)
{
   // code
}
x = (float)i;
Old code tends to persist so programmers may encounter this. The solution is to declare `i` outside.c++
int i;
for (i=0; i<10; i++)
{
   // code
}
x = (float)i;
```
Namespaces
We have seen the scope-resolution operator before:
c++
std::cout<<""Print something\n"";
In this case, std is a namespace.  A namespace is something like the ""family name"" of a group of variables and/or functions.  The standard namespace std includes all standard C++ functions and data structures and is brought in through standard headers.
A fully qualified name includes all namespace references.
c++
std::vector<std::string> words;
By employing namespaces, different libraries can define functions with the same name.  The ""family name"" namespace can then differentiate them.
Defining a Namespace
You can define your own namespaces.  Most usually the elements are classes, which we have not yet discussed, but that is not necessary.
c++
namespace blue {
   float x,y;
}
namespace yellow {
   float x,y;
}
Notice that we can have variables with the same names in different namespaces, because a namespace is a scoping unit.  We must reference the variables with their namespace name:
{{< code-download file=""/courses/cpp-introduction/codes/namespace.cxx"" lang=""c++"" >}}
Namespaces can be nested:
{{< code-download file=""/courses/cpp-introduction/codes/nestednamespace.cxx"" lang=""c++"" >}}
The Using Directive
When we insert a using statement into a scoping unit, we do not have to preface each member of the namespace with the name.  Most commonly we do this with the standard namespace.
```c++
include 
using namespace std;
int main() {
cout<<""Now we don't need the prefix\n"";

return 0;

}
```
In this case, we declare that we are using namespace std at a global scope through the rest of the file.  We can also use namespaces within a scoping unit, and that persists only within that unit.
Another form of the directive limits the reference to one item only.
The namespace must have been defined before we can do this.
{{< code-download file=""/courses/cpp-introduction/codes/namespacescope.cxx"" lang=""c++"" >}}
Using in Header Files
Introducing a using directive in a header file will bring the namespace into scope and can result in name collisions, especially if multiple headers are used.
It is strongly recommended that programmers use fully-qualified names in headers, with the using in any corresponding implementation files."
rc-learning-fork/content/courses/cpp-introduction/inheritance.md,"One of the foundations of object-oriented programming is inheritance.  Often, two objects are related and we can save coding by reusing code from one object in another.  Rather than retyping or cutting and pasting, we can simply pass down the members of one object to another.
The lower-level link is called the base class (or parent) and the subsequent one is called the derived class (or child).  
Inheritance should reflect an IS-A relationship between the two objects.  A cat is-a animal.  A rectangle is-a shape.  
Member Inheritance
Members declared in the base class cannot be inherited if they are declared private.  They must be public or protected to be transmitted.
The syntax to declare a derived class is
no-highlight
class child-class: access-mode parent;
The access category is one of private, public, or protected.  If not specified the usual default of private is assumed.  
C++ permits multiple parents, but this is generally discouraged.
no-highlight
class child-class: access-mode parent1, access-mode parent2;
The access-mode specifier for the base class affects how the members of that class are transmitted to the derived class. If it is public the public and protected members of the base class retain that access mode in the derived class.  If it is protected, both public and protected members are transmitted as protected.  In the private case, all members are transmitted as private.  In all cases, private members in the base class are not accessible directly to the derived class and an accessor must be provided if they should be available.  Declaring the base class public is the most common due to the restrictions imposed by the other access modes.
Example
{{< code-download file=""/courses/cpp-introduction/codes/inheritance.cxx"" lang=""c++"" >}}
Notice that an antelope is-a animal and a reptile is-a animal, but an antelope is not a reptile.  So derived classes do not need to have a direct relationship with one another aside from their inheritance.
We could introduce another level of derivations since ""Reptile"" is a broader category than ""Antelope.""  For example, we could declare class Mammal
c++
class Mammal: public Animal {
   protected:
      string furColor;
      string order;
      string getOrder();
};
We could then derive Antelope from Mammal.  Further subdivisions would be possible (class Ungulate and so forth).
Derived classes can continue like this in an inheritance chain indefinitely, though it would be poor practice to set up more than a few links in the chain.   
{{< diagram >}}
graph TD;
A(Animal) --> B(Reptile)
A(Animal) --> C(Mammal)
C(Mammal) --> D(Antelope)
{{< /diagram >}}
Constructors
Constructors, destructors, and friend classes are not inherited from the base class by derived classes.  We can still create constructors, and the derived class can invoke the parent constructor.
{{< code-download file=""/courses/cpp-introduction/codes/child_constructor.cxx"" lang=""c++"" >}}
In this example, the child inherits the getName accessor from the parent.
But age does not refer back to the parent, since that variable occurs only in the child, so it must be explicitly declared.
The constructor for the Child class invokes the Parent constructor and then sets the age.  
In C++11 the using keyword may be employed to bring in the base constructor.
c++
class Child: public Parent {
    private:
        int age;
    public:
        using Parent::Parent;
        void setAge();
        int getAge();
};
The Parent constructor cannot be used to set the new member age, so a mutator would be defined.  For this reason, some software engineers recommend keeping the older parent constructor syntax if the derived class defines its own constructor.
Exercises

Add a new class Mammal as sketched above.  Derive Antelope from that. Add an attribute scaleColor to the Reptile class.

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/inherit_chain.cxx"" lang=""c++"" >}}
{{< /spoiler >}}

Create a constructor for Animal that sets the name, food, foodQuantity, and vocalization.  Pass it through to the descendant classes and in each one, add the attributes new to that class.  Remove functions made unnecessary by the constructor.  Optionally implement the using syntax in the Antelope class.  Depending on your compiler version, you may need to add a flag -std=c++11 or equivalent.

{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/constructor_chain.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
Extra Exercises
Clean up the solution to Example 2 by declaring attributes private or protected and implementing all required accessors and mutators.  
Optionally, implement Animal in its own interface and implementation files.  Include its implementation header into the source with 
```c++
include ""animal.h""
``
You will need to compile your source files separately and linkanimal.o` appropriately."
rc-learning-fork/content/courses/cpp-introduction/declarations.md,"Like most compiled languages, C++ is statically typed .  All variables must be declared to be of a specific type before they can be used.  A variable’s type cannot be changed once it is declared.
C++ is (nearly) strongly typed.  Mixed-mode expressions are limited and many conversions must be explicit.
Variables are declared by indicating the type followed by a comma-separated list of variables followed by a semicolon.
c++
int i, j, k;
float  x, y;
Initializing at Compile Time
Variables can be declared and initialized at the same time:
c++
float x=1.e.8, y=42.;
int i, j, k, counter=0;
C++, but not C, permits two additional formats for initialization.  Constructor initialization encloses the initial value in parentheses.  Uniform initialization uses curly braces.
c++
float x(0.);
int   i{0};
int   j=2;
Example
Start Geany or your preferred editor or IDE.  Type
{{< code file=""/courses/cpp-introduction/codes/variables.cxx"" lang=""c++"" >}}
Variables can be declared and initialized anywhere in the body of the code as long as they are declared before they are used.  There are some implications for variable scope, which we will discuss later.  
{{< code file=""/courses/cpp-introduction/codes/vardecls.cxx"" lang=""c++"" >}}
Auto
A recent introduction to the C++ (since C++11) standard is the auto declaration.  We have learned that the compiler is able to infer the type of literals.  Thus, it can deduce the type of variables that are initialized to literal values.  Moreover, if a variable is initialized to a variable previously declared to be a named type, the compiler can assign the same type to the new variable.
```c++
float x=12;
auto  y=x;
auto i=12;
Any `auto` variable _must_ be initialized.c++
auto z;
no-highlight
error: declaration of ‘auto z’ has no initializer
``
If initializing anautovariable with a literal, be sure the type of the literal is what you intend.  Keep in mind that3and3.0are _distinct_ types to the compiler.  Thetypeid` built-in will return the type.
{{< code file=""/courses/cpp-introduction/codes/autos.cxx"" lang=""c++"" >}}
On Unix with g++ this results in
no-highlight
f d i
Recall that the literal 3.0 is a double.
If multiple assignments are made with auto they must be the same.

Wrong:
auto i=0, f=3.14159;
Right:
auto i=0;
auto f=3.14159;
Decltype
Similar to auto, we can use the C++11 declaration decltype to declare that a variable is of the same type as another, previously-declared variable.
c++
float x;
decltype(x) y;
Variable y will take the same type as x.  This declaration was intended primarily for more advanced applications such as lambda functions and templates, where it may be difficult to determine the type at compile time.
Qualifiers
Constants
Programmers can declare a variable to have a fixed value that cannot be changed. In C++ we add the qualifier const to these variables.
const double pi=3.141592653589793;
Variables declared const must be initialized when they are declared.  Any attempt to change them will result in a fatal compiler error.
Volatiles
A volatile variable is one that the compiler may not change and that may be beyond the control of the program itself.  The value of such a variable must always be read directly from main memory.  An example might be a program that obtains data from an instrument.  We do not want the compiler to modify the instrument readings so we would declare those variables volatile.
c++
volatile float voltage;"
rc-learning-fork/content/courses/cpp-introduction/type_conversions.md,"Type Conversions
As we have seen with the example of dividing two integer, operators are defined on specific types and return a specific type.  What if we write 2./3?  The first operand is a double, whereas the second is an integer.  This is called a mixed expression.  For consistency, one type must be converted to match the other before the operator is applied.  
Implicit Conversions
Most compilers will automatically cast numeric variables to make mixed expressions consistent.  The 
hierarchy, from lowest to highest, is bool -> char -> short int -> int -> unsigned int -> long -> unsigned -> long long -> float -> double -> long double.
Each variable will be promoted until all in the mixed expression are the same type.
Explicit conversion also occurs when a variable or literal of one type is assigned to a
variable of another type.  If the conversion is legal, the compiler will force
the type of the quantity on the right-hand side of the assignment to match the declared type of the variable on the left-hand side.  
The rules for numerical type conversions may result in some surprises.  For example, when a float is converted to a double, the extra bits in the significand are filled (""padded"") with zeros.  There is no magic that tells the compiler how to extend it ""correctly.""
This can result in loss of precision or even seemingly-bizarre results, such as when a signed int is converted to an unsigned int.
To illustrate:
{{< code file=""/courses/cpp-introduction/codes/conversions.cxx"" lang=""c++"" >}}
The result on a Unix system with g++ is
no-highlight
Signed to unsigned:
-1100020
4293867276
Float to int:
4.78000020980835
4
Double to float:
3.141592653589793
3.141592741012573
Notice that the compiler prints out 16 digits of the float variable when told to do so, even though they are incorrect past the 7th decimal place.  Moreover,
conversion to int from any floating-point type truncates, it does not round.
Explicit Conversion
Explicit type conversion is also called casting.
Use explicit casting to be clear, or in circumstances such as argument lists where the compiler will not do it.
Explicit casting among numeric types:
c++
r=(float) i;
j=(int) w;
d=(double) f;
The same phenomena apply as for implicit conversions when one numeric type is converted to another. 
{{< code file=""/courses/cpp-introduction/codes/casts.cxx"" lang=""c++"" >}}
The result on a Unix system with g++ is
no-highlights
Cast 0.3333333432674408
Literals 0.3333333333333333
Recall that literal floating-point values are typed as double by C++.
The above format was inherited from C.  C++ also supports a functional notation:
r=float(i);
j=int(w);
d=double(f);
All these conversions are called C style casts.  They are very permissive.
This is not a problem for ordinary numeric variables as long as the programmer understands the rules.  It can become a problem when converting from one pointer type to another.
We will discuss this in more detail when we talk about pointers.
Newer C++ style casts look like templates.
The static cast is most widely used and behaves similarly to the C-style casts.
c++
int n=100000;
double s = static_cast<double>(n);
The dynamic_cast is mostly used for pointers or reference to classes and is beyond our scope here.  The constant_cast can be used to both cast and alter the value of a variable declared const and likewise is beyond our scope in this introduction. Finally, a reinterpret_cast is defined; it is used to convert pointers from one type to another and also will not be discussed in this introduction.
String/Numeric Interconversions
C++ has introduced stringstreams for this purpose.
Stringstreams are internal string buffers.
In this example we have included the line 
c++
using namespace std;
for convenience.  We will discuss namespaces in when we talk about 
scope.  In short, it makes the standard namespace the default, so that we may omit in before keywords such as cout and string.
```c++
include 
include 
include 
using namespace std;
int main() {
    string age;
    int iage;
    iage=39
//Convert numeric to character:
stringstream ss;
ss<<iage;       //load iage into buffer
age=ss.str();

//Convert character to numeric:
    age='51'
    stringstream ss2(age);
    ss2>>iage;
}
```
This may make more sense once we understand file input/output.
C++11 String Conversions
The C++11 standard has introduced several type-conversion functions:
{{< table >}}
| Function    |  Conversion | 
|-------------|-------------|
|std::stoi  |  string to integer |
|std::stol  |  string to long    |
|std::stoul    |  string to unsigned integer | 
|std::stoll    |  string to long long | 
|std::stof     |  string to float |
| std::stod    |  string to double |
| std::stold   |  string to long double|
| std::to_string    |  number to string |
| std::to_wstring   |  number to wide string |
{{< /table >}}
These are in the standard string class for C++11 and above.  Some compilers may require the addition of a -std=c++11 flag to access them."
rc-learning-fork/content/courses/cpp-introduction/containers_templates.md,"A container is a data structure that can store a group of items, which typically are related and are individually accessed by standard methods.  By this definition a simple array can be considered a container, though some strict computer-science definitions disallow a data structure with a fixed size from the category.  C++ containers generally permit resizing of the data structure by inserting or deleting elements or even by clearing all elements.
C++ implements most containers as templates.  Templates are an implementation of generic programming.  In generic programming, algorithms are expressed in a way that can apply to more than one type.  Templates can be functions or classes and, for C++14 and later, variables.  We will cover classes later but in brief, a class is a data structure that encompasses related variables and the procedures that operate on them.  We can declare variables whose type is the class; this is called instantiation and each variable is an instance of the class.  We then access the variables and procedures through the name of the variable.  For a template class, we pass through a type for the variable as a parameter when it is declared.
c++
  myTemplateClass<float>  myfloat;
  myTemplateClass<double> mydouble;
The angle brackets are included and enclose the specific type.
Associated with many templated containers are iterators.  An iterator is a variable that can be used to access elements.  It has properties similar to pointers.  It can be incremented by adding integers to the current value and dereferenced with the * operator.  
Most C++ programmers primarily use templates through libraries.  These libraries provide many data structures and perform a number of tasks that occur frequently in programming, thus relieving every programmer of the effort of writing his or her own code. 
The Standard Library
The C++ Standard Library is defined as part of the language definition.  It incorporates much of the Standard Template Library or STL.  The STL was initially an add-on to C++, developed with the intention of exploring generic programming.
The standard library is huge, with many subcomponents.  We have been using it regularly already; every invocation of it requires the prefix std::.
```c++
include 
int main() {
    std::cout<<""Hello World\n"";
}
A few examples we have seen so far have employed the shortcut `using`, which we will discuss when we study [scope](/courses/cpp-introduction/scope), but although common practice, often it is best to avoid it.c++
include 
using namespace std;
int main() {
    cout<<""Hello World\n"";
}
```
The standard library consists of containers, algorithms, iterators, and a number of general-purpose and numerical sub-libraries.  A comprehensive list is here.
Useful Standard Containers
Several templated containers are available in the standard library.
Array
The array container ""wraps"" C-style fixed-size arrays.  Unlike a bare array, the container carries metadata, including the array size and other descriptive elements.  It also does not ""decay"" to a pointer when passed to a function, meaning that it doesn't ""forget"" the metadata within the function.
{{< code file=""/courses/cpp-introduction/codes/std_array.cxx"" lang=""cxx"" >}}
A standard array is a sequence and we can use a range-based for loop with it.  Note also the peculiar declaration of the loop variable in the three-element loop; this is required because the array container defines its size and index variables to be of an unsigned int of type size_type. Strictly speaking, it should always be used for loop variables for standard containers, since they are defined internally that way.  The intention was to allow for very large array sizes without overflowing a loop variable.  A standard int or long will nearly always work, and the compiler will not complain, but best practice is to make the loop variable match the internal definition.  The size_type depends on the vector template so would have to be declared std::vector<float>::size_type; a shortcut that is safe, yet less wordy, is to use std::size_t.
c++
   for (std::size_t i=0; i<x.size(); i++) {
      //code
   }
The std::size_t type is guaranteed to be large enough to index any object possible within the limitations of the compiler.
The array container has some drawbacks.  It is one dimensional only; no multidimensional arrays are allowed.  It cannot be dynamically allocated (there is some capability for that in the C++20 standard, but it is limited); only literals or const variables are allowed for its declaration.  Therefore, we will look at another, very widely used, container, the vector.
Vector
The vector container is similar to the array. It represents a one-dimensional ordered sequence of elements. Elements are accessed by integers 0..N-1 (for size N).
But unlike arrays, vectors are dynamic.  It's possible to enlarge and shrink them.
Initializing vectors:
{{< code-download file=""/courses/cpp-introduction/codes/std_vector.cxx"" lang=""c++"" >}}
Many operations are defined for a vector.  These are some of the most commonly used:
{{< table >}}
|---------------------|---------------------|
|  V.push_back(item)  |  Append item to V |
|  V.at(index)        |  Access index with bounds checking ([] does no checking) |
|  V.start()          |  Starting point for iterator  |
|  V.end()            |  End point (beyond last element) of iterator |
|  V.size()           |  Number of elements
|  V.clear()          |  Empty V and make it size 0
{{< /table >}}
Vectors make some tradeoffs with memory and speed that array containers do not; this is to allow for adding elements even if the initial declaration is static. The vector type is also built upon C-style arrays, so certain operations, especially insertion, can be relatively slow.  However, their flexibility usually more than makes up for this.
A full reference guide can be found here.
Exercise
Consult the vector documentation and write a program to do the following:
1. Define a vector of type int with N elements.  Initialize N to 10.
2. Initialize each element to 2k+1 for k=0 to N-1
3. Append -20 to the end of the vector
4. Insert the value 15 after position 3
5. Add 100 to element 0. Replace element 1 with 102.  Use two different methods to access the element.
6. Use an iterator with begin and end to print the contents.
7. Use a range for loop to print the elements.
Hints: 
For a templated container like a vector, we can't assume an integer for iterating so we use a type std::vector<type>::iterator to declare it. You will need it for begin and end in a loop.  It can also be used to insert.
Insert is a bit tricky.  It has several forms but the simplest is <vec>.insert(location, const<type>& value), e.g. myvec.insert(myvec.begin()+2,42).
You may use an integer for appropriate loop variables, but remember that size_type is more correct.  You may also use using namespace std if you wish.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/vector.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
The sample uses endl (output an end-of-line character) rather than \n.  There is a slight difference between the two which we will discuss later."
rc-learning-fork/content/courses/cpp-introduction/statements.md,"An expression is a combination of variables, operators, and function invocations that can result in a unique evaluation.
C++ expressions are much like those of other languages.
c++
a+3*c
8.d0*real(i,dp)+v**3
sqrt(abs(a-b))
A || B
myfunc(x,y)
If expressions are the ""words,"" then statements are the ""sentences"" of a programming language.  A statement is a combination of expressions and operators such as assignment (= in most languages) which describes a command to the compiler.
Statements in C++ terminate with a semicolon ;
c++
w=a+3*c;
z=myfunc(x,y);
if (A||B) C=0.;
The length of a statement is in principle unlimited, but in practice it is limited by the compiler and the ability of humans to follow a sequence of symbols.  Always keep in mind that the compiler is never confused, but humans are often confused.
Comments
One of the most important types of statement is the comment.  A comment is ignored by the compiler. C++ supports two types, C-style comments and C++ comments.
C-style comments are usually used for comment blocks in C++.  The first line must start with /* and the comment continues until the compiler reaches */.
Whitespace and line breaks are included in the comment.
c++
/* This code prints ""hello.""
   Author:  A. Programmer
   Date:  Today
*/
The C++ comment starts with //.  It may begin anywhere on the line.  Anything following // is ignored up to the end of the line.
c++
// A comment
// Another line needs another marker
if (A==B) {  // Check it out
Code Blocks
A code block is a set of statements that are functionally equivalent to a single statement.  For example, the language specifies that the if construct is followed by a statement.  But it is rare that one statement suffices for the algorithm, so a code block must be used.
In C++, blocks are enclosed in curly braces {}.
Two choices for layout are widely used.  The first is ""same line"" where the opening curly brace is placed on the same line as the introductory clause.  The closing curly brace then aligns with the first letter of the statement.
c++
    if ( A || B ) {
        C=0.;
        D=1.;
    }
The second option is to place the opening curly brace on the line below
c++
    if ( A || B ) 
    {
        C=0.;
        D=1.;
    }
The first style saves space and the closing curly brace aligns visually with the code construct to which it belongs.  In the second style, the opening and closing curly braces are aligned.  Unless one style or the other is imposed by a particular coding project or employer, programmers should choose one and be consistent.
Blocks also have important implications for scope."
rc-learning-fork/content/courses/cpp-introduction/classes.md,"Object-Oriented Programming
An object is a data structure which has associated data (variables) and behaviors (procedures).
Objects work on their own data, communicating with outside units through an interface.
Objects encapsulate related concepts and keep them unified.  They may hide data from code outside the object.
Generally speaking, an object would contain the procedures required to update and maintain its state. 
In most object-oriented programming languages, objects are abstract concepts that are represented in code by classes. 
Object-oriented programming (OOP) grew from programming fields that simulated real-world, physical objects and it may be easiest to grasp by thinking of that model.  These objects are well-defined and their interactions can be managed through the interface that each object provided.  A user interface is a good example.  We have various objects such as figures, text, menus, and so forth. The figures might be one-dimensional (lines, arrows, etc.) or two-dimensional (rectangles, ellipses, etc).  They have characteristics such as length/radius, they have a state such as current size, color and location of their centers, and they have behaviors such as moving or rotating.  Text has location and the content of the string that it displays.  All these objects interact with each other, as the user takes various actions or the program executes.
For another example, return to the Employee struct.  The struct collected data about an employee into a unified type.  However, actions may be associated with an employee.  Salaries could be raised, addresses could be updated, employees could join or quit the company, and so forth. We would implement procedures to carry out these actions and include them in the object.
Classes in C++
We define a class with the keyword class.
A variable that is a member of the class is often called an attribute.  A method is a procedure that is a member of a class.  An invocation of a method may be called sending a message.
The class definition contains full declarations of attributes, but only the 
prototypes of the methods.
For now we will declare all members public so that they will be directly accessible by code that uses the class.
c++
class MyClass{
   public:
     double var1, var2;
     int var3;
     double function1(double x, double y);
     double function2(double x);
};
The methods are defined using the scoping operator as class::function.
c++
double MyClass::function1(double x,double y) {
    return var1*x+var2*y;
}
double MyClass::function2(double x) {
   var1=x*var2;
   return;
}
Notice that function2 does not return anything explicitly.  This is because it acts upon a member of the class.  Such methods should not return their results.  In contrast, function1 returns a value to the outside caller. 
An instance of a class is a variable of that class.  Objects are represented in code by instances and sometimes the terms are used somewhat interchangeably.
c++
MyClass A, B
A and B are instances of MyClass.   
As for structs, we reference the members through the . operator.
c++
   A.var1=10.;
   A.var2=11.;
   A.var1=7;
   A.function1(2.,9.);
Constructors and Destructors
A class always has a constructor and a destructor.  If not provided by the programmer, the compiler will try to fill them in.
The constructor is automatically called when an instance of the class is created.
The constructor is invoked when a variable of the class type is declared.
For a declaration of type pointer-to-class the constructor is called by the new operator.
The constructor has the same name as the class.
The destructor is called when the object is released.
The destructor has the same name as the class but preceded by ~.
Explicit destructors are often not required.
```c++
class MyClass{
   public:
      double var1, var2;
      int var3;
      MyClass(double v1, double v2, int v3);
      ~MyClass(); // destructors never have arguments
      double function1(double x, double y);
      double function2(double x);
};
MyClass::MyClass(double v1, double v2, int v3) {
   var1=v1; var2=v2; var3=v3;
   return;
}
MyClass::~MyClass(){
//Not much to do in this example, usually would not implement
}
double MyClass::function1(double x,double y) {
   return var1x+var2y;
}
double MyClass::function2(double x) {
   var1=x*var2;
   return;
}
```
Like a struct, a class is a scoping unit.  Variables belonging to the class are in scope only within the class (specifically, its instance).
this
An instance variable is always passed to a method.  This variable stands for the current instance, that on which the method is invoked.  Some languages such as Python require that it be the first argument to any method.
In Python it is conventionally represented by self.
python
def a_method(self,x,y):
   self.z=x+y
C++ does not pass the instance variable explicitly.
If you need access to it in a method, use the this pointer.
Since it is a pointer, it requires the arrow operator.
One example where it might be required is using the same variable name for an argument and an attribute.
c++
MyClass::MyClass(x,y,z) {
   this->x=x;
   this->y=y;
   this->z=z;
}
Exercise
Convert the Employee struct to a class.  Write a constructor for it. Incorporate an updateSalary method that takes a raise percentage (or fraction, your choice), computes the new salary, and updates the employee attribute. Why does it not have to take the employee's
current salary as a parameter?
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/employees.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/pointers_mem.md,"We have learned that variables are ""nicknames"" for the contents of a specific location in memory.  In many languages, including C++, we can also define variable that contain the actual address of a memory location.  These variables are called pointers because they ""point to"" memory directly.
Since an ordinary variable is also associated with memory, we can use the reference operator & to obtain the location in memory.  Inversely, given a location, we can use the dereference operator * to get the value stored there.
{{< code file=""/courses/cpp-introduction/codes/ref_deref.cxx"" lang=""c++"" >}}
Pointer Variables
Pointer variables are declared using the dereference operator.
c++
int* p;
The type of p is pointer to int.  
Spaces are ignored by the compiler, so
c++
int* p;
int *p;
and even
c++
int * p;
are equivalent.  However, be aware that
c++
int* p, i, j;
does not declare pointers to int p,i, and j in C++.  Only p is a pointer in this declaration.  For this reason it is recommended to keep declarations of pointers separate.
The choice between int* p and int *p is largely a matter of taste and emphasis.  Some programmers believe that int* p is more appropriate for C++ whereas int *p is more ""C like.""  However, there is no rule.  As for curly braces, programmers should be consistent in notation.
Example:
{{< code-download file=""/courses/cpp-introduction/codes/pointers.cxx"" lang=""c++"" >}}
The NULL Pointer
If the memory location referenced by a pointer is invalid, the pointer can be set to a special value, NULL.  NULL will never compare as equality to any valid pointer value. 
It is frequently used to initialize a pointer variable.
c++
float *p=NULL;
Setting a pointer variable to NULL can prevent referencing an unintended memory location.
Memory Allocation and the New Operator
C-Style Allocation
Previously we saw that arrays could be initialized with a variable that was set at compile time and declared const.   However, this limits the flexibility of the code.  Anytime a change is required, the code must be recompiled.  Modifying hard-coded values is also error-prone.  Finally, in many cases we need to be able to adapt the code to the circumstances of a particular run.  This is especially true for programs that use arrays, since they are frequently used to represent grids or other numerical entities, and the program should be capable of running at different resolutions without the need to recompile each time.
C has a set of alloc routines of which malloc (memory allocation) is most frequently used.  The malloc function allocates a block of memory of the specified number of bytes.  It must be told the number of bytes to obtain by the programmer.  Nearly always the sizeof function must be invoked to be sure the correct number of bytes is allocated.
Malloc returns a pointer of type void so it must be cast to the desired type.
These blocks are raw chunks of memory, but can be referenced as the C-style arrays we have already seen.
c++
    float *y = (float *)std::malloc(N*sizeof(float));
    for (int i=0; i<N; ++i) {
        std::cout<<y[i];
    }
    std::cout<<""\n"";
Malloc and the related routines calloc (contiguous allocation) and realloc (reallocate) are supported by C++.
{{< code file=""/courses/cpp-introduction/codes/malloc.cxx"" lang=""c++"" >}}
The compiler is not required to initialize the memory returned by malloc, but at least some do, initializing it to zero.
If the block cannot be acquired, the return value of the pointer is NULL.
New
The older C-style allocation functions are prone to errors and are generally useful only for built-in types.  C++ defines the new operator to allocate memory for any type for which a constructor exists.  The constructor is a function that tells the compiler how to set up a variable of a particular type.  Programmers write their own for classes but the compiler provides constructors for built-in types.
The syntax of new is
c++
<type>* var=new <type>[N];
where the angle brackets indicate an option and are not typed into the code.
{{< code file=""/courses/cpp-introduction/codes/new.cxx"" lang=""c++"" >}}
This creates an array of N floating-point numbers.
There are several differences between malloc and new, the most important of which is that new uses the constructor, plus the item count, to compute the number of bytes of memory to allocate.  Generally, new is the ""C++ way"" even for built-in types and its use is recommended over malloc.
Multidimensional C-Style Arrays with New
We will only illustrate 2d arrays here; the concept can be extended to higher-dimensional arrays.
Two-dimensional arrays are 1-d arrays of pointers to a one-dimensional array.
{{< code-download file=""/courses/cpp-introduction/codes/twod.cxx"" lang=""c++"" >}}
A two-dimensional array can be declared with two asterisks, indicating that the variable is a ""pointer to pointer.""
Memory Deallocation
If memory is allocated, it must be released (deallocated) or the result may be a memory leak.  If new (or malloc) is called, a pointer is returned to a block of memory.  Another call to new can be made returning to the same variable; C++ will not stop this.  When the old pointer value is overwritten, access to its corresponding block of memory is lost, but the memory is still allocated and is not available for any other use.  
{{< code-download file=""/courses/cpp-introduction/codes/leaker.cxx"" lang=""c++"" >}}
A memory leak can rapidly fill up memory.  In modern computer operating systems, running executables are restricted to a particular region of memory and an attempt to access memory outside that area may result in a segmentation violation.  If it runs long enough, a leaking executable will eventually encounter a segmentation violation or sometimes, at least on Linux, a bus error (a bus error occurs when an attempt is made to access a location in memory that does not exist).  Occasionally, however, leaking codes can still take down an entire system.   Therefore memory leaks must be avoided.
To release memory allocated with malloc or similar, use free.  As a general rule, each malloc must be paired with one free.
{{< code file=""/courses/cpp-introduction/codes/better_malloc.cxx"" lang=""c++"" >}}
For new use delete
{{< code file=""/courses/cpp-introduction/codes/better_new.cxx"" lang=""c++"" >}}
As for malloc and new, free is a function while delete is an operator.
If an array is created with new the correct form of delete places square brackets before the name of the pointer:
c++
   float *x =new float[N];
   delete [] x;
Otherwise use delete without square brackets.
c++
   float *x = new float;
   delete x;
Exercise
Correct the memory-leaking example.  Leave the multiple new statements (even though they have no consequences) and fix the code as is.  Is there a way to tell whether the fix was successful?
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/fixed_leaker.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/characters.md,"A char variable is 1 byte (8 bits).  It represents a single character.
c++
char letter;
A char is unsigned.  C++ supports the signed char type but it is not as commonly used.  Since a signed char is effectively an 8-bit integer, it can be used in arithmetical expressions and some programmers who must write for devices with limited memory use the char type to save space.  A signed char can also be promoted to int consistently. 
Another use of char is to act as a byte type since C, and older C++, do not support byte.  Newer C++ standards (from C++17) and compilers that support them offer a type std::byte, but this is defined in terms of unsigned char and is a class, not a primitive type.
Byte types or their equivalents offer direct access to memory, which is organized by bytes.
C-Style Strings
C-style strings are actually arrays of individual characters.  They must be declared with a fixed size, or allocated.
```c++
   char cstr[8];
cstr=""Hello"";
The compiler automatically terminates each C-style string with an invisible ""null"" character.  If the assigned value does not fill the array, it will be padded with blanks.  It may not exceed the length of the string.  The size must account for the null terminating character, soc++
   char greeting[5]=""Hello"";
will result in an error, butc++
   char greeting[6]=""Hello"";
```
works.
A C-style string may only be initialized to a quoted string when it is declared.
c++
   char greeting[6];
   greeting=""Hello"";
is invalid. 
C Style Character Operations
C++ supports C-style string functions.  Include the <cstring> header.
{{< table >}}
|    Function   |      Operation    |   Usage     |
|:-------------:|:-----------------:|:-----------:|
|   strcpy      |  copy str2 to str1 |  strcpy(str1,str2)  |
|   strcat      |  concatenate str2 to str1|  strcat(str1,str2)  |
|   strcmp      |  compare two strings |  strcmp(str1,str2)  |
|   strlen      |  length of string (excludes null)  |  strlen(str)  |
{{< /table >}}
Individual characters may be addressed using bracket notation.  Each character is one item, and the count begins from zero and goes to strlen-1.
c++
   char greeting[8]=""Hello"";
   std::cout<<greeting[0]<<""\n"";
   std::cout<<greeting[2]<<""\n"";
Example:
{{< code-download file=""/courses/cpp-introduction/codes/cstr.cxx"" lang=""c++"" >}}
In the above code, pay attention to the lines
```c++
    std::cout<<strlen(greeting)<<""\n"";
    std::cout<<strcat(greeting,musical_instr)<<""\n"";
    std::cout<<greeting<<""\n"";
    std::cout<<strlen(greeting)<<""\n"";
char str[6];
strcpy(str,greeting);
std::cout<<str<<""\n"";

What result did this code yield?  On a Linux system with g++ the output wasno-highlight
5
HelloCello
HelloCello
10
HelloCello
``
The size ofgreetingwas doubled (not counting null terminators) even though it was declared size 6.  The compiler did not check for this.  Thestrcpy` function then copied it to another variable of size 6.
The result is a buffer overflow.  
To see what can happen, compile and run the following code
{{< code-download file=""/courses/cpp-introduction/codes/buffer_oflow.cxx"" lang=""c++"" >}}
Type in a short username (any string), then type Eleventy as your password. It should work as expected.  Now try typing a username that is longer than 10 characters and see what happens.
If using C-style strings and functions, guard against this by using
|    Function    |      Operation    |   Usage     |
|--------------|-------------------|-------------|
|   strncpy      |  copy str2 to str1, max n bytes of str2 |  strncpy(str1,str2,n)  |
|   strncat      |  concatenate str2 to str1, max n bytes of str2|  strncat(str1,str2,n)  |
One way to ensure that n is correct is to use sizeof, which returns a value in bytes.
c++
strncpy(str1,str2,sizeof(str1)-1);
str1[strlen(str1)]='\0';
We must explicitly add the null character to the end of the target of the copy or even strncpy will overflow the buffer.
The strncat function is more difficult to use correctly since it appends $n$ bytes from str2 regardless of the size of str1.  
In general, it is best to avoid fixed-size char variables as much as possible, because C++ (and C) does not check C-style array bounds. Similar problems can occur with numerical arrays, but in those cases the result is typical a segmentation fault. Buffer overflows in characters can result in insecure programs.
Since we are programming in C++, not C, for most purposes it is better to use C++ strings (see here),
which do not have these disadvantages."
rc-learning-fork/content/courses/cpp-introduction/file_io.md,"File Streams
Standard streams are automatically opened.  Other files must be opened explicitly.
Files can be input streams (ifstream), output streams (ofstream), or either/both (fstream).
```c++
include 
orc++
include 
include 
```
as needed.
Open
First a stream object must be declared.
c++
  ifstream input;
Then the stream can be attached to a named file
c++
  input.open(inFileName);
This assumes the file exists and is opened for reading only.
For output use
c++
  ofstream output;
  output.open(outFileName);
This file will be emptied if it exists or created if it does not exist, and will be opened in write-only mode.
To open read/write use
c++
   fstream myfile;
   myfile.open(myfileName);
Modifiers
We can control the characteristics of the file with modifiers

ios::in      open for input (read). Default for ifstream.
ios::out     open for output (write). Default for ofstream.
ios::binary  open as binary (not text)
ios::app     append
ios::trunc   if file exists, overwrite (default for ofstream)

Use a pipe (|) to combine them
c++
   ofstream myfile;
   myfile.open(""myfile.dat"",ios::binary | ios::app);
Inquiring
All inquiry methods return a bool (Boolean).  To check whether a file is open
c++
   myfile.is_open()
To check whether a file opened for reading is at the end
c++
   myfile.eof()
To test for any error condition
c++
   myfile.good()
Close
Much of the time, it is not necessary to close a file explicitly.  Files are automatically closed when execution terminates.
If many files are opened, it is good practice to close them before the end of the run.
c++
   myfile.close();
Rewind
An open unit can be rewound.  This places the file pointer back to the beginning of the file.
The default is to rewind a file automatically when it is closed.
These are C-style functions and are in <cstdio>.
c++
   rewind(mystream)
You can also seek to position 0
c++
   fseek(mystream,0,SEEK_SET)
where rewind clears the end-of-file and error indicators, whereas fseek does not.
Writing to a File
We write to a file much like to a standard stream.  In this example, we assume that var1, var2, and var3 are arrays or vectors of length nlines.
c++
  ofstream out(""outfile.txt"");
  out<<""column1,column2,column3\n"";
  for (int i=0;i<nlines;++i) {
      out<<var1[i]<<"",""<<var2[i]<<"",""<<var3[i]<<""\n';
  }
Reading from a File
The extraction operator works on file objects as well as on cin:
{{< code file=""/courses/cpp-introduction/codes/simple_read.cxx"" lang=""c++"" >}}
Although this method works, it has a number of drawbacks.  As we learned for console IO, the extraction operator assumes the separator is whitespace. It can be thrown off when attempting to convert a string into a numerical type.  It also is inflexible. We will discuss more advanced methods in the next chapter.
Exercise
Write a program that creates a file mydata.txt containing four rows consisting of
no-highlight
1 2 3
4 5 6
7 8 9
10 11 12
Either rewind or reopen the file and read the data back.  Write a loop to add 1 to each value and print each row to the console.  Note that if you choose to rewind, the file will have to be opened read/write.  If you close it and reopen it, it will have to be reopened in write mode.  You may use a statically-sized array for the data.  In the next chapter we will learn a more flexible method of reading lines of files.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/read_write_file.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/operators.md,"Operators are characters or groupings of characters that take some number of variables or literals as operands, apply some specific mathematical, logical, or translational operation, and return a result.  Operators are defined by each programming language, although basic ones are often the same or similar.  The majority are mathematically binary operators, i.e. they take two operands, though nearly all languages have some unitary operators and a few have operators that take three or more operands.  Each operand must be of the specific types for which an operator is valid.
Arithmetic Operations
Arithmetic operators are defined on integers, floats, and doubles.
+ - add, subtract
* /multiply, divide
Operators are applied in a particular order.  This is called precedence.
First (equal status):  * /
Second (equal status):  + -
Evaluation is left to right by precedence unless parentheses are used to group operators and operands.
The mnemonic PEMDAS is sometimes applied--ParenthesesExponentsMultiplicationDivisionAdditionSubtraction--but remember that MD and AS are equals within their ranking.  C++ does not provide an exponential operator so more correctly it would be PMDAS.  Exponentiation is supplied by the pow built-in function.
c++
pow(x,3)
Even if the exponent is an integer, pow evaluates it as if both are floating-pint numbers.
Special Considerations for Integer Operators
In C++ 2/3 is always zero!  Why?
Because 2 and 3 are both integers, so / is an integer operation that yields an integer result
Exercise:
What is 9/5?
The remainder can be obtained with the % operator. 
c++
k=n%m
It is defined as
$$  n-floor(n/m) x m $$
It is mathematically well-defined for negative integers, but the results for such arguments are not generally what most programmers expect.  
Example:
{{< code-download file=""/courses/cpp-introduction/codes/testmod.cxx"" lang=""c++"" >}}
Assignment Operators
The simple assignment of a value to a variable is through the equals sign =.
c++
a=b;
C++ supports several compound assignment operators; the first operator specifies the arithmetic or other operation to be performed on the variable on the left-hand side, then the result is assigned back to the same variable.
c++
a+=b;
//Equivalent to
a=a+b;
There is no rule that assignment operators (aside from =) must be used but they save space and typing.
c++
a+=b;
a-=b;
a*=b;
a/=b;
a%=b;
For the special case of adding or subtracting 1, special increment and decrement operators are defined.
c++
++i;
--i;
i++;
i--;
Beware when assigning the result of an increment or decrement to another variable.  The ""prefix"" increment/decrement operators shown here first add or subtract, then change the value of the variable.  They are exactly equivalent to i+=1 and i-=1 respectively. The ""post"" operators i++ and i-- do not change the value of the variable before incrementing or decrementing.
{{< code file=""/courses/cpp-introduction/codes/incdec.cxx"" lang=""c++"" >}}
no-highlight
 i is: 3
 j is: 2
 i is: 4
 j is: 4
Exercise
Run the following program.  Modify the values of the different variables and see what happens.
{{< code-download file=""/courses/cpp-introduction/codes/expressions.cxx"" lang=""c++"" >}}
Conditional Operators
Comparison Operators
These are used to compare numerical values.  They can also compare character and string variables; ordering is determined by the character encoding.  They return a Boolean value.
{{< table >}}
|   Symbols   |   Meaning  |
|-------------|------------|
|   ==        |  Equality  |
|   !=        | Not equal  |
|    <        | Less than  |
|    <=       | Less than or equal  |
|    >        | Greater than  |
|    >=       | Greater than or equal to  |
{{< /table >}}
Boolean Operators
{{< table >}}
|   Operator    |   Meaning   |
|---------------|-------------|
|   !           |   Negation of what follows |
|   &&          |     and     |
|   ||          |     or      |
{{< /table >}}
It is important to note that || is an inclusive or.  It evaluates to true if either operand is true.  This is different from many human languages, for which ""or"" is generally, though not always, exclusive.  An exclusive ""or"" is true only if exactly one of the conditions is true.
   You can have cake or ice cream (but not both).
An exclusive or can be constructed with
c++
(a && !b) || ( a && b)
where a and b are Boolean expressions.  
""Truth tables"" define the outcome of Boolean operators.  This example is for ""and.""
{{< table >}}
|   Operator    |   Operand 1   | Operand 2 |  Result |
|---------------|---------------|-----------|---------|
|     &&        |   true        |  true     |  true   |
|     &&        |   false       |  true     |  false  |
|     &&        |   true        |  false    |  false  |
|     &&        |   false       |  false    |  false  |
{{< /table >}}
Conditional Operator Precedence
Like arithmetic operators, conditional operators have a precedence ordering.

! has the highest rank
>,>=,<,<= are equal and outrank == or !=
==,!= are equal and outrank &&
&& outranks ||

Exercise
Experiment with different values for the variables in this code.
{{< code-download file=""/courses/cpp-introduction/codes/conditional_operators.cxx"" lang=""c++"" >}}
Bitwise Operators
Bitwise operators are defined for ""integer type"" variables, e.g. int, short i
nt, unsigned int, char, unsigned char etc.  They return another ""integer type"" variable.
{{< table >}}
|   Operator    |   Meaning   |
|---------------|-------------|
|   &           |     bitwise and     |
|   \|           |     bitwise or      |
|   ^           |     bitwise xor (exclusive or)      |
{{< /table >}}"
rc-learning-fork/content/courses/cpp-introduction/console_io.md,"Most operating systems have some type of console.  Output written to the console appears as text on a terminal or equivalent.  Geany opens a window for console output when a program is executed.
We have been using console output via std::cout up till now without learning much about it.  It is associated with one of the standard streams of C++ input/output.
Streams
C++ handles input and output through a concept called streams.  A stream is an abstraction of a device (a keyboard, a monitor, in the past possibly a tape device, and so forth) to which characters can be printed or from which characters can be read.
A stream may be buffered; that is, the messages are accumulated in memory until the buffer is filled, when they are flushed (written) to output.  
The standard streams are incorporated with the header
```c++
include 
```
The iostream library is part of the C++ standard libraries set and contains stream objects for standard streams.  They correspond to the standard streams of Unix or macOS.  Windows handles console IO differently, but iostream can map the stream objects to its equivalents.
{{< table >}}
| cin | standard input |
| cout | standard output |
| cerr | standard error |
| clog | standard logging |
{{< /table >}}
The cin object is for input; it is a member of the istream (input stream) class.  If a console is attached the others all output is to the same device (the screen).  They are members of the ostream (output stream) class.
Unix and macOS shells can redirect standard output and standard error to separate files.
Standard output is for ""normal"" output whereas standard error is used for error messages. 
On Linux cin and cout are buffered, whereas cerr is not.
Clog is attached to the standard-error stream but is buffered. 
The ""c"" is for character, which is the type these objects can handle.  For wide characters (two or more bytes) the streams wcin, wcout, wcerr, and wclog are available.
Output
```c++
include 
//write to standard output
  std::cout<<var1<<"" ""<<var2<<"" ""<<var3<<""\n""
  std::cout<<var1<<"" ""<<var2<<"" ""<<var3<<std::endl
``
The double less-than symbol<<` is the insertion operator.  It inserts characters into the stream.  It is able to convert the values of variables into strings provided that how to do it is defined for the type of the variable.  Built-in types already provide those instructions; programmers can write those instructions for their own types.
The insertion operator will not print anything other than exactly what it is told to print.  It does not add any spaces between variables; they must be explicitly printed as in the example above.
The special symbol ""\n"" represents the ""newline"" character.  This terminates the current line and advances to the next line.  Cout, cerr, and clog  will not add a newline character unless told to do so.  Newline is just another character, so it is accumulated in the usual output buffer for the stream.  The endl manipulator also inserts a newline, but in addition it flushes the output buffer.  
C++ also supports the C printf function.  The <cstdio> header provides it.
{{< code file=""/courses/cpp-introduction/codes/out_printf.cxx"" lang=""c++"" >}}
More details are available at references such as this.
Input
```c++
include 
//read from standard input
  std::cin>>var1>>var2
``
The double greater-than symbol>>` is the extraction operator.  It ""extracts"" characters from the stream.  Similarly to the insertion operator, it can convert variables from characters to a type provided it is defined for that type.
The cin method reads variables separated by whitespace (spaces or tabs).
It will wait indefinitely until all variables specified are entered.  If the input is not of the correct type, it will fail silently and, depending on the compiler, may assign zeros or nonsense to subsequent variables.
Exercise
Try different input values for this code.  Try entering floats or words.
{{< code-download file=""/courses/cpp-introduction/codes/read_cin.cxx"" lang=""c++"" >}}
Formatted Input/Output
Formatted input is rarely needed but can be used to control whitespace behavior, which is particularly useful for character (char) input.
Some documentation is here
Formatted output permits greater control over the appearance of output.  Compilers tend to let their default output sprawl.
Formatted output also allows programmer control over the number of decimal digits printed for floating-point numbers.
Manipulators
C++ uses manipulators to modify the output of the stream methods cin and cout.
They are introduced with the iomanip header.
```c++
include 
``
A few basic manipulators:
* Output
   *endl//flushes the output and inserts newline
   *ends//outputs null character (C string terminator)
   *boolalpha// true/false printed for Booleans
   *left/right/internal//left/right/internal for fillers.
* Input
  *ws// reads and ignores whitespace
  *skipws/noskipws` //ignore/read initial whitespace as characters (skip is the default)
These manipulators stay in effect in a given output stream until cancelled.

setw(n) //Set width the output quantity will occupy
setprecision(n) //Set number of places printed for floating-point numbers
fixed/scientific //Fixed-point format or scientific notation format
setfill(c) //Set a filler character c
setbase(n) //Output in base n (options are 8, 10, or 16, or 0 which reverts to the default of decimal).

Example
{{< code-download file=""/courses/cpp-introduction/codes/out_manip.cxx"" lang=""c++"" >}}
A more complete list of manipulators is here.
Exercises

Write a program that computes pi using a trig identity such asp=4\*atan(1). Remember

include 

Using double precision, print pi in
Scientific notation
Scientific notation with 8 decimal places

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/print_pi.cxx"" lang=""c++"" >}}
{{< /spoiler >}}

In an “infinite” while loop:

Request an integer from the user without advancing to a new line, e.g.
“Please enter an integer:” 
If the integer is 1, print “zebra”.  If it is 2, print “kangaroo”.  If it is anything else except for zero, print “not found”.  If it is 0, exit the loop.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/kangaroo.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/enums.md,"The predefined types available in C++ are not sufficient for most non-trivial programming requirements.
In C++ terminology, nearly any type that is not a native type is said to be user-defined.  Arrays, pointers, and references fall into this category.
We have already discussed those types and will focus here on more advanced user-defined types.
Enumerations
One of the simplest user-defined types is the enumeration or enum.
An enumeration associates integers with names.  By default, the integers begin at 0 and increment by 1 until each name has been assigned a value.
Unscoped Enums
For compatibility with C, C++ supports unscoped enums.  These are in scope
throughout the unit in which they are declared.
c++
enum WeekDays { Sun, Mon, Tue, Wed, Thu, Fri, Sat };
Quotes are not used for the enum names because these are not literal strings.
Variables can be declared of an enum type.
WeekDays today;
The possible values of an enum variable are restricted to the integers declared in the enum.
c++
today=Fri
std::cout<<today<<""\n"";
Enums can start at a value other than 0.
c++
enum Spring { Mar=3, Apr, May };
If a value is specified and subsequent ones are not, the rule of incrementing by one is followed.
c++
enum Cards { Diamonds, Spades=4, Clubs, Hearts };
Diamonds=0, Spades=4, Clubs=5, and Hearts=6.
Specific values may be given to each name.  The same value may be reused, though this is usually not a good idea.
c++
enum MonthDays { Jan=31, Feb=28, Mar=31, Apr=30, May=31};
Scoped Enums
Unscoped enums can result in conflicts.  Consider the example of
c++
enum color {red, yellow, orange};
enum fruit {apple, orange, pear};
The compiler does not allow this, because ""orange"" has already been declared when it sees the second enum.  
To handle this, C++11 introduced the scoped enum or class enum.  This uses the keyword class.
c++
enum class color {red, yellow, orange};
enum class fruit {apple, orange, pear};
We then use the scope-resolution operator to refer to the names in the enum.
c++
color paint == color::red;
An enum is a type and therefore converting its values to another type generally requires a cast.  Unscoped enums can be cast to int implicitly, but a scoped enum must use an explicit cast.
{{< code-download file=""/courses/cpp-introduction/codes/enum.cxx"" lang=""c++"" >}}
The static_cast is a cast that occurs at compile time.  For native types it is essentially the same thing as the ordinary cast.  It can also be used for user-defined types, as we have illustrated for the enum in the above example.  It takes a templated type in angle brackets as the indicator to which it should cast its argument. 
Typedef
It may be convenient to name a new user-defined type, or to rename an existing type, so that new variables can be declared by name.  We use typedef to create a new name for our type.  The syntax is
no-highlight
typedef existing newname
We can then declare variables of newname type.
Examples:
```c++
typedef float real;
typedef boost::multi_array Array2D;
real x;
   Array2D gridArray(boost::extents[nrows][ncols]);
```
Typdef is particularly useful when working with templated types whose declarations may be long and awkward, as in the Boost array example above.  Unlike user-defined types such as structs the new ""type"" is merely a synonym for an existing type.
Typedefs are very common in C code, because C requires using the struct keyword to declare variables of that type.
```c
struct myStruct {
   int myvar;
   float anothervar;
};
struct myStruct var1;
This would frequently be declared withc
typedef struct myStruct {
   int myvar;
   float anothervar;
} aStruct;
aStruct var1;
```
In contrast to C, C++ does not require the enum, struct, and class keywords in variable declarations, as long as there is no ambiguity.  Programmers are advised to avoid creating this ambiguity, but some libraries may not adhere to this principle.  Ambiguity can commonly occur when a struct contains a member with the same name as a typedef.
```c++
struct myStruct {
   int myvar;
   float anothervar;
}
myStruct var1;
```
Structures are described in detail in the next chapter."
rc-learning-fork/content/courses/cpp-introduction/subprograms.md,"A subprogram is a self-contained, but not standalone, program unit.  It performs a specific task, usually by accepting parameters and returning a result to the unit that invokes (calls) it.
Subprograms are essential to good coding practice.  Among other benefits, they are
  * Reusable.  They can be called anywhere the task is to be performed.
  * Easier to test and debug than a large, catch-all unit.
  * Effective at reducing errors such as cut-and-paste mistakes.
Other general names for subprograms are routines, procedures, and methods. The word ""method"" is generally reserved for procedures defined within an object, but it is not conceptually different from any other subprogram. 
Subprograms must be invoked or called in order for any of their code to be executed.  
Functions and Subroutines
Functions take any number (up to compiler limits) of arguments and return one item.  This item can be a compound type.
Functions must be declared to a type like variables.  The return statement returns the result to the caller.
Subroutines take any number of arguments (up to the compiler limit) and return any number of arguments.  Bidirectional communication can take place through the argument list.  Strictly speaking, all subprograms in C++ are functions, but the ability to declare a void return ""type"" and to pass by reference means some are effectively subroutines. 
Variables in the argument list are often called dummy arguments since they stand for actual arguments that are defined in the calling unit.  Like any variable, they must be declared explicitly.  In C++ these declarations are included in the argument list.  In C++ either the function or its prototype must appear before any invocation.  The prototype consists only of the declaration of the function along with its argument list.  Only the types are required in the argument list of a prototype; dummy variables are optional.  The prototype provides the compiler with information about the number and type of arguments to the function, which enables it to check each invocation to ensure that the argument lists match.
C++ does not define or expect a keyword for a function declaration. The parentheses following the name are required, however.
```c++
//Prototype
float myfunc(float, float, float);
//Definition
float myfunc(float v1, float v2, float v3) {
    return v1*v2-v3;
}
``
The prototype is frequently declared in a header file ending in.hor.hpp. The function definition is generally then in a corresponding.cppor.cxx` file. 
Functions are invoked by name.  They can enter into an expression anywhere on the right-hand side of the assignment operator (=).
c++
z=4.*myfunc(var1,var2,var3)
The names of the actual arguments when the function is invoked need not be the same as those of the dummies, but the number and type must match.
A ""subroutine"" would be declared void (so it has no return value).  If it is a utility routine, such as to print a message, it does not require an argument list.  In C++ (but not C) an empty argument list in the declaration is equivalent to a single void argument.
{{< code-download file=""/courses/cpp-introduction/codes/printme.cxx"" lang=""c++"" >}}
Default (Optional) Arguments
The programmer can provide default values for dummy arguments in a function.  If not passed, these take their default values.
c++
int myfunc(int i, int j=0, int k=1) {
//code
}
This function could be called as
c++
myfunc(i,j,k);
myfunc(i,j);
myfunc(i);
Since they are not required, default arguments are also said to be optional.  Values passed explicitly through the argument list override the defaults.
Default arguments may be set in either the declaration (prototype) or the definition of the function, but not both.  The values are set at compile time.
{{< code-download file=""/courses/cpp-introduction/codes/default_args.cxx"" lang=""c++"" >}}
C++ does not support what other languages call ""keyword"" arguments. Optional arguments must be kept in position and may not be rearranged or skipped.
Exercises

Write a program that evaluates the function
$$f(x)=\frac{1}{\pi (1+x^2)}$$
for 401 values of x equally spaced between -4.0 and 4.0 inclusive.
Put the values into an array or vector variable x.  Use variables for the starting and ending values of x and the number of values.
Write a function to evaluate f(x) for any given real (scalar) value of x and call it each time through your loop.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/function1.cxx"" lang=""c++"" >}}
{{< /spoiler >}}
Print the values and the corresponding function evaluation to a comma-separated-values (CSV) file.  Use software such as Excel, Python, Matlab, or anything else you know to plot the result.

Modify your program from Exercise 1 to use a prototype in a header file ending in .h, with the implementation in a .cpp or .cxx file.  Refer to the chapter on linking if you need a refresher on building with multiple files.  This simple example probably does not require a Makefile.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/function2.h"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/function2.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/advanced_io.md,"Simple stream input/output covers much of what we need for basic programs, but as we move to more sophisticated codes we may find that we need more control, especially for reading files.
Stringstreams
An input/output stream is a sequence of characters.  A string is also a sequence of characters.  With the right definitions and methods, we can thus treat a string as a stream.  C++ does this with the stringstream class.
In particular, the insertion and extraction operators are defined on the stringstream object.
Stringstreams are very commonly used to convert from a string to a different type, often a numerical type, and vice versa.  As long as >> and << know how to do the conversion for the target type, as they must for an ordinary istream or ostream objects, the operators can perform the same conversion for a stringstream.  Since the >> operator breaks on whitespace, it can also be used to split a string.
To convert from string to numeric, we create a new stringstream from the string.  We now have a string buffer holding the string.  We now read from that buffer into the numeric variable using the extraction operator.
c++
   std::stringstream ss(num_str) 
   float num;
   ss>>num;
To go the other direction we write the number into an empty stringstream buffer.  We must then extract the string using a built-in method.
c++
   std::stringstream st;
   float num=10.4;
   st<<num;
   std::string num_str=st.str();
Putting this together gives us this example:
{{< code file=""/courses/cpp-introduction/codes/str_stream.cxx"" lang=""c++"" >}}
Reading from the Command Line
Command-line options are strings that follow the name of the executable.
no-highlight
./myexec first second 10
The command line is contained in a two-dimensional character array (one dimension for the characters, the other for multiple character groups).  It is called argv.  The element argv[0] is the name of the executable.  The integer argc is the length of argv.  These variables must be specified as arguments to main if you wish to read command-line arguments.
We can read strings only.  You must convert if necessary to a numerical type using stringstreams.
{{< code-download file=""/courses/cpp-introduction/codes/cl.cxx"" lang=""c++"" >}}
Getline
The getline function reads an entire line at once, as a single string.  This means that we will need to handle the input ourselves, converting as appropriate.
{{< code file=""/courses/cpp-introduction/codes/getline_read.cxx"" lang=""c++"" >}}
Getline's name is a little misleading.
Getline actually reads to a delimiter.  The default delimiter is newline      \n.
c++
   getline(istream,string)  // reads to newline
   getline(istream,string,delim) // reads to delimiter character
The delimiter character is discarded from the string.
Getline can also be used for standard input.
Example:
c++
  cout<<""Enter your name:"";
  getline(cin,name);
Reading a CSV file

We often need to read files where each line contains several fields separated by a comma or other delimiter.  For example: read four values from each line for 200 lines, ignoring the second column values.

{{< code-download file=""/courses/cpp-introduction/codes/read_csv.cxx"" lang=""c++"" >}}
Getline is used twice, once to read the line as a string and again to split the line on commas.  In this case we know that we have four fields in each line so we declare an array of strings.  More generally, we could use a vector and push_back after getline reads the next chunk to the delimiter.  To read the subunits of the line, we declare a stringstream and use that as the stream buffer, rather than a file descriptor.
Exercises

Download the file cpi.csv.  Examine the file.  Write a program that will read the file.  Store the first column in a vector year and the second column in another vector cpi.  Be sure to skip the header line.  It is not necessary to read any data from the header.  

As an alternative to converting with a stringstream, you can convert a C++ string to a C-style string with the c_str() method.
c++
mystr.c_str()
You can then use the atoi and atof functions that operate on C strings to convert to integer and float respectively.
{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/read_cpi.cxx"" lang=""c++"" >}}
{{< /spoiler >}}

Write a program that creates a file mydata.txt containing four rows consisting of
1, 2, 3
4, 5, 6
7, 8, 9
10, 11, 12
Rewind the file and read the data back.  Write a loop to add 1 to each value and print each row to the console.

{{< spoiler text=""Example Solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/read_write_csv.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/linkers_libraries.md,"Linkers and Libraries
When the executable is created any external libraries must also be linked.
The compiler will search a standard path for libraries.  This path is dependent on both the operating system and the compiler, but on Unix this is typically /usr/lib, /usr/lib64, /usr/local/lib, /lib.
If you need libraries in other locations, you must give the compiler the path. -L followed by a path works, then each library must be named with the pattern libfoo.a or libfoo.so and be referenced -lfoo.
Subdirectories are not included in the library search paths so a -L flag is required even when the path begins with a standard location.
Example:
g++ –o mycode –L/usr/lib64/foo/lib mymain.o mysub.o -lfoo
A library ending in .a is static.  Its machine-language code will be physically incorporated into the executable.  If the library ends in .so it is dynamic.  It will be invoked by the executable at runtime.
MacOS is similar to Unix but generally uses different paths for the system libraries.
Working with libraries on Windows is somewhat more complex.  Dynamic libraries are called DLLs (dynamically linked libraries) and end in .dll, but generally code links an ""import library"" that ends in .lib.  Although the underlying DLL is supposed to be universal, these import library files are compiler dependent.  Microsoft Visual Studio will automatically link to the .lib but MingGW uses a more Unix-like format by default and expects a .so suffix.  Libraries provided by Windows will be in MSVC (Microsoft Visual C++) format so translation may be required.  MinGW-64 provides the gendef tool that can create a definition file from a DLL, which can then be used to produce a MinGW import library.  See the MinGW-64 documentation for specifics.  The MinGW-64 FAQ is also helpful.
Headers and the C Preprocessor
Most C and C++ libraries require include files, also called header files.
Indeed, some libraries consist only of headers. 
The headers must be made available to your source code, in addition to the bodies of the libraries being linked into the binary.
The headers are brought in through the C Preprocessor (CPP).  CPP directives begin with a hash mark # in the first column.  
The #include directive physically copies the referenced file at the specified point into an intermediate text file used by the compiler.  The processor will have a built-in search path for headers and those headers that are in that path are referenced with angle brackets.
```c++
include 
include 
Any `.h` or `.hpp` suffix is usually omitted.  C libraries are prefixed with `c` in addition to dropping the suffix.
include 
include 
``
These files arestdio.handmath.h` respectively in C programs.
For local variables, quotation marks tell the preprocessor to look in the current directory/folder relative to the file that includes the header.  If not found there, most systems will move on to the system folders.
```c++
include ""myheader.h""
Sometimes a longer path is used.  The processor will start looking in its search path (system folders or current directory as appropriate).c++
include 
include ""subs/mysubs.h""
``
Paths to headers that are not in the default search path nor in the current directory 
can be specified at compile time with the-I` flag.
Example:
bash
g++ –c –I/usr/lib64/foo/include mymain.cxx
Many C++ ""community"" codes use the CMake build system.  CMake generates a Makefile but uses its own search mechanism for headers and libraries.  See for example this tutorial.
Compiler Libraries
If the compiler is used to invoke the linker, as we have done for all our examples, it will automatically link several libraries, the most important of which for our purposes are the runtime libraries.  An executable must be able to start itself, request resources from the operating system, assign values to memory, and perform many other functions that can only be carried out when the executable is run.  The runtime libraries enable it to do this.  As long as all the program files are written in the same language and the corresponding compiler is used for linking, this will be invisible to the programmer.  Sometimes, however, we must link runtime libraries explicitly, such as when we are mixing languages (a main program in Fortran and some low-level routines in C, or a main program in C++ with subroutines from Fortran, for instance).  
All C++ compilers also provide a set of standard libraries that implement many of the features of the language, such as the data structures defined in the Standard Template Library or STL. These libraries generally require headers to define their functions.  We have already been using the iostream library but there are many others.
```c++
include 
include 
include 
Most C++ compilers are also C compilers, and vice versa, and use the C runtime library, which is fairly minimal.  For example, it is possible to use gcc to compile C++ code by linking the standard library.  In the following example the program used `sqrt` so it was necessary to link the math library libm.so explicitly; C++ does not require this.no-highlight
gcc vardecls.cxx -lstdc++ -lm
The Gnu C++ standard libraries are in libstdc++.so on Unix.  It is obviously much simpler to typeno-highlight
g++ vardecls.cxx
```
Compiling and Linking Multiple Files with an IDE
Our discussion of building your code has assumed the use of a command line on Unix.  An IDE can simplify the process even on that platform.
We will use Geany for our example; more sophisticated IDEs have more capabilities, but Geany illustrates the basic functions.
Using Microsoft tools such as Visual Studio or VSCode on Windows may be desirable if you will need to link to Microsoft or other vendor-provided libraries.
We have three files in our project, example.cxx, adder.cxx and adder.h.  The main program is example.cxx.  It needs adder.cxx to create the executable.  We must open the two files in Geany.  Then we must compile (not build) each one separately.  Once we have successfully compiled both files, we open a terminal window (cmd.exe on Windows).  We navigate to the folder where the files are located and type 
g++ -o example example.o adder.o
Notice that we name the executable the same as the main program, minus the file extension.  This follows the Geany convention for the executable.  It is not a requirement but if Geany is to execute it, that is the name for which it will look.
You can run the executable either from the command line (./example may be required for Linux) or through the Geany execute menu or gears icon.
If Geany is to run a multi-file executable then the main program file must be selected as the current file as well as match the name of the executable.
{{< figure src=""/courses/cpp-introduction/img/Geany5.png"" width=500px caption=""Executing the example program"" >}}
The file ending in .h is called a header file. The best practice in C++ is to separate the interface into a header file and the implementation into the source file. We will discuss this, as well as the preprocessor commands, when we cover functions.
This process becomes increasingly cumbersome as projects grow in number and complexity of files.  The most common way to manage projects is through the make utility, which we will examine next."
rc-learning-fork/content/courses/cpp-introduction/project1.md,"Write a program to compute the day of the week for any date of the Gregorian calendar. Here is the formula: 
W=(C+Y+L+M+D ) mod 7
Y is the last two digits of the actual year and D is the actual day. 
You need to obtain the value of C from the following rule for the years: 
* If year is in the 1400s, 1800s, 2200s, C=2 
* If year is in the 1500s, 1900s, 2300s, C=0
* If year is in the 1600s, 2000s, 2400s, C=5 
* If year is in the 1700s, 2100s, 2500s, C=4 
Months are numbered from 1 in the usual way, but (from January) M is 0, 3, 3, 6, 1, 4, 6, 2, 5, 0, 3, 5 
The only tricky part of this algorithm is L, the number of leap days that have occurred since the beginning of the century of the given date. 
To obtain this:
1. Integer divide the last two digits of the year by 4 to obtain the number of “ordinary” leap years in the century up to that year, not counting the century year itself if applicable. 
2. Obtain the remainder of the two digits and 4. If it is not a century year and the remainder is 0 the year is a leap year, otherwise it is not. If the year itself is a century year see Step 3. 
3. If the century (1400, 1500, etc.) was evenly divisible by 400 then the century year is a leap year, otherwise it is not. Thus 2000 was a leap year but 1900 was not. So add 1 for centuries divisible by 400 and 0 otherwise. 
4. If your date is January 1-February 29 of a leap year, subtract 1. 
Try to devise a method to obtain the last two digits on your own. Print the day of the week as a word (Monday, Tuesday, etc.). Remember that Sunday is the first day of the week and it will be counted as 0 in this algorithm. 
Test your program first with your own birthdate. Then test with the following dates: 
* Today’s date 
* December 25, 1642 (Note: this is Newton’s birthdate in the Julian calendar, but use it as a Gregorian date) 
* October 12, 1492 
* January 20, 2000 
* December 11, 2525
Try to write and test your own program before peeking at the sample solution.
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/day_of_week.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/encodings_strings.md,"Like everything else in the computer, characters must be represented by a sequence of 0s and 1s.  A catalogue of these representations is usually called an encoding.
The basic character set used by C++ is ASCII, the American Standard Code for Information Interchange. Internationally, it is sometimes known as US-ASCII.  Originally, 7 bits were used to represent data, resulting in a total of 128 (27) available characters.  The first 32 are non-printing characters that were mainly needed by the mechanical devices for which the encoding was developed.  Only a few non-printing characters are still used, among them line feed, carriage return, and even ""bell.""
{{< code file=""/courses/cpp-introduction/codes/ringer.cxx"" lang=""c++"" >}}
ASCII now uses 8 bits (extended ASCII) which doubles the number of available characters, but the first 128 character codes are the same as 7-bit ASCII.  It was not as well standardized as ASCII, though standards exist for Latin alphabets (ISO 8859-1, ISO Latin 1) and Cyrillic (ISO 8859-5).  Nearly all programming languages continue to restrict the characters allowed for statements to the original ASCII, even when larger character sets are the default and may be used for comments and output.
Strings
A string is a sequence of characters of variable length.
Using strings requires a header:
```c++
define 
The string is a _class_ , which is a little beyond our scope right now.  But we can still use basic functions without understanding the class.c++
string str, str1, str2;
str.size();  // length of string
str1+str2; // concatenate two strings
str.substr(2,5); // substring (counts from 0)
str[n];  // nth character (counts from 0), no checking
str.at(n);  // nth character (counts from 0), checks bounds for error
```
Many useful operations are available to work with strings.  Most of them have more options than presented here; see documentation for more details.
{{< table >}}
|    Method    |      Operation    |   Usage     |
|--------------|-------------------|-------------|
|   clear      |  delete all characters |  str.clear()  |
|   append      |  add characters to the end |  str.append(str1)  |
|   compare      |  compare two strings |  str1.compare(str2)  |
|   insert      |  add characters from position p  |  str.insert(p,str1)  |
|   replace      |  replace n characters from position p with str1 |  str.replace(p,n,str1)  |
|   find      |  find start position str1 in str  |  str.find(str2)  |
|   c_str      |  convert to C-style character array  |  str.c_str()  |
{{< /table >}}
Example:
{{< code-download file=""/courses/cpp-introduction/codes/strings.cxx"" lang=""c++"" >}}
Exercise
In the above code, change ""This"" to ""That"" in newtitle.
{{< code-download file=""/courses/cpp-introduction/solns/strings.cxx"" lang=""c++"" >}}
Strings, Wide Strings, and Unicode
Even extended ASCII accommodates far too few characters to accommodate more than a handful of alphabets, much less other writing systems.  Unicode was created to address this.  The first 128 codes are still the 7-bit ASCII codes even with the millions available through Unicode.
The characters must still be encoded and there are multiple standards for Unicode.  One of the most widely used is UTF-8, which has become a Web standard.  It is variable-width; characters are encoded in one to four bytes, with the first 128 characters one byte in size and corresponding exactly to ASCII.  The string class can support UTF-8 directly.  However, some software, especially on Windows, may use the UTF-16 encoding.  Windows uses the ""wide string"" or wstring_t class for much of its character support; this makes cross-platform coding awkward. We will not go deeply into Unicode support but here is an example:
{{< code file=""/courses/cpp-introduction/codes/unicode.cxx"" lang=""c++"" >}}
Interested students can find discussions online of Unicode support.  Those who wish to write for Windows in particular may need to examine Microsoft-specific documentation."
rc-learning-fork/content/courses/cpp-introduction/project5.md,"Download the file vabirds.csv.
1. Create a struct birdData in files birdData.h and birdData.cxx. 
2. Write the following functions, implemented in birdDat.cxx. 
  * Write a stats procedure that takes only an instance of the struct and returns the mean and standard deviation of the observations for that instance. Hint: one way to return multiple items in C++ is to use a vector.
  * Write a minmax procedure that takes an instance of the type and the array of years and returns the maximum observed, the minimum observed, and the years for maximum and minimum.  You may use the maxval, minval, maxloc, and minloc intrinsics.
   * Write a main program that uses your struct.  Look up the sort 
Write a read_data routine that uses the number of items in the file, corrected for the header and the two footers, to allocate a vector of bird_data types.
Loop through this vector to load the data for each species.
The read_data routine should return the vector of years and the vector of bird_data types.  
Request a species name from the user.  Find the species in your vector of types 
and print its mean, standard deviation, and results from minmax. Print some appropriate message if the species is not found.  Compute a vector of the means for all species.
Use the built-in sort to sort the means vector.  However, we also want the permutation vector, which is a vector of the indices of the original positions.
For example, if after the sort the permutation vector is (17,3,55,11,23, and so forth) that means that the element that was previously 17 is now the first in the new array, and so on.
Using online resources, figure out how to do this.  Hint: you will need to use a ""custom comparitor.""
Test the user input portion for 

TurkeyVulture

TuftedTitmouse 

ElegantTrogon
For this project you can require an exact match of the species name.  (Note that no spaces are allowed and words are separated by capitalization; we would have to do more sophisticated string handling if we were to allow spaces and variations in capitalization.)
{{< spoiler text=""Sample solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/birdstruct/birdData.h"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/birdstruct/birdData.cxx"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/birdstruct/birdstats.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/variables.md,"In programming, a variable is similar, but not identical to, the variable familiar from mathematics.  In mathematics, a variable represents an unknown or abstract entity.  In programming, a variable represents a location in memory.
Computer memory consists of individual elements called bits, for 
binary digit.  Each bit is ""off"" or ""on"", represented by 0 and 1.  Bits are usually grouped into units of 8, called a byte.  The bytes are organized into words.  The number of bits in a word determines whether the computer is ""32 bits"" or ""64 bits"".  Nearly all modern hardware is 64 bits, meaning that each word of memory consists of 8 bytes.  Words are numbered, starting from 0.
Each variable has a type.  Types are a way of representing values as patterns of bits.  Some of these types, particularly those that represent numeric values, are defined by hardware operations in the computer's CPU.  Others can be defined by the programmer, but even these derived types are represented as combinations of the primitive types.
Remember that computers do not use base 10 internally.
Precision is the number of digits that are accurate, according to the requirements of the IEEE standard.  Please note that compilers will happily output more digits than are accurate if asked to print unformatted values.
Like most programming languages, C++ is case-sensitive.  Variables Mean and mean and even mEan are different to the compiler.  
Moreover, like most compiled languages. C++ is statically typed .  All variables must be declared to be of a specific type before they can be used.  A variable’s type cannot be changed once it is declared.
C++ is nearly strongly typed.  Mixed-mode expressions are limited and most conversions must be explicit.
Naming
Variable names must consist of only letters from the Latin alphabet, digits, or underscores.  They must begin with a letter or an underscore.  Spaces and special characters other than the underscore are not permitted.  The number of characters in a name is limited only by the system (compiler and platform) but programmers are advised to choose names that are descriptive but not overlong.
Good descriptive names may consist of several words or parts of words.  Since C++ is case-sensitive, a popular way to distinguish the segments is camel case.
This omits underscores and uses capitalization to separate components.  The customary version for most C++ programmers starts the name lower case and capitalizes subsequent components, with the possible exception of the names of classes.
c++
isValid
startDate
class Animal
Variables and other identifiers may not be the same as the list of reserved words in C++. A list defined by the standard is here.  Some compilers may define additional keywords.
Basic Types
Integers
Integers are quantities with no fractional part.  C++ supports signed and unsigned integers.  Signed integers take on all values within the available range.  Unsigned integers represent only non-negative values.
Signed integers are represented internally by a sign bit followed by a value in binary.  Remember that computers do not use base 10 internally.
Unsigned integers omit the sign bit and use all the available bits for the value.
C++ supports several categories of integer, differing by the number of bits to represent them and whether they are signed or unsigned.
The C++ standard does not specify the number of bits in an integer, only the minimum for each category.
{{< table >}}
|    Declaration    | Minimum Number of Bits   |  Minimum Range  |
|-------------------|--------------------------|-----------------|
|    short          |      16              |  -32,768 to 32,767   |
|  unsigned  short  |      16              |  0 to 65,535         |
|  int              |      16, usually 32  |  -32,768 to 32,767   |
|  unsigned int     |      16, usually 32  |  0 to 65,535         |
|  long             |      32              |  -2,147,483,648 to 2,147,483,647  |
|  unsigned long    |      32              |  0 to 4,294,967,295               |
|  long long        |      64              | -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807  |
|  unsigned long long |    64              |  0 to 18,446,744,073,709,551,615  |
{{< /table >}}
The programmer can use the sizeof function to determine the actual size of a type on the system in use.  It returns a result in bytes.  On most computing platforms, the int and long are the same (32 bits or 4 bytes).
{{< code file=""/courses/cpp-introduction/codes/ints.cxx"" lang=""c++"" >}}
The output for this code on an Intel-based Linux computer using the g++ compiler was
no-highlight
2,4,8,2,8,8,8
Floating Point Numbers
Floating-point numbers are representations of the mathematical real numbers.
However, due to the inherent finiteness of the computer, they have distinct
properties.

There is a finite number of floating-point numbers. Therefore, many (indeed, infinite) real numbers will map to the same floating-point number.
They are represented with a form of scientific notation, so their distribution on the number line is not uniform.
They are commutative but not associative or distributive, in general.  That
is,
$r+s=s+r$
$(r+s)+t \ne r+(s+t)$
$r(s+t) \ne rs+rt$

Floating-point numbers are defined by the IEEE 754 standard.  They consist of a sign bit, an exponent, and a significand.  All modern hardware uses base 2 so the exponent is a power of 2.
For input and output, these binary numbers must be converted to and from decimal (base 10), which usually causes a loss of precision at each
conversion.  Moreover, some numbers can be represented exactly given the available bits in base 10 but not in base 2 and vice versa, which is another source of error.  Finally, most real numbers cannot be represented exactly in the relatively small number of bits in either base 2 or base 10.
The most common types of floating-point number supported by hardware are single precision, which occupies 32 bits, and double precision, which takes up 64 bits.
As for integers, the C++ standard specifies only minimum ranges.  On nearly all general-purpose hardware, single-precision floating point is float and double-precision floating point is double.
{{< table >}}
|   Precision  |  Exponent Bits |  Significand Bits | Exponent Range (base 2) | Approximate Decimal Range  |  Approximate Decimal Precision |
|--------------|----------------|-------------------|-------------------------|----------------------------|--------------------------------|
| Single       |  8    |  23  |  -126/127 | ±2 x 10-38 to ±3 x 1038 | 7 digits |
| Double       |  11   |  52  |  -1022/1023 |  ±2.23 x 10−308 to ±1.80 x 10308 |  16 digits |
{{< /table >}}
C++ specifies a long double type but requires only that it be at least equivalent to double.
{{< code file=""/courses/cpp-introduction/codes/floats.cxx"" lang=""c++"" >}}
The output of the above program on the same Intel-based Linux computer with g++ was
no-highlight
4,8,16
The IEEE 754 standard also defines several special values.  The ones most frequently encountered by programmers are Inf (infinity), which may be positive or negative and usually results from an attempt to divide by zero, and NaN (not a number), which is the defined result of mathematically illegal operations such as $\sqrt{-1}$.
The number of bits is not a function of the OS type.  It is specified by the standard.
Boolean
Booleans represent truth value.  The name of the type is bool and the only permitted values are true and false.
Internally, true is 1 and false is 0, but it’s easier for humans to read and remember true/false.
c++
bool isValid;
Since they are integers they can be used in mathematical expressions, though this can become confusing.  
Literals
Literals are specific values corresponding to a particular type.  The compiler infers the type from the format.
{{< table >}}
|  Value  |  Type |
|---------|-------|
|  3      |  int  |
| 3.2     | double |
| 3.213e0 | double |
| ""This is a string"" | string |
| ""Isn’t it true?"" | string |
| true    |  bool  |
{{< /table >}}
Note that the default type for a floating-point literal is a double."
rc-learning-fork/content/courses/cpp-introduction/building.md,"The ""traditional"" development environment for compiled languages was a text editor and a command line.  Many programmers continue to use these successfully, but modern tools can greatly improve programmer productivity.  Some such tools are especially recommended for the Windows operating system, since it does not support command-line usage as cleanly as Linux or macOS.
Compilers
Compilers are sophisticated software packages.  They must perform a complex analysis of the code, translate it to machine language, and invoke a linker to create an executable.  This ""big picture"" view and direct machine language output is what enables compiled programs to run generally with much higher performance than interpreted scripts.  Compilers also offer a large number of compile-time options that can significantly impact the performance and sometimes the results of the executable.  
Many compilers are available, but we will focus on those from three vendors.
Gnu Compiler Collection
The Gnu Compiler Collection is a well-established, free and open-source bundle. The base compiler is gcc for C.  Several add-on languages are supported, the most widely used of which are g++ (C++) and gfortran (Fortran).  
NVIDIA HPC SDK
The NVIDIA HPC SDK is another free (though not open-source) compiler suite for C/C++/Fortran.  Formerly the Portland Group compilers, it is a general-purpose package but is oriented toward extensions for programming NVIDIA GPUs.  These compilers are nvcc, nvc++, and nvfortran. 
Intel Compilers
The Intel compilers have a reputation for producing the fastest executables on Intel architectures.  Most high-performance computing sites provide the commercial Intel suite icc, icpc, and ifort.  Intel's Parallel Studio package also ships with high-performance Math Kernel Libraries (MKL), MPI (IntelMPI), and threading (tbb).  The Parallel Studio package is available on the UVA HPC system.
Intel has recently released the oneAPI Toolkits. They are free but not open source, and are supported only through a community forum.  In order to obtain the ""classic"" compilers described above, the HPC Toolkit must be installed.  The newer compilers provided in the Base Toolkit for Intel are icx, and icpx.  Both the classic and the new Fortran compilers ifort and ifx are in the HPC Toolkit.
Integrated Development Environments
An Integrated Development Environment (IDE) combines an editor and a way to compile and run programs in the environment.
A well-known IDE for Microsoft Windows is Visual Studio. This is available through the Microsoft Store; it is not free for individuals.
macOS uses Xcode as its native IDE. Xcode includes some compilers, particularly for Swift, but it can manage several other languages.  Available at the App Store and free.
A full-featured cross-platform IDE is [Eclipse] (http://www.eclipse.org/).  Free.
A lighter-weight IDE for Windows and Linux is [Code::Blocks] (http://www.codeblocks.org/).  Free.
Windows programmers using Intel's oneAPI distribution must also install Visual Studio.  
An increasingly popular IDE is Visual Studio Code (VSCode) from Microsoft. It is also cross-platform, with versions available for Windows, macOS, and Linux.  It does not support C, C++, or Fortran by default; extensions must be installed to provide syntax highlighting and debugging for those languages.  C and C++ are installed with one extension that can be found at the top of the list.  
In our examples, we will use a very lightweight IDE called Geany since it is free, easy to install and use, and  works on all three platforms.  It is more of a programmer's editor than a full-featured IDE, but it does include some build tools.
Building an Executable
Creating an executable is generally a multistep process.  Of course, the first step is the preparation of a source file.  The convention for C++ was once a file extension of .cpp, but that conflicts with the name of the preprocessor (C PreProcessor) so .cxx is increasingly common.  We will use .cxx but .cpp still works.
From each source file, the compiler first produces an object file.  In Unix these end in .o, or .obj on Windows.
This is the compilation step.
Object files are binary (machine language) but cannot be executed.  They must be linked into an executable by a program called a linker (also called a loader).  The linker is normally invoked through the compiler.  The entire process of compiling and linking is called building the executable.
If not told otherwise a compiler will attempt to compile and link the source file(s) it is instructed to compile.  If more than one file is needed to create the executable, linking will not work until all object files are available, so the compiler must be told to skip that step.
For Unix compilers the -c option suppresses linking.  The compiler must then be run again to build the executable from the object files.
The linker option -o is used to name the binary something other than a.out.
Unix and macOS do not care about file extensions, but Windows will expect an executable to end in .exe.
Command Line
For full control and access to more compiler options, we can build from the command line.  For Linux and Mac this is a terminal application.  On Windows, use a command prompt for gcc.  The Intel oneAPI distribution ships with an integrated command prompt in its folder in the ""applications"" menu; this command prompt is aware of the location of the compiler executables and libraries.
Example
g++ -c mymain.cxx
g++ -c mysub.cxx
IDEs generally manage basic compiler options and usually name the executable based on the project name.  Our examples of command line usage will all assume a Unix operating system; there are some differences between Linux and macOS, with larger differences for Windows.  On macOS and especially Windows, using an IDE makes code management simpler.  Using Geany as our example, clicking the icon showing a pyramid pointing to a circle will compile the current file without attempting to invoke the linker.  The brick icon builds the current file, so it must be possible to create a standalone executable from a single file."
rc-learning-fork/content/courses/cpp-introduction/project4.md,"Download the file bodyfat.csv.  This is a dataset of body fat, age, height, and weight for a set of participants in a study. BMI categories are as follows:
{{< table >}}
|Severely underweight |  BMI < 16.0 |
|Underweight          | 16 <= BMI < 18.5 |
|Normal               | 18.5 <= BMI < 25 |
|Overweight           | 25 <= BMI < 30 |
|Obese Class I        | 30 <= BMI < 35 |
|Obese Class II       | 35 <= BMI < 40 |
|Obese Class III      | BMI > 40       |
{{< /table >}}
Write a bmistats module containing functions for the following:
1. Convert pounds to kilograms.  Use the actual conversion factor, not the approximate one.  Look it up on Google.
2. Convert feet/inches to meters.  Look up the conversion factor, do not guess at it. 
3. Compute BMI.
4. Determine where the BMI falls in the table supplied and return that information an appropriate form. 
Write a file stats that implements the following:
1. Mean of an array 
2. Standard deviation of an array 
3. Outlier rejection using Chauvenet’s criterion.  Pseudocode given further down.
Write a main program that implements the following:
1. Uses your other files
2. Reads the input file into appropriate arrays (use one-dimensional arrays for this project).  Don't assume you know the length of the file (but you can assume the number of header lines is fixed).
3. Pass appropriate arrays to a subroutine that computes an array of BMI data based on height and weight and returns the BMI array.
4. Rejects the outlier(s).  The function should return an array of logicals that you can apply to the original data using WHERE or similar.  Create new arrays with the outlier(s) deleted. 
Write a file that contains the corrected data for bodyfat and BMI.  Use Excel or whatever you normally use to plot BMI as a function of percentage body fat. 
Be sure to plot it as a scatter plot (points only, no connecting lines).  
Chauvenet’s criterion: It’s not the state of the art but works pretty well.
1. Compute the mean and standard deviations of the observations.
2. Compute the absolute values of the deviations, i.e. abs(A-mean(A))/std(A)
3. Use the tails devs=devs/sqrt(2.)
4. Compute the probabilities prob=erfc(devs) : erfc is an intrinsic in any fairly recent Fortran compiler.
5. The criterion is that we retain data with prob>=1./(2*N_obs) (number of observations).
{{< spoiler text=""Example solution"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/stats.cxx"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/bmistats.h"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/bmistats.cxx"" lang=""c++"" >}}
{{< code-download file=""/courses/cpp-introduction/solns/bmi.cxx"" lang=""c++"" >}}
{{< /spoiler >}}"
rc-learning-fork/content/courses/cpp-introduction/setting_up.md,"Linux
For users of the University of Virginia's cluster, first load a compiler module.
module load gcc
This command brings a newer gcc, g++, and gfortran into the current environment. Now load Geany.
module load geany
geany&
For personal use, compilers, Geany, and the other cross-platform IDEs are available for all popular Linux distributions and can be installed through the distribution's package manager or by downloading from the vendor (e.g. the NVIDIA HPC SDK).  Most workstation users do not install environment modules, so the module command would not be required.  However, it may be convenient if you wish to install multiple compilers.  At least one implementation of software modules is usually available for a given distribution.
GCC
The gcc compiler is a standard part of all Linux distributions.  However, it is usually necessary to add g++ and gfortran separately.
Intel OneAPI
Download the Linux version from Intel.  Installing the HPC Toolkit as well as the required Basic Toolkit is recommended.
NVIDIA HPC SDK
Download and install the package.  If your distribution is not supported with a package, you may have t
o download the tarball and set paths appropriately.
To use a different compiler with Geany, from the Build menu choose Set Build Commands. To use a different compiler with Geany, from the Build menu choose Set Build Commands.  See the chapter on building your codes for compiler names and some options.
IDEs for Mac and Windows
On Mac and Windows, an IDE can be installed in the usual way for those platforms, either through the application ""store"" or starting at the package's home page.
Mac OS
GCC
Install Xcode from the App Store.  This will install gcc and g++.  You may also wish to install the gcc package from homebrew since it will install the Xcode command-line tools for you.
Intel oneAPI
Download the Mac version from Intel.  It may be useful to install the HPC Toolkit as well as the required Basic Toolkit. 
The NVIDIA HPC SDK is not available for Macs.
Geany can be installed from its homepage.  Other options, such as VSCode, can be installed similarly.
Windows
There are several compiler options for Windows.  Visual Studio supports C and C++.  There are also several free options.
GCC
A popular vehicle for using the Gnu compilers on Windows is Cgwin.  Cygwin also provides a large number of other Unix tools.  The Gnu compiler suite is installed through the Cygwin installer.
Recently, Microsoft has released the Windows Subsystem for Linux (WSL).  This is not a virtual machine but is a type of Linux emulator.  It is a full-featured command-line-only Linux environment for Windows, but the X11 graphical user interface is not supported.
A drawback to both Cygwin and the WSL is portability of executables.  Cygwin executables must be able to find the Cygwin DLL and are therefore not standalone.
WSL executables only run on the WSL.  For standalone, native binaries a good choice is MingGW.  MinGW is derived from Cygwin.
MinGW provides a free distribution of gcc/g++/gfortran.  The standard MinGW distribution is updated fairly rarely and generates only 32-bit executables.  We will describe MinGW-w64, a fork of the original project.
{{< figure src=""/courses/cpp-introduction/img/MinGW1.png"" width=500px >}}
MinGW-w64 can be installed beginning from the MSYS2 project.  MSYS2 provides a significant subset of the Cygwin tools.  Download and install it.
{{< figure src=""/courses/cpp-introduction/img/MSYS2.png"" width=500px >}}
Once it has been installed, follow the instructions to open a command-line tool, update the distribution, then install the compilers and tools. 
A discussion of installing MinGW-64 compilers for use with VSCode has been posted by Microsoft here. 
Intel oneAPI
First install Visual Studio.
  Individual developers and most academic users are eligible for the free community edition.  After installing VS, download and install the Basic Toolkit.  The HPC Toolkit is also recommended; it is installed after Basic.
NVIDIA HPC SDK
Download and install the package when it is available.
Environment Variables in Windows
To use any of these compilers through an IDE, they must be added to the Path environment variable.  You must use the path you chose for the installation.  The default is C:\msys64\mingw64\bin for the compilers.
Control Panel->System and Security->Advanced system settings->Environment Variables
{{< figure src=""/courses/cpp-introduction/img/WindowsEV.png"" width=412px >}}
Once you open Path, click New to add to the Path
{{< figure src=""/courses/cpp-introduction/img/WindowsPath.png"" width=500px >}}
To test that you have successfully updated your path, open a cmd window and type
g++
You should see an error
g++: fatal error: no input files
Compiling Your First Program
We will show Geany and VSCode on Windows.  Both look similar on the other platforms.  
Open Geany (or VSCode).  Type in the following
{{< code file=""courses/cpp-introduction/codes/hello.cxx"" lang=no-highlight >}}
{{< figure src=""/courses/cpp-introduction/img/Geany1.png"" width=500px  >}}
Syntax coloring will not be enabled until the file is saved with a file extension that corresponds to the language.  Save this file as hello.cxx.  The coloring will appear.
{{< figure src=""/courses/cpp-introduction/img/Geany2.png"" width=500px >}}
The appearance is similar in VSCode.
{{< figure src=""/courses/cpp-introduction/img/VSCode.png"" width=500px >}}
In Geany, click the Build icon (a brick wall).  A message confirming a successful compilation should be printed.
{{< figure src=""/courses/cpp-introduction/img/Geany3.png"" width=500px >}}
Now click the Execute button.  A new window will open and the message will be printed.
{{< figure src=""/courses/cpp-introduction/img/Geany4.png"" width=500px caption=""Executing the Hello World program"" >}}
Build Commands in Geany
If you wish to use a different compiler suite from the default GCC with Geany, or if you need to change the compiling or linking commands for any other reason, you can modify the build commands through the Build->Set Build Commands dialogue.
{{< figure src=""/courses/cpp-introduction/img/GeanyBuildTools.png"" width=500px caption=""Edit the build commands Geany will use"" >}}"
rc-learning-fork/content/courses/cpp-introduction/_index.md,"C++ is a versatile, widely-used compiled language.  This short course covers the basics, with some discussion of features of the C++11 standard.  Newer codes increasingly use C++14 constructs but not all compilers support those by default yet. 
The best way to learn a programming language is to use it.  We strongly urge you
 to attempt the exercises and projects to practice your coding skills."
rc-learning-fork/content/courses/cpp-introduction/boost.md,"One of the most popular add-on libraries for C++, especially numerical or scientific programming, is Boost.  Boost is not included with any compilers, but is generally easy to obtain.  For Linux systems generally it is available through the package manager.  
Installing Boost
Boost uses its own build system on Unix and macOS.  On Windows there is an experimental CMake system but generally the ""bjam"" builder is still used.
In all cases, installing Boost requires some familiarity with using a command line.
Unix and macOS
On a Linux system, the simplest way to install Boost is to utilize the package manager of the distribution.  For example on Ubuntu the command is
no-highlight
sudo apt install libboost-all-dev
or on Fedora (or Centos 8 and beyond)
no-highlight
sudo dnf install boost
sudo dnf install boost-devel
If you do not have superuser (sudo) permission, or you wish to install Boost somewhere other than the main system libraries, follow the general instructions.
The default prefix for installation from b2 install is /usr/local.  This is true for both Linux and macOS.  If you wish to install to /usr/local, which is normally in the system search paths, you will need to run the installation command with sudo
no-highlight
cd path/to/boost/source
./bootstrap.sh
sudo ./b2 install
If you do not have sudo privileges or you wish to install it someplace else, keep in mind that this will affect how you reference the headers and libraries with -I and -L.  You must provide the prefix to the installation directory to the bootstrap.sh script to install someplace other than the default location.
no-highlight
./bootstrap.sh --prefix=/home/mst3k/boost
./b2 install
Windows
Installation on Windows is somewhat more complicated than on Unix variants.  First download the zipped source file.
1. Start a command window. If you wish to install Boost in a standard search path, you will need to run it as administrator.
2. Unzip the .zip file into a folder of your choice C:\yourpath.  It will unpack to a folder with a version number, which may vary from our example C:\yourpath\boost_1_76_0
3. Create some folders, e.g.
      mkdir C:\boost-build
      mkdir C:\yourpath\boost_1_76_0\boost-build
      mkdir C:\boost
4. From the command prompt, cd C:\yourpath\boost_1_76_0\tools\build
5. Runboostrap.bat gcc6. Install the build system withb2 --prefix=""C:\boost-build"" install7. Modify your session PATH withset PATH=%PATH%;C:\boost-build\bin8. Return to your source directorycd C:\yourpath\boost_1_76_09. Build withb2 --build-dir=""C:\install\boost --prefix=""C:\boost"" --build-type=complete toolset=gcc install10. This will install to ""C:\boost\include\boost-1_76_0"" and ""C:\boost\lib""
      You may remove temporary unpacking and build directories if you wish.  You may also move the header files up to C:\boost\include if you prefer.  Remember thatwill use-I` to start looking for that subdirectory.
Using Boost is probably simplest with a Makefile.  The example is for Windows with a particular choice of location for the boost header files and libraries.  Change the locations for -I and -L as appropriate if they are located elsewhere on your system.  Please refer to the earlier chapter for a review of setting up Makefiles.  This example goes with a standard Boost example.
If you have installed Boost onto a Linux or macOS system to a system default search location such as /usr or /usr/local you will not need to specify the -I or -L paths at all.  The example makefile assumes installation in system paths in the compiler's default search paths.
Boost MultiArrays
C-style arrays are widely used but lack some important features, such as the ability to check whether indexes outside the array bounds are referenced.
The C++ STL array type has more features, but is limited to one dimension.  A workaround is to ""stack"" vectors, since a vector of a vector amounts to a 2-dimensional structure.
```c++
include 
using namespace std;
vector> A;
```
The Boost library provides a popular alternative, the MultiArray. This structure can be N-dimensional, its bounds (extents) can be checked, and it can be reshaped and resized.  The price is that it can be slow.
{{< code-download file=""/courses/cpp-introduction/codes/boost_array.cxx"" lang=""c++"" >}}
Exercise
If you have succeeded in installing Boost, or you have access to a system where it has been installed, download and run the above boost_array.cxx program."
rc-learning-fork/content/courses/containers-for-hpc/minimal.md,"The industry standard of restricting containers to just the application and its dependencies often results in better security and smaller size. See how the use of multi-stage builds and scratch/distroless base images can reduce the image size by as much as 99% in real applications.
Prerequisites:
- Building Containers [Docker]

Review of Best Practices
In the previous chapter, we worked through an example of lolcow and saw how following best practices can reduce the image size drastically.
0. Package manager cache busting
bash
apt-get update && apt-get install ...
1. Clean up

apt
bash
rm -rf /var/lib/apt/lists/*
conda
bash
conda clean -ya
pip (no separate clean up command)
bash
pip install --no-cache-dir ...
Must occur in the same RUN statement as the installation step

2. Only install what's needed
bash
--no-install-recommends
3. Base image

For OS and common tools (e.g. python/conda, GCC) start from official image
Do not reinvent the wheel


Know which variant to use (e.g. devel vs runtime, slim)
Read their overview



Exercise: QIIME 2
QIIME 2 is a popular bioinformatics software. Can you suggest any potential improvement to the Dockerfile? 
Answer

With our [Dockerfile](https://github.com/uvarc/rivanna-docker/blob/master/qiime2/2020.8/Dockerfile), we managed to reduce the image size by half.



Multi-Stage Build
The objective is to minimize image size without loss of functionality. What's needed to build/install an application is not always needed at runtime. By separating the buildtime and runtime stages, we'll see how to achieve up to 99% reduction in image size in real applications.
""Disk space is cheap so why should I care?""

Minimize vulnerabilities/attack surface
Fewer things to maintain
Reduce overhead

Buildtime $\neq$ runtime dependency

Multiple FROM statements (each defines a stage)
Build stage:
install buildtime dependencies
install software


Production stage:
use base/runtime base image instead of devel (jre vs jdk)
only install runtime dependencies (e.g. cmake, go not needed)
copy software from build stage



Reference: Use multi-stage builds
Multi-stage Dockerfile template
```dockerfile
FROM base1 AS build
install buildtime dependencies
install software
FROM base2
COPY --from=build /path/to/file/in/base1 /path/to/file/in/base2
install runtime dependencies
ENV PATH=/path/to/binary:$PATH
ENTRYPOINT [""binary""]
```

base1 and base2 may or may not be the same
Use AS <stage> to name a stage
Use COPY --from=<stage> to copy from a particular stage
You can have more than 2 stages

Exercise: LightGBM
LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is an open source project by Microsoft.


Examine the official Dockerfile. Can you identify any problems?
Answer

nvidia/cuda:cudnn-devel as base image  (>1 GB)
Clean up in separate RUN statements





Let's try to build an image of the command line interface (CLI) alone. Copy the Dockerfile. Remove the Tini, Conda, Jupyter sections and everything related to python/conda. Build the image and note the image size.
Answer
```dockerfile
FROM nvidia/cuda:8.0-cudnn5-devel

Global

apt-get to skip any interactive post-install configuration steps with DEBIAN_FRONTEND=noninteractive and apt-get install -y
ENV LANG=C.UTF-8 LC_ALL=C.UTF-8
ARG DEBIAN_FRONTEND=noninteractive

Global Path Setting

ENV CUDA_HOME /usr/local/cuda
ENV LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:${CUDA_HOME}/lib64
ENV LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/usr/local/lib
ENV OPENCL_LIBRARIES /usr/local/cuda/lib64
ENV OPENCL_INCLUDE_DIR /usr/local/cuda/include

SYSTEM

update: downloads the package lists from the repositories and ""updates"" them to get information on the newest versions of packages and their
dependencies. It will do this for all repositories and PPAs.
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    wget \
    bzip2 \
    ca-certificates \
    libglib2.0-0 \
    libxext6 \
    libsm6 \
    libxrender1 \
    git \
    vim \
    mercurial \
    subversion \
    cmake \
    libboost-dev \
    libboost-system-dev \
    libboost-filesystem-dev \
    gcc \
    g++
Add OpenCL ICD files for LightGBM
RUN mkdir -p /etc/OpenCL/vendors && \
    echo ""libnvidia-opencl.so.1"" > /etc/OpenCL/vendors/nvidia.icd

LightGBM

RUN cd /usr/local/src && mkdir lightgbm && cd lightgbm && \
    git clone --recursive --branch stable --depth 1 https://github.com/microsoft/LightGBM && \
    cd LightGBM && mkdir build && cd build && \
    cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ .. && \
    make OPENCL_HEADERS=/usr/local/cuda-8.0/targets/x86_64-linux/include LIBOPENCL=/usr/local/cuda-8.0/targets/x86_64-linux/lib
ENV PATH /usr/local/src/lightgbm/LightGBM:${PATH}

System CleanUp

apt-get autoremove: used to remove packages that were automatically installed to satisfy dependencies for some package and that are no more needed.
apt-get clean: removes the aptitude cache in /var/cache/apt/archives. You'd be amazed how much is in there! the only drawback is that the packages
have to be downloaded again if you reinstall them.
RUN apt-get autoremove -y && apt-get clean && \
    rm -rf /var/lib/apt/lists/*
```
2.24 GB




Rewrite the Dockerfile using a multi-stage build based on OpenCL.

Use nvidia/opencl:devel as the build stage
Remove everything related to CUDA since it is not relevant
Redefine the OpenCL environment variables as:
    dockerfile
    ENV OPENCL_LIBRARIES=/usr/lib/x86_64-linux-gnu \
        OPENCL_INCLUDE_DIR=/usr/include/CL
Keep the same dependencies
Remember to clean up at the right place
Ignore the command under # Add OpenCL ICD files for LightGBM
Instead of mkdir foo && cd foo use WORKDIR foo (see documentation)
Use the same command to install LightGBM, but replace the paths with the corresponding OpenCL environment variables
Add an ENTRYPOINT


Use nvidia/opencl:runtime as the production stage
The runtime dependencies are libxext6 libsm6 libxrender1 libboost-system-dev libboost-filesystem-dev gcc g++
Remember to copy from the build stage


Build the image and compare the image size with step 2.

Answer
```dockerfile
FROM nvidia/opencl:devel AS build
ENV LANG=C.UTF-8 LC_ALL=C.UTF-8
    OPENCL_LIBRARIES=/usr/lib/x86_64-linux-gnu
    OPENCL_INCLUDE_DIR=/usr/include/CL
ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    wget \
    ca-certificates \
    libglib2.0-0 \
    libxext6 \
    libsm6 \
    libxrender1 \
    git \
    cmake \
    libboost-dev \
    libboost-system-dev \
    libboost-filesystem-dev \
    gcc \
    g++ && \
    rm -rf /var/lib/apt/lists/*
WORKDIR /usr/local/src/lightgbm
RUN git clone --recursive --branch stable --depth 1 https://github.com/microsoft/LightGBM && \
    cd LightGBM && mkdir build && cd build && \
    cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=${OPENCL_LIBRARIES}/libOpenCL.so -DOpenCL_INCLUDE_DIR=${OPENCL_INCLUDE_DIR} .. && \
    make OPENCL_HEADERS=${OPENCL_INCLUDE_DIR} LIBOPENCL=${OPENCL_LIBRARIES}
FROM nvidia/opencl:runtime
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    libxext6 libsm6 libxrender1 libboost-system-dev libboost-filesystem-dev gcc g++ && \
    rm -rf /var/lib/apt/lists/*
COPY --from=build /usr/local/src/lightgbm/LightGBM/lightgbm /lightgbm
ENTRYPOINT [""/lightgbm""]
```
374 MB (84% reduction)




Challenge: Verify that the two containers have the same performance on Rivanna's GPU node. Follow the tutorial example. Run the same job without using GPU. How much faster is it with GPU?



Base Images Without OS
Do we really need an operating system?

Not always!
No /bin/sh, ls, cat, ...
Shell-less containers are supported by Singularity 3.6+


No package manager
In production stage, typically there's no RUN; just COPY
Workflow:
Target a specific stage to build: docker build --target=build .
Find shared libraries of binary: ldd
Copy binary and libraries to production stage
Build production



Example base images

Scratch
Literally start from scratch!
A ""no-op"" in the Dockerfile, meaning no extra layer in image
Build other base images (beyond scope of this workshop)
Minimal image with a single application


Distroless
Derived from Debian
Support for C/C++, Go, Rust, Java, Node.js, Python




Exercise: fortune from scratch
This exercise illustrates how we can cherry-pick files from the package manager that are essential to the application.


The Ubuntu base image shall be our basis of comparison. Copy the Dockerfile and build the image.
```dockerfile
FROM ubuntu:16.04
RUN apt-get update && apt-get install -y --no-install-recommends \
        fortune fortunes-min && \
    rm -rf /var/lib/apt/lists/*
ENV PATH=/usr/games:${PATH}
ENTRYPOINT [""fortune""]
```


Find the dependencies for fortune:

docker run --rm -it --entrypoint=bash <img>
ldd /usr/games/fortune


For your reference, the content of the packages can be found here:
- fortune-mod package list
- fortunes-min package list



Having identified the necessary files to copy, add a second stage FROM scratch to your Dockerfile. Only COPY what's necessary. Build and compare image sizes.
Answer
```dockerfile
FROM ubuntu:16.04 AS build
RUN apt-get update && apt-get install -y --no-install-recommends \
        fortune fortunes-min && \
    rm -rf /var/lib/apt/lists/*
FROM scratch
fortune
COPY --from=build /usr/games/fortune /usr/games/fortune
COPY --from=build /usr/lib/x86_64-linux-gnu/librecode.so.0 /usr/lib/x86_64-linux-gnu/librecode.so.0
COPY --from=build /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libc.so.6
COPY --from=build /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2
fortunes-min
COPY --from=build /usr/share/doc/fortunes-min/ /usr/share/doc/fortunes-min/
COPY --from=build /usr/share/games/fortunes/ /usr/share/games/fortunes/
ENV PATH=/usr/games:${PATH}
ENTRYPOINT [""fortune""]
```
The image size comparison is 130 MB vs 4 MB, a 97% reduction.



Exercise: (Trick) Question
Can you build an image for lolcow (equivalent to fortune|cowsay|lolcat; see previous workshop for details) from scratch/distroless? 
Exercise: LightGBM from scratch
Revisit the LightGBM Dockerfile you prepared previously. Enter the image layer of the build stage and run ldd to find the libraries needed by LightGBM. In the production stage, use scratch as the base image. Do not use any RUN statements in the production stage. You must include this line:
dockerfile
COPY --from=build /etc/OpenCL/vendors/nvidia.icd /etc/OpenCL/vendors/nvidia.icd
Build the image and compare image sizes.
Answer

```dockerfile
FROM nvidia/opencl:devel AS build

ENV LANG=C.UTF-8 LC_ALL=C.UTF-8
    OPENCL_LIBRARIES=/usr/lib/x86_64-linux-gnu
    OPENCL_INCLUDE_DIR=/usr/include/CL
ARG DEBIAN_FRONTEND=noninteractive

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    wget \
    ca-certificates \
    libglib2.0-0 \
    libxext6 \
    libsm6 \
    libxrender1 \
    git \
    cmake \
    libboost-dev \
    libboost-system-dev \
    libboost-filesystem-dev \
    gcc \
    g++ && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /usr/local/src/lightgbm
RUN git clone --recursive --branch stable --depth 1 https://github.com/microsoft/LightGBM && \
    cd LightGBM && mkdir build && cd build && \
    cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=${OPENCL_LIBRARIES}/libOpenCL.so -DOpenCL_INCLUDE_DIR=${OPENCL_INCLUDE_DIR} .. && \
    make OPENCL_HEADERS=${OPENCL_INCLUDE_DIR} LIBOPENCL=${OPENCL_LIBRARIES}

FROM scratch

COPY --from=build /usr/local/src/lightgbm/LightGBM/lightgbm /lightgbm
COPY --from=build \
    /lib/x86_64-linux-gnu/libc.so.6 \
    /lib/x86_64-linux-gnu/libdl.so.2 \
    /lib/x86_64-linux-gnu/libgcc_s.so.1 \
    /lib/x86_64-linux-gnu/libm.so.6 \
    /lib/x86_64-linux-gnu/libpthread.so.0 \
    /lib/x86_64-linux-gnu/

COPY --from=build \
    /usr/lib/x86_64-linux-gnu/libOpenCL.so.1 \
    /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.65.1 \
    /usr/lib/x86_64-linux-gnu/libboost_system.so.1.65.1 \
    /usr/lib/x86_64-linux-gnu/libgomp.so.1 \
    /usr/lib/x86_64-linux-gnu/libstdc++.so.6 \
    /usr/lib/x86_64-linux-gnu/

COPY --from=build /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2

COPY --from=build /etc/OpenCL/vendors/nvidia.icd /etc/OpenCL/vendors/nvidia.icd

ENTRYPOINT [""/lightgbm""]
```

The image size is merely **10.7 MB**, 99.5% smaller than what we started with. There is no loss in functionality or performance.


We submitted a pull request that has been merged.

Example: TensorFlow distroless
TensorFlow is a popular platform for machine learning. It is an open source project by Google.
The TF 2.3 container that you used in the previous workshop is actually based on distroless, which is why you were not able to run ls inside the container.

Dockerfile
18% image size reduction
PR approved and merged


Dynamic vs Static Linking
The above procedure, while impressive, may be tedious for the average user. All the examples so far are based on dynamic linking, where the shared libraries of an executable are stored separately. If you are compiling code from source, you may choose to build a static binary (e.g. -static in GCC) so that all the necessary libraries are built into the binary.
Exercise: Linking against OpenBLAS
OpenBLAS is a linear algebra library. The code in this exercise is taken from its user manual. It is based on dgemm which performs the matrix operation:
$$ \alpha A B + \beta C, $$
where $A_{mk}, B_{kn}, C_{mn}$ are matrices and $\alpha, \beta$ are constants. For details please visit the Intel tutorial page.


Select an appropriate base image. (Hint: You will be compiling C++ code.)


Install libopenblas-dev via the package manager.


Copy the code to the same directory as your Dockerfile.
```
include 
include 
void main()
{
    int i=0;
    double A[6] = {1.0,2.0,1.0,-3.0,4.0,-1.0};       
    double B[6] = {1.0,2.0,1.0,-3.0,4.0,-1.0};
    double C[9] = {.5,.5,.5,.5,.5,.5,.5,.5,.5}; 
    cblas_dgemm(CblasColMajor, CblasNoTrans, CblasTrans,3,3,2,1,A, 3, B, 3,2,C,3);c
for(i=0; i<9; i++)
    printf(""%lf "", C[i]);
printf(""\n"");

}
```
To copy it into the Docker image, add these lines:
dockerfile
WORKDIR /opt
COPY cblas_dgemm.c ./


Compile the code with this command:
    gcc -o cblas_dgemm cblas_dgemm.c -lopenblas -lpthread


Build the image and note the image size. You should get this output:
    11.000000 -9.000000 5.000000 -9.000000 21.000000 -1.000000 5.000000 -1.000000 3.000000
(Optional) Read the Intel tutorial to figure out what the matrices $A, B, C$ are. Do the math and verify that you get the same result.


Find the necessary libraries and add a second stage from scratch. Compare the image size between the two stages.
Answer
```dockerfile
FROM gcc:10.2 AS build
RUN apt-get update && apt-get install -y --no-install-recommends libopenblas-dev && \
    rm -rf /var/lib/apt/lists/*
WORKDIR /opt
COPY cblas_dgemm.c ./
RUN gcc -o cblas_dgemm cblas_dgemm.c -lopenblas -lpthread
FROM scratch
COPY --from=build /opt/cblas_dgemm /cblas_dgemm
COPY --from=build \
    /lib/x86_64-linux-gnu/libc.so.6 \
    /lib/x86_64-linux-gnu/libm.so.6 \
    /lib/x86_64-linux-gnu/libpthread.so.0 \
    /lib/x86_64-linux-gnu/
COPY --from=build /usr/lib/x86_64-linux-gnu/libopenblas.so.0 /usr/lib/x86_64-linux-gnu/libopenblas.so.0
COPY --from=build \
    /usr/local/lib64/libgcc_s.so.1 \
    /usr/local/lib64/libquadmath.so.0 \
    /usr/local/lib64/libgfortran.so.5 \
    /usr/local/lib64/
COPY --from=build /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2
ENV LD_LIBRARY_PATH=/usr/local/lib64:$LD_LIBRARY_PATH
ENTRYPOINT [""/cblas_dgemm""]
```
1.29 GB vs 42.9 MB (97% reduction).




Re-compile the code with static linking by adding a -static flag. In the production stage simply copy the binary. Compare image sizes.
Answer
```dockerfile
FROM gcc:10.2 AS build
RUN apt-get update && apt-get install -y --no-install-recommends libopenblas-dev && \
    rm -rf /var/lib/apt/lists/*
WORKDIR /opt
COPY cblas_dgemm.c ./
RUN gcc -o cblas_dgemm cblas_dgemm.c -lopenblas -lpthread -static
FROM scratch
COPY --from=build /opt/cblas_dgemm /cblas_dgemm
ENTRYPOINT [ ""/cblas_dgemm"" ]
```
26.6 MB (98% reduction).



This exercise illustrates that it is easier to build a minimal container of a static binary.
Exercise: Linking against LibTorch
LibTorch is the C++ frontend of PyTorch. This exercise is based on the ""Writing a Basic Application"" section of the PyTorch tutorial.


Select an appropriate base image. (Hint: You will be compiling C++ code.)


You will need these additional packages:

Build tools: build-essential cmake
Download and decompress: wget ca-certificates unzip



Find the download link for LibTorch under the ""Install PyTorch"" section at https://pytorch.org/. Select ""None"" for CUDA. Download the file to /opt in the image. Hints:

In your wget command, you may want to rename the output file using -O libtorch.zip.
Remember to decompress.



Copy these two files to the same directory as your Dockerfile.

dcgan.cpp

```
include 
include 
int main() {
    torch::Tensor tensor = torch::eye(3);
    std::cout << tensor << std::endl;
}
```

CMakeLists.txt

```
cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(dcgan)
list(APPEND CMAKE_PREFIX_PATH ""/opt/libtorch/share/cmake/Torch"")
find_package(Torch REQUIRED)
add_executable(dcgan dcgan.cpp)
target_link_libraries(dcgan ""${TORCH_LIBRARIES}"")
set_property(TARGET dcgan PROPERTY CXX_STANDARD 14)
```
To copy them into the Docker image, add these lines:
dockerfile
WORKDIR /opt/dcgan
COPY dcgan.cpp CMakeLists.txt ./


Build the code. The typical procedure is:
    mkdir build
    cd build
    cmake ..
    make


Find the necessary libraries and add a second stage from scratch. Compare the image size between the two stages.
Answer
```dockerfile
FROM gcc:10.2 AS build
RUN apt-get update && apt-get install -y --no-install-recommends \
        build-essential cmake \
        wget ca-certificates unzip && \
    rm -rf /var/lib/apt/lists/*
WORKDIR /opt
RUN wget -q https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.7.1%2Bcpu.zip -O libtorch.zip && \
    unzip libtorch.zip && rm libtorch.zip
WORKDIR /opt/dcgan
COPY dcgan.cpp CMakeLists.txt ./
RUN mkdir build && cd build && cmake .. && make
FROM scratch
COPY --from=build /opt/dcgan/build/dcgan /dcgan
COPY --from=build \
    /lib/x86_64-linux-gnu/libc.so.6 \
    /lib/x86_64-linux-gnu/libdl.so.2 \
    /lib/x86_64-linux-gnu/libm.so.6 \
    /lib/x86_64-linux-gnu/libpthread.so.0 \
    /lib/x86_64-linux-gnu/librt.so.1 \
    /lib/x86_64-linux-gnu/
COPY --from=build \
    /opt/libtorch/lib/libc10.so \
    /opt/libtorch/lib/libgomp-75eea7e8.so.1 \
    /opt/libtorch/lib/libtorch.so \
    /opt/libtorch/lib/libtorch_cpu.so \
    /opt/libtorch/lib/
COPY --from=build \
    /usr/local/lib64/libgcc_s.so.1 \
    /usr/local/lib64/libstdc++.so.6 \
    /usr/local/lib64/
COPY --from=build /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2
ENV LD_LIBRARY_PATH=/usr/local/lib64:$LD_LIBRARY_PATH
ENTRYPOINT [""/dcgan""]
```
1.98 GB for build stage vs 314 MB for production stage (85% reduction).




Challenge: The above image cannot make use of GPU. Build an image for GPU. Hints:

You do not need a physical GPU to build an image for GPU.
Pick a nvidia/cuda base image. Read their overview page on Docker Hub to decide which flavor to use.
Choose a CUDA version to get the download link for LibTorch on the PyTorch webpage.




Challenge: Can you build dcgan on Rivanna without using a container? Why (not)?


Challenge: Can you build a static binary of dcgan? Why (not)?



Summary
{{< table >}}
| App | Single-stage image size | Final image size | Reduction (%) |
|---|---:|---:|---:|
| fortune | 130 MB | 4 MB | 97.0 |
| LightGBM | 2.24 GB | 10.7 MB | 99.5 |
| dgemm | 1.29 GB | 26.6 MB | 98.0 |
| dcgan | 1.98 GB | 314 MB | 85.5 |
{{< /table >}}

References

UVA Rivanna-Docker GitHub
Dockerfiles by UVA Research Computing
Tips


Best practices for writing Dockerfiles
Use multi-stage builds
Google Distroless GitHub
Matthew Moore, Distroless Docker: Containerizing Apps, not VMs, swampUP (2017)
"
rc-learning-fork/content/courses/containers-for-hpc/building-docker.md,"Docker, Dockerfile, Docker Hub
Starting from a simple Dockerfile, we will adopt best practices sequentially and see their effect.
Prerequisites


Command line (Unix)


Install Docker
To follow along on your own computer, please install Docker Desktop and register for a free account on Docker Hub. Both can be found here.
After the installation, open a terminal (""cmd"" on Windows) and make sure you can execute the command docker run hello-world successfully.


Set BUILDKIT_PROGRESS=plain for plain output (or remember to run docker build --progress=plain ...).



Building for (not on) Rivanna

Docker
No Docker on Rivanna
Docker Hub
Can be converted into Singularity


Singularity
Users cannot build on Rivanna (needs sudo privilege)
Singularity Library/Hub (more limitations)
Refer to workshop in Spring 2020



Intro to Dockerfile: lolcow
fortune | cowsay | lolcat

fortune cookie
talking cow
rainbow color

Steps:
1. Choose a base image
2. Install software dependencies (if any)
3. Install software
Step 1: Choose a base image
Use FROM to specify the base image. In this example, we'll use Ubuntu 22.04. You do not need to install this on your computer - Docker will pull from Docker Hub when you build it.
dockerfile
FROM ubuntu:22.04

OS: ubuntu, debian, centos, ...
Doesn't have to be a bare OS
python, continuumio/miniconda3, node, nvidia/cuda, etc.



Dockerfile reference
Steps 2 & 3: Install software
Use RUN to specify the actual commands to be executed (as if you were to type them on the command line).
```dockerfile
FROM ubuntu:22.04
RUN apt-get install fortune cowsay lolcat
```
Save this file as Dockerfile and run docker build . Does it work?
We need to update our package list. Let's modify our Dockerfile and build again.
```dockerfile
FROM ubuntu:22.04
RUN apt-get update
RUN apt-get install fortune cowsay lolcat
```
This time it still failed due to the prompt for confirmation. To pass ""yes"" automatically, add -y.
```dockerfile
FROM ubuntu:22.04
RUN apt-get update
RUN apt-get install -y fortune cowsay lolcat
```
This finally works. It returns an image ID that we can call to run it:
bash
docker run --rm -it <img>
But it only returns a shell prompt where fortune, cowsay, lolcat don't seem to work. What's wrong?
Summary so far

Build:
Update package manager
Automatic yes to prompt


Run:
Use --rm to remove container after it exits
Use -it for interactive processes (e.g. shell)


Problems:
User needs to know path to executable
User just wants to run ""lolcow""



Use ENV to set environment variable
This is equivalent to export PATH=/usr/games:${PATH} but it is preserved at runtime. In doing so we can execute fortune, cowsay, and lolcat directly without specifying the full path.
```dockerfile
FROM ubuntu:22.04
RUN apt-get update
RUN apt-get install -y fortune cowsay lolcat
ENV PATH=/usr/games:${PATH}
```
Use ENTRYPOINT to set default command
```dockerfile
FROM ubuntu:22.04
RUN apt-get update
RUN apt-get install -y fortune cowsay lolcat
ENV PATH=/usr/games:${PATH}
ENTRYPOINT fortune | cowsay | lolcat
```
Finally, we can simply run docker run --rm -it <img> to get the desired behavior. You now know how to build a working Docker container.
4 Best Practices
While our container is functional, there is a lot of room for improvement. We shall look at some important best practices for writing Dockerfiles.
0. Package manager cache busting
The idea of ""cache busting"" is to force update whenever a change is made to install. This ensures we get the latest packages (especially critical security updates) should we make changes and rebuild the image in the future.
```dockerfile
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y \
        fortune cowsay lolcat
ENV PATH=/usr/games:${PATH}
ENTRYPOINT fortune | cowsay | lolcat
```

Save this as Dockerfile0, which will be the basis for comparison
For consistency we shall use the same tag as the number

docker build -t <user>/lolcow:0 -f Dockerfile0 .
1. Clean up
Almost all package managers leave behind some cache files after installation that can be safely removed. Depending on your application, they can easily accumulate up to several GBs. Let's see what happens if we try to clean up the cache in a separate RUN statement.
```dockerfile
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y \
        fortune cowsay lolcat
RUN rm -rf /var/lib/apt/lists/*  # clean up command for apt
ENV PATH=/usr/games:${PATH}
ENTRYPOINT fortune | cowsay | lolcat
```
bash
docker build -t <user>/lolcow:0.5 -f Dockerfile0.5 .
docker images | grep lolcow
You should see that there is no difference in the image size. Why?

Each statement creates an image layer.
If you try to remove a file from a previous layer, Docker will make a ""whiteout"" so that you can't see it, but the file is still there.
The file can be retrieved.
This is not just a size issue but also a security pitfall.

Very important! You must remove files in the same RUN statement as they are added.
```dockerfile
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y \
        fortune cowsay lolcat && \
    rm -rf /var/lib/apt/lists/*
ENV PATH=/usr/games:${PATH}
ENTRYPOINT fortune | cowsay | lolcat
```
bash
docker build -t <user>/lolcow:1 -f Dockerfile1 .
docker images | grep lolcow
Now you should see that the clean-up is effective.
2. Only install what's needed
The apt package manager often recommends related packages that are not really necessary. To disable recommendation, use --no-install-recommends.
```dockerfile
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y --no-install-recommends \
        fortune fortunes-min cowsay lolcat && \
    rm -rf /var/lib/apt/lists/*
ENV PATH=/usr/games:${PATH}
ENTRYPOINT fortune | cowsay | lolcat
```

You may need to specify extra packages
fortune itself provides the executable without the message database
fortunes-min contains the message database


See how Ubuntu reduced image size by 60%

3. Use a smaller base image
For installation of common packages, you may consider Alpine.

BusyBox + package manager + musl libc (beware of compatibility issues)
Presentation on Alpine Linux from DockerCon EU 17 

Look for slim variants (e.g. debian:buster-slim) of a base image, if any.
```dockerfile
FROM alpine:3.17
RUN echo ""@testing http://dl-cdn.alpinelinux.org/alpine/edge/testing"" >> /etc/apk/repositories && \
    apk add --no-cache fortune cowsay@testing lolcat@testing
ENTRYPOINT fortune | cowsay | lolcat
```
Note: An ENV statement is not needed here because the executables are installed under /usr/bin.
Image size comparison
bash
$ docker images | grep lolcow | sort -nk 2 | awk '{print $1, $2, $NF}'
<user>/lolcow    0      207MB
<user>/lolcow    0.5    207MB
<user>/lolcow    1      167MB
<user>/lolcow    2      154MB
<user>/lolcow    3      45.9MB

| Version | Description | Reduction (MB) | % |
|---|---|---:|---:|
|0  |(Basis of comparison) | - | - |
|0.5|Clean up in separate RUN  | 0 | 0 |
|1  |Clean up in same RUN      |40 | 19 |
|-  |Install only what's needed  |13 | 6 |
|2  |Combination of previous two |53 | 26 |
|3  |Alpine base image           |161| 78 |
Reference: Best practices for writing Dockerfiles
Summary

Choose a base image (FROM)
Install software dependencies (RUN)
Install software (RUN)
Clean up (in same RUN statement as installation)
Define environment variables (ENV)
Define default command (ENTRYPOINT)

Push to Docker Hub
You can push your image to Docker Hub easily. First, let's set our lolcow version 3 as the latest.
bash
docker tag <user>/lolcow:3 <user>/lolcow:latest
Then sign in to Docker Hub and push as follows:
bash
docker login
docker push <user>/lolcow:latest
Docker Hub interface
In your browser, go to https://hub.docker.com/r/<user>/lolcow.

Overview: 
Sync with GitHub to update README.md; or
Use docker-pushrm


Tags:
List all versions
View image history if Dockerfile not provided
Compressed size is much smaller than size on disk




Case Studies (hands-on)
By now, we know how to write a simple Dockerfile to install software using the distro's package manager. In practice, we may encounter software that does not exist in the package list. How do we deal with such cases?
Compiled language (C++)
https://github.com/lilab-bcb/cumulus_feature_barcoding
Hints:
- You do not have to start from a bare OS. Search for gcc on Docker Hub.
- Install build-essential if you are starting from a bare OS.
- Version pinning - to choose a specific version, download from https://github.com/lilab-bcb/cumulus_feature_barcoding/releases (you will need wget).
Interpreted language (Python)
https://docs.qiime2.org/2022.8/install/native/#install-qiime-2-within-a-conda-environment
Hints:
- Click on ""Linux"" to get the URL for the yaml file. Download the yaml file in the same directory as your Dockerfile.
- You do not have to start from a bare OS in your Dockerfile. Search for miniconda3 on Docker Hub.
- (Recommended) There is a much faster dependency solver than conda - micromamba. See here for instructions.
- Use the suggested COPY and ENTRYPOINT statements.
- After you're done, compare with the official Dockerfile and image size. What is the biggest reason for the difference?
General Remarks

Play with different base images and package managers.
If you encounter a Docker statement that you have not used before, first check the official documentation for best practices.
A comprehensive list of dependencies may be lacking. Some developers may not specify any at all. You will have to rely on a combination of experience, error message, and web search. (Most likely all of the above.)
Especially for Python packages, versions may be too permissive or too restrictive such that, in either case, future installation of the application will fail. (I have encountered both.) Tweak the versions until it works.
The next step is ""multi-stage build"" which is covered in the Minimal Containers workshop. There you will learn how to distinguish between buildtime versus runtime dependencies and separate them out.

Clean Up
If you build containers often, you can run out of disk space quickly. To clean up:

Run docker rmi <IMAGE_ID> to remove a specific image.

Run docker system prune to clean up cache. (This will not affect images that are tagged.)
```bash
$ docker system prune
WARNING! This will remove:
  - all stopped containers
  - all networks not used by at least one container
  - all dangling images
  - all dangling build cache
Are you sure you want to continue? [y/N] y
```



References

UVA Rivanna-Docker GitHub
Dockerfiles by UVA Research Computing
Tips


Best practices for writing Dockerfiles
Natanael Copa, Small, Simple, and Secure: Alpine Linux under the Microscope, DockerCon EU (2017)
"
rc-learning-fork/content/courses/containers-for-hpc/intro.md,"https://www.docker.com/resources/what-container
Why use software containers?

Simple Containers simplify software installation and management.
Portable You can build an image on one machine and run it on another.
Reproducible Versioning and freezing of containers enable data reproducibility.

Characteristics
Containers
- share the OS kernel of the host
- virtualize the OS instead of hardware
- have much less overhead and faster deployment than a VM
Apptainer/Singularity is designed for HPC

Does not require sudo privilege to run (unlike Docker)
Interoperates well with HPC resource managers in multi-node environments
Easily makes use of GPUs, high speed networks, parallel filesystems
Able to convert Docker images
"
rc-learning-fork/content/courses/containers-for-hpc/building-apptainer.md,"Introduction
Apptainer vs Singularity
Apptainer is a continuation of the Singularity project. Since our migration to Apptainer on Dec 18, 2023, users can now build containers natively on HPC.
Previous workflow:
- build Docker container on personal computer
- upload (push) to a registry
- download (pull) from registry
Motivation
Containerization provides an isolated environment, which can be useful in these cases:

application and/or its dependencies are incompatible with system/module libraries
preserve environment independently of the host OS and software stack
bypass software installation request ticket
customization for self/lab

Example: lolcow
```bash
$ apptainer run lolcow.sif

< Beware of low-flying butterflies. >

    \   ^__^
     \  (oo)\_______
        (__)\       )\/\
            ||----w |
            ||     ||

```
What command is actually being executed?
```bash
$ apptainer inspect --runscript lolcow.sif
!/bin/sh
fortune | cowsay | lolcat

```
{{< warning >}}
Inspect the runscript before running an image!
{{< /warning >}}
Setup


(Optional) Cache
The default cache directory is ~/.apptainer. If you are an active container user it can quickly fill up your home. You can change it to scratch:
{{< code-snippet >}}export APPTAINER_CACHEDIR=/scratch/$USER/.apptainer{{< /code-snippet >}}
Otherwise, remember to clean up periodically.
1. We have suppressed certain output from the apptainer command. To see the complete output, type \apptainer.
1. Load the Apptainer module: module load apptainer


Definition File
The definition file is a set of instructions that is used to build an Apptainer container:

base OS or base container
files to add from the host system
software to install
environment variables to set at runtime
container metadata

This is a skeleton:
```bash
Bootstrap: ...   # ""Header""
From: ...        #
%files           # ""Section""
    ...          #
%post
    ...
%environment
    ...
%runscript
    ...
%labels
    ...
%help
    ...
```
Header

At the top of the def file
Sets the base OS or base container

Bootstrap (mandatory)
This is the very first entry. It defines the bootstrap agent:

docker
library
shub
and many more

From (mandatory)
Define the base container.
From: [<collection>/]<container>[:<tag>]
Section
Each section starts with %. All sections are optional.
%files
Copy files into the container.
%files
    <source1> [<destination1>]
    <source2> [<destination2>]
    ...

Files are always copied before the %post section.

%post
Installation commands. Example:
%post
    apt-get update
    apt-get install -y lolcat
%environment
Define environment variables (set at runtime). Not available at build time. Example:
%environment
    export LC_ALL=C
%runscript
List of commands to be executed upon apptainer run.
%labels
Add metadata in the form of key-value pairs. Example:
%labels
    Author Ruoshi Sun
%help
Text to be displayed upon apptainer run-help.
Hands-on exercise: lolcow
fortune | cowsay | lolcat

fortune cookie
talking cow
rainbow color

Steps:
1. Choose a base image
2. Install software dependencies (if any)
3. Install software
Step 1: Choose a base image
Use Bootstrap and From to specify the base image. In this example, we'll use Ubuntu 22.04. You do not need to install this on your computer - Apptainer will pull from Docker Hub when you build it.
bash
Bootstrap: docker
From: ubuntu:22.04

OS: ubuntu, debian, centos, ...
Doesn't have to be a bare OS
python, continuumio/miniconda3, node, nvidia/cuda, etc.



Steps 2 & 3: Install software
{{< info >}}
For this application the package manager will take care of all the dependencies.
{{< /info >}}
In %post specify the actual commands to be executed (as if you were to type them on the command line).
{{< code-snippet >}}Bootstrap: docker
From: ubuntu:22.04
%post
    apt-get install fortune cowsay lolcat
{{< /code-snippet >}}
Save this file as lolcow.def and run apptainer build lolcow.sif lolcow.def. Does it work?
We need to update our package list. Let's modify our definition file and build again.
{{< code-snippet >}}Bootstrap: docker
From: ubuntu:22.04
%post
    apt-get update
    apt-get install fortune cowsay lolcat
{{< /code-snippet >}}
This time it still failed due to the prompt for confirmation. To pass ""yes"" automatically, add -y.
{{< code-snippet >}}Bootstrap: docker
From: ubuntu:22.04
%post
    apt-get update
    apt-get install -y fortune cowsay lolcat
{{< /code-snippet >}}
This finally works.
bash
$ apptainer build lolcow.sif lolcow.def
$ apptainer run lolcow.sif
But it only returns a shell prompt where fortune, cowsay, lolcat don't seem to work. What's wrong?
Summary so far

Build:
Update package manager
Automatic yes to prompt


Problems:
User needs to know path to executable
User just wants to run ""lolcow""



Use %environment to set environment variable
This is equivalent to export PATH=/usr/games:${PATH} but it is preserved at runtime. In doing so we can execute fortune, cowsay, and lolcat directly without specifying the full path.
{{< code-snippet >}}Bootstrap: docker
From: ubuntu:22.04
%post
    apt-get update
    apt-get install -y fortune cowsay lolcat
%environment
    export PATH=/usr/games:${PATH}
    export LC_ALL=C
{{< /code-snippet >}}
Use %runscript to set default command
{{< code-snippet >}}Bootstrap: docker
From: ubuntu:22.04
%post
    apt-get update
    apt-get install -y fortune cowsay lolcat
%environment
    export PATH=/usr/games:${PATH}
    export LC_ALL=C
%runscript
    fortune | cowsay | lolcat
{{< /code-snippet >}}
Save this as lolcow_0.def, which will be the basis for comparison.
Two Best Practices
While our container is functional, there is room for improvement. We shall look at some important best practices.
1. Clean up
Package managers usually leave behind some cache files after installation that can be safely removed. Depending on your application, they can easily accumulate up to several GBs.
{{< code-snippet >}}Bootstrap: docker
From: ubuntu:22.04
%post
    apt-get update
    apt-get install -y fortune cowsay lolcat
    rm -rf /var/lib/apt/lists/*  # clean up command for apt
%environment
    export PATH=/usr/games:${PATH}
    export LC_ALL=C
%runscript
    fortune | cowsay | lolcat
{{< /code-snippet >}}
Save this as lolcow_1.def.
2. Only install what's needed
The apt package manager often recommends related packages that are not really necessary. To disable recommendation, use --no-install-recommends.
{{< code-snippet >}}Bootstrap: docker
From: ubuntu:22.04
%post
    apt-get update
    apt-get install -y --no-install-recommends fortune fortunes-min cowsay lolcat
    rm -rf /var/lib/apt/lists/*  # clean up command for apt
%environment
    export PATH=/usr/games:${PATH}
    export LC_ALL=C
%runscript
    fortune | cowsay | lolcat
{{< /code-snippet >}}
Save this as lolcow_2.def.

You may need to specify extra packages
fortune itself provides the executable without the message database
fortunes-min contains the message database


See how Ubuntu reduced image size by 60%

Image size comparison
bash
$ ll -h lolcow*.sif
... 86M ... lolcow_0.sif
... 54M ... lolcow_1.sif
... 48M ... lolcow_2.sif

| Version | Description | Reduction (MB) | % |
|---|---|---:|---:|
|0  |(Basis of comparison) | - | - |
|1  |Clean up              |32 | 37 |
|-  |Install only what's needed  |6 | 7 |
|2  |Combination of previous two |38 | 44 |
Sandbox
As we have experienced from the previous section, we may need to iteratively troubleshoot the container build process and, unlike Docker which caches the image layers that finished successfully, Apptainer goes through the whole process every time. So during the exploratory phase, we adopt a more ""interactive"" approach via sandbox.

Create and use a writable directory
Useful for debugging container build process

bash
$ apptainer build --sandbox <directory> <URI/IMG>
$ apptainer shell -w --fakeroot <directory>
Apptainer> ... (installation commands) ...
{{< warning >}}
Technically you can build the sandbox into a container, but this is not recommended. Write down all the commands in the right order into a definition file for reproducibility.
{{< /warning >}}
Exception: Alpine
Alpine is a Linux distribution with a very slim base image. However, due to a known bug you cannot build it through a definition file in Apptainer.
bash
/.singularity.d/libs/fakeroot: eval: line 140: /.singularity.d/libs/faked: not found
fakeroot: error while starting the `faked' daemon.
sh: you need to specify whom to kill
FATAL:   While performing build: while running engine: exit status 1
Let's try to build a lolcow container in Alpine via a sandbox.
```bash
$ apptainer build --sandbox alpine docker://alpine:3.17
$ apptainer shell -w alpine
Apptainer> echo ""@testing http://dl-cdn.alpinelinux.org/alpine/edge/testing"" >> /etc/apk/repositories
Apptainer> apk add fortune cowsay@testing lolcat@testing
Apptainer> rm /var/cache/apk/*
Apptainer> exit
$ apptainer build lolcow_3.sif alpine
$ apptainer exec lolcow_3.sif sh -c ""fortune|cowsay|lolcat""

/ ""All my life I wanted to be someone; I \
| guess I should have been more          |
| specific.""                             |
|                                        |
\ -- Jane Wagner                         /

    \   ^__^
     \  (oo)\_______
        (__)\       )\/\
            ||----w |
            ||     ||

```
Note the container size - only 14MB! This is 84% smaller than what we had before.
{{< info >}}
Why can't you run ""apptainer exec lolcow_3.sif fortune|cowsay|lolcat""?
{{< /info >}}
Registry
A container registry is a repository for container images. Here we examine two popular choices.
Docker Hub
The Apptainer/Singularity SIF is supported by Docker Hub. Register for a free account.
{{< info >}}Replace ""myname"" with your actual username in the following commands.{{< /info >}}
Login:
```bash
$ apptainer remote login --username myname docker://docker.io
$ apptainer remote list
...
Authenticated Logins
=================================
URI                 INSECURE
docker://docker.io  NO
```
Push:
bash
$ apptainer push lolcow_0.sif oras://docker.io/myname/lolcow:0
Check:
https://hub.docker.com/r/myname/lolcow/tags
{{< info >}}Best practice: Sign your containers; see here.{{< /info >}}
{{< warning >}}While Apptainer can convert a Docker image into SIF, you cannot run SIF with Docker. You are simply using Docker Hub to host your SIF - it is not converted into Docker.{{< /warning >}}
UVA Research Computing container resources
Docker Hub account
Repository of Dockerfiles and definition files
GitHub Packages

Create a personal access token
Login
    bash
    apptainer remote login --username myname docker://ghcr.io
    [paste your token]
Push
    bash
    apptainer push lolcow_0.sif oras://ghcr.io/myname/lolcow
Check https://github.com/users/myname/packages/container/package/lolcow

Case Studies
Python
While you can create a conda environment locally, you cannot directly migrate it onto another machine. A container ensures an identical environment with only a one-time installation.
{{< info >}}Unless you need the entire Anaconda distribution you should opt for a slimmer base image (e.g. miniconda, micromamba) to create your own environment.{{< /info >}}
{{< info >}}Base images already exist for popular deep learning frameworks such as PyTorch and Tensorflow. There is no need to install them by yourself.{{< /info >}}
Exercise
Your project requires PyTorch 2.1.2, Numpy, Seaborn, and Pandas. Write the corresponding Apptainer definition file.
Hints:
- Find the appropriate base image from here.
- Pull the base image and examine it first. Does it already provide some packages?
{{< info >}}While PyTorch runs on a GPU, you do not need to build the container on a GPU.{{< /info >}}
{{< info >}}You will likely run out of memory when building large containers (over a few GBs). Request an interactive job to build on a compute node in the largemem partition.{{< /info >}}
R
Rocker provides many base images for all R versions (see here):
- rocker/r-ver: basic R installation
- rocker/rstudio: with RStudio Server
- rocker/tidyverse: plus tidyverse and dependencies
- rocker/shiny: shiny server
{{< info >}}If you want to build a custom R container start with one of the Rocker images. Building R, RStudio Server, etc. from source can be very tedious!{{< /info >}}
Exercise
Your project requires R 4.3.2, dplyr, ggplot2, and RcppGSL. Write the corresponding Apptainer definition file.
Hints:
- Pick an appropriate base image - there are two viable choices here.
- Pull the base image and examine it first. Does it already provide some packages?
- In the definition file, install CRAN packages via R -e ""install.packages('...')"".
- Load the three packages. Do they all succeed? If not, how can you fix it?
Multistage Build
By distinguishing between buildtime-dependencies vs runtime-dependencies, it is possible to reduce the image size drastically via a multistage build. (My experience with some extreme cases is that only 1% is needed at runtime.)
This is beyond the scope of the workshop, but you are welcome to browse the Apptainer documentation and Appendix 2 on Minimal Containers.

References

Apptainer User Guide
Definition File
Definition File vs Dockerfile
GitHub Packages
"
rc-learning-fork/content/courses/containers-for-hpc/using.md,"Log on to our HPC cluster

SSH client or FastX Web
Run hdquota
Make sure you have a few GBs of free space


Run allocations
Check if you have hpc_training




Basic Apptainer commands
Pull
To download a container hosted on a registry, use the pull command. Docker images are automatically converted into Apptainer format.
apptainer pull [<SIF>] <URI>

<URI> (Unified resource identifiers)
[library|docker|shub]://[<user>/]<repo>[:<tag>]
Default prefix: library (Singularity Library)
user: optional; may be empty (e.g. apptainer pull ubuntu)
tag: optional; default: latest


<SIF> (Singularity image format)
Optional
Rename image; default: <repo>_<tag>.sif



Pull lolcow from Docker Hub
bash
apptainer pull docker://rsdmse/lolcow
Inspect
Inspect an image before running it via inspect.
apptainer inspect <SIF>
bash
$ apptainer inspect lolcow_latest.sif 
org.label-schema.build-arch: amd64
org.label-schema.build-date: Monday_8_January_2024_10:21:0_EST
org.label-schema.schema-version: 1.0
org.label-schema.usage.apptainer.version: 1.2.2
org.label-schema.usage.singularity.deffile.bootstrap: docker
org.label-schema.usage.singularity.deffile.from: rsdmse/lolcow
Inspect runscript
This is the default command of the container. (Docker ENTRYPOINT is preserved.)
apptainer inspect --runscript <SIF>
```bash
$ apptainer inspect --runscript lolcow_latest.sif 
!/bin/sh
OCI_ENTRYPOINT='""/bin/sh"" ""-c"" ""fortune | cowsay | lolcat""'
...
```
Run
There are three ways to run a container: run, shell, exec.
run
Execute the default command in inspect --runscript.
CPU: apptainer run <SIF> = ./<SIF>
GPU: apptainer run --nv <SIF> (later)
bash
./lolcow_latest.sif
shell
Start an Apptainer container interactively in its shell.
apptainer shell <SIF>
bash
$ apptainer shell lolcow_latest.sif
Apptainer>
The change in prompt indicates you are now inside the container.
To exit the container shell, type exit.
exec
Execute custom commands without shelling into the container.
apptainer exec <SIF> <command>
bash
$ apptainer exec lolcow_latest.sif which fortune
/usr/bin/fortune
Bind mount

Apptainer bind mounts these host directories at runtime:
Personal directories: /home, /scratch
Leased storage shared by your research group: /project, /standard
Your current working directory


To bind mount additional host directories/files, use --bind/-B:

bash
apptainer run|shell|exec -B <host_path>[:<container_path>] <SIF>

Exercises

For each of the three executables fortune, cowsay, lolcat, run which both inside and outside the lolcow container. Which one exists on both the host and the container?
a) Run ls -l for your home directory both inside and outside the container. Verify that you get the same result. b) To disable all bind mounting, use run|shell|exec -c. Verify that $HOME is now empty.
View the content of /etc/os-release both inside and outside the container. Are they the same or different? Why?
(Advanced) Let's see if we can run the host gcc inside the lolcow container. First load the module: module load gcc
Verify that the path to gcc (hint: which) is equal to $EBROOTGCC/bin.
Verify that $EBROOTGCC/bin is in your PATH.
Now shell into the container (hint: -B /apps) and examine the environment variables $EBROOTGCC and $PATH. Are they the same as those on the host? Why (not)?
In the container, add $EBROOTGCC/bin to PATH (hint: export). Is it detectable by which? Can you launch gcc? Why (not)?




Container Modules
Apptainer module
The apptainer module serves as a ""toolchain"" that will activate container modules. You must load apptainer before loading container modules.
See what modules are available by default:
bash
module purge
module avail
Check the module version of Apptainer:
bash
module spider apptainer
Load the Apptainer module and check what modules are available:
bash
module load apptainer
module avail
You can now load container modules.
Container modules under apptainer toolchain
The corresponding run command is displayed upon loading a module.
```bash
$ module load tensorflow
To execute the default application inside the container, run:
apptainer run --nv $CONTAINERDIR/tensorflow-2.13.0.sif
$ module list
Currently Loaded Modules:
  1) apptainer/1.2.2   2) tensorflow/2.13.0
```

$CONTAINERDIR is an environment variable. It is the directory where containers are stored.
After old container module versions are deprecated, the corresponding containers are placed in $CONTAINERDIR/archive. These are inaccessible through the module system, but you are welcome to use them if necessary.


Exercise

What happens if you load a container module without loading Apptainer first?
    bash
    module purge
    module list
    module load tensorflow
Check the versions of tensorflow via module spider tensorflow. How would you load a non-default version?
What is the default command of the tensorflow container? Where was it pulled from?


Container Slurm job (TensorFlow on GPU)

Computationally intensive tasks must be performed on compute nodes.
Slurm is a resource manager.
Prepare a Slurm script to submit a job.

Copy these files:
bash
cp /share/resources/tutorials/apptainer_ws/tensorflow-2.13.0.slurm .
cp /share/resources/tutorials/apptainer_ws/mnist_example.{ipynb,py} .
Examine Slurm script:
```bash
!/bin/bash
SBATCH -A hpc_training      # account name
SBATCH -p gpu               # partition/queue
SBATCH --gres=gpu:1         # request 1 gpu
SBATCH -c 1                 # request 1 cpu core
SBATCH -t 00:05:00          # time limit: 5 min
SBATCH -J tftest            # job name
SBATCH -o tftest-%A.out     # output file
SBATCH -e tftest-%A.err     # error file
VERSION=2.13.0
start with clean environment
module purge
module load apptainer tensorflow/$VERSION
apptainer run --nv $CONTAINERDIR/tensorflow-$VERSION.sif mnist_example.py
```
Submit job:
bash
sbatch tensorflow-2.13.0.slurm
What does --nv do?
See Apptainer GPU user guide
```bash
$ apptainer shell $CONTAINERDIR/tensorflow-2.13.0.sif
Apptainer> ls /.singularity.d/libs
$ apptainer shell --nv $CONTAINERDIR/tensorflow-2.13.0.sif
Apptainer> ls /.singularity.d/libs
libEGL.so         libGLX.so.0              libnvidia-cfg.so           libnvidia-ifr.so
libEGL.so.1       libGLX_nvidia.so.0           libnvidia-cfg.so.1         libnvidia-ifr.so.1
...
```

Custom Jupyter Kernel
""Can I use my own container on JupyterLab?""
Suppose you need to use TensorFlow 2.17.0 on JupyterLab. First, note we do not have tensorflow/2.17.0 as a module:
bash
module spider tensorflow
Go to TensorFlow's Docker Hub page and search for the tag (i.e. version). You'll want to use one that has the -gpu-jupyter suffix. Pull the container in your account.
Installation
Manual

Create kernel directory

bash
DIR=~/.local/share/jupyter/kernels/tensorflow-2.17.0
mkdir -p $DIR
cd $DIR

Write kernel.json

{
 ""argv"": [
  ""/home/<user>/.local/share/jupyter/kernels/tensorflow-2.17.0/init.sh"",
  ""-f"",
  ""{connection_file}""
 ],
 ""display_name"": ""Tensorflow 2.17"",
 ""language"": ""python""
}

Write init.sh

```bash
!/bin/bash
module load apptainer
apptainer exec --nv /path/to/sif python -m ipykernel $@
```

Change init.sh into an executable
bash
chmod +x init.sh

Easy to automate!
JKRollout
This tool is currently limited to Python. The container must have the ipykernel Python package.
text
Usage: jkrollout sif display_name [gpu]
    sif          = file name of *.sif
    display_name = name of Jupyter kernel
    gpu          = enable gpu (default: false)
bash
jkrollout /path/to/sif ""Tensorflow 2.17"" gpu
Test your new kernel

Go to https://ood.hpc.virginia.edu
Select JupyterLab
Partition: GPU
Work Directory: (location of your mnist_example.ipynb)
Allocation: hpc_training


Select the new ""TensorFlow 2.17"" kernel
Run mnist_example.ipynb

Remove a custom kernel
bash
rm -rf ~/.local/share/jupyter/kernels/tensorflow-2.17.0

References

Apptainer User Guide
Overview
Bind Path and Mounts


"
rc-learning-fork/content/courses/containers-for-hpc/_index.md,"Overview
This short course is an introduction to using and building software containers, structured as follows:

Introduction to Software Containers
Using Containers on HPC [Apptainer]
pull and convert Docker containers
inspect a container
run containers interactively and non-interactively
navigate container modules
submit container jobs via Slurm
create custom Jupyter kernels


Building Containers on HPC [Apptainer]
definition file
best practices
registry
case studies


Appendix 1: Building Containers [Docker]
Dockerfile
best practices
DockerHub
case studies


Appendix 2: Minimal Containers [Docker]
multistage build
OS-less base images
dynamic vs static linking



The chapters are mutually independent, except for ""Minimal Containers"" that requires ""Building Containers [Docker]"" or equivalent knowledge.
You are strongly encouraged to repeat the examples and do the exercises.
Prerequisites

Linux command line.
Rivanna account for the main chapters.
Docker installation and DockerHub account for the appendices.
"
rc-learning-fork/content/courses/introduction_to_mpi/part_1.md,"Part 1 covers general concepts and collective communications.
Please download the slides.
Download the appropriate MPI Guide

C/C++

Fortran

Python
Also download the labs and sample code for your language (C++ use C):

MPI Lab1
mpi1.c

mpi1.f90

mpi1.py
Fortran users may download my random.f90 module

random.f90
Some sample solutions

Monte Carlo Pi in Python

Find maximum with gather"
rc-learning-fork/content/courses/introduction_to_mpi/part_2.md,"Part 2 is an introduction to intermediate programming with MPI. Point-to-point communications are covered.
Please download the slides
If you have not already downloaded the appropriate MPI Guide:

C/C++

Fortran

Python
Also download the labs and sample code for your language (C++ use C):

MPI Lab2

heatedplate.c

heatedplate.cxx

heatedplate.f90

heatedplate.py
Some solutions

Token ring

Heated plate in C++

Heated plate in Python"
rc-learning-fork/content/courses/introduction_to_mpi/_index.md,"This course is an introduction to programming with MPI. Attendees should be proficient at programming in Fortran, C/C++, or Python."
rc-learning-fork/content/courses/r-shiny/customization/method4.md,"How do I customize my app so the font and colors match my university's?
{{< figure src=""/courses/r-shiny/customization/img/uva-website.png"" >}}"
rc-learning-fork/content/courses/r-shiny/customization/method2-shinythemes.md,"

install.packages(""shinythemes"")


Add library(shinythemes) to app.R or ui.R


Add the theme argument to your fluidPage(), navbarPage(), etc. Set theme with shinytheme(""<theme>"")


{{< figure src=""/courses/r-shiny/customization/img/shinytheme-code.png"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method3-skin.md,"Change color of dashboard
You can change the color of your dashboard with the skin argument (similar to theme with shinythemes)
{{< figure src=""/courses/r-shiny/customization/img/dashboard-skin.png"" >}}"
rc-learning-fork/content/courses/r-shiny/customization/method4-sass.md,"Modifying CSS


Requires you to know at least some CSS


Each color value appears in multiple places



Syntactically Awesome Style Sheets
CSS with Superpowers
{{< figure src=""/courses/r-shiny/customization/img/sass.png"" width=""30%"">}}

Sass allows you to change colors and fonts using variables
{{< figure src=""/courses/r-shiny/customization/img/sass-compiler.png"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method1.md,"{{< figure src=""/courses/r-shiny/customization/img/defaults.png"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method4-theming.md,"The thematic package enables automatic theming of plots
```
library(thematic)
thematic_on()
onStop(thematic_off)
```
{{< figure src=""/courses/r-shiny/customization/img/plot.png"" caption=""Great for dark mode!"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method4-branding.md,"{{< figure src=""/courses/r-shiny/customization/img/brand.png"" caption=""https://brand.virginia.edu/"">}}"
rc-learning-fork/content/courses/r-shiny/customization/intro.md,"

Accepting the Shiny Defaults


Shiny Themes


Shiny Dashboards


Bootstraplib

"
rc-learning-fork/content/courses/r-shiny/customization/method4-preview.md,"
Add bs_themer() to your server function

```
library(shiny)
library(bslib) # step 1
...
server <- function(input, output){
   bs_themer()
##cool reactive logic##
}
```
{{< figure src=""/courses/r-shiny/customization/img/bslib.gif"" caption=""Adjust colors and fonts"">}}

When you make changes in the Preview tool, code for updating your theme is printed to the console. Copy the updates to your Shiny app.
```
my_theme <- bs_theme()
This line is copied from the console (may need to update theme name)
my_theme <- bs_theme_update(my_theme, bg = ""#4B3E23"", fg = ""#000000"")
```

How do I find out Bootstrap variable names?


List of Bootstrap sass variables:
https://rstudio.github.io/bslib/articles/bs4-variables.html


bslib commands:
    https://rstudio.github.io/bslib/reference/index.html

"
rc-learning-fork/content/courses/r-shiny/customization/method3-components.md,"

Header


Sidebar


Body


{{< figure src=""/courses/r-shiny/customization/img/blank-dashboard.png"" >}}"
rc-learning-fork/content/courses/r-shiny/customization/method2-nonshinythemes.md,"

Download the CSS files


Add .css file to www folder in app directory


Set theme = ""myfile.css""


Try this with the ""Quartz"" theme!"
rc-learning-fork/content/courses/r-shiny/customization/method1-bootstrap.md,"

Shiny's default UI uses Bootstrap


Bootstrap is a CSS framework developed by Twitter


Easy to customize if you're comfortable with HTML and CSS


Millions of websites use Bootstrap
{{< figure src=""/courses/r-shiny/customization/img/bootstrap.png"">}}
{{< figure src=""/courses/r-shiny/customization/img/rc-website.png"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method3-sidebar-menu.md,"Add menu items to the sidebar


We'll add menuItems that behave like tabs, similar to tabPanel


menuItems in the sidebar correspond to tabItems in the body


{{< figure src=""/courses/r-shiny/customization/img/menu-items-code.png"" >}}
{{< figure src=""/courses/r-shiny/customization/img/menu-items.png"" >}}"
rc-learning-fork/content/courses/r-shiny/customization/method4-bslib.md,"Need to install 2 packages (requires shiny>=1.6)


sass: compile sass to css in R


bslib: customize Bootstrap in R


https://rstudio.github.io/bslib/

Unfortunately doesn't work with Shiny Dashboards -- use fresh instead!
https://dreamrs.github.io/fresh/articles/vars-shinydashboard.html"
rc-learning-fork/content/courses/r-shiny/customization/method4-start.md,"
Load bslib
Instantiate a bs_theme object
Set theme to your bs_theme object in the UI

```
library(shiny)
library(bslib) # step 1
my_theme <- bs_theme() # step 2
ui <- fluidPage(
  theme = my_theme, # step 3
  ...
)"
rc-learning-fork/content/courses/r-shiny/customization/method2-bootswatch.md,"All Shiny Themes come from bootswatch.com
{{< figure src=""/courses/r-shiny/customization/img/bootswatch.png"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method2.md,"The shinythemes package helps you spice up your app
{{< figure src=""/courses/r-shiny/customization/img/shinythemes.png"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method3-boxes.md,"Add app components to the body with boxes
Boxes need to be put in a fluidRow or column
{{< figure src=""/courses/r-shiny/customization/img/boxes-code.png"" >}}
{{< figure src=""/courses/r-shiny/customization/img/boxes.png"" >}}"
rc-learning-fork/content/courses/r-shiny/customization/_index.md,"Learn how to ""prettify"" our apps and prepare them for publication!
{{< figure src=""/courses/r-shiny/customization/img/shiny-logo.png"" caption=""Shiny R package logo"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method3.md,"Packages like flexdashboard and shinydashboard let you create dashboards for your users
{{< figure src=""/courses/r-shiny/customization/img/dashboard.png"" caption=""https://gallery.shinyapps.io/087-crandash/"">}}"
rc-learning-fork/content/courses/r-shiny/customization/method3-flex-vs-shiny.md,"{{< table >}}
| flexdashboard    | shinydashboard      |
|      ---           |       ---             |
| R Markdown         | Shiny UI code         |
| Super easy         | Not quite as easy     |
| Static or dynamic  | Dynamic               |
| CSS flexbox layout | Bootstrap grid layout |
{{< /table >}}
We'll just touch on shinydashboard here
http://rstudio.github.io/shinydashboard/get_started.html"
rc-learning-fork/content/courses/r-shiny/introduction/intro-computer-setup.md,"What You'll Need


Development server (just your computer!)


Deployment server (shinyapps.io, UVA Microservices)


Development Server
This is where you will write and test the code for your Shiny app. You can interact with your app, but no one else can.
{{< figure src=""/courses/r-shiny/introduction/img/development-server.jpg"" >}}
You will spend most of your time on your development server.
Set Up Your Development Server
Option 1: Clone or Download the GitHub Repository

Run git clone https://github.com/uvarc/shiny-workshop in the terminal
-OR- Go to https://github.com/uvarc/shiny-workshop and click green ""Code"" button, then click ""Download ZIP""


Option 2: Create a New R Project (need Git installed for this option)

File > New Project > Version Control > Git
Repository URL: https://github.com/uvarc/shiny-workshop
Create Project

After downloading the code, run packages-for-workshop.R

Deployment Server
Once you're ready to share your app, it's time to move it to a deployment server (i.e. deploy your app).
{{< figure src=""/courses/r-shiny/introduction/img/deployment-server.jpg"" >}}
Once deployed on the deployment server, your development server is no longer serving your app. This means any changes you make locally will need to be pushed to the deployment server before they're visible to the world.
Set Up Your Deployment Server
Create a free account on shinyapps.io.
Can host 5 apps on shinyapps.io for free.
You can also host your apps on UVA servers: 
https://www.rc.virginia.edu/userinfo/microservices/"
rc-learning-fork/content/courses/r-shiny/introduction/ui-frameworks.md,"

Replace UI_starting.R with UI_fluid_page.R in the app.R.


Run the app


Pretty ugly, right? Now try replacing tagList with fluidPage



fluidPage()
A fluid page layout consists of rows which in turn contain columns


Rows ensure that items appear on the same line as long as the browser is wide enough (hence the fluid)


Columns define how much horizontal space elements should occupy. Horizontal space is defined by a 12-unit wide grid


Adds some Bootstrap styling (framework for designing websites--developed by Twitter)



fluidRow() and column()
Changing tagList to fluidPage didn't do much
Let's add some fluidRows() and column() functions to create this:
{{< figure src=""/courses/r-shiny/introduction/img/fluid-wide.png"" >}}
The solution is in UI_language_soln.R
fluidPage() is responsive


Try adjusting the size of your browser window.


The text adjusts so that it all fits within a single window--no need to scroll left and right!


Other Layouts
Check out the Shiny cheatsheet to see other types of layouts
https://shiny.rstudio.com/images/shiny-cheatsheet.pdf


tabsetPanel() + tabPanel()


sidebarLayout() + sidebarPanel()/mainPanel()


splitLayout()


wellPanel()


navbarPage()


navlistPanel()

"
rc-learning-fork/content/courses/r-shiny/introduction/project3.md,"It's time to try coding your own Shiny app. With this app, a user will be able to select a data frame from a list of choices, and then select a response and explanatory variable. The app will have 3 types of functionality: 


Plot a scatterplot of the data frame (response vs explanatory)


Show the head of the data frame


Print out a regression report

"
rc-learning-fork/content/courses/r-shiny/introduction/overview.md,"Shiny is an R package for developing interactive web applications, or apps. Shiny apps are just webpages!
Webpages are made up of two main components.


HTML/CSS: what your app looks like (the form)


JavaScript: what your app does (the function)


The difference between Shiny apps and regular webpages: Shiny apps are powered by an R session.

Creating a Shiny app
To create a Shiny app, we need:

Regular R stuff: writing code to manipulate and analyze data, visualize with graphs, etc...




To create a user interface


To connect 1 to 2 with reactive logic

"
rc-learning-fork/content/courses/r-shiny/introduction/reactives.md,"We've already seen some examples with our render functions, but what exactly is reactivity?
More Harry Potter Comparisons
Remember, the Knight Bus is our connection between regular R code (Muggle World) and the world of reactives (Wizarding World)
knight_bus <- function(input, output, session) {
    reactive code here!
}"
rc-learning-fork/content/courses/r-shiny/introduction/overview-metaphor.md,"Shiny Bridges Two Worlds of R Programming
In Harry Potter, the Knight Bus connected the non-magical, or ""Muggle"", world to the magical Wizarding World. Similarly, Shiny connects regular R code to the magical world of ""reactives"" (we will talk more about these later).
| 1. Muggle World        | 2. Wizarding World    |
|         ---                |           ---             |
| - Regular R code           | - Reactives               |
| - Functions, packages, etc | - Functions w/o arguments |
|                            | - Values that can't be changed with <- |
|                            | - Packaged in a server function |
The Wizarding world can reach out to the Muggle world, but not the other way around."
rc-learning-fork/content/courses/r-shiny/introduction/reactives-kb.md,"{{< table >}}
| inputs | conductors | observers |
|  ---   |    ---     |    ---    |
| {{< figure src=""/courses/r-shiny/introduction/img/input.png"" >}} | {{< figure src=""/courses/r-shiny/introduction/img/conductor.png"" >}} | {{< figure src=""/courses/r-shiny/introduction/img/observer.png"" >}} |
| actionButton() | reactive() | observe() |
| selectInput()  | eventReactive() | observeEvent() |
| reactiveValues() | renderText() | textOutput() |
{{< /table >}}


Inputs are reactive sources. Reactive endpoints are called observers because they observe changes to inputs.


Conductors act like both sources and endpoints. They depend on inputs, but they also provide inputs to other reactives.

"
rc-learning-fork/content/courses/r-shiny/introduction/project3-step2.md,"Having all the outputs on one page can be messy.


Put each output widget on its own tab with an informative label


Put the input widgets in the same tab as the table display (head of data frame)


Make sure the input widget/table display tab is displayed when you start the app

"
rc-learning-fork/content/courses/r-shiny/introduction/reactives-example.md,"server <- function(input, output) {
    output$distPlot <- renderPlot({
        hist(rnorm(input$obs))
    })
}
In this example, input$obs is a reactive source, and output$distPlot is a reactive endpoint.


Clicking or typing into input widgets will set some sort of value, or reactive source.


A reactive endpoint is an object that appears in the app, like a plot, text, or values. Reactive endpoints re-execute when their reactive inputs change.

"
rc-learning-fork/content/courses/r-shiny/introduction/project3-step3.md,"Write some reactive logic so that the appropriate choices appear for the response and explanatory input widgets
Hint: use updateSelectInput(). The variable names will be returned by names(Raw_data()). Should you use a eventReactive() or observeEvent() to accomplish this?"
rc-learning-fork/content/courses/r-shiny/introduction/reactives-connections-states.md,"Reactives have connections

Inputs have outward connections
Observers have inward connections
Conductors have both inward and outward connections

Reactive States
Reactives have 2 kinds of states

Their value, e.g. numerical/text value, plot, image
Their validity--validated, invalidated



Reactives are invalidated when their inputs change value. They become valid again when they recalculate or re-execute based on their input's new value.
"
rc-learning-fork/content/courses/r-shiny/introduction/intro-workshop-files.md,"File Organization
Project Directory


slides/: contains all the RMarkdown files/slides for this workshop

Knit MAIN.Rmd to see the main slideshow
Each section is a separate RMarkdown file (feel free to take notes in these!)



projects/: contains all the code we'll be working with today

Each app will live in a separate directory



sandbox/: place to put code snippets or quick demos

"
rc-learning-fork/content/courses/r-shiny/introduction/reactives-vs-observers.md,"Reactives: calculate or cache a value, meant to be used as a variable, has a result
Observers: for side effects--runs some code when a dependency changes, doesn't yield a result, isn't used as input for other expressions, not assigned to a variable (e.g., updating a menu based on a selection)

{{< figure src=""/courses/r-shiny/introduction/img/dog.jpeg"" >}}

Imagine a delivery man rings your doorbell to deliver a package. You head to the front door to receive it. At the same time, your pet dog starts wildly barking at the door. You open the door and take the package from the delivery man.

In this example:


The delivery man ringing the doorbell is a reactive input.


You going to the front door is a conductor.


Your dog barking is an observer. The dog is reacting to the doorbell ringing, but the barking has nothing to do with package delivery.


You taking the package is a reactive endpoint (not a side effect).

"
rc-learning-fork/content/courses/r-shiny/introduction/project3-step4.md,"Write reactive logic to:


Display the head of the selected data frame in a table (only the response and explanatory variables)


Plot response versus explanatory in a scatterplot


You'll be using the render functions to create the displays. Is there a way you can create a common object for that can be used by both of the render functions? Hint: use reactive()"
rc-learning-fork/content/courses/r-shiny/introduction/reactives-modularizing.md,"Adding an expression
This script still calculates the hypotenuse but splits the calculation into two parts


C2 <- reactive(input$A^2 + input$B^2, label = ""C2"")


output$C <- renderText(sqrt(C2()))


Run pythagorean-2.R and change the input values.


Stop the app


Run reactlogShow()


Reactives are lazy and only update when they need to

"
rc-learning-fork/content/courses/r-shiny/introduction/project3-step5.md,"Display a summary of the linear model response ~ explanatory.
Hints:
- You can use paste() to construct the formula

Try renderPrint
"
rc-learning-fork/content/courses/r-shiny/introduction/project3-step1.md,"Start with app-0.R. It's a working app-see for yourself!
Even though the app has very little functionality, the script is already crowded.


Pull out the UI and put it into a file ui.R. Do the same for server.


Source these into app-0.R.


Make sure the app still works!

"
rc-learning-fork/content/courses/r-shiny/introduction/overview-project0.md,"The Main File, app.R
Go to projects -> project0-first-app and open app.R.


This file contains all the components of the app: the user interface and reactive logic.


The UI and reactive logic can be written in separate .R files that are sourced in app.R or all put in the same file. 


In this case the UI is in user_interface.R and the reactive logic is in reactive_logic.R.



shinyApp()
app.R is just a regular R script

Adding the call to shinyApp() at the end of the script changes the ""Run"" button to ""Run App""

Starting in the Muggle World
Let's start with muggle.R (our run-of-the-mill R code)


We load the libraries we need


We create a function muggle_plot that takes variable names from the diamonds dataset as inputs and generates a scatterplot


Test it out if you want by uncommenting and running the last line of the script


User Interface
Open user_interface.R


The function tagList takes HTML functions as inputs and creates a list of HTML components


Try running the first line, h2(""A very basic Shiny app"") in the console (make sure you have shiny loaded)


Functions like h2, p, and actionButton are wrappers for HTML code (essentially strings)


The first argument in actionButton, plotOutput, and textOutput are IDs. We will use these IDs in the next part


Reactive Logic
Open reactive_logic.R


We will connect the UI to the Muggle code with reactive logic


We are assigning a function to reactive_logic with three arguments: input, output, and session. The arguments are always the same, but reactive_logic is usually called server.


Functions renderPlot and renderText are assigned to output$IDname. They correspond to the UI functions plotOutput and textOutput


Invoking the App
shinyApp(user_interface, reactive_logic)
To run our app, we use the command shinyApp. The first argument is the UI, and the second argument is our reactive logic.
Deploying the App


Put all the files your app needs in a single directory (the name of the directory will be the name of the app)


Make sure there is a file called app.R with the call to shinyApp()


Run rsconnect::deployApp(""/path/to/app/directory""). You may need to connect your shinyapps.io account to deploy an app for the first time.


Your Turn: Modify the App
Try the following on your own!


Hide the button message until the button has been clicked 3 times.


Add selectInput dropdown menus for the X and Y variables.


(Superstar) Connect the selectInput menus to the plot (we haven't covered this yet)

"
rc-learning-fork/content/courses/r-shiny/introduction/reactives-reactlogshow.md,"Project 2: Pythagorean Theorem
This script calculates the hypotenuse of a right triangle using 
output$C <- renderText({
    sqrt(input$A^2 + input$B^2)
  })


Run pythagorean-1.R and change the input values a few times.


Stop the app


Run reactlogShow() in the console (may need to install first)


A log will open in your browser
reactlogShow() chart


Click the forward arrow in the log window


When input$A or input$B's value changed, it became invalidated.


This then invalidates output$C


output$C becomes validated once it recalculates

"
rc-learning-fork/content/courses/r-shiny/introduction/reactives-diagram.md,"{{< figure src=""/courses/r-shiny/introduction/img/reactivity.png"" >}}"
rc-learning-fork/content/courses/r-shiny/introduction/ui-widgets.md,"

Go to projects/project1-ui and take a look at UI_starting.R and Knight_bus.R.


Run app.R.


Widgets are the different buttons and fields we see on a webpage. 


We only see the inputs from UI_starting.R because nothing is connected to the outputs.



Input Widgets
The Shiny Widget Gallery


You can try out and play with Shiny input widgets on RStudio's website.


The website shows you how the values change when you modify the input widgets. The site will also show you the code you need to include the widget in your own app.


https://shiny.rstudio.com/gallery/widget-gallery.html

Output Widgets


Output widgets are similar to input widgets. However, output widgets require a render function to be visible in the app.


Each output widget has its own corresponding render function.


{{< figure src=""/courses/r-shiny/introduction/img/output-widgets.png"" >}}

Playing with Widgets
Add some input widgets to UI_starting.R


actionLink


checkboxInput


radioButtons


textInput


Make sure the app still works with your changes!

Connecting Inputs to Outputs
Add some reactive logic to Knight_bus.R so that the text output felix (output$felix) displays the selected choice from annie (input$annie)"
rc-learning-fork/content/courses/r-shiny/introduction/ui.md,"There are 3 steps to building a UI:


Instantiating input and output widgets


Frameworks and layouts


Organizing widgets within the framework

"
rc-learning-fork/content/courses/r-shiny/introduction/_index.md,"Learn how to create your own web apps in R using Shiny! In this workshop, we will go over Shiny basics and will code an app together from scratch.
{{< figure src=""/courses/r-shiny/introduction/img/shiny-logo.png"" caption=""Shiny R package logo"">}}"
rc-learning-fork/content/courses/r-shiny/introduction/intro-creating-new-app.md,"How do you create a new blank Shiny app?
Old Faithful
Using New File -> Shiny Web App... creates a living, breathing Shiny App
{{< figure src=""/courses/r-shiny/introduction/img/old-faithful.png"" >}}
A Better Way to Do It


Open a new R script file.


Start typing shinyapp and press Tab to autocomplete. This will expand into a ""snippet"" of code--the skeleton of a Shiny App.


Save the file in the sandbox folder and run the app.


It's still a working Shiny app--it just doesn't do anything. Starting from the snippet is less error-prone than creating a new project and deleting the guts.
Stop an app by clicking the STOP button in the console."
rc-learning-fork/content/courses/fiji-image-processing/scripting/index.md,"{{< figure library=""true"" src=""fiji.png"" >}}
{{% link-button href=""/scripts/fiji/fiji-example-scripts.zip"" icon=""fa fa-download"" %}}Scripts{{% /link-button %}}
This chapter is an introduction to the scripting interface of the Fiji application, an open-source and enhanced version of the popular ImageJ program used for scientific image processing.  Here you will learn how to write scripts for automated execution of image processing pipelines and batch processing of multiple image files in Fiji.
Step-by-step instructions are provided for developing scripts in Jython, an implementation of the popular Python language for Java platforms. Example scripts are provided for the Jython and BeanShell languages.
Introduction & Fiji Programming Tools {#intro-id}
Installation {#installation-id}
Fiji is a stand-alone application that can be downloaded from the Fiji website. It is available for Mac OSX, Windows, and Linux platforms.  

macOS The Fiji application should be installed in the user's home directory rather than the default Applications folder.
Windows 7 & 10: The Fiji application should be installed in the user's home directory rather than the default C:\Program Files directory.
Linux: The Fiji application should be installed in a directory where the user has read, execution, and write permissions, e.g. the user's home directory.

Download the Example Scripts
Go to this tutorials landing page and click on the Code button. 
The Application Programming Interface (API) {#api-id}
Fiji and ImageJ are written in Java. The application programming interface (API) defines the various Java packages, classes, methods, and constants that a programmer can use for development of custom Fiji & ImageJ scripts. The API is well documented on these public sites:

ImageJ (https://imagej.nih.gov/ij/developer/api/)
Fiji (https://javadoc.scijava.org/Fiji/)

Fiji provides convenient programming wrappers for these Java classes so you can write your scripts in:

ImageJ macro language: simple, slow, not very versatile
Jython:  Python syntax with a few limitations, easy to learn, very versatile
BeanShell:  Syntax similar to Java, versatile
Several others…

Fiji provides a richer programming environment than ImageJ and it is recommended to use Fiji instead of ImageJ for any script development.
Step-by-step instructions in this tutorial are provided for developing scripts in Jython. The download section contains scripts written in the Jython and BeanShell languages.

The Script Editor {#script-editor-id}
To start the script editor in Fiji go to menu File > New > Script….
{{< figure src=""fiji-script-editor.png"" >}}

The top pane provides the editor. Multiple scripts can be open at the same time and will show up as separate tabs.
The bottom pane shows output (e.g. produced by print statements) and any errors encountered during script execution.

Script editor menus:

File:         Load, save scripts
Edit:     Similar to word processor functionality (Copy, Paste, etc.)
Language: Choose language your script is written in with syntax highlighting
Templates:    Example scripts
Tools:        Access to source code of a class in the ImageJ/Fiji package
Tabs:     Navigation aid in script


The Macro Recorder {#macro-recorder-id}
In the Fiji menu, go to Plugins > Macros… > Record.
{{< figure src=""fiji-macro-recorder.png"" >}}

The Macro Recorder logs all commands entered through the Fiji graphical user interface (GUI). It is useful to convert these GUI actions into script commands.
In the Record drop-down menu, select BeanShell.
Clicking the Create button copies the code to a new script in the Script Editor.
Keep the Recorder window open throughout this tutorial.

The Console Window {#console-id}
In the Fiji menu, go to Window > Console.

The Console window shows the output and logging information produced by running plugins and scripts.


Jython {#jython-id}
Jython is an implementation of the Python 2.7 programming language designed to run on the Java platform.

Jython/Python are interpreter languages. The code is evaluated line-by-line when the script is run (executed). This is different from compiled languages.
Some Python packages (like numpy) are not available in Jython.  
Jython scripts can use ImageJ/Fiji packages and classes via specific import statements.
In Python many problems can be solved with less code than in other languages.  Still the code is easy to read. 

Variables {#variables-id}
A variable is used to store values or references to other objects/functions in the computer's memory.

Values are assigned to variables in statements with an =.
The identifier to the left of = defines the variable name, the expression to the right of the =defines the value to be assigned to the variable.
Variable names have to start with a letter and cannot contain any special characters, e.g. ?, ,, etc.. Variable names are case-sensitive, so my_name is not the same as My_name or my_Name.

Let's create five variables, x, y, my_name, is_it_Monday, and my_list.  
x = 10
y = 2.3
my_name = ""Santa Claus""
is_it_Monday = False
my_list = [2,3,5,7,11]

x is assigned an integer value of 10.
y is a float variable, i.e. a number with decimals.
my_name is assigned a sequence of characters, called a string. Strings are defined by enclosing single ' or double "" quotes.
is_it_Monday is assigned a boolean value. A boolean can be either True or False.
my_list contains a sequence of data elements, in this case integers. In Python/Jython, you can create lists containing elements with mixed types, even nested lists, e.g. [1.0, 'abc', 1.2, ""hello""].


Conditionals: if Statements {#conditionals-id}
Often different blocks of code need to be executed depending on a specific condition.  This is achieved by using if, elif, else code blocks.
if <Boolean expression1>:
   statement(s)
elif <Boolean expression2>:
   statement(s)
else:
   statement(s)
Example:
x = 100
y = 200
if x < y:
    print ""x is smaller than y.""
elif x == y:
    print ""x equals y""
else:
    print ""x is larger than y""
Note:

The if, elif, and else keywords need to start at the same indentation level. Each of these lines end with a :, signifying the beginning of a new code block.
The statement(s) to be executed need to be indented by at least a single space relative to the if, elif, and else keywords.
The elif and else statements are optional. Multiple elif blocks can be placed between the if and else blocks.
Any expression that evaluates to a boolean (True or False) can be used as conditional for the if and elif statements.
An elif block is only executed if all preceding if and elif conditions evaluated to False, and the conditional expression in this elif statement evaluates to True.
The else block is only executed if all preceding if and elif conditions evaluated to False.  
The == is the operator to test for equality. Do not confuse it with =, which is used to assign values to variables.    


Code Loops {#loops-id}
Like most languages, Python and Jython offer loop constructs to repeatedly execute the same code block.
For loops:
for loops are executed a fixed number of iterations. It is possible to exit early but this must be added to the code.
for <item> in <iterator>:
    statement(s)
else:  # optional
    statement(s)
Example:
for i in range(20):
   print i

The else clause is optional and only executed when the loop completes the iterator.
The for and else keywords need to start at the same indentation levels. Each of these lines end with a :.
The statement(s) to be executed need to be indented by at least a single space relative to the for and else keywords.


While loops:
while loops do not have a predetermined number of iterations. They terminate when some condition is evaluated as False.
while <Boolean expression>:
    statement(s)
else: # optional
    statement(s)
Example:
x = -20
y = -5
while (x<0 and y<0):
    print ""x="", x, ""y="", y
    x = x - y
    y = y + 1
else:
    print ""Exited loop with: x:"", x, "", y:"", y

The else clause is optional and executed when the loop completes the iterator.
break and continue statements can be added inside the code loop to exit the loop early, or to skip to the next iteration, respectively.

Functions {#functions-id}
A function is a code block that groups related statements that perform a specific task or job within a larger program.

With functions, we can break down a program into a series of well-defined tasks which are easier to write and debug.
Functions allow us to reuse code.
Functions reduce ""cut-and-paste"" errors.  
If your code performs a task more than once, write a function.

In Jython and Python this could look like this:
def add_values(value1, value2):
    result = value1 + value2
    return result

The declaration of a function starts with the keyword def followed by the function's name (i.e. add_values) and a list of parameters (i.e. (value1,value2)).
The content of the function is defined by the indented code block following the def function declaration.  Consistent indentation (with spaces) is required in order for the Interpreter to figure out what lines are part of this function.
The return statement defines values that are passed to the code block that invoked execution of the function, i.e. the caller. If no return statement is defined, the function returns None.

Example for invocation of the function add_values:
sum = add_values(2.3, 4.1)
print sum

In this example we are calling the function add_values, passing the numbers 2.3 and 4.0 as arguments.
The function performs the operation on the passed arguments and the result is stored in our custom variable sum.

Importing packages {#import-id}
Just like Python, Jython provides an easy way to get access to classes and functions that are defined in external packages, outside your script.

The import statement specifies the names of external packages, classes and functions your script wants to use (the namespace).
The ImageJ API documentation provides the Java package names and classes.

Here's an example. Let's say we want to use the ImageProcessor class in our Jython script.  The ImageProcessor class is defined in the ij.process Java package so the equivalent Jython import statement would look like this:
from ij.process import ImageProcessor

Script Example {#jython-scripts-id}
Jython scripts are simple text files that use specific syntax and structure. Jython scripts (or scripts in any other programing language) should not be edited in word processing programs like Microsoft Word which add invisible formatting characters that mess with programming language syntax.
```
from ij import IJ
def print_info(imp):
    """"""This function prints basic image info.""""""
    print ""\nTitle:"", imp.getTitle() # print title
# output image dimensions
print ""Width x Height:"", imp.getWidth(), ""x"", imp.getHeight()

Main code block
imp = IJ.getImage()
print_info(imp)  
```

import statements define code that your script needs to work but that is organized in packages/modules outside your current script. An overview of the standard Fiji/ImageJ packages can be found here.
Indentation levels define code blocks, e.g. conditionals, code loops, functions.
The def keyword defines the beginning of a function, followed by the function's name and argument list. The content of the function is defined by the indented code block following the def function declaration.
Empty lines are ignored by the interpreter but can improve readability of the code.
The # defines a comment. All characters following a # on that line are ignored by the code interpreter.  
Text between a pair of triple quotes """""" defines a special comment type, called doc string, that is used to annotate an entire program or function.


Fiji Scripts in Jython {#fiji-id}
Images in Fiji: ImagePlus, ImageStack, ImageProcessor {#image-handling-id}

An ImagePlus object contains an ImageProcessor (2D image) or an ImageStack (3D, 4D or 5D image). It is used to show the image in a window and also includes metadata (e.g. spatial calibration, the directory/file where it was read from, acquisition info, etc.).
The ImageProcessor contains the pixel data (8-bit, 16-bit, 32-bit float, or RGB) of the 2D image and some basic methods to manipulate it.
An ImageStack is a group of ImageProcessors of the same type and size. ImageStack allows the implementation of multi-dimensional images, e.g. with x, y, z, time, and channel axes.


Get Current Image {#current-image-id}
In Fiji, go to File > Open Samples > Blobs.
```
from ij import IJ                       # find the IJ class
imp = IJ.getImage()                     # get active Image
print ""\nTitle:"", imp.getTitle()        # output image title
output image dimensions
print ""Width x Height:"", imp.getWidth(), ""x"", imp.getHeight()
```

The IJ.getImage() command retrieves the image from the currently active image window in Fiji.  We assign the returned ImagePlus object to the variable imp.
The ImagePlus class provides numerous methods that can be accessed via the imp object instance, e.g. imp.getTitle(), imp.getWidth(), imp.getHeight(), etc.
The print statement outputs Strings and values in the Script Editors console pane.

Create the script:
In the Fiji Script Editor, go through these steps:

Create a new file (File > New).
Go to Language and select Python.
Copy the script above into the script editor.
Go to File > Save as... and save the file as Get_Image.py.
Click the Run button.

Output:
```
Started Get_Image.py at Tue Jun 04 00:50:07 EDT 2019
Title: blobs.gif
Width x Height: 256 x 254
```

Image Dimensions {#image-dimensions-id}
Fiji and ImageJ can handle multidimensional images, e.g. volumetric images or time-lapse series, up to five dimensions. Each image (ImagePlus object) has at least two dimensions, i.e. width (x-axis) and height (y-axis). In addition, ImagePlus object can have multiple color channels, z-focal planes (called slices), and/or timepoints (called frames). Lastly, images can have different pixel encodings, e.g. 8-bit, 16-bit, 32-bit, RGB that define an image's dynamic range (e.g. number of distinct intensity values per pixel) and color representation.
The ImagePlus class provides convenient methods to get this information.
```
from ij import IJ
imp = IJ.getImage()
width = imp.getWidth()            # get number of pixels along x-axis
height = imp.getHeight()          # get number of pixels along y-axis
channel_no = imp.getNChannels()   # get number of channels
slice_no = imp.getNSlices()       # get number of slices
frame_no = imp.getNFrames()       # get number of frames
bitdepth = imp.getBitDepth()      # bits per pixel
bpp = imp.getBytesPerPixel()      # bytes per pixel (there are 8 bits in a byte)  
```

Image Calibration {#image-calibration-id}
Many image formats allow inclusion of image annotations (metadata) in addition to the pixel data. An important piece of information is the spatial pixel calibration, e.g. the definition of pixel size in real-world physical units. For example, a pixel may correspond to 200 nanometer in x-y dimension.
Let's assume that we have a variable imp as reference to an ImagePlus object. We can get and set an image's calibration with the following ImagePlus class methods:
Get copy of image calibration
calibration = imp.getCalibration().copy()
Set image calibration
new_imp.setCalibration(calibration)

Creating a New Image {#new-image-id}
New images can be created by initializing an ImagePlus object with an ImageProcessor object instance.  The following ImageProcessor subclasses can be used, depending on bit-depth and color mode of the desired image.
| ImageProcessor subclass | Mode | bits/pixel |
| --- | --- | --- |
| ByteProcessor | grayscale | 8-bit int |
| ShortProcessor | grayscale | 16-bit int |
| FloatProcessor | grayscale |32-bit float |
| ColorProcessor | color | 8-bit int per channel |

Example
```
from ij.process import ByteProcessor, ShortProcessor,  \
                                     FloatProcessor, ColorProcessor
from ij import ImagePlus
width = 200                                  # in pixel
height = 100                                 # in pixel
bp = ByteProcessor(width, height)            # create ImageProcessor
imp_bp = ImagePlus(""New 8-bit image"", bp)    # create ImagePlus with specific title and ImageProcessor object
imp_bp.show()                                # show image window
sp = ShortProcessor(width, height)
imp_sp = ImagePlus(""New 16-bit image"", sp)
imp_sp.show()
fp = FloatProcessor(width, height)
imp_fp = ImagePlus(""New 32-bit image"", fp)
imp_fp.show()
cp = ColorProcessor(width, height)
imp_cp = ImagePlus(""New RGB image"", cp)
imp_cp.show()
```
This script creates four new images, each with 200 x 100 pixels but with different pixel encodings. Take a look at each image window's header and note the differences.
{{< figure src=""fiji-newimages.png"" >}}

Pixel encoding (8-bit, 16-bit, 32-bit, RGB).
Memory footprint in kilobytes (20K, 39K, 78K, 78K).


Duplicating an Image {#image-duplication-id}
The Duplicator class provides a convenient way to create an exact copy of an existing ImagePlus object.
```
from ij import IJ
from ij.plugin import Duplicator
imp = IJ.getImage()
copied_imp = Duplicator().run(imp)
copied_imp.show()
```
Note that the duplicated image will be named DUP_<original_title>. You can change the title with the setTitle(string) method.
copied_imp.setTitle(""Perfect copy"")

Opening and Saving Image Files {#image-io-id}
Open Images
Fiji can read many different image file formats. This versatility is now provided via the integrated Bio-Formats plugin.
| Class/Method | Behavior |
| --- | --- |
| IJ.open() |   interactive dialog |
| IJ.openImage() | non-interactive, using default image display |
| loci.formats.ImageReader | non-interactive, configurable image display |

Save Images
Fiji can also save image files in various common formats, including TIF, OME-TIF, BMP, PNG, JPEG, AVI, etc.).
| Class/Method | Behavior |
| --- | --- |
| ij.io.FileSaver | interactive dialog |
| IJ.saveAs () | non-interactive, simple image writer (TIF, JPEG, GIF, PNG, etc.) |
| loci.formats.ImageWriter | non-interactive, advanced with many formats |

Interactive Image Opening and Saving (with Dialog)
from ij import IJ
from ij.io import FileSaver
Open file (interactive dialog)
imp = IJ.open()
imp = IJ.getImage()
imp.setTitle(""copy.tif"")
Save file in original format (interactive dialog)
fs = FileSaver(imp)
fs.save()        # could also use fs.saveAsTiff(), fs.saveAsPng(), etc.

Non-interactive Image Opening and Saving (without Dialog)**
```
from ij import IJ
import os
from os import path
Open file from URL or storage device (non-interactive)
filesource = ""http://imagej.nih.gov/ij/images/blobs.gif""
imp = IJ.openImage(filesource)
imp.show()
Create new output directories and filename
homedir = path.expanduser(""~"")              # home directory
outputdir = path.join(homedir, ""workshop"", ""images"")    # full dir path
print outputdir
if not path.isdir(outputdir):         
    os.makedirs(outputdir)
Save file as .tif (non-interactive)
outputfile = os.path.join(outputdir, ""blobs-copy.tif"")
IJ.saveAs(imp, ""tiff"", outputfile)
print ""Saved file:"", outputfile
```
Check the output in the console window. The image should have been saved to the workshop/images subdirectory in your home directory.
If you cannot find your saved file, try this:

Click on the blobs-copy.tif image window,
Go to Image > Show Info….
View path entry in Info for blobs-copy.tif window.

Note that file paths use \ on Windows but / on Linux and OSX platforms. Avoid hardcoding the \ or / and use os.path functions to create platform appropriate path names.

IJ.run {#ij-run-id}
The IJ.run command can be used to execute many of the commands available through the Fiji graphical user interface.
Basic syntax: IJ.run(image, command, option)

image : reference to ImagePlus object
command   : String
option : String, in some cases just """" (empty String)

The Macro Recorder is an excellent tool to convert a function accessible via the Fiji menu into an IJ.run statement. Let's try it.
Record

Ensure that the Macro Recorder window is open.  If not, go to Plugins > Macro > Recorder.
In the Recorder window’s Record drop-down menu, select BeanShell.
In the Recorder window select and remove any code.
In Fiji, go to File > Open Samples > Boats
Go to Process > Filters > Gaussian Blur, a dialog will pop up:
Enter a Sigma value of 5.
Click Ok.


Go to File > Save As… > Tiff.
Browse to your workshop/images directory (if it does not exist yet, create it), use blurry_boats.tif as file name  and click Save.
In the Recorder window, click Create. This should bring up the Script Editor window with a new script file containing the code copied from the Recorder. If you see more than three lines of code, remove everything that starts before imp = IJ.openImage(…..

Convert to Jython script

In the Script Editor , go to Language, and select Python.
Insert an from ij import IJ statement at the top of the script.
Remove semicolons (;) at the end of the last three lines so your script looks like this (ignore the file path; yours will be different):
```
from ij import IJ

imp = IJ.openImage(""http://imagej.nih.gov/ij/images/boats.gif"")
IJ.run(imp, ""Gaussian Blur..."", ""sigma=5"")
IJ.saveAs(imp, ""Tiff"", ""/Users/mst3k/blurry_boats.tif"")
``
4. Save your script asIJ_Run.py`. Close all image windows.
5. Run the script.
Check your workshop/images directory for the blurry_boats.tif file.

Working with ImageStack {#imagestack-id}
In Fiji, go to File > Open Samples > Mitosis. This image has the following dimensions:

2 Channels
5 Focal Planes (Slices, Z)
51 Timepoints (Frames)

The multidimensional image (represented by an ImagePlus object imp) contains an ImageStack object to manage the different image planes. An ImageStack contains a group of ImageProcessors of the same type and size.
The ImageStack object of an ImagePlus object (imp) can be retrieved like so:
stack = imp.getStack()
The following code provides a reference to the ImageProcessor for a particular channel, slice, and frame in the imp ImagePlus object:
index = imp.getStackIndex(channel_no, slice_no, frame_no)
stack = imp.getStack()
ip = stack.getProcessor(index)
Conversely, we can replace an ImageProcessor in an ImageStack (referenced by the stack variable) like so:
stack.setProcessor(new_ip, index)     # provide new ImageProcessor object, and stack position to place it
Important: The new ImageProcessor object has to be of the same type, width, and height as all the other ImageProcessors in the stack.

Regions-of-Interest (ROIs) {#rois-id}
ROIs define groups of pixels in an image. An ROI can be used to:

Measure pixel values
Change pixel values
Mark image regions in non-destructive overlays (without changing pixel values)

ROI Types (classes):

Roi           
Line          
OvalRoi       
PolygonRoi
ShapeRoi
TextRoi
ImageRoi

ROIs can be managed directly or through the RoiManager class.
ROI: Cropping an Image {#roi-crop-id}
In Fiji, go to File > Open > Samples > Clown
```
from ij.gui import Roi
from ij import IJ
from ij.plugin import Duplicator
get current image
imp = IJ.getImage()
create rectangular ROI centered in image
rel_size = 0.5                  # 0.0 < rel_size < 1.0
width = int(rel_size * imp.getWidth())
height = int(rel_size * imp.getHeight())
top_left_x = int(0.5 * (imp.getWidth() - width))
top_left_y = int(0.5 * (imp.getHeight()- height))
roi = Roi(top_left_x, top_left_y, width, height)
copy image, set ROI on copied image, crop, set title and show image
cropped_imp = Duplicator().run(imp)
cropped_imp.setRoi(roi)
IJ.run(cropped_imp, ""Crop"", """")     # modifies passed ImagePlus object!
cropped_imp.setTitle('Cropped')
cropped_imp.show()
```
Run the script in the Script Editor.
Fill ROI with a Given Value {#roi-fill-id}
```
from ij import IJ, ImagePlus
from ij.process import FloatProcessor
from array import zeros
from random import random
from ij.gui import Roi, OvalRoi  
Create a new ImagePlus filled with noise
width = 1024
height = 1024
pixels = zeros('f', width * height)
for i in xrange(len(pixels)):
    pixels[i] = random()
fp = FloatProcessor(width, height, pixels)
imp = ImagePlus(""Random"", fp)  
Fill a rectangular region of interest with a value of 2.0
roi = Roi(40, 100, 400, 300)
fp.setRoi(roi)
fp.setValue(2.0)
fp.fill()  
Fill a oval region of interest with a value of -2.0
oroi = OvalRoi(500, 300, 150, 550)
fp.setRoi(oroi)
fp.setValue(-2.0)
fp.fill(oroi.getMask())  # Attention! Required for non-rectangular ROIs  
imp.show()
```
Changing ROI Properties {#roi-properties-id}
The appearance of an ROI object (roi) can be changed.
Border Color
from java.awt import Color
roi.setStrokeColor(Color.RED)
Border Width
roi.setStrokeWidth(2.0)
Fill Color
from java.awt import Color
roi.setFillColor(Color.YELLOW)
Set ROI Position {#roi-set-id}
The positioning of an ROI object (roi) can be set for a specific channel, slice, frame in a given image (imp) with multi-dimensional ImageStack.
index = imp.getStackIndex(channelNo, sliceNo, frameNo)
roi.setPosition(index)
Note: If index is 0 (default), the Roi applies to all image planes in the stack.

Particle Analysis {#particle-analysis-id}

In the Macro Recorder window’s Record drop-down menu, select BeanShell. Remove any code.
In Fiji, go to File > Open Samples > Blobs
Go to Process > Filters > Median…
Set Radius : 2
Click Ok.


Go to Process > Binary > Options…
Check Black background box.
Click OK.


Go to Image > Adjust Threshold…
Select Default and Red in drop-down menu
Uncheck Dark background box.  
Click Apply.


Go to Process > Binary > Watershed
Go to Analyze > Set Measurements
Select options as shown.
Click Ok.


Go to Analyze > Analyze Particles…
Select options as shown.
Click Ok.



{{< figure src=""fiji-particle-analysis.png"" >}}
Convert the Macro recordings into a Python script, remember to do the following in the Script Editor:

In the Script Editor, go to Language, and select Python.
Insert an import statement at the top the script
Remove semicolons (;) at the end of the lines.

The script should look like this:
```
from ij import IJ
imp = IJ.openImage(""http://imagej.nih.gov/ij/images/blobs.gif"")
IJ.run(imp, ""Median..."", ""radius=2"")
IJ.run(imp, ""Options..."", ""iterations=1 count=1 black"")
IJ.setAutoThreshold(imp, ""Default"")
IJ.run(imp, ""Convert to Mask"", """")
IJ.run(imp, ""Watershed"", """")
IJ.run(imp, ""Set Measurements..."", ""area mean min centroid integrated display redirect=None decimal=3"")
IJ.run(imp, ""Analyze Particles..."", ""size=0-Infinity display exclude clear summarize add"")
imp.show()
```
Save the script as Blob_Analysis.py, and run the script.
You should see the following results tables.
{{< figure src=""fiji-particle-analysis-results.png"" >}}

RoiManager {#roi-manager-id}
The RoiManager class implements the Analyze > Tools > ROI Manager function. The Particle Analyzer can also use the Roi Manager to store ROIs identified during the analysis, see Particle Analysis.
Using the RoiManager class in scripts:
Get reference to system default instance
from ij.plugin.frame import RoiManager
rm = RoiManager.getInstance2()
Get number of ROIs managed by RoiManager
count = rm.getCount()
Get all ROIs as a list
rois = rm.getAllRoisAsArray()
Get specific ROI at specific index position
roi = rm.getRoi(index)
Remove all ROIs
rm.reset()
Add ROI
roi = OvalRoi(100, 150, 50, 50)
rm.addRoi(roi)
Run operation on ROI: combine, select, save, open, etc.
rm.runCommand(command_string)

ImageStatistics {#imagestats-id}
The ImageStatistics class provides access to pixel-based statistics, including the histogram, of an entire image or ROI selection in an image.
Common statements:
Define measurement options: bitwise OR combination of constants
from ij.process import ImageStatistics as IS
options = IS.MEAN | IS.AREA | IS.STD_DEV  # many others

Measure entire image (imp)
imp.setRoi(null)   # remove any ROI from image
stats = imp.getStatistics()

Measure particular ROI (roi) in image (imp)
imp.setRoi(roi)
stats = imp.getStatistics()

Get histogram for image (imp) as a list of intensity counts:
stats = imp.getStatistics()
histo_values = stats.getHistogram()

ResultsTable {#resultstable-id}
The ResultsTable class is used for storing measurement results and strings as columns of values.
Common statements:
Get default table used by Analyze > Measure
from ij.measure import ResultsTable
rt = ResultsTable.getResultsTable()
Create new empty table
rt = ResultsTable()
Show table with title. The title Results is reserved for default table for Analyze > Measure.
rt.show(""My Results"")   # passed argument becomes table title
Get number of table rows
rt.getCount()
Get list of float values in column (by column header)
col_index = rt.getColumnIndex(""Area"")
rt.getColumn(col_index)
Add value to specific column in last row
rt.addValue(""Area"", 23.0)
Delete column
rt.deleteColumn(""Area"")
Delete row (by row index)
rt.deleteRow(row_index)

Custom Dialog Windows: Collecting User Input {#dialogs-id}
Custom language agnostic dialogs can be created using the SciJava@Parameter annotation.

Script parameters are a feature of ImageJ2; they will work in Fiji but will not work in plain ImageJ1.
Basic Syntax:
Parameter declarations begin with #@.  Each such line contains a single parameter declaration or script directive and nothing else.
#@ Type variableName will declare an input of the indicated type, assigned to the specified name. The use of a space between #@ and Type is encouraged, but not required.
#@output Type outputName will declare the variable of the specified name as an output parameter with the given type. The Type parameter is optional, as the output will be treated as Object be default. (For the output directive and other script directives, no space is allowed between #@ and the directive.)



| Data type | Widget type   | Available styles |
| --- | --- | --- |
| boolean / Boolean | checkbox | |
| byte / short / int / long | numeric field | slider / spinner / scroll bar |
| Byte / Short / Integer / Long | numeric field | slider / spinner / scroll bar |
| Float | numeric field | slider / spinner / scroll bar |
| BigInteger / BigDecimal   | numeric field | slider / spinner / scroll bar |
| char / Character / String |text field | text field / text area / password |
| Dataset | ImagePlus | (>=2 images) triggers a dropdown list |
| ColorRGB | color chooser | |
| Date | date chooser | |
| File | file chooser   | open / save / file / directory / extensions |
Source: https://imagej.net/Script_Parameters
Example:
```
@String(label=""Please enter your name"",description=""Name field"") name
@OUTPUT String greeting
greeting = ""Hello, "" + name + ""!""
```
When you run the script, a dialog will pop up asking for your input. Type in a name (any character sequence) and click OK. The output message will be shown in a separate window.
{{< figure src=""fiji-greeting-script.png"" >}}

Batch Processing of Image Files {#batch-processing-id}
Often we want to apply the same image processing method to a set of images.

It is convenient to organize such image files into a single directory or group of directories.
We already created a series of images and saved them in the workshop/images directory.  Let’s produce a simple batch processing script that applies a Gaussian filter to all images in that directory.

Simple Template:
```
@ File (label=""Input directory"", style=""directory"") inputdir
@ File (label=""Output directory"", style=""directory"") outputdir
from ij import IJ
import os
from os import path
def process_file(f):
    print ""Processing"", f
    imp = IJ.openImage(f)
    IJ.run(imp, ""Gaussian Blur..."", ""sigma=2"");
    return imp
def save_as_tif(directory, image):
    title = image.getTitle().split(""."")[0] + ""-blurred.tif""
    outputfile = path.join(outputdir, title)
    IJ.saveAs(image, ""TIFF"", outputfile)
    print ""Saved"", outputfile
Main code
inputdir = str(inputdir)
outputdir = str(outputdir)
if not path.isdir(inputdir):
    print inputdir, "" is does not exist or is not a directory.""
else:
    if not path.isdir(outputdir):
        os.makedirs(outputdir)
    filenames = os.listdir(inputdir)
    tif_files = [f for f in filenames if f.split(""."")[-1] == ""tif""]
    for tif_file in tif_files:
        fullpath = path.join(inputdir, tif_file)
        imp = process_file(fullpath)
        save_as_tif(outputdir, imp)
    print ""Done.\n""
```

Installing Scripts as Plugins {#install-plugins-id}
Once you have completed script development, you can install the script in Fiji.  It will show up as a selectable entry in the Plugins menu.

Note: Your script name needs to include at least one underscore (_). Example: Basic_Crop.py

There are two installation options:

Option A) Manual process:
Copy the script to the plugins directory in your Fiji install tree. You can create a subdirectory in plugins (e.g. plugins/workshop) and place your script there if you like to group related scripts.


Option B) Fiji installer:
In Fiji, go to Plugins > Install….
In the first pop-up dialog browse to and select the script. Click Open.  
In the second dialog window,  select the ""plugins"" directory (or a subdirectory within plugins) to where the script should be installed. Click Save.



After installing the script as plugin, restart Fiji.
Go to File > Plugins and verify that the script is available.  The _ and file extension are stripped from the plugin name. Example: Basic_Crop.py is installed as Basic Crop

Exercises {#exercises-id}
Beginner {#beginner-ex-id}
Project 1: Modify the Get Current Image example script to print the number of channels, number of focal planes (z), and number of timepoints. Open images of your choice from Fiji > Open Samples and compare the output when running the script.
Project 2: Open the Macro Recorder, delete any code in the recorder pane. Record the following steps and convert the macro recording into Jython script. Hint: IJ.run

Go to File > Open Samples > Fluorescent Cells.
Split the channels (Image > Color > Split Channels).
Merge the channels again as Composite image, but swap the red and green channel.

Project 3: Implement the crop ROI example script with functions.
Project 4: Modify the processing function in Batch Processing example script to apply a different image filter. Hint: IJ.run

Intermediate {#intermediate-ex-id}
Project 5: Modify the crop ROI example script to have it create multiple cropped images of various sizes in a code loop. Hint: For loops
Project 6: Modify the processing function in Batch Processing example script to prompt user for sigma value to be used by the Gaussian filter. Hint: Custom Dialog Windows. The custom sigma value should be passed from the main code block as an argument to the process function. Hint: Functions
Project 7: Let's try to improve the particle analysis by implementing the following features:

Duplicate the original image: Duplicator class
Access to ROIs identified by Particle Analyzer: RoiManager class
Measuring area, pixel intensities, etc. for each ROI: ImageStatistics class
Creating a customized results table and save it: ResultsTable class
Custom Dialog for User Input to specify particle size: Custom User Dialogs


```
@ Float (label=""Min particle size"", value=50) min_size
@ Float (label=""Max particle size"", value=200) max_size
@ Boolean (label=""Save ROIs"", value=false) save_rois
@ File (label=""Output directory"", style=""directory"") outputdir
from ij import IJ
from ij.plugin import Duplicator
from ij.plugin.frame import RoiManager
from ij.measure import ResultsTable
from ij.process import ImageStatistics as IS
from os import path
open image and create copy
original = IJ.openImage(""http://imagej.nih.gov/ij/images/blobs.gif"")
original.show()
imp = Duplicator().run(original)
imp.setTitle(""Mask"")                           # rename the copy            
IJ.run(imp, ""Median..."", ""radius=2"");
IJ.run(imp, ""Options..."", ""iterations=1 count=1 black"")
IJ.setAutoThreshold(imp, ""Default"")
IJ.run(imp, ""Convert to Mask"", """")
IJ.run(imp, ""Watershed"", """")                   # break up particle clumps
IJ.run(imp, ""Set Measurements..."", ""area mean min centroid integrated display redirect=None decimal=3"")
hardcoded: ""size=50-Infinity display exclude clear summarize add""
moptions = ""size="" \
    + str(min_size) + ""-"" + str(max_size) \
    + "" display exclude clear summarize add""
IJ.run(imp, ""Analyze Particles..."", moptions)
imp.show()
get default instance of RoiManager(used by ParticleAnalyzer)
rm = RoiManager.getInstance2()
get list of all ROIs
rois = rm.getRoisAsArray()
set measurement options
options = IS.MEAN | IS.AREA | IS.CENTROID
create Results Table
rt = ResultsTable()
iterate over all ROIs
for roi in rois:
    original.setRoi(roi)
    stats = original.getStatistics(options)      # measure
    rt.incrementCounter()                        # advance row counter
# add values to various columns
rt.addValue(""Label"", original.getTitle())
rt.addValue(""Mean"", stats.mean)
rt.addValue(""Area"", stats.area)
rt.addValue(""Centroid X"", stats.xCentroid)
rt.addValue(""Centroid Y"", stats.yCentroid)

show custom table
rt.show(""Blob Results"")
save contents of ResultsTable object as .csv file
outputdir = str(outputdir)
resultsfile = path.join(outputdir, ""blobs.csv"")
print ""Results file:"", resultsfile
rt.saveAs(resultsfile)
if save_rois:
    roifile = path.join(outputdir, ""blobs-rois.zip"")
    rm.deselect()                                # ensure all ROIs rather than selected are saved
    rm.runCommand(""Save"", roifile)
```

Expert {#expert-ex-id}
Project 8:  Modify the processing function in Batch Processing example script to apply a median filter to a circular shaped ROI in the center of the image. The ROI diameter should be half of the width or height (whichever is smaller) of the image. The radius of the median filter should be requested from the user. Hint: Setting ROI, Hint: Recording filter functions, Hint: Custom Dialog Windows
Project 9: Let’s create a script that performs the following operations on a multidimensional ImageStack:

Extract the central focal image planes of the last channel for all timepoints.
Save each extracted image plane as a separate .tif image file in a designated directory.
The pixel size calibration of the original image should be retained in each saved image plane.

```
from ij import IJ
from ij import ImagePlus
import os
from os import path
imp = IJ.openImage(""http://imagej.nih.gov/ij/images/Spindly-GFP.zip"")
channel_no = imp.getNChannels()             # last channel            
slice_no = 1 + imp.getNSlices() // 2        # integer division to get center slice  
stack = imp.getStack()                      # get ImageStack object
calibration = imp.getCalibration().copy()   # get calibration 
create output dir workshop/images in home directory if it does not exist
outputdir = path.join(path.expanduser(""~""), ""workshop"", ""images"")
print outputdir
if not path.isdir(outputdir):
    os.makedirs(outputdir)
iterate over all frames and save central slice of last channel
for frame_no in range(1, imp.getNFrames() + 1):
    stack_index = imp.getStackIndex(channel_no, slice_no, frame_no)
    ip = stack.getProcessor(stack_index)
# remove extension from image title and add frame no as suffix
title = imp.getTitle().split(""."")[0] + ""-"" + str(frame_no)
single_imp = ImagePlus(title, ip)
single_imp.setCalibration(calibration)

# create file name with absolute path (incl directory name) and save
outputfile = path.join(outputdir, title)
IJ.saveAs(single_imp, ""Tiff"", outputfile)

```
Project 10: Modify Project 9 to include the following features:

Apply either a Gaussian or Median filter of a chosen radius to the extracted image plane.
Downsample the extracted image plane by a certain factor (see next instruction). Pay to attention to where to place that command in relation to the setCalibration() command.
Include a custom dialog that
lets the user choose the filter mode (""Gaussian Blur..."" or ""Median..."") and filter radius (sigma for Gaussian),
lets the user specify the downsampling fraction (0 < fraction <= 1.0). 1.0 would imply no downsampling. Hint: Image > Adjust > Size.



Resources {#resources-id}
Fiji Scripting

Tutorial: https://syn.mrc-lmb.cam.ac.uk/acardona/fiji-tutorial/
Tips for Developers: https://imagej.net/Tips_for_developers
API: https://imagej.nih.gov/ij/developer/api/
SciJava: https://javadoc.scijava.org/Fiji/
Advanced: https://imagej.net/tutorials/imagej2-python

General Scripting

Python: https://learning.rc.virginia.edu/courses/programming_python_scientists_engineers/
"
rc-learning-fork/content/courses/fiji-image-processing/introduction/index.md,"{{< figure src=""/img/fiji.png"" >}}
{{% link-button href=""/data/intro-fiji-images.zip"" icon=""fa fa-download"" %}}Data{{% /link-button %}}
Fiji Installation
Fiji is available on Windows, Mac, and Linux platforms and can be downloaded directly
from the Fiji website: https://fiji.sc/. Fiji is a portable application,
meaning it doesn't require an installer. To use Fiji, simply download and unzip the application,
and then you are able to start it.

macOS: The Fiji application should be installed in the user’s home directory rather than the default Applications folder.
Windows 7 & 10: The Fiji application should be installed in the user’s home directory rather than the default C:\Program Files directory.
Linux: The Fiji application should be installed in a directory where the user has read, execution, and write permissions, e.g. the user’s home directory.

Introduction
Image Processing & Analysis

Image Filtering (removal of noise and artifacts)
Image Enhancement (highlight features of interest)
Image Segmentation (object recognition)
Quantitative Image Feature Description (object measurements)

Image Processing Applications

Photography & cinematography
Digitization of non-digital documents -> text mining
Natural sciences, engineering
Forensics, e.g. fingerprint matching
Quality control, e.g. in semiconductor fabrication
Satellite surveillance, e.g. weather forecasting, terrain usage analysis
Autonomously operating machinery (cars, etc.)
Automatic cataloging of pictures on the web

The Digital Representation of an Image

2D images are 2D arrays of numbers organized in rows and columns
The intersection of row and column is a pixel
Each pixel is represented as an intensity value (e.g. between 0-255)

{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-1.png"" >}}

{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-2.png"" >}}
Fiji can handle image files in various file formats and pixel encodings, e.g. color images (RGB, HSB, etc.), grayscale images, and binary images.
Color images use 8-bit encoding for each channel (e.g. RGB: 256 intensity values for red, green, and blue).
Pixels in grayscale images may be encoded as 8-bit integer (256 intensity levels), 16-bit integer (65536 intensity levels), or 32-bit float (decimal number) values.
Binary images are a special type of 8-bit grayscale images that only contain the pixel values 0 (black) or 255 (white). They are used for masking and segmentation of object areas of interest in an image.
Image Processing & Analysis
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-3.png"" >}}
Image processing and analysis procedures often share a common workflow as shown here.
The original image (or raw data) that serves as input for the image processing pipeline may contain background noise that may need to be removed by applying specific image filters.
The cleaned-up image may then be processed to enhance certain features, e.g. highlight object edges.
The enhanced image can then be converted into binary image masks that define groups of pixels as objects. Based on these masks, objects of interest can be classified and analyzed for size, geometry, pixel intensities, etc..
Basic Operations in ImageJ/Fiji

Handling Image Files
Working with Image Channels
Image Montage and Cropping
Image Scale Bars

Handling of Image Files
Images and Image Stacks in Fiji
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-4.png"" >}}
Working with Image Channels
Fiji has built-in tools to manipulate multichannel composite and RGB images. For this example, open the 5-dimensional (x, y, z, color, time) image stack.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-5.png"" >}}
1 Go to File > Open Samples > Mitosis (26MB, 5D stack). An image window should open as shown here.
2 Open the Channels Tool dialog with Image > Color > Channels Tool….
3 In the Channels Tool dialog window, change from Composite to Color. Move the channel slider and note how the display updates based on the selected channel.
4 Switch back to Composite mode. Uncheck selected channels and note how the channel overlay changes based on the selected images.
5 Change to Grayscale. Although the color information has been converted to gray, the pixel intensity values remain unchanged.
6 Change back to Color, Click More >>, select a new color.
7 Change to Composite mode.

Splitting Channels
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-6.png"" >}}
1) Go to File > Open Samples > Mitosis (26MB, 5D stack).
2) Go to Image > Color > Split Channels.
Individual image channels are displayed in separate windows. This operation only works on multichannel image stacks or RGB images.

Merging Channels
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-7.png"" >}}
Images displayed in separate windows can be merged into multichannel composite images. The input images need to be of the same type (either 8-bit or 16-bit pixel encoding). You can merge up to 7 different images into a single multichannel image.
1) Go to File > Open Samples > Fluorescent Cells.
2) Go to Image > Color > Split Channels.
3) Go to Image > Color > Merge Channels.
4) Set the green channel to None and leave the other default settings. Click OK.
The resulting image should contain a merge of the red and blue channels without the green channel component.

Changing Image Color Assignment
The color representation in 8-bit and 16-bit images is defined by a color Lookup Table (LUT).
For single channel images, you can change the LUT by running these commands:
1) Image > Lookup Tables.
For a multichannel image, you can change the LUT by running these commands:
1) Image > Color > Channels Tools.
2) In the Channels Tools window, switch to Color mode and select a channel of interest.
3) Click More >>, select a new color.

Image Montage and Cropping
Making an Image Montage
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-8.png"" >}}
Images can be laid out in a grid-like fashion, aka a montage. Given the regular arrangement of the images in a grid layout, all image tiles used for the montage have to be of the same x-y dimension and same image type (e.g. all 8-bit, 8-bit, 32-bit, or RGB).
The easiest way to create a montage is to create an image stack that contains all the images for the individual image tiles and then use the Fiji Montage tool. Here is an example:
1) Close All Image Windows: File > Close All.
2) File > Open Samples > Fluorescent Cells.
3) Image > Color > Split Channels.
4) Close Blue Image.
5) Select Red Image.
6) Go to Image > Lookup Tables > Grays.
7) Select Green Image.
8) Go to Image > Lookup Tables > Grays.
9) Image > Color > Merge Channels (green and red channel); select Keep Sources.
10) For each of the three images:
  a) Click on grayscale image window to make it the active window,
  b) then go to Image > Type > RGB Color.
11) Go to Image > Stacks > Images to Stack.
12) Go to Image > Stacks > Make Montage.(Columns: 3, Scale factor: 1.0)

Image Size: Cropping
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-select-tools.png"" >}}
Images can be cropped around regions-of-interest (ROIs) that can be drawn with the selection tools in the Fiji Toolbar.
1) Go to File > Open Samples and select any image.
2) Click Rectangular selection icon, then draw rectangular box in image.
3) Go to Image > Crop. This crops image to size of rectangular selection box.
Note that the image will be cropped for all z-focal planes, all channels, and all timepoints.

Image Scale Bars
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-10.png"" >}}
Many scientific instruments used for image acquisition store additional metadata in the image files that contain calibrations defining the size of a pixel (or 3D voxel) for that image or image stack in physical units (e.g. mm, nm, etc.). These values define the scale of an image.
Taking a look at the image window right below the image title indicates whether the image scale (e.g. pixel/voxel size) has been set or not.

If the scale is not set, the image dimensions are displayed as NNN x NNN pixels, see top left panel.
If the scale is set, the image dimensions are displayed as NNN x NNN , see bottom left panel.


Defining an Image Scale
You can set or modify an image's scale following these steps:
1) Go to Analyze > Set Scale....
2) In the top box of the dialog window, enter the number of pixels that correspond to a known physical distance.
3) In the second box, enter the physical distance represented by the pixel distance.
4) Enter the physical units label, e.g. mm, nm, etc.. The label um is interpreted as micrometer.
* If Global is checked, the new scale (or scale removal) operation is applied to all open images.
5) Click OK.

Adding a Scale Bar
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-11.png"" >}}
For presentation purposes it is frequently desired to display a scale bar in the image. This can be done following these steps:
1) Go to Analyze > Tools > Scale Bar....
2) Define positioning of the scale bar and its size. Check the Overlay box and click OK.
2) Go to Image > Overlay > Show Overlay/Hide Overlay to toggle between showing and hiding the scale bar.


The Overlay will not affect the pixel values of the image region under the scale bar.  It is non-destructive.


Saving the image in TIF format retains the non-destructive overlay.


Saving the image in JPG format burns the overlay into the image.



Useful ImageJ/Fiji Keyboard Shortcuts
| Shortcut             | Operation                    |
| --------             |  --------                    |
| Ctrl + I         | Show image file info         |
| Ctrl + Shift + D | Duplicate current image      |
| Ctrl + Shift + X | Crop image                   |
| Ctrl + Shift + I | Invert image                 |
| Ctrl + A         | Select All (entire image)    |
| Ctrl + Shift + A | Deselect marked region       |
| Ctrl + M         | Measure (in selected region) |
| Ctrl + T         | Add selection to ROI Manager |
*On a Mac, use the Command key instead of the Ctrl key.

Image Filtering & Enhancement
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-12.png"" >}}

Applying Filters: Noise Removal
Image noise is an undesirable by-product of image capture that adds spurious and extraneous information.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-13.png"" >}}

How Do Filters Work?
Fiji provides several standard filters that can be applied to images. Here we are just exploring a few of them.
1) Open image file noisy_image.tif from the Tutorial Example Files.
2) Create three copies of the image file by pressing Ctrl + Shift + D (three times).
3) Go to Process > Filters and experiment with the Mean, Median, Gaussian, and Unsharp Mask filters on the four images.
4) Experiment with settings and click Preview.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-14.png"" >}}
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-15.png"" >}}

Comparing Average (Mean) and Median Filters


Mean: Replaces pixel value with average value in kernel region.


Median: Replaces pixel value with median value in kernel region.


{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-16.png"" >}}

Other Commonly Used Filters
| Filter                 | Effect                                                                    |
| ------                     | ------                                                                        |
| Convolve                   | Convolution with custom filter                                            |
| Gaussian Blur              | Gaussian filter, reasonable edge preservation                             |
| Median                     | Median of neighbors, non-linear, good for salt-and-pepper noise reduction |
| Mean                       | Average of neighbors, smoothing                                           |
| Minimum                    | Smallest value of neighbors, smoothing                                    |
| Maximum                    | Largest value of neighbors, smoothing                                     |
| Variance                   | Variance of neighbors, indicator of image textures, highlight edges       |

Removing Periodic Noise (Advanced)
Periodic noise as shown by the banding pattern observed in this image cannot be removed with the kernel-based standard filters described above. However applying Fast-Fourier Transform (FFT) analysis can reveal the periodicity of such noise represented by the high intensity spots in the FFT image (top middle panel). By masking these hotspots (bottom middle panel) and applying the inverse FFT one can retrieve the original image without the periodic noise (right panel).
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-17.png"" >}}
Applying Filters: Edge Detection
Edge detection is commonly used to define the outline of objects of interest. It is implemented by applying special convolution kernels.
Note that edge detection algorithms can be very sensitive to noise. So it is generally advised to remove noise with other filters first (e.g. median filter which preserves edges quite well) before applying the edge detection filter (see images below).
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-18.png"" >}}
Image Segmentation


Image Segmentation is the process that groups individual image pixels that represent specific objects.


It often involves the application of a variety of image pixel filters.


It requires binary (black and white) image masks and object shape descriptors (morphometric parameters).



Example: Blob Analysis
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-19.png"" >}}
To illustrate typical steps in image segmentation and analysis, we are going to process the blobs-shaded.tif image that is part of the [Tutorial Example Files] (#example-files).
Analysis Goals:


Separate dark spots as individual objects from background.


Reject small objects (debris) by size exclusion.


Count total number of objects and compute average object size.


Measure size (2D surface area) of each object.



Separating ""Blobs"" from Background
In order to separate the dark spots from the background we will be using intensity thresholding. The idea is that all pixels with an intensity below (or equal) the threshold go in one bin, all other pixels go into another bin. Pixels in one bin are considered to belong to objects of interest while those in the other bin are considered background. This is the principal method to create binary images.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-20.png"" >}}
1) Open image file blobs-shaded.tif.
2) In menu, go to Image > Adjust > Threshold….
3) Click on image window, then click on Auto button in Threshold window. If you lost track of the Threshold window, goto Windows and select it from the list.
4) In the Threshold dialog window, uncheck the Dark Background box.  In the drop-down boxes select Default and Red.
5) Move the sliders and observe the change of the red pattern overlaying the image. Click Reset to remove the overlay. Don’t click Apply yet!

What is the problem?


Correction of Uneven Background Signal
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-21.png"" >}}
A Background Correction Image
1) Go to Image > Duplicate, in the Title box enter: Bg.
2) Go to Process > Filters > Gaussian Blur….


Check the Preview box.


Try different Sigma Radius values: e.g. 1, 20, 100.


3) Finally, set Sigma Radius to 60.
4) Click OK.

Performing the Background Correction
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-22.png"" >}}
5) Go to Process > Image Calculator….


From the Image1 drop-down, select the blobs-shaded.tif image.


Select the Divide Operation.


From the Image2 drop-down, select the Bg image.


Check the Create new window box.


Check the 32-bit (float) result box.


Click OK.


6) Click on the resulting window that contains the corrected image.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-23.png"" >}}
7) Go to Image > Rename... and enter Corr as a new name for the corrected image.
8) Go to Image > Type > 8-bit to change the image from 32-bit float to 8-bit integer encoding.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-24.png"" >}}

Creating a Binary Object Mask
9) Go to Process > Binary > Options. Check the Black background box and click OK. This controls the display mode of thresholded images and also the mode that the Particle Analyzer uses to identify objects (see below). In our example objects of interest will be displayed white on black background.
10) Go to Image > Adjust Threshold.


In the Threshold dialog window, choose the Default algorithm.


Choose the Red overlay.


Deselect Dark Background and deselect Stack histogram.


Click on the Corr image window and click Auto in the Threshold window. Note the uniform detection of dark spots indicated by the red overlay. Note the bimodal distribution of the pixel intensity values in the thresholding histogram. Compare this thresholding to the one obtained for the original image blobs-shaded.tif.


{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-25.png"" >}}
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-27.png"" >}}
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-28.png"" >}}

{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-29.png"" >}}
11) In the Threshold dialog window, click Apply.


If you see a message like: “Convert to 8-bit mask or set background pixels to NaN”, it means that you are running the thresholding on a 32-bit image.  Convert the Corr image to 8-bit first (see step 8).


If you see black objects on white background, check Process > Binary > Options (step 9).


Redo the thresholding on “Corr” image.


12) Go to Image > Rename and enter Mask as a new image title.
The result after steps 1-12 should be a binary mask image that shows white bobs on a black background.

Splitting Joined Objects
The previous steps produced a binary mask, but closer inspection reveals that some blob objects are fused. These appear as peanut-shaped objects. We will apply a watershed algorithm to separate these fused objects.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-32.png"" >}}
13) Click on the Mask image window and go to Process > Binary > Watershed.
14) Save the image obtained after the watershed operation as blobs-watershed.tif.


The watershed algorithm only works on binary images, e.g. the black and white “blob” mask.


It only works for objects that show significant opposing convex-to-concave curvature (neck regions).



Analyze Particles
Now we are ready to identify and extract information from each blob.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-34.png"" >}}
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-33.png"" >}}
15) Go to Analyze > Analyze Particles....


In the Analyze Particles dialog, set up the analysis parameters as shown.


Click OK.


The option Show Outlines leads to the creation of a new image that contains the outline and object ID (red numbers).
The option Add to Manager sends the definition for each identified particle, i.e. dark blob, to the Region-of-Interest Manager, (ROI Manager for short).

The ROI Manager
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-35.png"" >}}
Regions-of-interest (ROIs) define groups of pixels in an image. ROIs can have different shapes, e.g. line, rectangular, ellipsoid, polygonal, text, or irregular. An ROI can be used to

Measure pixel values in a specific image region.
Change pixel values in a specific image region.
Mark image regions in non-destructive overlays (without changing pixel values).

When we use the Particle Analyzer with the Add to Manager option, the ROI for each blob is sent to the Roi Manager.
ROI Properties
Via the Properties button, the ROI Manager provides functions to change properties of selected ROIs, e.g. name, Z-position, outline color & width, and fill color. Multiple ROIs can be selected by holding the Ctrl (command on a Mac) or Shift keys while clicking on the ROIs.
ROI Import & Export
The ROI Manager can save selected ROIs to a file or open ROIs from a file. Typically, the ROI files have a .roi or .zip extension. The Open & Save functions are available after clicking the More>> button in the ROI Manager.


Save: Export All Selected ROIs to File


Open: Import ROIs from ROI file


This only affects selected ROI.


If no ROIs are selected, new properties are be applied to all ROIs in Manager.



Image Object Measurements
Set Measurements
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-37.png"" >}}
Fiji can measure a variety of parameters for each region-of-interest, or an entire image. The specific measurements to be calculated by the Particle Analyzer can be set in the Set Measurements dialog.
1) Go to Analyze > Set Measurements....


Adjust settings as shown in screenshot. Click OK.


Settings affect future measurements, not existing ones.


2) For quick measurements, draw a selection on the image and press Ctrl + M.

Measurements occur for current selected region in the current image.

Example:
1) Draw a selection on image
2) Use Ctrl + M.
3) Change settings in Set Measurements window.
4) Use Ctrl + M.
5) Select an ROI in the ROI Manager. Click Measure or use Ctrl + M to create measurements for the currently selected ROI.

Analyze Particles with Custom Parameters
The Particle Analyzer allows filtering of identified ROIs by size and circularity (range 0.0-1.0). A perfect circle has a circularity of 1.0. Let's filter the identified blobs and discard any ROI that has an area <150 square pixels.
1) Go to Edit > Selection > Select None (Ctrl + Shift + A).
2) Go to Analyze > Analyze Particles....
3) In the Analyze Particles dialog window, set up the analysis parameters as shown in the screenshot.
4) Click OK.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-38.png"" >}}
Based on our selection, the Particle Analyzer creates two results tables, titled Results and Summary. The type of  measurements computed (e.g. Mean, Median, etc.) depend on the settings in the Set Measurements dialog.


Each line in the Results table reflects measurements for a single ROI.


The Summary table, as the name implies, provides summary statistics for all ROIs.


{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-39.png"" >}}

Measuring Pixel Intensities
Let's go back and modify the measurement selection.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-40.png"" >}}
1) Go to Analyze > Set Measurements....
2) In the Set Measurements dialog window, set up the analysis parameters as shown in the screenshot.
3) Click OK.
4) Select all ROIs:

Go to Window > ROI Manager.
Click on first ROI, hold Shift key,
Scroll down, click last ROI in list.

5) Measure ROIs in the Mask image

Click on black/white mask image
In ROI Manager window, click Measure. This adds measurements for all selected ROIs to the Results table.
Click on the Results window, go to File > Rename, and enter Mask-Results as a new name.
If you don’t rename the window, the Results will be overwritten by new measurements.

6) Measure ROIs in the original image

Click on blobs-shaded.tif image
In ROI Manager window, click Measure. This adds measurements for all selected ROIs to the Results table.


Now we can compare the intensity measurements for the corresponding ROIs in the Mask and the blobs-shaded.tif images.

What do you observe?

{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-41.png"" >}}
Additional ROI Manager Functions

ROIs that are listed in the ROI Manager window can be saved to a file (single ROI or multiple ROIs).
Conversely, ROIs can be loaded from file into the ROI Manager and (re-)applied to an image.
Hand-drawn ROIs created with the ROI tools can be added to the ROI-Manager.
ROIs can be renamed and colored.
ROIs can be combined to create ROIs with complex shapes.

Go to the Exercises section for ideas to explore ROI-Manager functionality.

3D Image Reconstruction
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-42.png"" >}}
Isolating the Brain Segmentation
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-43.png"" >}}
Creating the Brain Segmentation
1) Apply gaussian filter (0.5).
2) Apply local thresholding (Grey Median, Otsu).
3) Run particle analysis (1000-infinity particle size).
4) Manual clean up segmentation mask as needed.
5) Save brain segmentation mask as TIF image stack.
These steps have been executed for you. The resulting brain segmentation mask is saved as brain-mask.tif in the Tutorials Example Files.
Creating the Composite 3D Rendering
1) Go to File > Open Samples > T1-Head.
2) Go to Image > Type > 8-bit.
3) Go to File > Open and select brain-mask.tif see Tutorials Example Files.
4) Go to Process > Image Calculator:

Select t1-head.tif as Image 1.
Select the AND operation.
Select brain-mask.tif as Image 2.
Check the Create new window box.

This will create a new grayscale window with the brain proper that corresponds to the mask.
5) Go to Image > Color > Merge Channels: t1-head.tif (gray), brain (magenta)
6) Go to Plugins > 3D Viewer.

Select Display as Volume. Leave the other default settings.

8) In 3D Viewer, go to Edit > Change Transparency: Change skull transparency to 75%.
9) Go to View > Create 360 degree animation to view a rotating animation of the rendered head volume.
{{< figure src=""/courses/fiji-image-processing/introduction/3d-brain-rendering.gif"" >}}


Better: Export surfaces in Fiji to .stl files.


Process .stl surfaces for high quality rendering in other software, e.g. Paraview, Blender, etc..



Fiji/ImageJ Tutorials and Resources
https://imagej.nih.gov/ij/docs/examples/
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-44.png"" >}}

Exercises
1. Saving files in different file formats
Fiji supports many different file formats.
1) Use File > Open Samples to open an image of your choosing
2) Go to File > Save As and pick a file format to export the current image to.
Note that some image file formats use compression algorithms that may reduce the pixel resolution or
dynamic range of an image’s intensity values. The TIF file format is very versatile because it can store
image data without resolution loss, it can handle multi-image files (i.e. image stacks or multi-page),
store image annotations as file internal metadata, and can be read by many programs.
File import/export functionality is further extended by the Bio-Formats plugin (Plugins > Bio-Formats).
See https://docs.openmicroscopy.org/bio-formats/6.0.0/

2. Image Channels
2.1 Splitting and Merging Channels
1) Go to File > Open Samples > Mitosis (26MB, 5D stack).
2) Go to Image > Color > Split Channels.
3) Go to Image > Color > Merge, merge the two channels but assign new colors to each channel.
What happens if you assign one of the images to multiple different colors?

2.2 Manipulating Color Lookup Tables
Color lookup tables (LUTs) can be used on 8-bit grayscale, 16-bit grayscale, and 8-bit Color images. Note that 8-
bit color is not the same as RGB color. LUTs for 16-bit grayscale will be resampled to 8-bit (256 colors).
1) Go to File > Open Samples > Clown
2) Go to Image > Zoom > In.
3) Go to Image > Color > Edit LUT.
Did this work? Look at the image information in the clown.jpg window. Alternatively, go to Image > Show
information and look at the Bits per pixel entry. What is the image format?
4) Click on the clown.jpg image and convert the image to 8-bit color (Image > Type > 8-bit Color) Keep 256
colors (the default). Click OK.
5) Go to Image > Color > Edit LUT. The window shows the LUT color palette
6) Double-click on a LUT tile and edit its RGB values to pure blue. Click OK and repeat this for other LUT
entries, change the color representation as you wish.
What do you observe?

3. Image Montage
For this exercise we are creating a montage of RGB images, each image representing the center focal plane at a
single timepoint within an x-y-color-t timelapse series. Experiment with creating different image tile sizes and
border widths.
1) Close all images (File > Close All).
2) Go to File > Open Samples > Mitosis (26MB, 5D stack).
3) Go to Image > Type > RGB Color. Keep all slices and frames selected.
4) Go to Image > Duplicate. Check the Duplicate Hyperstack box, enter Slices (z): 3 (central focal plane),
enter Frames (t): 1-51. Click OK.
5) Click on the newly created stack. It should contain images for a single focal plane over multiple
timepoints (frames).
6) Save this image stack as a TIF file to your hard drive so you can create new montage layouts without
having to go through steps 1-5 again.
7) Go to Image > Stacks > Make Montage. In the dialog box set the following values:

Columns: 10
Rows: 6
Scale factor: 0.25
Border width: 5
Check Use foreground color

8) Click OK.
Select the image stack created in step 5 (or reopen the stack saved in step 6) and create new montages with
modified montage settings. To change the foreground color for the border, double-click on the Color Picker in
the Fiji toolbar.
{{< figure src=""/courses/fiji-image-processing/introduction/intro-fiji-45.png"" >}}

4. Image Scale Bars
Scale bars can be added to any image. Before a scale bar can be drawn, the pixel size has to be defined.
1) Go to File > Open Sample and select an image.
2) Repeat step 1 and open a second image.
3) Go to Analyze > Set Scale.
4) If the image does not have a scale bar, the entries for Distance in pixels and Known distance will be set
to zero values and Scale label will show  at the bottom the dialog box.
5) Change the values for Distance in pixels and Known distance to non-zero values. Leave the Global box
unchecked.
6) Enter a value for Unit of length. This can be any character sequence. If you enter “um” (without the
quotes) the unit will be set to μm.
7) Click Ok.
Look at the active image window (currently in front). The scale is indicated below the window title. The scale of
the second window should remain unaltered.
8) Go to Analyze > Set Scale again.
9) Change the Distance in pixels, Known distance, and Unit of length values.
10) Check the Global box.
11) Click Ok.
Note that the scale has changed for all open images. To remove a scale bar from an image, go to Analyze > Set
Scale and press the Click to Remove Scale button. If the Global box is checked, it will remove the scale on all
open images, otherwise the scale will be removed for the current image only.

5. Custom Image Filters
To apply a filter to images, a convolution kernel has to be created. The built-in filters have standardized
convolution kernels. You can create your own convolution kernels to create custom filters. In this example we
create a simple edge detector, called the Sobel operator.
1) Open the noisy.tif image.
2) Apply a filter to remove the salt-and-pepper noise, e.g. with the median filter.
3) Duplicate the image.
4) Go to Process > Filters > Convolve.
5) In the textfield of the dialog box, enter these numbers (each number separated by a white space).
-1 0 1
-2 0 2
-1 0 1
6) Check the Preview box. Experiment with changing the kernel values; make sure to keep the square
matrix format.
7) Click OK.
What edges are highlighted in the image? Can you modify the kernel to detect the missing vertical edges?
8) Create another kernel with these values and apply it to the copied image obtained in step 3:
-1 -2 -1
0 0 0
1 2 1
What edges are being detected now?
9) Go to Process > Image Calculator.
10) Select the output image from step 7 as Image 1.
11) Change Operation to Max.
12) Select the output image from step 8 as Image 2.
13) Check the Create new window box.
14) Check the 32-bit (float) result box.
15) Click OK.
The resulting image should show a combination of edges detected in steps 7 and 8. Can you create four kernels
to detect all vertical and horizontal edges?

6. Image Segmentation & Object Measurements
For this exercise we assume that we already have a binary mask (thresholded) image, and we will use the
Particle Analyzer for image segmentation and object measurements.
1) Open the blobs-watershed.tif image. This should be a binary image.
2) Go to Analyze > Set Measurements and change the selection of measurement options.
3) Go to Analyze > Analyze Particles and modify the Size (pixel^2), Circularity (1.0 refers to perfect circular
shaped objects), and Show options.
4) Click OK.
5) Repeat steps 2-4 and experiment with different settings.
Depending on your selection you will see a results table with data for each detected object (if Display results is
checked), a summary table for all results (if Summarize is checked), and the ROI-Manager (if Add to Manager is
checked) with a list of ROIs corresponding to the individual objects.
6) Go to File > Open Samples > Blobs.
7) Click on the blobs-watershed.tif image.
8) Go to Analyze > Set Measurements, check the Min & max gray value and the Mean gray value boxes,
and change the Redirect to option to blobs.gif. This means that object detection is performed on the
blobs-watershed.tif binary mask, but pixel intensity measurements will be performed on the blobs.gif
image due to the redirect.
9) Go to Analyze > Analyze Particles. Click OK.
Repeat the measurement results with and without redirect and compare the changes in the results for Min &
max gray value and the Mean gray value.

7. ROI Manager
The ROI Manager is very useful to handle multiple ROIs. It allows saving/loading ROIs and also to
combine simple shaped ROIs into more complex ones.
1) Open an image of your choice.
2) Use any of the selection tools from the Fiji toolbar to draw a selection area onto
the image.
3) Go to Selection > Edit > Add to Manager. This will add the current selection to the ROI
Manager.
4) Draw a new region of interest and go to Selection > Edit > Add to Manager.

7.1 Saving ROIs to file
The ROI Manager should show two ROIs.
5) In the ROI Manager, click on one of the ROIs.
6) In the ROI Manager, click More>> and then Save. In the dialog box enter a name for the ROI file and
click Save.
7) In the ROI Manager, hold the Shift key and select both ROIs.
8) Click More>> and then Save. In the dialog box enter a name for the ROI file and click Save. This will save
multiple ROI definitions into a single file.
Notes:


For a single ROI, the default ROI file extension is .roi. For multiple ROIs, the default file extension is .zip.


If none of the ROIs are selected, the operations in the ROI Manager are applied to all ROIs.



7.2 Opening ROIs from file
1) In the ROI Manager, select all ROIs and click Delete.
2) Click on More>> and then click Open. Select the ROI file saved under exercise 7.1 step 8 (multiple ROIs).
3) Click on one of the ROIs in the ROI Manager. This should create corresponding selection in the current
image.
Note: You can open another image and reload saved ROIs to apply them to the new current image.

7.3 Changing ROI display properties
Each ROI has multiple properties, e.g. position, color, etc.
1) In the ROI Manager, select the first ROI (or reload from file).
2) Click on Properties.
3) Change the Name to ROI_1, the Stroke color to blue, and the width to 2. Click OK.
4) Select the second ROI.
5) Click on Properties.
6) Change the Name to ROI_2, the Stroke color to red, the width to 1, and the Fill color to cyan. Click OK.
7) Click OK.
If you have multiple ROIs selected OR none at all, the properties will be applied to all ROIs.

7.4 Combining ROIs
Simple ROI shapes can be combined into more complex ones.
1) In the current image, draw a new selection that partially overlaps the first ROI.
2) Add the selection to the ROI Manager. It should show as the last one in the ROI Manager list.
3) In the ROI Manager, hold the Ctrl (Windows) or Command (Mac) key and click on the first and
third ROI.
4) A context menu should pop up; if not, click on More >>. In the ROI Manager’s context menu,
select OR (Combine). The menu will disappear. In the ROI Manager, click on Add [t] button. A
new ROI will appear at the bottom of the ROI Manager list.
5) Select the last ROI in the list (the one you just created). It encompasses the combined area of
the first and third ROI.
6) Repeat steps 3 and 4 but choose different ROI combination methods, e.g. And, XOR. The And
operation produces a new ROI that contains only the overlapping region of the selected ROIs,
while the XOR operation creates a combined ROI that excludes any overlap between the
selected ROIs."
rc-learning-fork/content/courses/fiji-image-processing/fiji-omero/index.md,"Introduction to OMERO
OMERO is an image management software package that allows you to organize, view, annotate, analyze, and share your data from a single centralized database. With OMERO, you and your collaborators can access your images from any computer without having to download the images directly to your computer.
In this chapter you will learn to view and manipulate images through the Fiji
/ImageJ software package.
For more details, review the OMERO tutorial or visit the Research Computing website describing the UVA Omero Database Service.

Installation of Fiji and the OMERO Plugin


Install the Fiji application on your computer as described in this chapter.


Start Fiji and go to Help > Update. This starts the updater which looks for plugin updates online.


In the ImageJ Updater dialog window, click on Manage Update Sites. Ensure that the boxes for the following plugins are checked:


Java-8


Bio-Formats


OMERO 5.4




Click Close. This will take you back to the ImageJ Updater window.


Click Apply Changes.


Restart Fiji.


Download the Example Scripts
To follow along, you can download the Jython scripts presented in this tutorial through this link.
Check your OMERO Database Account


If you are accessing the OMERO Database from an off-Grounds location, you have to connect through a UVA Virtual Private Network (VPN).  Please follow these instructions to set up your VPN.


Open a web browser and go to http://omero.hpc.virginia.edu. Login to the OMERO web interface is described here.


Username: Your computing ID


Password: Your personal password


If you are new to UVA's OMERO Database, a temporary password had been emailed to you.  
Please change your temporary password when you log in for the first time to the Omero database server as described in these instructions. 


You may be a member of multiple groups. For this workshop we want to specify  omero-demo as the default group.


If you cannot log in, please submit a support request.  Select Omero Image Analysis as Support Category.  

Working with the OMERO Web Client and Desktop Applications
OMERO provides a group of client software to work with your imaging data.


OMERO.web Client: 
OMERO.web is a web-based client for OMERO.  At UVA you can use it by connecting to http://omero.hpc.virginia.edu. The Web client can be used to view images and metadata, and manage image tags, results, annotations, and attachments. It cannot be used to import data or measure regions-of-interest (see OMERO.insight).


View Data


Import Data


View Image Metadata


Manage Image Tags


Manage Results and Annotations




OMERO.insight: 
The two main additional features of OMERO.insight which are not available yet for OMERO.web are:


The Measurement Tool, a sub-application of ImageViewer that enables size and intensity measurements of defined regions-of-interest (ROIs), and 


image import.




OMERO.importer: 
The OMERO Importer is part of the OMERO Desktop client but can also be run as a standalone application.


OMERO.cli: 
The OMERO.cli tools provide a set of administration and advanced command line tools written in Python.


For this workshop we will be using the OMERO Web client.
Projects, Datasets, Screens, Plates and Images
For this workshop we will be using prepared image sets and also upload new image data to your accounts.  Omero organizes images in nested hierarchies of Projects and Datasets or Screens and Plates. The hierarchical nature is visualized as tree structures in the OMERO.web and OMERO.insight clients.  
Images can be linked to individual projects, datasets, screens, or plates. A given image can be linked to multiple folders, allowing you to logically assign the same image to different folders without copying it. You can think of it like smart folders or symbolic links.
In addition, each group has a folder Orphaned Images that contains images that have not been linked to any projects, datasets, screens, or plates.
Note:
Each project, dataset, screen, plate, and image has a unique numerical ID that unambiguously identifies the data artifact.  We use these numerical IDs rather than image, folder, or file names to get access to these elements.
Sample Data
Sample data has been added to the Fiji Omero Workshop project.  Inside the blue project folder you will find several dataset subfolders. 


Group: omero-demo (Group ID: 53)


Project: Fiji Omero Workshop (Project ID: 130)


Make note of the Group ID and Project ID.
Your Personal Projects and Datasets
Let's create a new project and dataset through the OMERO web client. 


In the left Explore panel, click on omero-demo label and switch to All Members.  Now you can see all data shared within the omero-demo group.


Right-click on the avatar with your name (or the ""All Members"" label). You can also just right click folder icon labeled Fiji Omero Workshop. In the popup menu, select Create new > Project. 


Name the new Project <YOUR_UID>_workshop, for example mst3k_workshop. 


Click on the blue folder icon of your new project and take note of the Project ID. We will need this to direct Fiji where to load data from or save data to.  No right click on your blue project folder icon and create a new dataset.  Right-click on the dataset icon and take note of the Dataset ID.


After the project is generated, your user interface should look like this:
    

Interactive Use of Fiji and OMERO
Uploading Images with the OMERO.insight client
Here we will demonstrate, the upload of a set of image files via the OMERO.insight client. The import process is described in our OMERO.insight Import Tutorial. 
After the upload, the files are located in the following Project and Dataset: 


Project: Fiji Omero Workshop (Project ID: 130)


Dataset: HeLa Cells (Dataset ID: 265)


Note that images cannot be uploaded with the web-based OMERO.web client.

Uploading Images with Fiji
Images that have been opened and are displayed in the Fiji graphical user interface can be uploaded to OMERO using the File > Export > OMERO... command. The naming of the tool can be confusing -- it's all a matter of perspective: Export from Fiji equates to an Import in OMERO.  To minimize possible confusion we avoid these terms for the purpose of this workshop and refer to upload for any process that sends data to OMERO and download for any process that retrieves data from OMERO.
Before you begin, you need to know the dataset ID that the image should be linked to in OMERO. You can use the OMERO.web, OMERO.insight, or OMERO.cli tools to look up an appropriate dataset ID for this. Keep in mind that you can upload images only to those Projects and Datasets that you own.


If you choose a dataset ID -1, the image will be added to the Orphaned Images in your default OMERO group.  For this workshop, we have chosen the omero-demo group as default. 


Alternatively, you can export the image to a specific dataset in your default group, you just need to provide the dataset ID when executing the upload command.


Note: Image uploads via the built-in OMERO Export function is limited to your default group. This is not an issue if you belong to a single group.  If you are a member of multiple groups, you can change your default group via the OMERO.web client. 
Exercises
Export RGB image


File > Open Sample > Leaf (36k)


File > Export > OMERO...
Since we chose Dataset: -1, the uploaded image will end up in Orphaned Images for your group. 


Go to OMERO webclient (http://omero.hpc.virginia.edu) and look for the uploaded image in Orphaned Images.





Export image stack to a specific dataset


In the OMERO webclient, create a new dataset under your personal workshop project. Make note of the dataset ID.


File > Open Samples > T1 Head (2.4M, 16-bit)


File > Export > OMERO...


Question: What happens when you repeatedly upload the same image to Omero?

Uploading Results with Fiji


In the OMERO.web client, make note of a dataset ID where you would like to upload a new image and associated results file to.  You make pick the dataset ID from the previous exercise.


In Fiji, go to File > Open Samples > Blobs (25K)


Let's apply some basic filtering and simple image segmentation to identify all cellular outlines in the image:
a. Go to Process > Filter > Median.  In the popup dialog enter a Radius of  3.0 and click OK.  This will smooth out some of the image's intrinsic noise without degrading the object outlines.
b. Go to Image > Adjust Threshold.  In the popup dialog choose the Default thresholding algorithm, uncheck the Dark Background box and click Apply. The image should have been converted to a binary mask with white objects ona black background.
        

c. Go to Analyze > Set Measurements.... In the popup dialog specify the parameters as shown in this screenshot. Click OK.
d. Go to Analyze > Analyze Particles and set up the parameters as shown. Click OK.
        

e. These steps should create a Results and a Summary table.


Go to File > Export > OMERO....  Enter the dataset ID you chose under step 1 and click OK.


{{< figure src=""/courses/fiji-omero/fiji-omero-blobs-export.png"" >}}


Click on the Results table window and go to File > Save As.  Save the file as Results.csv on your computer.  Repeat the same for the Summary table.  


Upload the results files: Go to the OMERO web client, open the Dataset folder that you had chosen for the image upload. On the right side of the webpage, expand the Attachments pane and click on the + icon. In the popup dialog, click Choose File and select the saved csv files. Click Accept.  


Now you have analyzed the image and uploaded the image as well as all results to the OMERO dataset.

Scripting
Fiji provides convenient programming wrappers for the Fiji/ImageJ and OMERO functions that allow you to develop your scripts in a variety of programming languages:

ImageJ macro language: simple, slow, not very versatile
Jython: Python syntax with a few limitations, easy to learn, very versatile
BeanShell: Syntax similar to Java, versatile
Several others…

Fiji provides a richer programming environment than ImageJ and it is recommended to use Fiji instead of ImageJ for any script development.  Our Fiji/ImageJ: Script development for Image Processing tutorial provides a more general introduction to this topic.
Example Scripts
To follow along, you can download the Jython scripts presented in this tutorial through this link.
The Script Editor {#script-editor-id}
We'll be using the built-in Script Editor in Fiji to run our scripts.  To start the script editor in Fiji go to menu File > New > Script….
{{< figure src=""/courses/fiji-omero/fiji-script-editor.png"" >}}

The top pane provides the editor. Multiple scripts can be open at the same time and will show up as separate tabs.
The bottom pane shows output (e.g. produced by print statements) and any errors encountered during script execution.

Script editor menus:

File:         Load, save scripts
Edit:     Similar to word processor functionality (Copy, Paste, etc.)
Language: Choose language your script is written in with syntax highlighting
Templates:    Example scripts
Tools:        Access to source code of a class in the ImageJ/Fiji package
Tabs:     Navigation aid in script


The Macro Recorder {#macro-recorder-id}
The Macro Recorder logs all commands entered through the Fiji graphical user interface (GUI). It is useful to convert these GUI actions into script commands.

In the Fiji menu, go to Plugins > Macros… > Record.
In the Record drop-down menu, select BeanShell.
Clicking the Create button copies the code to a new script in the Script Editor.


The Console Window {#console-id}
In the Fiji menu, go to Window > Console.

The Console window shows the output and logging information produced by running plugins and scripts.



Connecting to OMERO
In order to get full access to OMERO's programming interface, we will now use a more advanced approach to establish an authenticated connection with the OMERO database.  We need instances of three classes: LoginCredientials, SimpleLogger, and Gateway.  The central one for the configuration is LoginCredentials which has to be initialized with user specific credentials and database host information.
Our script would not be very useful or secure if we had to hardcode these values. Fortunately we can use the SciJava@Parameter annotation to prompt the script user for the relevant information:
```python
@ String (label=""Omero User"") username
@ String (label=""Omero Password"", style=""password"") password
@ String (label=""Omero Server"", value=""omero.hpc.virginia.edu"") server
@ Integer (label=""Omero Port"", value=4064) port
```
These four lines at the top of our scripts are sufficient to create a dialog window that prompts the user for information that will be populated in the username, password, host, and port variables. With these variables in place we can now establish a connection to the OMERO database server.
python
cred = LoginCredentials()
if group_id != -1:
    cred.setGroupID(group_id)
cred.getServer().setHostname(server)
cred.getServer().setPort(port)
cred.getUser().setUsername(username)
cred.getUser().setPassword(password)
simpleLogger = SimpleLogger()
gateway = Gateway(simpleLogger)
e = gateway.connect(cred)
The return value of the connect method is stored as a boolean value in the variable e. If e==True, the connection was established; if e==False, the connection failed.  We can reuse this code block for most of our OMERO scripts.
It is very important to close the connection to the database at the end of your script, like this:
python
gateway.disconnect()

Getting the Basic OMERO Dataset Info
OMERO organizes users in groups. Each user can be a member of multiple groups. Images are organized in Projects and Datasets, or in Screens and Plates. The following script, Omero_Info.py, connects a user to a remote OMERO instance and shows a list of:

the groups that the user belongs to and the associated group ID. This ID is important when you want to access images stored for a particular group;
the projects and datasets for a particular group (specified via unique group ID);
and a list of images, organized by project and dataset, that the user has access to in a particular group.

The following script, Omero_info.py establishes a connection to the OMERO database and outputs your OMERO group memberships, as well as a list of all of your projects, datasets, and images. The code contains separate functions to connect to the database, retrieve information from the database, and parse the data into a set of tables.  If you're just starting with programming, you may find it helpful to work through our Fiji Scripting and other tutorials on our learning portal.
(Click on the black triangle next to View to take a look at the script.)

View Omero_Info.py script

{{< highlight python ""linenos=table,linenostart=1"" >}}
#@ String (label=""Omero User"") username
#@ String (label=""Omero Password"", style=""password"") password
#@ String (label=""Omero Server"", value=""omero.hpc.virginia.edu"") server
#@ Integer (label=""Omero Port"", value=4064) port
#@ Integer (label=""Omero Group ID"", min=-1, value=-1) group_id


# Basic Java and ImageJ dependencies
from ij.measure import ResultsTable
from java.lang import Long
from java.lang import String
from java.util import ArrayList

# Omero dependencies
import omero
from omero.gateway import Gateway
from omero.gateway import LoginCredentials
from omero.gateway import SecurityContext
from omero.gateway.exception import DSAccessException
from omero.gateway.exception import DSOutOfServiceException
from omero.gateway.facility import BrowseFacility
from omero.log import SimpleLogger


def connect(group_id, username, password, server, port):    
    """"""Omero Connect with credentials and simpleLogger""""""
    cred = LoginCredentials()
    if group_id != -1:
        cred.setGroupID(group_id)
    cred.getServer().setHostname(server)
    cred.getServer().setPort(port)
    cred.getUser().setUsername(username)
    cred.getUser().setPassword(password)
    simpleLogger = SimpleLogger()
    gateway = Gateway(simpleLogger)
    e = gateway.connect(cred)
    return gateway


def get_groups(gateway):
    """"""Retrieves the groups for the user""""""
    currentGroupId = gateway.getLoggedInUser().getGroupId()
    ctx = SecurityContext(currentGroupId)
    adminService = gateway.getAdminService(ctx, True)
    uid = adminService.lookupExperimenter(username)
    groups = []
    for g in sorted(adminService.getMemberOfGroupIds(uid)):
        groupname = str(adminService.getGroup(g).getName().getValue())
        groups.append({
            'Id': g,
            'Name': groupname,
        })
        if g == currentGroupId:
            currentGroup = groupname     
    return groups, currentGroup


def get_projects_datasets(gateway):
    """"""Retrieves the projects and datasets for the user""""""
    results = []
    proj_dict = {}
    ds_dict = {}
    groupid = gateway.getLoggedInUser().getGroupId()
    ctx = SecurityContext(groupid)
    containerService = gateway.getPojosService(ctx)

    # Read datasets in all projects
    projects = containerService.loadContainerHierarchy(""Project"", None, None) # allowed: 'Project"", ""Dataset"", ""Screen"", ""Plate""
    for p in projects:                # omero.model.ProjectI
        p_id = p.getId().getValue()
        p_name = p.getName().getValue()
        proj_dict[p_id] = p_name
        for d in p.linkedDatasetList():
            ds_id = d.getId().getValue()
            ds_name = d.getName().getValue()
            results.append({
                'Project Id': p_id,
                'Project Name': p_name,
                'Dataset Id': ds_id,
                'Dataset Name': ds_name,
                'Group Id': groupid,
            })
            ds_dict[ds_id] = ds_name     

    # read datasets not linked to any project 
    ds_in_proj = [p['Dataset Id'] for p in results]
    ds = containerService.loadContainerHierarchy(""Dataset"", None, None)
    for d in ds:                # omero.model.ProjectI
        ds_id = d.getId().getValue()
        ds_name = d.getName().getValue()
        if ds_id not in ds_in_proj:
            ds_dict[ds_id] = ds_name
            results.append({
                'Project Id': '--',
                'Project Name': '--',
                'Dataset Id': ds_id,
                'Dataset Name': ds_name,
                'Group Id': groupid,
            })
    return results, proj_dict, ds_dict         


def get_images(gateway, datasets, orphaned=True):
    """"""Return all image ids and image names for provided dataset ids""""""
    browse = gateway.getFacility(BrowseFacility)
    experimenter = gateway.getLoggedInUser()
    ctx = SecurityContext(experimenter.getGroupId())
    images = []
    for dataset_id in datasets:
        ids = ArrayList(1)
        ids.add(Long(dataset_id))
        j = browse.getImagesForDatasets(ctx, ids).iterator()
        while j.hasNext():
            image = j.next()
            images.append({
                'Image Id': String.valueOf(image.getId()),
                'Image Name': image.getName(),
                'Dataset Id': dataset_id,
                'Dataset Name': datasets[dataset_id],
            })
    if orphaned:
        orphans = browse.getOrphanedImages(ctx, ctx.getExperimenter()) # need to pass user id (long)
        for image in orphans:
            images.append({
                'Image Id': String.valueOf(image.getId()),
                'Image Name': image.getName(),
                'Dataset Id': -1,
                'Dataset Name': '',
            })  
    return images


def show_as_table(title, data, order=[]):
    """"""Helper function to display group and data information as a ResultsTable""""""
    table = ResultsTable()
    for d in data:
        table.incrementCounter()
        order = [k for k in order]
        order.extend([k for k in d.keys() if not d in order])
        for k in order:
            table.addValue(k, d[k])
    table.show(title)


# Main code
gateway = connect(group_id, username, password, server, port)

groups, current_group = get_groups(gateway)
show_as_table(""My Groups"", groups, order=['Id', 'Name'])

all_data,_,datasets = get_projects_datasets(gateway)
show_as_table(""Projects and Datasets - Group: %s"" % current_group, all_data, order=['Group Id', 'Dataset Id', 'Dataset Name', 'Project Name', 'Project Id'])

gateway.disconnect()    
{{< /highlight >}}


Downloading Images from the OMERO database
Let's try to download images from the database through a script.  The OMERO plugin provides simple download (aka import to Fiji) functionality to achieve this.


In the OMERO web interface, click on any image in the Fiji Omero Workshop project or your xxx_workshop project/dataset and note the Image ID displayed in the sidebar on the right side of the webpage. Image retrieval relies on these unique image identifiers.


Go back to the Fiji Script Editor and open the Omero_Image_Download.py script.


Run the script. A dialog window will open; enter these values:

Omero User: Your computing ID
Omero Password: Your OMERO Password
Omero Server: omero.hpc.virginia.edu
Omero Port: 4064
Omero Group ID: Enter 53 as ID for the omero-demo group, or use -1 to use your default group
Image ID: Enter the ID for an image that is part of your xxx_workshop dataset, or use 11980 from the example files. 



The script consists of the these core blocks:

Lines 1-6 define user input to connect to OMERO.
Lines 12-20 define a command variable that specifies OMERO connection and image parameters.
Line 21 executes the OMERO importer plugin that retrieves the image.

{{< highlight python ""linenos=table,linenostart=1"" >}}
@ String (label=""Omero User"") user
@ String (label=""Omero Password"", style=""password"") pwd
@ String (label=""Omero Server"", value=""omero.hpc.virginia.edu"") server
@ Integer (label=""Omero Port"", value=4064) port
@ Integer (label=""Omero Group ID"", min=-1, value=53) omero_group_id
@ Integer (label=""Image ID"", value=2014) image_id
from ij import IJ
from loci.plugins.in import ImporterOptions
Main code
command=""location=[OMERO] open=[omero:""
command+=""server=%s\n"" % server
command+=""user=%s\n"" % user
command+=""port=%s\n"" % port
command+=""pass=%s\n"" % pwd
if omero_group_id > -1:
    command+=""groupID=%s\n"" % omero_group_id
command+=""iid=%s] "" % image_id
command+=""windowless=true view=\'%s\' "" % ImporterOptions.VIEW_HYPERSTACK
IJ.runPlugIn(""loci.plugins.LociImporter"", command)
{{< / highlight >}}

Uploading Images to the OMERO database
Let's try to upload an image from Fiji to OMERO.


Go back to Fiji and then to File > Open Samples > Blobs.


Go back to the Fiji Script Editor and open the Omero_Image_Upload.py file.
{{< highlight python ""linenos=table,linenostart=1"" >}}
from ij import IJ
imp = IJ.getImage()
IJ.run(imp, ""OMERO... "", """")
{{< /highlight >}}


Run the script. The Export to OMERO dialog window will open. Enter the following values:


Server: omero.hpc.virginia.edu


Port: 4064


User: Your computing ID


Password: Your OMERO password


OMERO Dataset ID: Enter the ID for the xxx_workshop dataset that you created in the OMERO web interface. 


Check the Upload new image box.  Leave the other boxes unchecked.


Click OK.
If you see an error, make sure you entered the correct password and Dataset ID.  Note: you have to use your own project/dataset. 


Go to the OMERO website and refresh the page. Double-click on your xxx_workshop dataset icon to expand it. You should see the blobs.gif image.



Creating Key:Value Annotations
{{< figure src=""/courses/fiji-omero/fiji-omero-keyvalue.png"" >}}
OMERO allows you to link other pieces of information to your Project, Dataset, Screen, Plate or Image objects. This additional information is displayed on the right side in the OMERO web client, labeled under the General tab as Image Details, Tags, Key-Value Pairs, Tables, Attachments, Comments, and Ratings. In addition, there is the Acquisition tab that provides metadata information that was automatically extracted from the image file headers during import.  
For the remainder of this workshop, we will focus on Key-Value pairs and Attachments.  The key-value pairs are implemented as a dictionary (or HashMaps) that can be used to annotate individual images or whole datasets/project or plates/screens with additional information. Such information may include experimental conditions etc.. 
Let's look at an example:


In the OMERO webclient, expand the Fiji Omero Workshop project folder and the Sample Data dataset folder inside it. 


Click on the blobs.gif image.  In the general tab, you will see three entries under the Key-Value group. (You may have to click on the triangle next to the label to expand the tab and see it).


The values displayed are not particular meaningful, but they illustrate the concept. You can create and modify annotations interactively through the OMERO client.  In addition, you can manipulate key-value pairs (as well as other annotation categories) through Fiji scripts.

View Omero_Map_Annotation.py script

{{< highlight python ""linenos=table"" >}}
#@ String (label=""Omero User"") username
#@ String (label=""Omero Password"", style=""password"") password
#@ String (label=""Omero Server"", value=""omero.hpc.virginia.edu"") server
#@ Integer (label=""Omero Port"", value=4064) port
#@ Integer (label=""Omero Group ID"", min=-1, value=-1) group_id
#@ String (label=""Target"", value=""Image"", choices = [""Image"", ""Dataset"", ""Project""]) target_type
#@ Integer (label=""Target ID"", min=-1, value=-1) target_id

# Basic Java and ImageJ dependencies
from ij.measure import ResultsTable
from java.lang import Double
from java.util import ArrayList
from ij import IJ
from ij.plugin.frame import RoiManager
from ij.measure import ResultsTable

# Omero dependencies
import omero
from omero.log import SimpleLogger
from omero.gateway import Gateway
from omero.gateway import LoginCredentials
from omero.gateway import SecurityContext
from omero.gateway.model import ExperimenterData;

from omero.gateway.facility import DataManagerFacility
from omero.gateway.model import MapAnnotationData
from omero.gateway.model import ProjectData
from omero.gateway.model import DatasetData
from omero.gateway.model import ImageData
from omero.model import NamedValue
from omero.model import ProjectDatasetLinkI
from omero.model import ProjectI
from omero.model import DatasetI
from omero.model import ImageI


def connect(group_id, username, password, server, port):    
    """"""Omero Connect with credentials and simpleLogger""""""
    cred = LoginCredentials()
    if group_id != -1:
        cred.setGroupID(group_id)
    cred.getServer().setHostname(server)
    cred.getServer().setPort(port)
    cred.getUser().setUsername(username)
    cred.getUser().setPassword(password)
    simpleLogger = SimpleLogger()
    gateway = Gateway(simpleLogger)
    e = gateway.connect(cred)
    return gateway


def create_map_annotation(ctx, annotation, target_id, target_type=""Project""):
    """"""Creates a map annotation, uploads it to Omero, and links it to target object""""""
    # populate new MapAnnotationData object with dictionary
    result = ArrayList()
    for item in annotation:
        # add key:value pairs; both need to be strings
        result.add(NamedValue(str(item), str(annotation[item])))
    data = MapAnnotationData()
    data.setContent(result);
    data.setDescription(""Demo Example"");

    # use the following namespace if you want the annotation to be editable in the webclient and insight
    data.setNameSpace(MapAnnotationData.NS_CLIENT_CREATED);
    dm = gateway.getFacility(DataManagerFacility);
    target_obj = None

    # use the appropriate target DataObject and attach the MapAnnotationData object to it
    if target_type == ""Project"":
        target_obj = ProjectData(ProjectI(target_id, False))
    elif target_type == ""Dataset"":  
        target_obj = DatasetData(DatasetI(target_id, False))
    elif target_type == ""Image"":    
        target_obj = ImageData(ImageI(target_id, False))
    result = dm.attachAnnotation(ctx, data, target_obj)
    return result

# Main code
gateway = connect(group_id, username, password, server, port)
currentGroupId = gateway.getLoggedInUser().getGroupId()
ctx = SecurityContext(currentGroupId)   

# create a dictionary with key:value pairs
annotation = {'Temperature': 25.3, 'Sample': 'control', 'Object count': 34}

result = create_map_annotation(ctx, annotation, target_id, target_type=target_type)
print ""Annotation %s exported to Omero."" % annotation

gateway.disconnect()
{{< /highlight >}}



Batch Processing and Results Tables for OMERO Datasets
The previous examples demonstrated how to export local images to OMERO, or how to import OMERO images to a local workstation. As the final exercise, let's explore how an entire dataset comprised of many images can be downloaded from the remote OMERO instance, processed and analyzed locally, followed by an upload of the processed images and created results files back to the OMERO database.
The example script, Omero_Batch_Processing.py, consists of five key functions:

connect: Establishes a connection to the OMERO server with specific user credentials. It returns an instance of the OMERO  Gateway class that is used later to upload processed images to the same OMERO server instance.
get_image_ids: Gets a list of unique image IDs for a given dataset managed by the remote OMERO instance.
open_image: Downloads the image associated with an image ID and shows it in Fiji.
process: Applies a custom image processing routine to a given image. In this case a basic segmentation and counting of cells.
create_map_annotation: Uploads the cell count value to OMERO and links it to the original image.
upload_csv_to_omero: Converts an ImageJ ResultsTable into a csv file, uploads that csv file and links it ot the original image objects.
upload_image: Uploads an Image to a specific dataset managed by the remote OMERO instance.

Remember that the gateway connection needs to be closed at the end of the script.
{{< figure src=""/courses/fiji-omero/fiji-omero-batchprocessing.png"" >}}
To test this and see the process in action we will process a set of four images that has been deposited in the OMERO database. The setup is as follows:


Go to the OMERO webclient and make note of your Project ID, or you cna create a new project if you prefer. Again you need the ID.


In the Fiji Script Editor, open the Omero_Batch_Processing.py script and execute it.


In the popup window, specify the parameters as follows:
a. Replace the mst3k with your own credentials.
b. Omero Input Dataset ID: 265
c. Omero Output Dataset Name: Enter name to your liking
d. Omero Output Project ID:  Enter the ID that you looked up as step 1. The script will create a new dataset (with the name you chose) and place all the processed images in there.


Click OK. Watch the console output for logging messages.


After the script ru has completed, go to the OMERO webclient and open the Project that you had chosen to collect the output.  Look for the binary segmentation masks, the attached Results.csv files and the new Key-Value Pairs annotations for each image.



View Omero_Processing_Nuclei.py script

{{< highlight python ""linenos=table"" >}}
#@ String (label=""Omero User"") username
#@ String (label=""Omero Password"", style=""password"") password
#@ String (label=""Omero Server"", value=""omero.hpc.virginia.edu"") server
#@ Integer (label=""Omero Port"", value=4064) server_port
#@ Integer (label=""Omero Group ID"", min=-1, value=-1) omero_group_id
#@ Integer (label=""Omero Input Dataset ID"", min=-1, value=-1) dataset_id
#@ String (label=""Omero Output Dataset Name"", value=""Processed Images"") target_ds_name
#@ Integer (label=""Omero Output Project ID"", min=-1, value=-1) project_id


import os
import tempfile


from java.lang import Long
from java.lang import String
from java.lang.Long import longValue
from java.util import ArrayList
from jarray import array
from java.lang.reflect import Array
import java
from ij import IJ,ImagePlus
from ij.measure import ResultsTable
import loci.common
from loci.formats.in import DefaultMetadataOptions
from loci.formats.in import MetadataLevel
from loci.plugins.in import ImporterOptions

from loci.plugins.in import ImporterOptions

# Omero Dependencies
import omero
from omero.rtypes import rstring
from omero.gateway import Gateway
from omero.gateway import LoginCredentials
from omero.gateway import SecurityContext
from omero.gateway.facility import BrowseFacility
from omero.gateway.facility import DataManagerFacility
from omero.log import Logger
from omero.log import SimpleLogger
from omero.gateway.model import MapAnnotationData
from omero.gateway.model import ProjectData
from omero.gateway.model import DatasetData
from omero.gateway.model import ImageData
from omero.gateway.model import FileAnnotationData
from omero.model import FileAnnotationI
from omero.model import OriginalFileI
from omero.model import Pixels
from omero.model import NamedValue
from omero.model import ProjectDatasetLinkI
from omero.model import ProjectI
from omero.model import DatasetI
from omero.model import ImageI
from omero.model import ChecksumAlgorithmI
from omero.model.enums import ChecksumAlgorithmSHA1160

from ome.formats.importer import ImportConfig
from ome.formats.importer import OMEROWrapper
from ome.formats.importer import ImportLibrary
from ome.formats.importer import ImportCandidates
from ome.formats.importer.cli import ErrorHandler
from ome.formats.importer.cli import LoggingImportMonitor
from omero.rtypes import rlong


def connect(group_id, username, password, host, port):    
    '''Omero Connect with credentials and simpleLogger'''
    cred = LoginCredentials()
    if group_id != -1:
        cred.setGroupID(group_id)
    cred.getServer().setHostname(host)
    cred.getServer().setPort(port)
    cred.getUser().setUsername(username)
    cred.getUser().setPassword(password)
    simpleLogger = SimpleLogger()
    gateway = Gateway(simpleLogger)
    gateway.connect(cred)
    group_id = cred.getGroupID()
    return gateway


def open_image(username, password, host, server_port, group_id, image_id):
    command=""location=[OMERO] open=[omero:""
    command+=""server=%s\n"" % server
    command+=""user=%s\n"" % username
    command+=""port=%s\n"" % server_port
    command+=""pass=%s\n"" % password
    if group_id > -1:
        command+=""groupID=%s\n"" % group_id
    command+=""iid=%s] "" % image_id
    command+=""windowless=true ""
    command+=""splitWindows=false ""
    command+=""color_mode=Default view=[%s] stack_order=Default"" % ImporterOptions.VIEW_HYPERSTACK
    print ""Opening image: id"", image_id 
    IJ.runPlugIn(""loci.plugins.LociImporter"", command)
    imp = IJ.getImage()
    return imp


def upload_image(gateway, server, dataset_id, filepath):    
    user = gateway.getLoggedInUser()
    ctx = SecurityContext(user.getGroupId())
    sessionKey = gateway.getSessionId(user)

    config = ImportConfig()
    config.email.set("""")
    config.sendFiles.set('true')
    config.sendReport.set('false')
    config.contOnError.set('false')
    config.debug.set('false')
    config.hostname.set(server)
    config.sessionKey.set(sessionKey)
    config.targetClass.set(""omero.model.Dataset"")
    config.targetId.set(dataset_id)
    loci.common.DebugTools.enableLogging(""DEBUG"")

    store = config.createStore()
    reader = OMEROWrapper(config)
    library = ImportLibrary(store,reader)
    errorHandler = ErrorHandler(config)

    library.addObserver(LoggingImportMonitor())
    candidates = ImportCandidates (reader, filepath, errorHandler)
    reader.setMetadataOptions(DefaultMetadataOptions(MetadataLevel.ALL))
    success = library.importCandidates(config, candidates)
    return success


def get_image_ids(gateway, dataset_id):
    """"""Return all image ids for given dataset""""""
    browse = gateway.getFacility(BrowseFacility)
    experimenter = gateway.getLoggedInUser()
    ctx = SecurityContext(experimenter.getGroupId())
    images = []
    ids = ArrayList(1)
    ids.add(Long(dataset_id))
    j = browse.getImagesForDatasets(ctx, ids).iterator()
    while j.hasNext():
        image = j.next()
        images.append({
            'Image Id': String.valueOf(image.getId()),
            'Image Name': image.getName(),
            'Dataset Id': dataset_id,
        })
    return images


def create_map_annotation(ctx, annotation, target_id, target_type=""Project""):
    # populate new MapAnnotationData object with dictionary
    result = ArrayList()
    for item in annotation:
        # add key:value pairs; both need to be strings
        result.add(NamedValue(str(item), str(annotation[item])))
    data = MapAnnotationData()
    data.setContent(result);
    data.setDescription(""Demo Example"");

    #Use the following namespace if you want the annotation to be editable in the webclient and insight
    data.setNameSpace(MapAnnotationData.NS_CLIENT_CREATED);
    dm = gateway.getFacility(DataManagerFacility);
    target_obj = None

    # use the appropriate target DataObject and attach the MapAnnotationData object to it
    if target_type == ""Project"":
        target_obj = ProjectData(ProjectI(target_id, False))
    elif target_type == ""Dataset"":  
        target_obj = DatasetData(DatasetI(target_id, False))
    elif target_type == ""Image"":    
        target_obj = ImageData(ImageI(Long(target_id), False))
    result = dm.attachAnnotation(ctx, data, target_obj);
    return result


def upload_csv_to_omero(ctx, file, tablename, target_id, target_type=""Project""):
    """"""Upload the CSV file and attach it to the specified object""""""
    print file
    print file.name
    svc = gateway.getFacility(DataManagerFacility)
    file_size = os.path.getsize(file.name)
    original_file = OriginalFileI()
    original_file.setName(rstring(tablename))
    original_file.setPath(rstring(file.name))
    original_file.setSize(rlong(file_size))

    checksum_algorithm = ChecksumAlgorithmI()
    checksum_algorithm.setValue(rstring(ChecksumAlgorithmSHA1160.value))
    original_file.setHasher(checksum_algorithm)
    original_file.setMimetype(rstring(""text/csv""))
    original_file = svc.saveAndReturnObject(ctx, original_file)
    store = gateway.getRawFileService(ctx)

    # Open file and read stream
    store.setFileId(original_file.getId().getValue())
    print original_file.getId().getValue()
    try:
        store.setFileId(original_file.getId().getValue())
        with open(file.name, 'rb') as stream:
            buf = 10000
            for pos in range(0, long(file_size), buf):
                block = None
                if file_size-pos < buf:
                    block_size = file_size-pos
                else:
                    block_size = buf
                stream.seek(pos)
                block = stream.read(block_size)
                store.write(block, pos, block_size)

        original_file = store.save()
    finally:
        store.close()

    # create the file annotation
    namespace = ""training.demo""
    fa = FileAnnotationI()
    fa.setFile(original_file)
    fa.setNs(rstring(namespace))

    if target_type == ""Project"":
        target_obj = ProjectData(ProjectI(target_id, False))
    elif target_type == ""Dataset"":  
        target_obj = DatasetData(DatasetI(target_id, False))
    elif target_type == ""Image"":    
        target_obj = ImageData(ImageI(target_id, False))

    svc.attachAnnotation(ctx, FileAnnotationData(fa), target_obj)


def process_file(imp):
    """"""Run segmentation""""""
    print ""Processing"", imp.getTitle()
    title = imp.getTitle().split('.')[:-1]
    title = '.'.join(title) + ""_mask.ome.tiff""
    nimp = ImagePlus(title, imp.getStack().getProcessor(1))
    IJ.run(nimp, ""Median..."", ""radius=3"")
    IJ.run(nimp, ""Auto Local Threshold"", ""method=Bernsen radius=15 parameter_1=0 parameter_2=0 white"")
    IJ.run(nimp, ""Watershed"", """")

    IJ.run(""Set Measurements..."", ""area mean standard centroid decimal=3"")
    IJ.run(nimp, ""Analyze Particles..."", ""size=50-Infinity summary exclude clear add"")
    rt = ResultsTable.getResultsTable()
    rt.show(""Results"")

    imp.close()
    return nimp, rt 


def create_new_dataset(ctx, project_id, ds_name, ):
    dataset_obj = omero.model.DatasetI()
    dataset_obj.setName(rstring(ds_name))
    dataset_obj = gateway.getUpdateService(ctx).saveAndReturnObject(dataset_obj)
    dataset_id = dataset_obj.getId().getValue()

    dm = gateway.getFacility(DataManagerFacility)
    link = ProjectDatasetLinkI();
    link.setChild(dataset_obj);
    link.setParent(ProjectI(project_id, False));
    r = dm.saveAndReturnObject(ctx, link);
    return dataset_id 


# Main code
gateway = connect(omero_group_id, username, password, server, server_port)
currentGroupId = gateway.getLoggedInUser().getGroupId()
ctx = SecurityContext(currentGroupId)

image_info = get_image_ids(gateway, dataset_id)
tmp_dir = tempfile.gettempdir()
print tmp_dir

target_ds_id = create_new_dataset(ctx, project_id, target_ds_name)
for info in image_info:
    imp = open_image(username, password, server, server_port, omero_group_id, info['Image Id'])
    processed_imp, rt = process_file(imp)

    # Save processed image locally in omero_tmp dir
    imgfile = tempfile.TemporaryFile(mode='wb', prefix='img_', suffix='.tiff', dir=tmp_dir)

    #filepath = os.path.join(tmp_dir, processed_imp.getTitle())
    options = ""save="" + imgfile.name + "" export compression=Uncompressed""
    IJ.run(processed_imp, ""Bio-Formats Exporter"", options)
    # ignore changes & close
    processed_imp.changes=False 
    processed_imp.close()

    # uploaad image to a target dataset
    upload_image(gateway, server, target_ds_id, [imgfile.name])

    # create annotation
    annotation = {
        ""Cell count"": rt.size()
    }   
    create_map_annotation(ctx, annotation, info['Image Id'], target_type=""Image"")

    # export ResultsTable to csv file and link to image object
    file = tempfile.TemporaryFile(mode='wb', prefix='results_', suffix='.csv', dir=tmp_dir)
    rt.saveAs(file.name)
    #upload_csv_to_omero(ctx, file, ""Results.csv"", long(info['Image Id']), ""Image"")

# done, clean up    
shutil.rmtree(tmp_dir)
gateway.disconnect()    
print ""Done.\n""

{{< /highlight >}}


Resources {#resources-id}
OMERO

OMERO: https://www.openmicroscopy.org/omero/
OMERO User Support: https://help.openmicroscopy.org
UVA Research Computing: https://www.rc.virginia.edu
OMERO at the University of Virginia: https://www.rc.virginia.edu/userinfo/omero/overview/

Fiji Scripting

RC tutorial Fiji/ImageJ: Script development for Image Processing
Tutorial: https://syn.mrc-lmb.cam.ac.uk/acardona/fiji-tutorial/
Tips for Developers: https://imagej.net/Tips_for_developers
API: https://imagej.nih.gov/ij/developer/api/
SciJava: https://javadoc.scijava.org/Fiji/

General Python Programming

Introduction to Programming in Python
Programming in Python for Scientists and Engineers
"
rc-learning-fork/content/videos/matlab-parallel/index.md,"+++
title=""Parallel Computing with Matlab Videos""
date=""2019-06-03T21:13:14-05:00""
reading_time = false 
weight=1
+++
Running the Parallel Computing Workshop in Matlab's Live Editor
{{< video src=""Parallel_Computing_Workshop.mp4"" controls=""yes"" >}}
Submitting Matlab Parallel Jobs on Rivanna
{{< video src=""Rivanna.mp4"" controls=""yes"" >}}"
rc-learning-fork/content/videos/example/index.md,"+++
title=""Example Tutorial""
date=""2019-06-03T21:13:14-05:00""
reading_time = false 
weight=1
+++
{{< video src=""earth.mp4"" controls=""yes"" >}}"
rc-learning-fork/content/videos/matlab-data-science/index.md,"+++
title=""Data Science with Matlab Video""
date=""2019-06-03T21:13:14-05:00""
reading_time = false 
weight=1
+++
Introduction to Data Science with Maltab Workshop
{{< video src=""workshop-intro.mp4"" controls=""yes"" >}}"
rc-learning-fork/content/notes/slurm-from-cli/section4.md,"Environment Variables
An environment variable describes something about your working environment.  Some of them you can set or modify; others are set by the system. To see what is currently set in your terminal, run
bash
$printenv
To set an environment variable yourself, use the export command.
bash
$export VAR=value
When your job starts, SLURM will initialize several environment variables.  Many of them correspond to options you have set in your SBATCH preamble. Do not attempt to assign to any variable beginning with SLURM_
Some variables that you may wish to examine or use in your scripts:
{{< table >}}
|  Variable   |  Value  |
| ---------   |  -----  |
| SLURM_SUBMIT_DIR | Directory from which the job was submitted |
| SLURM_JOB_NODELIST | List of the nodes on which the job is running |
| SLURM_JOB_ID |  The numerical ID of the job |
| SLURM_NUM_TASKS |  The number of tasks (obtained from --ntasks) |
| SLURM_NTASKS_PER_NODE |  The number of tasks per node |
| SLURM_CPUS_PER_TASK |  The number of cpus (cores) assigned to each task |
{{< /table >}}
Interactive Jobs
Most HPC sites, including UVa's, restrict the memory and time allowed to processes on the login nodes.  Most jobs can be submitted through the batch system we have been discussing, but sometimes more interactive work is required.  For example
1. Jobs that must be or are best run through a graphical interface,
2. Short development jobs,
3. ""Computational steering"" in which a program runs for an interval, then the output is examined and parameters may be adjusted.
For most of these cases, we strongly recommend the use of the Open OnDemand Interactive Applications.  Jupyterlab is available to run notebooks.  Rstudio and Matlab Desktop are also available to run through this interface.  For more general work, including command-line options, the Desktop is usually the best option. It provides a basic terminal, but also access to other applications should they be needed.
For general-purpose interactive work with graphics, please use the Open OnDemand Desktop.  The X11 service that Linux uses for graphics is very slow over a network.  Even with a fast connection between two systems, the Desktop will perform better since the X11 server process and the programs that use it are running on the same computer.
If you must use a basic terminal for an interactive job, you must first use the command salloc.  This is the general Slurm command to request resources. This would be followed by srun to launch the processes.  However, this is complex and requires knowledge of the options, so we have provided a local ""wrapper"" script called ijob.
ijob takes options similar to those used with SBATCH, most of which are actually arguments to salloc.
bash
$ijob –c 1 –A myalloc -t <time> --mem <memory in MB> -p <partition> -J <jobname>
When the job starts you will be logged in to a bash shell in a terminal on the compute node.
{{< warning >}}
Never issue an sbatch command from within an interactive job (including OOD jobs).  The sbatch command must be used only to submit jobs from a login node.
{{< /warning >}}
Multicore and Multinode Jobs
One of the advantages of using a high-performance cluster is the ability to use many cores and/or nodes at once.  This is called parallelism.  There are three main types of parallelism.
You should understand whether your program can make use of more than one core or node before you request multiple cores and/or nodes. Special programming is required to enable these capabilities.  Asking for multiple cores or nodes that your program cannot use will result in idle cores and wasted SUs, since you are charged for each core-hour. The seff command can help with this.
High Throughput Serial Parallelism
High throughput parallelism is when many identical jobs are run at once, each on a single core.  Examples can include Monte-Carlo methods, parameter searches, image processing on many related images, some areas of bioinformatics, and many others.  For most cases of this type of parallelism, the best Slurm option is a job array.
When planning a high-throughput project, it is important to keep in mind that if the individual jobs are very short, less than roughly 15-30 minutes each, it is very inefficient to run each one separately, whether you do this manually or through an array.  In this case you should group your jobs and run multiple instances within the same job script.  Please contact us if you would like assistance setting this up.
Multicore (Threaded)
Shared-memory programs can use multiple cores but they must be physically located on the same node.  The appropriate Slurm option in this case is -c (equivalent to cpus-per-task).  Shared memory programs use threading of one form or another.  
Example Slurm script for a threaded program:
{{< code-download file=""/notes/slurm-from-cli/scripts/multicore.slurm"" lang=""bash"" >}}
Multinode (MPI)
In this type of parallelism, each process runs independently and communicates with others through a library, the most widely-used of which is MPI.  Distributed memory programs can run on single or multiple nodes and often can run on hundreds or even thousands of cores.  For distributed-memory programs you can use the -N option to request a number of nodes, along with ntasks-per-node to schedule a number of processes on each of those nodes.
{{< code-download file=""/notes/slurm-from-cli/scripts/multinode.slurm"" lang=""bash"" >}}
Hybrid MPI plus Threading
Some codes can run with distributed-memory processes, each of which can run in threaded mode.  For this, request --ntasks-per-node=NT and cpus-per-task=NC, keeping in mind that the total number of cores requested on each node is then $NT \times NC$.
{{< code-download file=""/notes/slurm-from-cli/scripts/hybrid.slurm"" lang=""bash"" >}}
GPU Jobs
We have a dedicated partition with nodes that are equipped with Graphical Processing Units (GPUs). Code must be built to take advantage of GPU resources. If the packages and your code are not written to use GPU resources, the GPUs will remain idle and you will be charged the SUs for them. Some popular Python packages that you could use are Pytorch or TensorFlow. We have some material on how to get started with using them in a Deep Learning setting.
GPU job scripts are similar to CPU scripts, but do require the addition of the --gres=gpu option. Example Slurm script requesting 1 GPU:
{{< code-download file=""/notes/slurm-from-cli/scripts/gpu.slurm"" lang=""bash"" >}}
The script uses the command nvidia-smi which detects GPU activity.
We have several different GPU types equipped on our nodes each offering varying amounts of memory. See our website for Hardware Specifications. In the example above, Slurm will choose whatever GPU is available. If you are working with larger models you may find that you need a GPU with more memory. To request a specific GPU, you add it to the gres Slurm option. If a GPU type has multiple options (for instance, we offer 40GB and 80GB A100 GPUs), there will be a constraint you can use to specify even further. Example Slurm script requesting 1 80GB A100 GPU node:
{{< code-download file=""/notes/slurm-from-cli/scripts/gpua100.slurm"" lang=""bash"" >}}
Note that the more specific your request is, the longer you will likely have to wait for the resource to be available.
Job Arrays
Many similar jobs can be submitted simultaneously through job arrays. There are some restrictions:

It must be a batch job.
Job arrays should be explicitly named with -J
It is generally prudent to separate stdout and stderror with -o and -e

A job array is submitted with sbatch --array=<range>, where range is two digits separated by a hyphen.
bash
$sbatch --array=0-30 myjobs.sh
An increment can be provided
bash
$sbatch --array=1-7:2 myjobs.sh
This will number them 1, 3, 5, 7
It is also possible to provide a list
bash
$sbatch --array=1,3,4,5,7,9 myjobs.sh
Each job will be provided an environment variable SLURM_ARRAY_JOB_ID and
each task will be assigned a SLURM_ARRAY_TASK_ID.  The ARRAY_JOB_ID is the overall jobid, whereas the ARRAY_TASK_ID will take on the values of the numbers in the specified range or list.
Slurm also provides two variables %A (global array ID) and %a (array task ID) which can be used in the -o and -e options.  If they are not used, then the different tasks will attempt to write to the same file, which can result in garbled output or file corruption, so please use them if you wish to redirect streams with those options.
To prepare a job array, set up any input files using appropriate names that will correspond to the numbers in your range or list, e.g.
myinput.0.in
myinput.1.in
...
myinput.30.in
You would submit a job for the above files with
bash
$sbatch --array=0-30
In your Slurm script you would use a command such as
bash
python myscript.py myinput.${SLURM_ARRAY_TASK_ID}.in
The script should be prepared to request resources for one instance of your program.
Complete example array job script:
{{< code-download file=""/notes/slurm-from-cli/scripts/array.slurm"" lang=""bash"" >}}
To cancel an entire array, cancel the global ID
bash
scancel 1283839
You can also cancel individual tasks
bash
scancel 1283839_11
Useful Commands
When you submit a job and it doesn't start or fails for an unknown reason it could be due to restraints in your account. This could include running out of storage space or SUs on your allocation. Additionally, it's useful to see how busy the queue is. The following subsections highlight how to identify these problems.
Allocations
Sometimes it’s useful to check how many SUs are still available on your allocation. The allocations command displays information on your allocations and how many SUs are associated with them:
$ allocations
Account                      Balance        Reserved       Available                
-----------------          ---------       ---------       ---------
hpc_training                 1000000               0          999882
running allocations -a <allocation_name> provides even more detail on when the allocation was last renewed and its members. E.g.
```
$ allocations -a hpc_training
Description StartTime           EndTime    Allocated   Remaining  PercentUsed Active 

new         2024-05-29 17:33:13 2025-05-29 1000000.000 999881.524        0.01 True   
Name   Active CommonName                     EmailAddress        DefaultAccount         

.
.
.
```
Storage Quota
One way to check your storage utilization is with the hdquota command. This command will show you how much of your home, scratch, and leased (if applicable) storage are being utilized. Below is the sample output for hdquota:
$ hdquota
Type             Location         Name                                        Size Used Avail Use%
====================================================================================================
home             /home            mst3k                                       50G   16G   35G  32%
Scratch          /scratch         mst3k                                       12T  2.0T   11T  17%
This is a useful command to check whether you’re running out of storage space or to see where files need to be cleaned up. For more detailed information on disk utilization you may also use the du command to investigate specific directories.
Queue limits and Usage
To gain information on the different queues you can use the qlist command. This will show the list of partitions, their usage, and the SU charge rate. You can use qlimits for information on each queue’s limits.
The sinfo command will provide some more detailed information on the health of each queue and the number of active nodes available. These commands can be useful in diagnosing why a job may not be running, or to better understand the queue usage for more efficient job throughput. More information on hardware specifications and queue information can be found here on our website.
Need Help
Research Computing is ready to help you learn to use our systems efficiently.  You can submit a ticket.  For in-person help, please attend one of our weekly sessions of office hours."
rc-learning-fork/content/notes/slurm-from-cli/section1.md,"Resources and Partitions
An HPC job is a description of the resources required, any preparatory steps such as loading modules or otherwise setting up an environment, and the commands to run the software, along with any postprocessing that may be appropriate.
The job is specified through a special form of script often called a batch script.  Usually it is written in bash.
Resources include the quantity of time requested, the amount of memory, the number of cores per node, and if appropriate the number of nodes or the number and/or architecture of GPU.
In the abstract, a queue is a sequence of jobs to be prioritized and handled. In a cluster, a queue, which Slurm calls a partition, is implemented with a group of compute nodes that provide a particular set of resources.
Slurm is a software package that manages the resources of the cluster and balances the demands of many competing job requests.  It consists of a workload manager, often called a scheduler, and a slurmd ""daemon"" which runs on each node and handles the execution and monitoring of the jobs.
Cores, Nodes, and Tasks
Hardware
The Slurm model is a cluster consisting of a number of nodes.  Each node is a separate server.  These servers are similar to an ordinary desktop computer, but are more reliable and usually provide more memory and cores that an ordinary desktop.
A core is a computing unit. It is part of a cpu.  
{{< alert >}}
Slurm began when cpus had only one core each. Beginning around 2005, cpus began to be divided into multiple cores.  But Slurm still refers to cores as ""cpus.""
{{< /alert >}}
Memory refers to random-access memory.  It is not the same thing as storage.  If a process reports running out of memory, it means RAM memory. Running out of disk space will result in a different error.
For more details about the structure of a computational cluster, see our introduction.
Processes and Tasks
A process can be envisioned an instance of an executable that is running on a particular computer.  Most executables run only a single process.  Some executables run threads within the root process.
Slurm refers to the root process as a task. By default, each task is assigned to one core.
Slurm Resource Requests
SLURM refers to queues as  partitions .  We do not have a default partition; each job must request one explicitly.
{{< table >}}
| Queue Name | Purpose | Job Time Limit | Max Memory / Node / Job | Max Cores / Node |
| :-: | :-: | :-: | :-: | :-: |
| standard | For jobs on a single compute node | 7 days | 1462 GB | 96 |
| gpu | For jobs that can use general purpose GPU’s (A40,A100,A6000,V100,RTX3090) | 3 days | 1953 GB | 128 |
| parallel | For large parallel jobs on up to 50 nodes (<= 1500 CPU cores) | 3 days | 750 GB | 96 |
| interactive | For quick interactive sessions (up to two RTX2080 GPUs) | 12 hours | 216 GB |  96  |
{{< /table >}}
To see an online list of available partitions, from a command line type
bash
$qlist
A more detailed view of the partitions and their limits is available through the command
bash
$qlimits
Batch Scripts
Jobs are described to the resource manager in the form of a script.  Typically this is written in the bash scripting language.  Bash is the default shell on most Linux-based systems, which includes the majority of HPC systems, so it is expected to be available to interpret the script.  However, Slurm accepts scripts in other languages if the interpreter is available.  We will consider only bash scripts in this tutorial.
To prepare a job, the user writes a script. The top of the script is a preamble that describes the resource requests. The rest of the script contains the instructions to execute the job. The script is then submitted to the Slurm system. The Slurm workload manager examines the preamble to determine the resources needed, while ignoring the rest of the script. It uses the resource request along with a fair share algorithm to set the priority of the job.  The job is then placed into the requested partition to wait for the resources to become available.  
Once the job starts, the slurmd daemon runs the script as an ordinary shell script. The preamble consists of comments (code that is not executed by the interpreter) so they are ignored. The rest of the script must be a valid bash shell script."
rc-learning-fork/content/notes/slurm-from-cli/section2.md,"Writing Batch Scripts
Batch scripts should be written on a cluster login node.  Please do not use your local computer to write them, as they may not work.  You must also use a text editor and not a word-processing program.
Several options are available to prepare batch scripts.  
Graphical Editors
You can log in to a FastX, which provides a MATE desktop environment. One of the tools is a graphical editor very similar to Notepad.  It is called pluma by MATE, but we have made it available as gedit if started from a terminal.  If you wish to start it from a menu, it is available from Applications→Accessories.
You can also use Open OnDemand's built-in file manager and editor.  Create a new file from the Files menu.  Select the file and choose Edit from the three-dot dropdown menu to the right of the file name.  This will open a very basic text editor.
Command-Line Editors
Editors available at the command line are nano, vim, and emacs.  Nano is a simple text-only editor.  Vim is also available text-only from a command line, but a graphical version called gvim can be invoked from a MATE Desktop through the Applications→Accessories menu. Emacs can also be started from the Accessories menu but, if a graphical environment, will start a graphical user interface.  If invoked within a text-only environment, it will fall back to a text interface.
Open OnDemand Tool
For a user-friendly introduction to creating Slurm scripts on-demand, we have implemented a Slurm script generator on Open OnDemand. To access the generator, go through Utilities→Slurm Script Generator on the top bar. This will present you with a fillable web form that generates a text file in real time with the details of your resource requests. You can then download the script created by the generator to your local workstation once completed. You can upload it to the cluster using any file transfer method you prefer.
Our First Slurm Script
This example illustrates the main parts of a Slurm script. 
In a bash script, any text beyond the # is ignored.
{{< code-download file=""/notes/slurm-from-cli/scripts/hello.slurm"" lang=""bash"" >}}
This script runs the Python code
{{< code-download file=""/notes/slurm-from-cli/scripts/hello.py"" lang=""python"" >}}
The Hello.slurm Script
The first line says that this script should be run by the bash interpreter.
```bash
!/bin/bash
```
The lines starting with #SBATCH are the resource requests.  They are called ""pseudocomments"" since they have meaning to Slurm.  There must be no space between # and SBATCH and the string must start in the first column of the line. 
```bash
SBATCH --nodes=1
SBATCH --ntasks=1
SBATCH --cpus-per-task=1 # total cores per task
SBATCH --mem=32000 # mb total memory
SBATCH --time=2:00:00
SBATCH --partition=interactive
SBATCH --account=hpc_training
``
Here we are requesting
  * 1 node, 1 task, 1 core
  * 32GB of memory (measured in MB). Strictly speaking this will be ""Gibibyes.""
  * 2 hours of running time.
  * The interactive partition (queue).  A partition must be specified.
  * The account (allocation) grouphpc_training`
The next lines set up the environment to run our job.
bash
module purge
module load miniforge
It is good practice to purge all modules first, since Slurm ""remembers"" any modules set in the environment where the script is launched.  Next we load the module we need to run our program, the Python distribution Miniforge.
Finally, we execute our job.
bash
python hello.py
We have chosen to name this script hello.slurm, but it can have any name.
Exercise 1
Using the Open OnDemand Slurm Script Generator, create a slurm script with the following resource requests:
 * 1 node, 1 task, 1 core.
 * 32GB of memory.
 * 2 hours of running time.
 * The interactive partition (queue).
 * The account (allocation) group hpc_training.
Using the displayed text file, compare your slurm script with our example hello.slurm. The requested resources should be the same. Once completed, download your slurm script and transfer it to the cluster by whatever means you wish. Also, download hello.py and transfer it to the cluster as it will be needed later.
Common Slurm Directives
The most commonly used Slurm directives are listed in the table below.  Many options have two versions, one with a single hyphen - followed by one letter, or two hyphens -- followed by a word and an equals sign =.  Some commands have no single-letter equivalent.
Angle brackets < > indicate a value to be specified, and are not typed.
{{< table >}}
|  Single-hyphen Option | Double-Hyphen Option|  Action |
|  -----  | -----| ---- |
| -a \<list>  | --array=\<list> | This is a job array with parameters in \<list> |
| -c \<ncpus> | --cpus-per-task=\<ncpus> | Number of cpus (cores) to be assigned to each task.  For threaded code. Ensures all cores are on the same node.|
| -C \<list> | --constraint=\<list> | Specify certain resource constraints |
| -D \<directory> | --chdir=\<directory> | Change to \<directory> before starting the job. Default is directory from which job is started. |
| -e \<name> | --error=\<filename> | Separate standard error from standard output and print to file \<name> |
| None | --export=\<vars> | Specify which environment variables are to be exported. Other options are  ALL (the default) or  NONE |
| | --gres=\<list> | Specify ""generic consumable resources."" For example  --gres=gpu:2 |
|-J \<jobname> | --job-name=\<jobname> | Specify a name of your choosing for the job rather than the default script name. |
| None | --mail-type=\<type> | Notify me by email upon certain events. Options are NONE (default) BEGIN (job begins),  END (job ends), FAIL (job fails) , REQUEUE (job is requeued), or ALL |
| None | --mail-user=\<email> |   Specify the email for notifications. |
| None | --mem=\<size[units]> |   Specify the total memory request per node, over the entire node. The default unit is megabytes. |
| None | --mem-per-cpu=\<size[units]> | Memory request per allocated core. Default unit is megabytes. |
| -n \<number> | --ntasks=\<number> | Request a total number of tasks over all nodes allocated. |
| None | --ntasks-per-node=\<ntasks> | Request that a minimum of ntasks be assigned to each node. 
| -N \<nnodes> | --nodes=\<nnodes> | Request nnodes nodes. Should be used only with MPI or other protocols able to use them. |
| -o \<filename> | --output=\<filename> | Specify a name of your choosing for the standard output file rather than the default of slurm\<jobid>.out. | 
| -p \<name>| --partition=\<names> | Specify the partition to run the job. |
| -t \<time> | --time=\<time> | Set the upper limit of the runtime. Format can be M (a number of minutes), MM:SS (minutes:seconds), HH:MM:SS (hours:minutes:seconds), D-H (days-hours), D-HH:MM (days-hours:minutes), or D-HH:MM:SS (days-hours:minutes:seconds). |
{{< /table >}}
See also our documentation for many more examples.
Modules
Any application software that you want to use will need to be loaded with the module load command.  
For example:
$ module load matlab
$ module load miniforge
$ module load goolf R
Modules need to be loaded any time that a new shell is created to set up the same working environment. This includes every time that you log out and back in, and every time that you run a batch job on a compute node.
Module Details
module avail – Lists all available modules and versions.
module spider – Shows all available modules
module key keyword – Shows modules with the keyword in the description
module list – Lists modules loaded in your environment.
module load mymod – Loads the default module to set up the environment for some software.
module load mymod/N.M – Loads a specific version N.M of software mymod.
module load compiler mpi mymod – For compiler- and MPI- specific modules, loads the modules in the appropriate order and, optionally, the version.
module purge – Clears all modules.
Learning more about a Module
To locate a python module, try the following:
$ module avail python
$ module spider python
$ module key python
To find bioinformatics software packages, try this:
$ module key bio
The available software is also listed on our website
Exercise 2
Try typing the command python in a terminal window. Why was it unable to find the executable? Now, load a module of your choosing that has python. Try the python command again. Purge your current modules and try python again.
Use module spider R to show the available R modules and how to load them. Using this information, why does the command module load R give an error?
Open hello.slurm using any text editor you prefer and add the lines needed to purge existing modules, load a module that provides python, and execute the hello.py script. For reference, check our example hello.slurm.
Working with Files and Folders
When using Slurm in terminal mode, you will probably want to create your own folders to organize your Slurm scripts, any input files, and the output.  You will need to be able to move around from one folder to another at the terminal.
By default, Slurm will start your job from the folder in which it was launched. You can change that with the -D option (directory) but many users simply navigate to the folder and type commands.
Creating Files and Folders
There are several options to create, rename, and move your files and folders. Note that folders are usually called ""directories"" in Unix.
FastX
Use the Caja file manager.  This shows up as a filing-cabinet icon in the upper-left corner of the ribbon of the MATE Desktop.  It can also be started from the menu Applications→System Tools→Caja. Caja's layout is very similar in appearance and behavior to Windows Explorer and similar tools.
Open OnDemand
Use the File Manager to create, rename, or move your folders.
Command Line
If you are familiar with the command line, you can use that. If you wish to learn it, you can go through our Unix Tutorials for Beginners, especially Tutorials 1--3.  You can also go through our HPC from the Terminal tutorial if you have not already done so.
Changing into a Directory
If you do not wish to learn the full command-line navigation, you will need to learn the cd command to get to your folder for launching your job.
Log into a terminal in FastX, or open a terminal through the Clusters tab in Open OnDemand.
The cd command stands for ""change directory."" It is followed by a path to that directory. In the examples below, mst3k is a generic user ID. Substitute your own.
bash
$cd myworkdir
$cd /scratch/mstk3/myprojectdir
$cd
The cd command with no options returns you to the top level of your home directory.  
You may also wish to learn pwd for ""print working directory"" so you can find out where you are.
bash
$cd shakespeare
$pwd
/home/mst3k/shakespeare
Exercise 3
Use FastX or Open OnDemand or the command line to create a new folder under your scratch directory. Practice changing into and out of it. Move hello.slurm and hello.py into the newly created folder.
Use FastX and Caja to navigate to your /scratch directory. To get there, click Go in the Caja menu. A textbox will open. Be sure that “search for files” is unchecked. Erase whatever is in the textbox and type /scratch/mst3k (substituting your own user ID). Still in FastX, open a terminal (the black box, or in the System Tools menu) and navigate to your new scratch folder."
rc-learning-fork/content/notes/slurm-from-cli/section3.md,"Running Jobs from Scratch
We recommend that you run your jobs out of your /scratch directory.
 * Your personal /scratch/mst3k folder has much more storage space than your home directory. 
 * /scratch is on a Weka filesystem, a storage system designed specifically for fast access.
 * /scratch is connected to the compute nodes with Infiniband, a very fast network connection.
{{< alert >}}
The scratch system is not permanent storage, and files older than 90 days will be marked for deleting (purging). You should keep copies of your programs and data in more permanent locations such as your home directory, leased storage such as /project or /standard, or on your lab workstation. After your jobs finish, copy the results to permanent storage.
{{< /alert >}}
Submitting a Job
Once we have navigated to the desired working directory in a terminal window, we use the sbatch command to submit the job. This assumes that your Slurm script is located in the current working directory.
bash
sbatch myjob.slurm
The system returns a JOBID.
We do not make the script executable.  The system handles that.
bash
$sbatch myjob.slurm
Submitted batch job 36805
Always remember that you submit your job script and not your executable or interpreter script.
Exercise
From your working directory where  hello.slurm is, submit the job.
Monitoring a Job
Once submitted, we can monitor our jobs.
Graphical Interface
The Open OnDemand Job Viewer (Jobs tab→Active Jobs) shows a Web-based view of jobs.  You can switch the dropdown between ""All Jobs"" and ""Your Jobs.""  You can also use the Filter textbox to select jobs by partition or another criterion.  In the Filter textbox you can enter multiple strings, which acts as ""and.""
Clicking the right-pointing arrow on the left side will cause a dropdown box to appear that will show the job status (Pending, Running, Completed) along with much other useful information.
Remember that this is a Web page and you will need to reload it in order to see changes in status.
Command Line
We use the squeue command to check on jobs from the terminal.
bash
$squeue
This shows all jobs.  To narrow that down we can use the -u (user) option or the -p (partition) option.
bash
$squeue -u mst3k
$squeue -p gpu
Job status is indicated by 
  * PD  pending
  * R   running
  * CG  exiting
```no-highlight
JOBID PARTITION     NAME     USER     ST    TIME  NODES  NODELIST(REASON)
36805  standard   myjob.sl  mst3k    R    1:45    1     udc-aw38-34-l
```
Jobs should rarely be observed in the CG state. If they are caught in that state they cannot be canceled by the user.  Exiting jobs will not charge for the time spent in that state.
For more information on a running job, similar to what you can see from the OOD Job Viewer, use the scontrol command.
bash
scontrol show job <jobid>
Deleting a Job
Open OnDemand
From the Job Viewer find your jobs.  If the job is pending or running, a red trash-can icon will appear under the ""Actions"" header.  Click the icon.  A dialog box will appear asking you to confirm the cancellation.
Command Line
To cancel a job use the scancel with the job ID. You can use squeue -u $USER to obtain your job IDs, but you must know the JID of the specific job you wish to cancel.
bash
$scancel 36805 #jobID
Be aware that if a job fails due to a system failure the time will not be charged, but if you cancel your job, or it fails due to inadequate resource request, your allocation will be charged for the time expended.
Exercise 4
Write a Slurm script that requests 30 minutes of time. Submit a job that will run for at least 30 minutes. It can be some software you use; if you do not have anything set up yet, write the preamble and then add the line
bash
sleep 30m
as the command.  You won't need to request a specific amount of memory. Submit this script and monitor your job’s status in the queue with squeue or the Active Jobs tab. Once it starts, get information about your job with scontrol, let it run for a minute, then cancel it with scancel. Practice with the terminal commands or the OOD GUI. Note that you will need your job’s ID for the last two commands.
{{< spoiler text=""Example script"" >}}
{{< code-download file=""/notes/slurm-from-cli/scripts/slow.slurm"" lang=""bash"" >}}
{{< /spoiler >}}
Examining Your Utilization
When your jobs have finished, you may wish to find out how much of the resource you utilized.  Two commands can be used for this purpose, sacct and seff.
sacct
As the name suggests, sacct will return accounting information about your job.  It is built-in to Slurm and does not know about local policies such as SU charges, but it will show you information about the job. It only works for jobs that have ended.
With no options it will show output for jobs run on the current date.
```bash
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 

56220974      mpi.slurm   parallel  hpc_build         10     FAILED      9:0 
56220974.ba+      batch             hpc_build          5     FAILED      9:0 
56220974.0   mpiheated+             hpc_build         10     FAILED      1:0 
56220992      mpi.slurm   standard  hpc_build         10  COMPLETED      0:0 
56220992.ba+      batch             hpc_build         10  COMPLETED      0:0 
56220992.0   mpiheated+             hpc_build         10  COMPLETED      0:0 
56221184      mpi.slurm   standard  hpc_build         10  COMPLETED      0:0 
56221184.ba+      batch             hpc_build         10  COMPLETED      0:0 
56221184.0   mpiheated+             hpc_build         10  COMPLETED      0:0 
56221192      mpi.slurm   standard  hpc_build         10  COMPLETED      0:0 
56221192.ba+      batch             hpc_build         10  COMPLETED      0:0 
56221192.0   mpiheated+             hpc_build         10  COMPLETED      0:0 
```
For a particular job, use the -j option.
```bash
$sacct -j 56221192
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 

56221192      mpi.slurm   standard  hpc_build         10  COMPLETED      0:0 
56221192.ba+      batch             hpc_build         10  COMPLETED      0:0 
56221192.0   mpiheated+             hpc_build         10  COMPLETED      0:0 
```
For more detail, specify the -o option and a list of fields. The list of available fields is returned by sacct -e and is lengthy. For example, if I use only one allocation I may not be interested in that field.
```bash
$sacct -o jobname,jobid,ncpus,nnodes,maxrss,state,elapsed -j 56221192
   JobName JobID             NCPUS   NNodes     MaxRSS      State    Elapsed 

mpi.slurm 56221192             10        1             COMPLETED   00:00:34 
     batch 56221192.ba+         10        1      4824K  COMPLETED   00:00:34 
mpiheated+ 56221192.0           10        1    108800K  COMPLETED   00:00:33 
```
The output from sacct can be heavily customized. For more information see the documentation.
Running sacct puts a load on the system and can be very slow, so please use it judiciously.
seff
The seff command returns information about the utilization (called the ""efficiency"") of core and memory. The output of seff will be returned in an email if you use END in Slurm's emailing feature. 
bash
$seff 56221192
Job ID: 56221192
Cluster: shen
User/Group: mst3k/users
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 00:05:17
CPU Efficiency: 93.24% of 00:05:40 core-walltime
Job Wall-clock time: 00:00:34
Memory Utilized: 1.04 GB (estimated maximum)
Memory Efficiency: 1.18% of 87.89 GB (8.79 GB/core)
Under most circumstances, for a cpu-only job the ""CPU"" (core) efficiency should be around 90% or better.  Please contact us if it is significantly lower than that.  Note that seff may be relatively inaccurate for very short jobs.
Core efficiency is more problematic for GPU jobs, since the key to efficient GPU utilization is maximizing the GPU computations and minimizing CPU work. Seff does not provide a GPU utilization metric at this time, but we may be able to help you if you are concerned about GPU utilization.
If your memory utilization is low and you have requested a specified amount, use sacct -o with at least the MaxRSS field to double-check. If you do not need as much memory as you thought, you may be able to save SUs and have a shorter queue wait time if you decrease it. 
Stream Output in Slurm
When running a program interactively, any output to the Unix standard streams will be printed directly to the user's console window.  However, programs running under the control of Slurm will not have a console attached. 
By default, SLURM redirects both standard output and standard error to a file called slurm-<jobid>.out.
You can change the name of this file with the -o or --output option in your script.
```bash
SBATCH --output=
orbash
SBATCH -o 
```
You can also separate standard-error output. Even if your program does not use standard error (not many do), Slurm uses it, so you may wish to keep that output distinct.
```bash
SBATCH --error=
orbash
SBATCH -e 
```
Text from standard input must be redirected from a file in your command line in the script.
bash
./myexec < myinput.txt
As an alternative to the Slurm options, you can also redirect standard output in the usual Unix manner if you prefer.
bash
./myexec < myinput.txt > myoutput.dat"
rc-learning-fork/content/notes/slurm-from-cli/_index.md,"{{< figure src=""/notes/slurm-from-cli/img/slurm_logo.png"" width=30% >}}
Slurm is a  resource manager (RM), also known as a  queueing system.
Resource managers are used to submit jobs on a computing cluster to compute nodes from an access point generally called a  login node.
Login nodes are intended for editing, compiling, and very short test runs.  Production jobs go to compute nodes through the queueing system."
rc-learning-fork/content/notes/omero-hands-on/index.md,"{{< slideshow folder=""slides/omero-hands-on"" ext=""png"" >}}
Import Images
Images can be imported into OMERO individually or in batches. You can import images 
through the OMERO.insight desktop client, or through the command line with OMERO.cli tools.
Using OMERO.insight

Click the Importer tool icon in the top toolbar. This will open the OMERO Importer tool.

{{< figure src=""/notes/omero-hands-on/omero-import-icon.png"" >}}

In the left window pane of the Importer tool, navigate to the directory containing the images or folders of images 
that you want to import to the OMERO database. 

{{< figure src=""/notes/omero-hands-on/omero-import-leftpane.png"" >}}

Click on the files or folders containing the images you want to import. To select 
multiple items, hold down the Ctrl (Windows) or Command (Mac) key while clicking. Once you 
are finished making your selections, click the "">"" button. This will open the Import Location 
menu.

{{< figure src=""/notes/omero-hands-on/omero-import-select.png"" >}}

In the Import Location menu, you can choose a Project, Dataset, or Screen* to import your 
images to, allowing you to organize your images before they are uploaded to the OMERO database. 
If your desired Project/Dataset/Screen doesn't exist yet, you can click the New button 
to create a new one.

{{< figure src=""/notes/omero-hands-on/omero-import-location.png"" >}}

Once your data and locations are selected, click the Add to the Queue button. This 
will close the Import Location menu and return you to the Importer tool. Click the 
Import button in the bottom right corner of the Importer tool to begin uploading your images.

{{< figure src=""/notes/omero-hands-on/omero-import-addqueue.png"" >}}
{{< figure src=""/notes/omero-hands-on/omero-import-startimport.png"" >}}
You can monitor the progress of your import and begin viewing images in OMERO before the

entire upload is complete.
{{< figure src=""/notes/omero-hands-on/omero-import-progress.png"" >}}
Once imported, you will see any new Projects or Datasets you created in the left window pane.

If you expand the Datasets, thumbnail versions of your images will appear in the center window 
pane of the OMERO.insight app.
{{< figure src=""/notes/omero-hands-on/omero-importedimages.png"" >}}
Using the Command Line
You can also use OMERO.cli to import images.
To import an individual image:
omero import BloodCells/EOSINOPHIL/_0_1845.jpeg
To import all the EOSINOPHIL images:
omero import BloodCells/EOSINOPHIL
Note that when importing images through the Command Line, they are imported to the Orphaned Images 
bin by default. To send them to a dataset, we can use Targeted Imports. If the Dataset doesn't exist 
yet, then OMERO will create the new Dataset for you.
omero import BloodCells/EOSINOPHIL Dataset:name:EOSINOPHIL
Research Computing can help you create scripts for bulk import of your data!
Organize Images
In OMERO, images can be organized using one of two different structures:


Project > Dataset


Screen > Plate > Well


Images can be organized using the first structure in either OMERO.insight or OMERO.web with drag-and-drop. 
You can also organize images into Projects > Datasets using OMERO.cli tools or OMERO.py tools for Python.
Currently, images can only be organized into Screen > Plate > Well using OMERO.py.
Project > Dataset
Using OMERO.insight or OMERO.web

Create a new Project or Dataset by clicking the New icon in the top left-hand menu of the Projects tab.

{{< figure src=""/notes/omero-hands-on/omero-new-project.png"" >}}

Type in a Project/Dataset title. Optionally, you can also add a description. Once complete, click the Create button.

{{< figure src=""/notes/omero-hands-on/omero-create-project.png"" >}}

You can now click and drag a Dataset to the Project where you would like it to reside.

Screen > Plate > Well
A simple Python and bash script can be used to import images using this structure.
{{< figure src=""/notes/omero-hands-on/omero-screen-plate.png"" >}}
Research Computing can help you create scripts for importing images as wells in a plate.
Tags
Tags are keywords that you can attach to images. You can filter images by a particular tag or 
tags, allowing for easy targeted image retrieval.
Adding Tags

Click the New Tag icon in the top left-hand menu of the Tags tab.

{{< figure src=""/notes/omero-hands-on/omero-new-tag.png"" >}}

Type in a name for your tag (description optional). Click the Create button.

{{< figure src=""/notes/omero-hands-on/omero-create-tag.png"" >}}

To add a tag to an image, first select the image. Then click the ""+"" sign under the Tags tab in the right-hand menu.

{{< figure src=""/notes/omero-hands-on/omero-tag-image.png"" >}}

Select your desired tag(s) on the left-hand side of the Tags Selection menu and click the "">"" arrow to add the tag. Click the Save button to save your added tags.

{{< figure src=""/notes/omero-hands-on/omero-add-tag.png"" >}}
Filtering by Tag
You can filter images by tag using one of two methods. Tagged images will appear in the center panel of the OMERO app.


Select the desired tag(s) from the Tags tab. 


Enter your desired tag in the top Search bar.


{{< figure src=""/notes/omero-hands-on/omero-filter-tag.png"" >}}
Annotations
You can annotate images with comments that can then be viewed by collaborators. You can also 
annotate images with customizable key-value pairs to attach further information to your data.
Comments


To add a comment on a selected image, simply select an image/dataset/project and navigate to the Comments tab of the right-hand menu.


Type your comment into the empty text box.


Click the Comment button to add your comment.


{{< figure src=""/notes/omero-hands-on/omero-add-comment.png"" >}}
Key-Value Pairs


To add a Key-Value pair, select an image/dataset/project and navigate to the Key-Value Pairs tab in the right-hand menu.


Double-click a cell the left-hand side of the table to modify a key. Double-click the corresponding right-hand cell to modify its value. Click the green ""+"" to add another key-value pair.


{{< figure src=""/notes/omero-hands-on/omero-key-value.png"" >}}
Third-Party Software
OMERO is compatible with a variety of third-party software packages that are commonly used 
in image processing. The examples below show how you can use OMERO with Fiji and MATLAB.
Fiji/ImageJ
Fiji/ImageJ is open-source software for scientific image processing. OMERO can be installed as a plugin for easy image import and export.
Examples of using OMERO with Fiji can be found in the online tutorial for Image Processing with Fiji and Omero.
MATLAB
OMERO can be used with MATLAB for image import and export, as well as for annotating images.
The following example script shows an example pipeline where:


Images are imported from OMERO to MATLAB


Image contrast is enhanced


Images are binarized


Cells are automatically counted


Binarized images are exported to OMERO with cell count annotated to the image

"
rc-learning-fork/content/notes/bash-scripting/getting_started.md,"A shell script is a text file so should be created using a text editor.  Do not use a word-processing program such as LibreOffice Write. Shell scripts should always be created on the operating system (Linux, macOS) on which they will run.
The first line should be
```bash
!/bin/bash
``
The#!` is often called a shebang.
After the shebang, we can add some shell commands.  
Example
Here is a simple example of a script that clears the screen, pauses for three seconds, then displays a quote from a Website.
{{< code-download file=""/notes/bash-scripting/scripts/qod.sh"" lang=""bash"" >}}
If you have downloaded the file onto a Linux system, you can run it directly.  Otherwise, either transfer it to your intended system or copy and paste it into a plain text file named qod.sh. 
Linux shells are indifferent to file suffixes. The sh is a convention, but other suffixes may be used; for example, when writing a shell script for a resource manager such as Slurm, we may wish to use a .slurm suffix.
To run the script, type
bash
bash qod.sh
This invokes bash to execute your script. Alternatively, you can modify your script to make it executable and run it:
bash
chmod 755 myscript.sh
./quod.sh"
rc-learning-fork/content/notes/bash-scripting/out.md,"
Introduction
to Bash Scripting

Katherine Holcomb
October 18\, 2017
 www.arcs.virginia.edu 
Outline

Scripting Introduction
Bash Script Structure
Hands-on:
                * Executing a Script
                * Variables and Expressions
                * Conditionals
                * Comparisons (Integer Values and Strings)
                * Loops
                * Command Line Arguments
                * String Operations
                * Arrays
                * Dealing with Files

More Tutorials and Resources

Linux Shell Scripting Tutorial:
http://bash.cyberciti.biz/guide/Main_Page
http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html
Advanced Scripting Tutorial
http://tldp.org/LDP/abs/html/
Regex Tutorials
http://www.regular-expressions.info/
http://www.zytrax.com/tech/web/regex.htm
Sed and (g)awk tutorial
http://www.grymoire.com/Unix/Sed.html
http://www.grymoire.com/Unix/Awk.html



What is a Script, What Can it be used for?
A Bash script is a plain text file that contains instructions for the computer to execute.
Scripts are interpreted at runtime and executed line-by-line.  Scripts are not standalone executables but must be run through an interpreter.
Anything that can be executed or evaluated on the bash command line can be placed into a script.
Frequently used to automate repetitive tasks:
            * File handling\, data backups
            * Schedule computing jobs\, e\.g\. on UVA’s Rivanna High\- Performance Computing Cluster\.

How to write a script
1) Bash shell environment to execute scripts







2) Needed: Text Editor to create and edit script files
            * - vi\, vim\, nano
            * - gedit through FastX
            * - avoid Windows Notepad\, Microsoft Word

Our Workspace Setup
Basic Unix commands:
cd \      changes current directory to \
cd ~/           changes to your home directory
pwd         shows full path of current directory
mkdir –p \        create entire directory \
ls          lists non-hidden files in current directory
ls *.sh           lists non-hidden files that end in .sh
mv    move or rename file
cd ~/
pwd
mkdir scripts
cd scripts
pwd
ls
Bash Script Structure: A Simple Example
Create script hello.sh
#!/bin/bash
#This is a simple bash script
echo ""Hello world!""  #print greeting to screen
echo ""I am done now!""
The first line tells the system to start the shell and execute the commands.  It is sometimes called a  “shebang” . An alternative is:
#!/ usr /bin/ env  _ bash_
Scripts can be annotated with comments\, characters after the  #  character are ignored by the interpreter
The third and fourth line are commands.
Executing A Script
Suppose the previous script was in a file  hello.sh
By default it is just text.  You must explicitly make it executable:
chmod u+x hello.sh
Then you can just invoke it by name at the command line:
./hello.sh
Alternative:  Run via bash interpreter
bash hello.sh
Comments and Line Continuations
All text from the  #  to the end of the line is ignored
To continue a statement on to the next line\, type  \ \
#This is a comment
echo ""This line is too long I  \
want it to be on two lines""
Multiple statements can be placed on the same line when separated by the  ;
a=20 ;  b=3
Sourcing

If you execute the script as a standalone script\, it starts a new shell -- that is what the  shebang  does -- if you do not use that you can type
bash myscript.sh
Sometimes you just want to bundle up some commands that you repeat regularly and execute them within the  current  shell.  To do this you should  not include _ a  _shebang \, and you must  source  the script
. \"
rc-learning-fork/content/notes/bash-scripting/_index.md,"The bash interpreter is the default shell on Linux.  A shell is the intermediary between the user and the Linux operating system when working at the command line.  Shells also can be programmed, much like other interpreted languages such as Python or R.  A program for an interpreter is usually called a script.
Scripts are simple ways of bundling up a series of commands to run in order.  They can be used to automate repetitive tasks, to run programs in the background, to handle conditions the program may encounter, and so forth.
Anything a user can type on the command line can be part of a shell script.
Before"
rc-learning-fork/content/notes/benchmark-parallel-programs/index.md,"Motivation
True or false:

""If I use more cores/GPUs, my job will run faster.""
""I can save SU by using more cores/GPUs, since my job will run faster.""
""I should request all cores/GPUs on a node.""

Show answer
1. Not guaranteed. 
2. False! 
3. Depends.


New HPC users may implicitly assume that these statements are true and request resources that are not well utilized. The disadvantages are:

Wasted SU. All requested cores/GPUs count toward the SU charge, even if they are idle. (SU is the ""currency"" for resource usage on a cluster. The exact definition will be presented later.)
Long waiting time. The more resources you request, the longer it is likely to wait in the queue.
Unpleasant surprise to see little or no performance improvement.

The premise of parallel scalability is that the program has to be parallelizable. Please read the documentation of your program to determine whether there is any benefit of using multiple cores/GPUs. (Exception: A serial program may need more memory than a single core can provide. You will need to request multiple cores, but this is a memory-bound issue and is different from the execution speed. We shall not consider this scenario for the remainder of the tutorial.)
What is benchmarking? And why/when should I benchmark?
A benchmark is a measurement of performance. We will focus on the execution time of a program for a given task. (This is also known as strong scaling in parallel computing.) However, even for a serial program, a benchmark can still be useful when you run the program on different platforms. For instance, if a certain task takes 1 hour to complete on your laptop and 5 hours on Rivanna, there could be something wrong with how the program was installed on Rivanna.
But doesn't benchmarking cost SU? Two things. First, the dev partition is the perfect choice for benchmarking and debugging purposes, as long as you stay within the limits. Remember, you do not need to complete the entire task if it takes too long; a fixed subtask would do. This could be an epoch in machine learning training, a time step in molecular dynamics simulation, a single iteration, etc., instead of reaching full convergence. This way you can perform the benchmark without spending SU.
Second, if the benchmark cannot be performed on dev for whatever reason (e.g. even the smallest subtask would take more than 1 hour, or the job needs more cores than the limit), it is true that you will have to spend SU for benchmarking, but you may still gain in the long term, especially if you are running many production jobs of a similar nature (high-throughput). If you manage to prevent overspending say 10 SU per job, then after 10,000 jobs you would have saved 100k SU, an entire allocation. (Linear term trumps constant, eventually.)
Besides the amount of hardware, sometimes certain parameters of a program can have a huge impact on performance as well (e.g. batch size in machine learning training, NPAR/KPAR in VASP). You will need to find the optimum parameter to achieve the best performance specific to your problem. This may not translate across platforms - what's optimal on one platform can be suboptimal on another, so you must perform a benchmark whenever you use a different platform!
Benchmarking will also help you get a better sense of the scale of your project and how many SU it entails. Instead of blindly guessing, you will be able to request cores/GPUs/walltime wisely.
Concepts and Theory
Definitions
Denote the number of cores (or GPU devices) as $N$ and the walltime as $t$. The reference for comparison is the job that uses $N=1$ and has a walltime of $t_1$.
Speedup is defined as $s=t_1/t$. For example, a job that finishes in half the time has a speedup factor of 2.
Perfect scaling is achieved when $N=s$. If you manage to halve the time ($s=2$) by doubling the resources ($N=2$), you achieve perfect scaling.
On Rivanna, the SU (service unit) charge rate is defined as
$$SU = (N_{\mathrm{core}} + 2N_{\mathrm{gpu}}) t,$$
where $t$ is in units of hours.
We can define a relative SU, i.e. the SU relative to that of the reference:
$$\frac{SU}{SU_1} = \frac{Nt}{t_1} = \frac{N}{s}.$$
In the case of perfect scaling, $N=s$ and so the relative SU is 1, which means you spend the same amount of SU for the parallel job as for its serial reference. Since sublinear scaling ($s<N$) almost always occurs, the implication is that you need to pay an extra price for parallelization. For example, if you use 2 cores ($N=2$) and reduce the walltime by only one-third ($s=1.5$), then the relative SU is equal to $N/s=1.33$, which means you spend 33% more SU than the serial job reference. Whether this is worth it will of course depend on:

the actual value of $s$,
the maximum walltime limit for the partition on Rivanna, and
your deadline.

Amdahl's Law
A portion of a program is called parallel if it can be parallelized. Otherwise it is said to be serial. In this simple model, a program is strictly divided into parallel and serial portions. Denote the parallel portion as $p$ ($0 \le p \le 1$) and the serial portion as $1-p$ (so that the sum equals 1).
Suppose the program takes a total execution time of $t_1$ to run completely in serial. Then the execution time of the parallelized program can be expressed as a function of $N$:
$$t = \left[(1-p) + \frac{p}{N}\right] t_1,$$
where the time spent in the serial portion, $(1-p)t_1$, is irrespective of $N$.
The speedup is thus
$$s=\frac{t_1}{t} = \frac{1}{1-p+\frac{p}{N}}.$$
As $N\rightarrow\infty$, $s\rightarrow 1/(1-p)$. This is the theoretical speedup limit.
Exercise: a) Find the maximum speedup if 99%, 90%, 50%, 10%, and 0% of the program is parallelizable. b) For $N=100$, what is the relative SU in each case?
Show answer
a) 100, 10, 2, 1.11, 1.
b) First calculate the actual speedup (not the theoretical limit): 50.25, 9.17, 1.98, 1.11, 1.
Relative SU: 1.99, 10.9, 50.5, 90.1, 100.
Notice how the wasted SU increases dramatically.


Exercise: What is the theoretical limit in relative SU?
$$\lim_{N\rightarrow\infty} \frac{SU}{SU_1}$$
Hint: Consider cases $s<N$ and $s=N$ separately.
Show answer
For perfect scaling, the relative SU is equal to 1 for all values of N.
Otherwise, no limit/undefined/goes to infinity.


Tools
The performance is usually tracked by the execution time.
time
The time command is included in most Linux distributions. You can simply prepend it to whatever command you want to time. For example:
```bash
$ time sleep 5
real    0m5.026s
user    0m0.001s
sys     0m0.006s
```
Notice there are 3 lines of output - real, user, and sys. A good explanation of these can be found here; for this tutorial, it is sufficient to use the real time. (The CPU time is given by user + sys, which in this case is almost 0 because the command is just sleep.)
perf
A more dedicated tool for performance measurement is perf. Instead of a single measurement, it is more accurate to run a benchmark multiple times and take the average. The perf tool contains built-in statistical analysis. Advanced users please refer to the official tutorial. Load the module if you would like to use perf: module load perf.
If you just want to get a rough idea with an error bar of say 5-10%, time suffices. The task should last significantly longer than 1 second.
Examples
Multi-core: Gaussian
Gaussian is a quantum chemistry software package that can run on multiple CPU cores. This example is the open-shell vanilomycin test (#0709) included with the software.
Setup
First load the module:
bash
module load gaussian
If you are using Gaussian only through a course:
bash
module load gaussian/grads16
Copy the input file:
bash
cp $g16root/g16/tests/com/test0709.com .
Prepare a SLURM script (job.slurm):
```bash
!/bin/bash
SBATCH -A 
SBATCH -p standard
SBATCH -t 10:00:00
SBATCH -N 1
SBATCH -c 1
SBATCH -C skylake
module purge
module load gaussian  # or gaussian/grads16
hostname
grep -c processor /proc/cpuinfo
cp $g16root/g16/tests/com/test0709.com .
g16 -p=$SLURM_CPUS_PER_TASK test0709.com
```
Note:
- The number of cores (#SBATCH -c <num>) is passed through the $SLURM_CPUS_PER_TASK environment variable to Gaussian's -p flag. This ensures consistency. 
- $g16root is an environment variable made available to you after you load the Gaussian module.
- The time command is not needed here because Gaussian will time the job automatically.
Submit the job:
bash
sbatch job.slurm
Benchmark
Modify the SLURM script to use 4, 8, 16, and 32 cores. Submit these jobs. The walltime can be read from the test0709.log file.
bash
$ grep Elapsed test0709.log
 Elapsed time:       0 days  0 hours 15 minutes 35.4 seconds.
You should obtain similar results as follows:
|N|Time (s)|Speedup|Relative SU|
|---|---:|---:|---:|
|1 |12323.9 |1.00 |1.00|
|4 | 2968.3 |4.15 |0.96|
|8 | 1528.5 |8.06 |0.99|
|16|  935.4 |13.18|1.21|
|32|  842.2 |14.63|2.19|
Don't get excited about the apparent superlinear scaling ($s>N$ for $N=4,8$) - it is within the margin of error.
The speedup is plotted below. Notice the perfect scaling up to $N=8$. The scaling performance worsens beyond 8 cores and drastically beyond 16. This does not mean 8 is the magic number to use for all Gaussian jobs - it only applies to calculations of a similar nature.
{{< figure src=""gaussian.png"" width=""600px"" >}}
Exercise: Does this obey Amdahl's Law? Why or why not?
Multi-GPU: PyTorch
PyTorch can make use of multiple GPU devices through the DistributedDataParallel (DDP) backend. This example is based on our PyTorch 1.7 container using the Python script provided by PyTorch Lightning with minor modifications. (The container uses CUDA 11 which is compatible with Rivanna hardware after the December 2020 maintenance.)
Setup


Download the container. The following command will create a Singularity container pytorch_1.7.0.sif.
bash
module load singularity
singularity pull docker://uvarc/pytorch:1.7.0


Copy the following as a plain text file (example.py).


```python
import os
import torch
from torch import nn
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
import pytorch_lightning as pl
class LitAutoEncoder(pl.LightningModule):
def __init__(self):
    super().__init__()
    self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))
    self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))

def forward(self, x):
    # in lightning, forward defines the prediction/inference actions
    embedding = self.encoder(x)
    return embedding

def training_step(self, batch, batch_idx):
    # training_step defined the train loop. It is independent of forward
    x, y = batch
    x = x.view(x.size(0), -1)
    z = self.encoder(x)
    x_hat = self.decoder(z)
    loss = F.mse_loss(x_hat, x)
    self.log('train_loss', loss)
    return loss

def validation_step(self, batch, batch_idx):
    x, y = batch
    x = x.view(x.size(0), -1)
    z = self.encoder(x)
    x_hat = self.decoder(z)
    loss = F.mse_loss(x_hat, x)
    self.log('val_loss', loss)
    return loss

def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
    return optimizer

dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())
train, val = random_split(dataset, [55000, 5000])
autoencoder = LitAutoEncoder()
trainer = pl.Trainer(max_epochs=1, gpus=1)
trainer.fit(autoencoder, DataLoader(train), DataLoader(val))
```

Prepare a SLURM script (job.slurm) that runs the above Python script. We would like to download the MNIST data files first and exclude the download time from the actual benchmark. (More on this later.) Also specify a GPU type for consistency.

```
!/bin/bash
SBATCH -A 
SBATCH -t 00:10:00
SBATCH -N 1
SBATCH --ntasks-per-node=1
SBATCH -p gpu
SBATCH --gres=gpu:1
module purge
module load singularity
time singularity run --nv pytorch_1.7.0.sif example.py
```

Submit the job.
bash
sbatch job.slurm

Benchmark
Set download=False in example.py:
python
dataset = MNIST(os.getcwd(), download=False, transform=transforms.ToTensor())
Resubmit the job to set the reference ($t_1$). Next, to make use of multiple GPU devices, use the ddp backend:
python
trainer = pl.Trainer(max_epochs=1, gpus=2, distributed_backend='ddp')
For this example, use the same number in --ntasks-per-node and --gres=gpu in your SLURM script. Also add srun (important):
bash
time srun singularity run --nv pytorch_1.7.0.sif example.py
Results on a K80 GPU:
|N|Time (s)|Speedup|Relative SU|
|---|---:|---:|---:|
|1|226 |1.00   |1.00|
|2|134 |1.69   |1.18|
|3| 95 |2.38   |1.26|
|4| 76 |2.97   |1.35|
The speedup is plotted below. Notice how the deviation from perfect scaling (light diagonal line) increases with $N$.
{{< figure src=""ddp_k80.png"" width=""400px"" >}}
The same benchmark was performed on RTX 2080Ti:
|N|Time (s)|Speedup|Relative SU|
|---|---:|---:|---:|
|1|171|1.00|1.00|
|2|108|1.58|1.26|
|3|79|2.16|1.39|
|4|64|2.67|1.50|
|5|57|3.00|1.67|
|6|52|3.29|1.82|
|7|51|3.35|2.09|
|8|49|3.49|2.29|
|9|49|3.49|2.58|
|10|49|3.49|2.87|
Notice the plateau beyond $N=6$, which implies that you should not request more than 6 GPU devices for this particular task. (A good balance between speed and SU effectiveness may be $2\le N \le 4$.)
{{< figure src=""ddp_rtx.png"" width=""400px"" >}}
Exercise: Deduce the parallel portion $p$ of this program using Amdahl's Law.
Show answer
Using $s=3.49$ as the theoretical speedup limit, $p=1-1/s=0.71$.


The performance of K80 vs RTX 2080Ti is compared below. On a single GPU device, the latter is 30% faster. 
{{< figure src=""k80_rtx.png"" width=""400px"" >}}
Remarks
A complete machine learning benchmark would involve such parameters as batch size, learning rate, etc. You may pass a sparse grid to locate a desirable region and, if necessary, use a finer grid in that region to identify the best choice.
Exercise: Revisit the true-or-false questions at the beginning of this tutorial and answer them in your own words.
Exercise: A user performed a benchmark on the standard partition and determined that a serial job would take 10 days to complete and that the theoretical speedup limit is 4. The entire project involves 1,000 such jobs. Assume that all 1,000 jobs can start running immediately. (The standard partition has a walltime limit of 7 days. No job extensions can be granted.)
a) What is the minimum amount of SU needed to finish the entire project?
b) The user has a deadline of 3 days. How many cores should the user request per job? How many extra SU will need to be spent compared to the minimum in a)?
c) Suppose the user did not perform the benchmark and just randomly decided to use 20 cores per job. How much time and how many SU will the user spend for this project? Compare your answer with b).
d) Repeat b) but this time the deadline is in 50 hours.
Show answer
a) Since each job could not finish within the 7-day limit using 1 core, we need to find the smallest $N$ such that $t\le7$ days. On one hand, the restriction is
$$s=\frac{t_1}{t}\ge \frac{10}{7};$$
on the other hand, we know $s_{\max}=1/(1-p) = 4$ so $p=0.75$ and that
$$s= \frac{1}{0.25+\frac{0.75}{N}}.$$
Combining these two equations:
$$\frac{1}{0.25 +\frac{0.75}{N}} \ge \frac{10}{7}$$
$$0.25+\frac{0.75}{N} \le \frac{7}{10}$$
$$N \ge \frac{0.75}{0.7-0.25} \approx 1.67.$$
Since $N$ must be an integer, the smallest solution is 2. For $N=2$, we obtain
$$s= \frac{1}{0.25+\frac{0.75}{2}} = 1.6$$
and
$$t= t_1/s = 10\times24/1.6 = 150\ \mathrm{hours}$$
or 6 days and 6 hours. Hence, each job takes $Nt=2\times 150=300$ SU and the entire project needs 300k SU.



b) Using the derivation in a) we find
$$N \ge \frac{0.75}{\frac{3}{10}-0.25} = 15.$$
The total amount of SU is
$$15\times(3\times24)\times 1000 = 1.08\mathrm{M}.$$
Compared to the minimum, the user needs to spend an extra 780k SU. 


c) For $N=20$ we obtain
$$s= \frac{1}{0.25+\frac{0.75}{20}} \approx 3.48$$
and
$$t = 10\times24/3.48 = 69\ \mathrm{hours}$$
or 2 days and 21 hours.
Each job costs $20\times69=1.38$k SU and the entire project needs 1.38M SU. Compared to b) the project takes 3 hours less but at an additional cost of 300k SU.


d) Unfortunately, the user will not be able to meet the deadline, since even with an infinite amount of cores each job would take $10\times24/4=60$ hours.



The moral of this exercise is two-fold: do your benchmark and plan ahead!"
rc-learning-fork/content/notes/pytorch-hpc/basics.md,
rc-learning-fork/content/notes/pytorch-hpc/index.md,
rc-learning-fork/content/notes/matlab-optimization/index.md,"Matlab Course: Optimization Techniques in MATLAB
Documentation: Optimization Toolbox (product page)
Documentation: Global Optimization Toolbox (product page)


The Optimization Toolbox
Video: Optimization Toolbox
Defining Optimizations Problems
Optimization Theory Overview
Choose Problem-Based or Solver-Based Approach
Solver-Based Optimization Problem Setup
Solve a Constrained Nonlinear Problem
Solving Optimizations Problems
Optimization Toolbox Solvers
Local vs. Global Optima
Optimization Decision Table
Set and Change Options
Choosing the Algorithm
Solver Outputs and Iterative Display
Improve Results
Nonlinear Programming
Solve nonlinear optimization problems
Unconstrained Nonlinear Optimization Algorithms
Constrained Nonlinear Optimization Algorithms
Tutorial for the Optimization Toolbox
Optimizing a Simulation or Ordinary Differential Equation
Fit an Ordinary Differential Equation (ODE)
Linear and Quadratic Programming
Solve linear optimization problems
Linear Programming Algorithms
Minimize quadratic functions subject to constraints
Quadratic Programming Algorithms
Mixed-Integer Linear Programming
Solve linear optimization problems with integer constraints
Mixed-Integer Linear Programming Algorithms
Traveling Salesman Problem
Multiobjective Optimization
Minimize multiple objective functions subject to constraints
Multiobjective Optimization Algorithms
Generate and Plot a Pareto Front
Multi-Objective Goal Attainment Optimization
Least Squares and Equation Solving
Least-Squares (Model Fitting) Algorithms
Equation Solving Algorithms
Large-Scale Constrained Linear Least-Squares
Nonlinear Data-Fitting
Documentations and Resources
Additional Optimization Toolbox Resources
Global Optimization Toolbox
Video: Global Optimization Toolbox
Solving Optimizations Problems
Optimization Workflow
Comparison of Six Solvers
Solver Behavior with a Nonsmooth Problem
Set and Change Options
View Options
Parallel Computing
GlobalSearch and MultiStart
Global or Multiple Starting Point Search
MultiStart Using lsqcurvefit or lsqnonlin
Set Start Points for MultiStart
Surrogate Optimization
What Is Surrogate Optimization?
Surrogate Optimization with Nonlinear Constraint
Modify surrogateopt Options
Pattern Search
Direct Search
Pattern Search Climbs Mount Washington
Effects of Pattern Search Options
Genetic Algorithm
Genetic Algorithm
Genetic Algorithm Options
Optimize an ODE in Parallel
Particle Swarm
Particle Swarm
Optimize Using Particle Swarm
Specifying Options for particleswarm
Tune Particle Swarm Optimization Process
Simulated Annealing
Simulated Annealing
Minimization Using Simulated Annealing Algorithm
Simulated Annealing Options
Multiobjective Optimization
Multiobjective Optimization
Compare paretosearch and gamultiobj
Documentations and Resources
Additional Global Optimization Toolbox Resources
Parallel Computing and Optimization
Parallel Computing
Matlab on Rivanna"
rc-learning-fork/content/notes/rapids/index.md,"What is RAPIDS?
RAPIDS is a suite of open source software libraries developed by NVIDIA to accelerate data science pipeline on GPUs. Each component is modeled after its CPU counterpart with minimal code change for the user.
Components
|Component| CPU API | Note|
|---|---|---|
| cupy | NumPy, SciPy | Technically not part of RAPIDS |
| cuDF | Pandas | |
| cuML | Scikit-learn | |
| cuGraph | NetworkX | |
| cuSignal | SciPy Signal| |
| cuSpatial | GeoPandas | |
| cuxfilter | crossfilter | |
We will focus on the first three components in this introductory workshop. The others are more specialized.
Prerequisites

NVIDIA GPU with compute capability 6.0+ (Pascal)
CUDA 11

Installation
The rapidsai module is available on Rivanna. It is backed by a container based on NVIDIA NGC.
To install on your own machine please visit https://rapids.ai/start.html.
Usage
JupyterLab
Select the ""RAPIDS x.y"" kernel.
Command line
bash
module load singularity rapidsai
Exercises
Clone https://github.com/rapidsai/notebooks. (The full size is near 1 GB.) We have also prepared a notebook cudf.ipynb under /project/apps_data/rapids. 


In cudf.ipynb, compare the performance of pandas and cudf starting from N = 100. Explore the behavior by varying N. Beyond which order of magnitude does cudf outperform pandas?


Which method's relative performance remains fairly constant? In other words, the ratio of the pandas execution time to the cudf execution time does not change much with respect to N.


Which method has the highest performance boost using cudf?


Repeat all of the above for other data types.


In cuml/notebook/kmeans_demo.ipynb, compare the performance of scikit-learn and cuml by varying N. Beyond which order of magnitude does cuml outperform scikit-learn?


There is a cell that checks the accuracy of cuml versus scikit-learn. Is this necessary?


Feel free to explore other notebooks.


General question: Why does the performance of RAPIDS depend on N? Why does the CPU API outperform RAPIDS when N is not big enough?


Remark: JupyterLab vs batch job
The JupyterLab environment is interactive which is great for debugging and testing. However, if the queue is busy you may need to wait for a long time. If your code can be executed non-interactively, we recommend converting it into a Python script so that you can submit it as a batch job. 
Converting a notebook into a Python script
The following command will convert your notebook mynotebook.ipynb into mynotebook.py.
bash
module load anaconda
jupyter nbconvert --to script mynotebook.ipynb
You may need to comment out Jupyter magic commands (e.g. %%time) before converting.
Batch job
Prepare a SLURM script job.slurm:
```bash
!/bin/bash
SBATCH -A mygroup             # your allocation account
SBATCH -p gpu                 # partition
SBATCH --gres=gpu:1           # number of GPUs
SBATCH -N 1                   # number of nodes
SBATCH -c 1                   # number of cores
SBATCH -t 10:00:00            # time
module purge
module load apptainer rapidsai
change x.y to the actual version
apptainer run --nv $CONTAINERDIR/rapidsai-x.y.sif mynotebook.py
```
Submit the job via sbatch job.slurm.

References and further reading

Official documentation
Workshop: High Performance Programming in Python
"
rc-learning-fork/content/notes/matlab-deep-learning/index.md,"{{< figure library=true src=matlab-logo.png width=30% height=30% >}}
If you are using MATLAB on your desktop computer, make sure you have the Deep Learning Toolbox and 
Deep Learning Toolbox Model for AlexNet Network installed. You can go to the Add-On Explorer to install 
these packages.
Using the Sample Dataset
To use the images in the sample dataset, first unzip the folder and add the folder and 
subfolders to your path. This will make the files visible to MATLAB.
You can then change directory into the DeepLearning folder. This is where we will be working 
for the remainder of this tutorial.
```matlab
unzip('DeepLearning.zip')
addpath(genpath('DeepLearning'))
cd DeepLearning
```
Example 1. Using a Pretrained Network
AlexNet
AlexNet is a neural network that was developed by Alex Krizhevsky at the University of Toronto 
in 2012. AlexNet was trained for a week on one million images from 1000 different categories.
In this example we will load AlexNet into MATLAB and use it to classify some images.
1. Load AlexNet
Load the pretrained network AlexNet into your MATLAB workspace as a variable net.
matlab
net = alexnet;
2. Load the image
Load the first sample image into the workspace as a variable img.
matlab
img = imread('file1.jpg');
Optionally, you can also view the image.
matlab
imshow(img)
{{< figure src=/notes/matlab-deep-learning/matlab-schnauzer.png >}}
3. Resize the image
AlexNet was trained on images that are 227 x 227 pixels in size. This means any images we want 
to classify with AlexNet must also be this size.
```matlab
% See the size of the image
imgSize = size(img)
% Resize the image
img = imresize('img', [227 227]);
imshow(img)
```
{{< figure src=/notes/matlab-deep-learning/matlab-schnauzer-resized.png >}}
4. Classify the image
The classify function takes a neural net and an image as inputs and returns a categorical prediction
as an output. 
matlab
pred = classify(net, img);
Try classifying the other images in the SampleImages folder. What results do you get?
Example 2. Perform Transfer Learning
Transfer Learning
Transfer Learning is the process of modifying a pretrained neural network to 
Datastores
Datastores are repositories for collections of data that are too large to fit in memory. 
Instead of storing all the pixel data in memory, datastores allow us to store just the 
filepaths and to read the image data into memory as needed.
1. Create an image datastore
Create an image datastore. We can label these images based on the folders in which they are organized. 
matlab
imds = imageDatastore('flowers', 'IncludeSubfolders', true, 'LabelSource', 'foldernames');
We can preview the first image in our datastore imds.
matlab
imshow(preview(imds))
{{< figure src=/notes/matlab-deep-learning/matlab-flower.png >}}
We can also inspect the labels of our images by extracting the Labels.
matlab
imds.Labels
2. Split the data
When training on new data, we generally want to reserve some of the data for testing. These 
data will not be used in training so that we don't overfit the network -- that is, so that the 
network isn't just good at classifying images it's already seen before. The test images will be 
used to evaluate the network's performance.
Typically we want to split our dataset into two subsets: train and test. Usually we use 80% 
of the data for training and use the remaining 20% for testing.
The splitEachLabel function allows us to divide the data proportionally within each folder/label.
By default, splitEachLabel will split the images based on alphabetical order, so we can use 
the 'randomized' option to randomly assign images to the training and test sets.
matlab
[train, test] = splitEachLabel(imds, 0.8, 'randomized');
3. Modify layers of AlexNet
AlexNet is made of 25 distinct layers. We can inspect these layers by looking at the Layers 
attribute of net (the variable in which we loaded AlexNet).
matlab
layers = net.Layers
Layer 1 is the input layer, which is where we feed our images.
Layers 2-22 are mostly Convolution, Rectified Linear Unit (ReLU), and Max Pooling layers. This is where 
feature extraction occurs.
Layer 23 is a Fully Connected Layer containing 1000 neurons. This maps the extracted features to each 
of the 1000 output classes.
Layer 24 is a Softmax Layer. This is where a probability is assigned to the input image for each output class.
Layer 25 returns the most likely output class of the input image.
When starting with a pretrained network, we typically want to modify just the last few layers to suit our 
particular problem. The feature extraction layers will adjust themselves based on the images we are training on -- 
no need to modify them ourselves!
First, we want to create a new Fully Connected layer fc with 5 neurons -- one for each of our flower labels. 
We will then replace the Fully Connected layer in layers with fc.
matlab
fc = fullyConnectedLayer(5);
layers(23) = fc;
We also want to replace the last layer with a new classification layer.
matlab
layers(end) = classificationLayer;
4. Set the training options
Now we want to train the network with our training data and new layers. Before we begin 
training, we want to set our training options, or hyperparameters.
More documentation about the different options can be found here.
| Training Option    | Description |
|----                | ----        |
| Solver Name        | The solver for the training network. MATLAB allows us to use different optimizers:  Stochastic Gradient Descent with Momentum sdgm, RMSProp rmsprop, and Adam adam. |
| MiniBatchSize    | Size of the mini-batch used for each training iteration. Rather than train the network on the whole  training set for each iteration, we can train on mini-batches, or subsets of the data. |
| MaxEpochs         | Number of times the training algorithm passes over the entire training set. |
| Shuffle            | Optional shuffling of the training data. Shuffling the training data allows you to train over different mini-batches for each epoch. | 
| InitialLearnRate | This controls how we quickly the network adapts. Larger learning rates mean the network makes  bigger adjustments after each iteration. A rate that is too large can cause the network to converge  at a suboptimal solution, while a rate that is too small can make the network learn too slowly.|
| Verbose            | Set to true if you want progress printed to the Command Window. |
| Plots              | Display training progress plots with the training-progress option. |

We can set our desired training options in a variable called options using the trainingOptions function.
```matlab
options = trainingOptions('sgdm', ...
   'MiniBatchSize', 10, ...
   'MaxEpochs', 2, ...
   'InitialLearnRate', 3e-4, ...
   'Shuffle', 'every-epoch', ...
   'Verbose', false, ...
   'Plots', 'training-progress');
```
5. Train the network
Now that we have our options set we can begin training the network on our new dataset. We 
will call our new neural network flwrnet.
We will use the trainNetwork function to train the network. As inputs, we will use our training dataset train, our modified layers layers, and our training options options.
matlab
flwrnet = trainNetwork(train, layers, options);
It will take several minutes to train the network.
{{< figure src=/notes/matlab-deep-learning/matlab-training-progress.png >}}
6. Evaluating performance
After training has completed, we can evaluate the performance of the network flwrnet using the reserved 
test dataset test.
```matlab
% Classify our test dataset
preds = classify(flwrnet, test);
% Extract the actual labels of the test dataset
actual = test.Labels;
% Count the number of predictions that match the actual label
numCorrect = nnz(preds == actual);
% Determine the fraction of correct predictions
fracCorrect = numCorrect/length(actual)
```
We can also create a Confusion Matrix Chart, which shows us the number of correct predictions for each output class. The confusion matrix also shows us the breakdown of how incorrect predictions were classified.
matlab
confusionchart(actual,preds)
{{< figure src=/notes/matlab-deep-learning/matlab-confusion-matrix.png >}}
7. Improving Performance
With our initial training options, our resulting network has so-so performance. We can try improving the performance by adjusting the training options.
MaxEpochs: We can increase the number of epochs over which we train the network. Generally, the longer we train the dataset, the more performance improves.
InitialLearnRate: If we set our initial learning rate too high, we can cause the network to converge at a suboptimal solution. To improve performance, you can try dividing your initial learn rate by 10 and retrain the network.
MiniBatchSize: You can try adjusting the mini-batch size. Smaller values typically mean faster convergence but more noise in the training process. Larger batch sizes mean more training time but generally less noise.
There are other training options that you can try adjusting that are dependent on the solver you chose. These options can be explored in the MATLAB documentation.
You can also improve performance by testing your network as you are training it; this process is called validation. In addition to 
setting aside some data for testing after training is complete, we also set aside a validation set. Every few iterations of training, we will classify the images of our validation set and assess the accuracy of the network. This allows us to see how prediction accuracy improves not only on our training data, but also on data the network hasn't seen before. The validation data isn't used to modify any of our network layers--it's just a check to see how training is coming along.
Example 3. Build a neural network
In some cases it may make more sense to train a network from scratch. This is particularly true if your dataset is very different from those that were used to train other networks.
In this example we will train a neural network to classify images of numerical digits. This uses images built into the MATLAB Deep Learning Toolbox.
{{< figure src=/notes/matlab-deep-learning/matlab-digits.png >}}
1. Create an image datastore
First we will create a datastore containing our images.
```matlab
% Retrieve the path to the demo dataset
digitDatasetPath = fullfile(matlabroot, 'toolbox','nnet','nndemos','nndatasets','DigitDataset');
% Create image datastore
imds = imageDatastore(digitDatasetPath, 'IncludeSubfolders', true, 'LabelSource', 'foldernames');
```
2. Split the data into training and test datasets
matlabn
[train, test] = splitEachLabel(imds, 0.8, 'randomized');
3. Define the layers of your network (the network architecture).
matlab
layers = [...
    imageInputLayer([28 28 1])
    convolution2dLayer(5,20)
    reluLayer
    maxPooling2dLayer(2,'Stride',2)
    fullyConnectedLayer(10)
    softmaxLayer
    classificationLayer];
4. Set your training options.
matlab
options = trainingOptions('sgdm', ...
    'MaxEpochs', 20, ...
    'InitialLearnRate', 1e-4, ...
    'Verbose', false, ...
    'Plots', 'training-progress');
5. Train the network
matlab
net = trainNetwork(train, layers, options);
6. Evaluate performance
```matlab
preds = classify(net, test);
actual = test.Labels;
numCorrect = nnz(preds == actual);
fracCorrect = numCorrect/length(actual);
confusionchart(actual, preds)
```"
rc-learning-fork/content/notes/globus-data-transfer/share.md,"Globus users are able to share data with anyone with a Globus account. All UVA Rivanna and Ivy users have Globus accounts (authenticate with Netbadge).
External collaborators don’t need to be affiliated with an institution using Globus in order for you to share data with them. Anyone can create a personal Globus account using @globusid.org
{{< figure src=""/notes/globus-data-transfer/imgs/globus_collab.png"" width=50% >}}
The instructions below show how to create a shared endpoint, a folder in which collaborators can upload and download data. Shared endpoints may be public (visible to the world!) or accessible only to users with permission.
Instructions 

Select the file or folder you want to share.
Click the Share button.
Click ""Add a Guest Collection.""
Enter a name and description for the Shared Endpoint. This should be unique and easy to remember.
Click ""Create Guest Collection"".
Click ""Add Permissions  Share With"".
Enter the UVA or Globus ID of the user you want to share with. Your collaborators must have a Globus ID; email is not sufficient.
Click ""Add"" and ""Add Permission"".

Optional: Add write permissions so the user can upload data. Enter an email message to the recipient if you wish.
{{< figure src=""/notes/globus-data-transfer/imgs/globus_setup_guest_collection.png"" width=50% >}}"
rc-learning-fork/content/notes/globus-data-transfer/transfer_select_folders.md,"We are now ready to initialize our transfer.
Instructions

Select the files or folders you want to transfer.
Select the destination for your files.
Click the highlighted “Start” button.

Navigate to the folder or files to be transferred on the source (here the personal collection).  Chose the files or folders you wish to transfer.
{{< figure src=""/notes/globus-data-transfer/imgs/globus_select_folder.png"" caption=""Select files or folders to transfer"" width=50% >}}
Navigate to the target folder on the destination (here the UVA Standard Security Collection).  You may need to move through several levels to find your target folder."
rc-learning-fork/content/notes/globus-data-transfer/overview.md,"Globus is a non-profit service for secure, reliable research data management developed and operated by the University of Chicago and Argonne National Laboratory, supported by funding from the Department of Energy, NSF, and the NIH. With Globus, subscribers can move, share, & discover data via a single interface – whether your files live on a supercomputer, lab cluster, tape archive, public cloud or your laptop, you can manage this data from anywhere, using your existing identities, via just a web browser.
Globus started as a pure transfer tool with two strengths:

Fast transfers over good networks
Robust transfers over flaky networks

Globus now has the add functionality of:

Data sharing and flexible access control
Identity management
A web GUI, scriptable command line tool, and powerful API
"
rc-learning-fork/content/notes/globus-data-transfer/transfer_setup_folders.md,"First we must assign the source and destination collections.
{{< figure src=""/notes/globus-data-transfer/imgs/globus_search_for_collection.png"" caption=""Searching for collections"" width=50% >}}
Start typing into the Collection textbox. A search bar will appear. Type until you find your personal collection name in the dropdown.  A green icon to the left of the name indicates an active collection.  Red icons show inactive collections.
{{< figure src=""/notes/globus-data-transfer/imgs/globus_finding_collection.png"" caption=""Finding your collection"" width=50% >}}
Do the same thing for the UVA collection you are targeting. Check again for the green ""stack"" icon for an active collection. In this case we also see a green ""columns"" icon.  This indicates the collection is managed. 
{{< figure src=""/notes/globus-data-transfer/imgs/globus_uva_standard_security_collection.png"" caption=""Find the managed collection"" width=50% >}}"
rc-learning-fork/content/notes/globus-data-transfer/troubleshooting.md,"Some problems occur frequently. Here are a few tips to solve them:
{{< table >}}
| Common Issues | Solution |
| --- | --- |
| I have admin privileges on my Health System computer. Why isn’t the Globus installation working? | Sometimes the Health System firewall prevents Globus software from connecting. Ask HIT to remote in and complete the installation. |
| Why won’t my transfer to Ivy storage start? | Globus doesn’t work while connected to the High Security VPN. Disconnect while transferring data. |
| Globus is transferring folders but they’re all empty. | There is probably a file with bad permissions or characters in the filename. Choose “Skip files with errors” in the Transfer options |
| I can’t connect to UVA Standard Security Storage. | When leaving UVA, your Eservices account can expire before your email – meaning no Globus access. |
{{< /table >}}
{{< figure src=""/notes/globus-data-transfer/imgs/globus_transfer_option_skip_files_with_errors.png"" width=50% >}}"
rc-learning-fork/content/notes/globus-data-transfer/setup.md,"Set up Globus once it is installed on your computer.
Instructions

Start the Globus Personal Connector application.
Choose a label for consent and click “Allow”. (The label you choose doesn't really matter.)
Choose a name for your Personal Collection. This is the name that you will see in the Globus collections list, so choose something descriptive enough that you know what it is and can quickly find it by searching (e.g. Martinez-Lab-Workstation, Zhang-Personal-Laptop).
Do NOT click the High Assurance checkbox! The UVA Ivy Data Transfer Node (UVA IVY-DTN) is already configured for sensitive data transfer. Checking the box is redundant conflicts with the default configuration.  The checkbox is not required at all for the Standard Security Storage collection. 
Click ""Save"" then ""Exit Setup"".

{{< figure src=""/notes/globus-data-transfer/imgs/globus_setup_personal_connector.png"" caption=""Setting up the Globus Personal Connector"" width=700px >}}"
rc-learning-fork/content/notes/globus-data-transfer/transfer_start.md,"You can optionally click the transfer options box to set specific parameters for your transfer.

By default, transfers on UVA DTNs are synced (option 1) and encrypted (option 5) – no need to select them.

{{< figure src=""/notes/globus-data-transfer/imgs/globus_transfer_options.png"" width=50% >}}

Files with errors will cause the entire transfer to fail – skip files with errors instead (option 6).

{{< figure src=""/notes/globus-data-transfer/imgs/globus_transfer_option_skip_files_with_errors.png"" width=50% >}}


You can schedule one-time and regular transfers with Timer.


When ready, click the blue Start button that points in the appropriate direction. When complete, the new folder will appear in the destination pane.


{{< figure src=""/notes/globus-data-transfer/imgs/globus_completed_transfer.png"" width=50% >}}"
rc-learning-fork/content/notes/globus-data-transfer/terminology.md,"Globus Terminology
Collection: A set of data that is linked to one or more folders on your computer or a remote server.
Personal Collection: A group of folders on a PC or workstation where you installed Globus.
Personal Connector: A program that runs in the background on your computer and allows you to connect to Globus through the Web app.
UVA Standard Security Storage: The collection for Research Project and Research Value storage.
UVA IVY-DTN: The collection for Ivy Central Storage.
Shared Collection: A folder that you can share with other Globus users."
rc-learning-fork/content/notes/globus-data-transfer/monitor.md,"By clicking on the ""Activity"" tab, you can check on the progress of transfers, monitor the effective transfer speed, and look for any failures."
rc-learning-fork/content/notes/globus-data-transfer/transferring_files.md,"Files are transferred with the Globus File Manager Web App. There are three ways to access the app:

Go straight to https://app.globus.org/file-manager
Go to https://www.globus.org/ -> Log In (top right corner)
Click Globus icon in Toolbar -> Web: Transfer Files

These instructions summarize the steps to set up and start a transfer.

Click the “Collection” field.
Click the “Your Collections” tab.
Select your Personal Collection.
Click “Transfer or Sync to…” in the gray menu.
Click the second “Collection” field.
Search for and select “UVA Standard Security Storage” or “UVA IVY-DTN”.
Select the files or folders you want to transfer.
Select the destination for your files.
Click the highlighted “Start” button.
"
rc-learning-fork/content/notes/globus-data-transfer/installation.md,"To transfer data to and from your computer, you will first need to install Globus Personal Connect. The following links provide instructions for installing Globus Personal Connect based on your machine's operating system.
{{< table >}}
| Platform | Installation instructions |
| --- | --- |
| Mac | https://docs.globus.org/how-to/globus-connect-personal-mac |
| Linux | https://docs.globus.org/how-to/globus-connect-personal-linux |
| Windows | https://docs.globus.org/how-to/globus-connect-personal-windows |
{{< /table >}}
The screenshots and set of instructions below show how to navigate to the installation links from the Globus homepage.
Instructions

Go to https://www.globus.org/
Click “I Want To…” > “Enable Globus on my system”
Scroll down to “Globus Connect Personal” (light blue box) and click the “Get Globus Connect Personal” link
Scroll down to “Install Globus Connect Personal” (light blue box)
Click the link for your operating system and follow the installation instructions

Download the software for your operating system:
{{< figure src=""/notes/globus-data-transfer/imgs/globus_download_personal_connector.png"" caption=""Find the link to download for your operating system"" width=700px >}}
Install the software:
{{< figure src=""/notes/globus-data-transfer/imgs/globus_install_personal_connector.png"" caption=""Download the application"" width=700px >}}"
rc-learning-fork/content/notes/globus-data-transfer/advantages.md,"Globus provides a secure, unified interface to your research data. Use Globus to ""fire and forget"" high-performance data transfers between systems within and across organizations.
{{< figure src=""/notes/globus-data-transfer/imgs/globus_advantages.png"" width=50% >}}
There are many advantages to using Globus: 

The Globus web app has an easy-to-use point-and-click interface.
Transfers faster than SCP/SFTP (usually by a factor of two).
Globus continues interrupted transfers – no need to restart.
Globus allows you to schedule regular transfers.
Get email notifications for successful or failed transfers.
Globus accounts are free! Collaborators don’t need a sponsored UVA account to use Globus.
VPN is not needed to transfer to and from UVA systems.
Approved for transferring sensitive data (HIPAA, CUI).

VPN or no VPN?
UVA Anywhere/More Secure VPN is not necessary for Globus. With just the web app, you can control transfers between systems that have Globus Personal Connect or Server installed.

The VPN will slow down transfers between your computer and Rivanna.
The High Security VPN completely blocks transfers between your computer and secure Ivy storage.
"
rc-learning-fork/content/notes/globus-data-transfer/add_locations.md,"When you first set up Globus, it only has access to certain folders of your local drive. You can add additional locations such as mapped network drives or external hard drives in the Globus Options/Preferences menu.
Instructions

Right-click Globus icon in toolbar
Click “Preferences” (Mac) or “Options” (Windows)
Click the Access tab
Click the “+”
Select the drive location and click “Open”
Navigate to the drive in the File Manager

{{< figure src=/notes/globus-data-transfer/imgs/globus_connect_options.png caption=""Select Options or Preferences from the Globus Connect menu."" >}}
{{< figure src=/notes/globus-data-transfer/imgs/globus_connect_add_location.png caption=""After clicking the +, navigate to the folder you wish to add."" >}}
Tips for Navigating to Mapped Drives

Click the Up button in the File Manager to navigate to higher level directories
On a Mac, mapped network drives will typically be located at /Volumes/drive_name
In Windows, network drives will be mapped to a drive letter (e.g., C: or Z:)
In Globus, Z:\Drive_Name\my_files becomes /Z/Drive_Name/my_files
"
rc-learning-fork/content/notes/globus-data-transfer/logging_in.md,"Most activities with globus will require logging in at their Website.
Instructions

Open the Globus Website and click “Log In”
Choose ""Use your existing organizational login""
Start typing  “University of Virginia” then select it in the dropdown.
Log in using Netbadge

Start from the Globus home page:
{{< figure src=""/notes/globus-data-transfer/imgs/globus_homepage.png"" width=50% >}}
Log in with your organization login (Netbadge)
{{< figure src=""/notes/globus-data-transfer/imgs/globus_login.png"" width=50% >}}"
rc-learning-fork/content/notes/globus-data-transfer/_index.md,"{{< figure library=""true"" src=""globus.png"" width=50% >}}
This tutorial will cover data transfer to and from UVA Research Computing storage systems using Globus software. Topics include: installing Globus, transferring files, monitoring large transfers, and sharing data with collaborators."
rc-learning-fork/content/notes/matlab-data-science/index.md,"The first 3 links below are to the MathWorks resource page on data science. At the bottom of the main resource
page are links to short videos, including the data science tutorial shown in the second link below.
The 4 links following the data science examples link are to the MathWork's Matlab Academy online courses in data science.
The two 'onramp' links are introductions to machine learning and deep learning using Matlab.
The next two links are in-depth courses in machine learning and deep learning using Matlab. Next to each section is an
estimate of the amount of time it would take to go through the material in the corresponding link.
The last link is to a video of using Matlab for data analytics.
Matlab for Data Science
https://www.mathworks.com/solutions/data-science.html
Matlab Data Science Tutorial (7 short videos)
https://www.mathworks.com/videos/series/data-science-tutorial.html
Matlab Data Science Examples
https://www.mathworks.com/solutions/data-science/resources.html
Machine Learning Onramp (2 hours)
https://matlabacademy.mathworks.com/R2020a/portal.html?course=machinelearning
Deep Learning Onramp (2 hours)
https://matlabacademy.mathworks.com/R2020a/portal.html?course=deeplearning
Machine Learning with MATLAB (16 hours)
https://matlabacademy.mathworks.com/R2020a/portal.html?course=mlml
Deep Learning with Matlab (16 hours)
https://matlabacademy.mathworks.com/R2020a/portal.html?course=mldl
Data Analytics with MATLAB (1 hour)
https://www.mathworks.com/videos/data-analytics-with-matlab-99066.html"
rc-learning-fork/content/notes/hpc-intro/wrapup.md,"If you have questions you can visit one of our online office hours Zoom sessions. Click on the ""Join us via Zoom"" button when a session is open.  Current hours are
Tuesdays:   3 pm – 5 pm
Thursdays:  10 am – noon
For specific help you can submit a ticket (this may open Netbadge).
"
rc-learning-fork/content/notes/hpc-intro/_index.md,"UVA's primary resource for high-performance computing provides a platform for computationally-intensive research across a variety of disciplines. 
{{< figure src=""/notes/hpc-intro/img/rivanna_racks.png"" caption=""UVA's HPC cluster"" >}}"
rc-learning-fork/content/notes/matlab-data-visualization/index.md,"MATLAB is mathematical computing software that combines an easy-to-use 
desktop environment with a powerful programming language. MATLAB can be used for data 
cleansing and processing, as well as data visualization. This tutorial will cover 
1. importing data from a variety of file types and formats, 2. data cleansing and 
manipulation, and 3. data visualization techniques. 
Downloading the Data
Throughout the tutorial we will be working with data from the National Health and Nutrition 
Examination Survey (NHANES). Download nhanes_matlab.xlsx.
Data Import
Importing Tabular Data
readtable
Creates a table by reading column oriented data from a file
T = readtable(filename)
readtable creates one variable in the table T for each column in the file filename.
Wholly numeric columns will be converted to a numeric array; a cell array will be generated 
from a column containing any non-numeric values.
| Readable File Formats | File Extensions |
| -----                     |  ------             |
| Delimited text files      | .txt, .dat, .csv    |
| Spreadsheet files         | .xls, .xlsb, .xlsx  |
Example
data = readtable('nhanes_matlab.xlsx');
While readtable is capable of reading Excel files, you will need to use readmatrix if you 
need to specify sheet names or a range of data. Both of these functions output a table.
Importing Data from Multiple Files
datastore
Read large collections of data
A datastore is simply a reference to a file or set of files. You tell MATLAB where to look 
for files with the datastore command.
Single File
ds = datastore(filename)
Multiple Files
ds = datastore(directory)
The datastore ds has many properties that you can modify so that MATLAB reads your data 
correctly (e.g. treating -999 as a missing value instead of a numeric data point).
Datastores are also useful if you are working with such a large amount of data that you 
wouldn't be able to load it all into memory. With a datastore you can tell MATLAB to read 
in the data incrementally, whether it's file by file or in 100-line chunks (MATLAB reads data in 20,000 
line chunks by default).
To read in data using a datastore, use the read or readall commands.
Read data incrementally
data = read(ds);
Read all data referenced by datastore ds
data = readall(ds);
See the MATLAB documentation on datastores to learn more about customizing your 
data import.
Example
```matlab
% Create datastore
ds = datastore('nhanes_matlab.xlsx');
% Set ReadSize property in ds to 50 so we only read in 50 lines at a time
ds.ReadSize = 50;
% Read in first 50 lines
data50 = read(ds);
% Read in next 50 lines
data100 = read(ds);
% Read in all data
data_all = readall(ds);
```
Importing Unstructured Data
Suppose you have an unstructured data file like the one below.
{{< figure src=""/notes/matlab-data-visualization/matlab-dataviz-1.png"" >}}
MATLAB is unable to read 
data automatically if each line doesn't have the same number of columns. We can use MATLAB's lower-level file import functions to read irregular data.
Using low-level file import requires three steps:


Open file (fid = fopen(filename), fid stands for file ID)


Read data


Close file (fclose(fid))


The first and last steps are pretty straightforward, so the rest of this section will 
focus on step 2. There are a couple ways we can read in the data.
fgetl
Read line from file
myLine = fgetl(fid)
Using in succession will allow you to continue reading the file line by line. Regardless of 
whether the data is numeric, the output of fgetl will be a string. This means you may have to 
parse and convert the data to the proper data type after import. You can learn more about 
this process from MATLAB's documentation on string manipulation.
textscan
Read formatted data from file
myData = textscan(fid, formatSpec)
textscan allows you to specify the format of a line of data up-front so that you don't have to 
manipulate strings unnecessarily. textscan also allows you to read multiple lines and to skip 
any columns you don't need. 
The output of textscan is a cell array (myData) where each cell contains the values from a 
single column. Each cell will contain a column vector (for numeric data) or column cell array (for non-numeric or mixed 
data).
textscan requires you to specify the format of your data in the variable formatSpec.
Below is a formatSpec for some example data.
{{< figure src=""/notes/matlab-data-visualization/matlab-dataviz-2.png"" >}}
```matlab
% This dataset is part of your installation of MATLAB!
% fullfile retrieves the full file path to the dataset.
filename = fullfile(matlabroot, 'examples', 'matlab', 'scan1.dat');
% Open the file
fid = fopen(filename);
% Format spec: it's a string
formatSpec = '%{MM/dd/uuuu}D %s %f32 %d8 %u %f %f %s %f';
% Read the data into using textscan
myData = textscan(fid, formatSpec);
% Close the file
fclose(fid);
```
More information about MATLAB's low-level file I/O can be found here: https://www.mathworks.com/help/matlab/low-level-file-i-o.html.
Data Cleansing
Working with Missing Data
When MATLAB imports data that has missing values for numeric variables, it replaces that 
instance with NaN, or Not-a-Number. This section discusses multiple ways you 
can handle missing data and NaNs.
Omitting NaNs
Calculating statistics on arrays that contain NaN results in another NaN. If we want to omit NaNs 
from our calculation, we can use the 'omitnan' option.
Example: Calculating mean
```
avgIncome = mean(data.Income, 'omitnan');
```
Other functions that can use the 'omitnan' option:
| Function Name | What It Does       |
| ---           | ---                |
| cov           | Covariance         |
| mean          | Mean               |
| median        | Median             |
| std           | Standard Deviation |
| var           | Variance           |
However, max and min omit NaNs by default, and adding the 'omitnan' flag will yield 
unexpected results.
Locating Missing Data and Deleting Incomplete Rows
ismissing
Find missing values in a table
TF = ismissing(A)
ismissing returns a logical array TF that is the same size as the table A. Values of 1 
in TF correspond to missing values in A at the same location.
any
Find non-zero elements in an array
missingRows = any(TF, 2)
any returns a logical array missingRows that is the same length as the input array TF.
Values of 1 in missingRows correspond to rows in TF that contain a 1. Because 1s in TF 
correspond to missing values in our original table A, values of 1 in missingRows also correspond 
to rows with missing data in A.
We have the number 2 as the second input in any. This is because by default any looks for 
non-zero elements in a column. Since we want to look for non-zero elements in rows, we need to specify 
that with the 2.
Logical Indexing
Remove rows with missing data
A(missingRows,:) = [];
Using our logical array missingRows, we can index into our table A and select all the 
rows in A that have missing data. With the colon operator :, we can also select the data from all 
the columns in those rows. If we select that data in A and set it equal to empty brackets, that will 
remove all those rows from A.
Example
```
% Read in data as table
data = readtable('nhanes_matlab.xlsx');
% Find missing data
missing = ismissing(data);
% Find rows that have missing data
missingRows = any(missing, 2);
% Remove rows with missing data from table
data(missingRows,:) = [];
```
Categorical Data and Set Operations
categorical
Assigns a value to each of a finite set of discrete categories
Consider the cell array below.
mySet = {'low', 'medium', 'low', 'low', 'high', 'medium', 'low'};
As humans, we understand that the array contains values that fall into 3 distinct categories: 
'low', 'medium', and 'high'. MATLAB doesn't necessarily know this and will treat all seven items in the 
array as individual values. With the categorical function, we can tell MATLAB to treat values with 
the same string as part of a single category. The output of the categorical function is a 
categorical array the same size as the input array.
```
mySet = categorical(mySet);
categories(mySet)
```
With the categories command, we can find out the different categories in our categorical array. 
As expected, our three categories are 'low', 'medium', and 'high'.
We can convert the text variables in our table to categorical arrays one at a time with the categorical command.
```matlab
% Reading the data into a table
data = readtable('nhanes_matlab.xlsx');
% Convert Gender variable to categorical array
data.Gender = categorical(data.Gender);
```
convertvars
Batch convert table variables to categorical arrays
T2 = convertvars(T1, vars, datatype)
We can use convertvars to create a new table T2 that converts all the variables in our table T1 to our desired data 
type, in this case categorical arrays. We list the names of the variables we want to convert in 
the cell array vars.
Example
```matlab
% Reading the data into a table
data = readtable('nhanes_matlab.xlsx');
% Convert text variables to categorical arrays
vars = {'Gender', 'Race'};
newdata = convertvars(data, vars, 'categorical');
```
We can replace vars with @iscell if we know we want to convert all cell arrays to in our 
table to categorical arrays.
newdata = convertvars(data, @iscell, 'categorical');
Why Use Categorical Arrays?

Several discrete data plot types require input data be categorical
Use less memory
ismissing is able to determine missing data in categorical arrays but not cell arrays

Analyzing Groups within Data
MATLAB Academy Exercises
Data Visualization
We will be looking at different examples of data visualization in MATLAB using a live script. 
Please download the script from this link.
plot and Modifying Plot Line Properties
Functions for Customizing Appearance


Figure Formatting GUI


Exporting and Saving Figures


Log-Scaled Axes


Bar Plots, Box Plots, and Histograms


Scatter Plots


Scatter Plot Matrix


3-D Surface Plots


Animation


Extra Exercises
NHANES


Create a scatter plot of Height vs Weight. Include labels on both axes and a title for your graph.


Create a new table in which all rows containing missing data, categorical or numerical, have been removed.


Create a scatter plot matrix to compare Weight, Height, and BPSys.


Create stacked bar plots showing the proportions of the Highest Level of Education reached at each Income.


Discretizing Continuous Data
Review Project: Fuel Efficiency
3D Data Visualization
The Graphics Objects Hierarchy"
rc-learning-fork/content/notes/biopython/index.md,"Introduction
From the official Biopython project website:

Biopython is a set of freely available tools for biological computation written in Python by an international team of developers.It is a distributed collaborative effort to develop Python libraries and applications which address the needs of current and future work in bioinformatics. The source code is made available under the Biopython License, which is extremely liberal and compatible with almost every license in the world.

This workshop assumes a working knowledge of the Python programming language and basic understanding of the concepts of online DNA and Protein sequence repositories.
Introductions to Python can be found here and here.

Getting Started
Python code examples
The Python scripts and data files for this workshop can be downloaded from here. On your computer, unzip the downloaded folder and use it as working directory for this workshop.
Python programming environment
The Anaconda environment from Anaconda Inc. is widely used because it bundles a Python interpreter, most of the popular packages, and development environments. It is cross-platform and freely available. There are two somewhat incompatible versions of Python; version 2.7 is deprecated but still fairly widely used. Version 3 is the supported version. 
Note: The latest Biopython package version (1.77+) requires Python 3.


Visit the Anaconda download website and download the installer for Python 3 for your operating system (Windows, Mac OSX, or Linux). We recommend to use the graphical installer for ease of use.


Launch the downloaded installer, follow the onscreen prompts and install the Anaconda distribution on your local hard drive.


The Anaconda Documentation provides an introduction to the Anaconda environment and bundled applications. For the purpose of this workshop we focus on the Anaconda Navigator and Spyder. 
Navigator
Once you have installed Anaconda, start the Navigator application: 
* Instructions for Windows
* Instructions for Mac
* Instructions for Linux
You should see a workspace similar to the screenshot, with several options for working environments, some of which are not installed. We will use Spyder which should already be installed. If not, click the button to install the package.

Spyder
Now we will switch to Spyder. Spyder is an Integrated Development Environment, or IDE, aimed at Python. It is well suited for developing longer, more modular programs. 

To start it, return to the Anaconda Navigator and click on the Spyder tile. It may take a while to open (watch the lower left of the Navigator). 
Once it starts, you will see a layout with an editor pane on the left, an explorer pane at the top right, and an iPython console on the lower right. This arrangement can be customized but we will use the default for our examples. Type code into the editor. The explorer window can show files, variable values, and other useful information. The iPython console is a frontend to the Python interpreter itself. It is comparable to a cell in JupyterLab.


Installation of the Biopython package
It is recommended to install the biopython package from PyPI using the pip install command. Detailed instructions are available here.
On your own computer:
Start the Anaconda Prompt command line tool following the instructions for your operating system.
* Start Anaconda Prompt on Windows
* Start Anaconda Prompt on Mac, or open a terminal window.
* Linux: Just open a terminal window.
At the prompt, type the following command and press enter/return:
bash
pip install biopython
This command will install the latest biopython package version in your current Anaconda Python environment.
On Rivanna (UVA's HPC platform):
Rivanna offers several Anaconda distributions with different Python versions. Before you use Python you need to load one of the anaconda software modules and then run the pip install command. 
bash
module load anaconda
pip install --user biopython
Note: You have to use the --user flag which instructs the interpreter to install the package in your home directory. Alternatively, create your own custom Conda environment first and run the pip install biopython command in that environment (without --user flag) 
Testing the Biopython Installation
Start the Spyder IDE (see here). In the IPython console pane, type the following command and press enter/return:
import Bio
print (Bio.__version__)
If the package is installed correctly, the output will show the biopython version number.

Bio Subpackages and Classes
The Bio package provides a large number of subpackages providing specific functionality. The Biopython website provides a full list of all subpackages. 
The following table shows an excerpt of that list relevant for this workshop.
| Subpackages/Classes | Purpose |
| - | - | 
| Bio.Entrez | Functions to retrieve Entrez records and associated data |
| Bio.ExPASy | Tools to access data hosted on the ExPASy protein databases |
| Bio.SwissProt | Tools to work with the sprotXX.dat file from SwissProt |
| Bio.Seq | Sequence datastructure (immutable=read-only) |
| Bio.MutableSeq | Sequence datastructure (mutable=modifiable |
| Bio.SeqRecord | Datastucture for Seq object plus enriched information |
| Bio.SeqIO | Read/write sequences (various file formats )| 
| Bio.AlignIO | A new multiple sequence Alignment Input/Output interface for BioPython 1.46 and later |
| Bio.Align.MultipleSeqAlignment | Tools for Code for working with sequence alignments |

Online Datasets and Databases
The Bio module provides several classes to process output and datasets provided by the following web services and tools:
* FASTA
* Blast output – both from standalone and WWW Blast
* Clustalw
* GenBank
* PubMed and Medline
* ExPASy files, like Enzyme and Prosite
* SCOP, including ‘dom’ and ‘lin’ files
* UniGene
* SwissProt
In this workshop we will explore options to download nucleotide and protein sequences from  Entrez and ExPASy.

Accessing NCBI's Entrez Databases
The Bio.Entrez submodule provides access to the Entrez databases. When you use this module you need to know the String descriptor of the database you want to query (aka its name).  A list of valid database names is provided in  column three of this table.
Note: Please review the General Introduction to the E-utilities for accessing the Entrez Application Programming Interface Program. The E-utilities limit the frequency of API calls and your IP address may be blocked if you continuously exceed the limit.
Basic Steps:

Provide an email address. This is required! Entrez.email = ""your@somewhere"".
Use an E-utility to get a handle for the data of interest, e.g. handle = Entrez.esearch(...).
Use handle to read or parse data with handle.read() or handle.parse().
Close the handle with handle.close().

Also read ""What the heck is a handle?""
Find Database Records:
Let's find the protein records associated with the human Pax6 gene and download the associated sequences in FASTA format.
To search the database we use the Entrez.esearch function.  We need to specify the database via the db, argument and specify a search term (provided as a list of Strings). 
The following code is available in the entrez-fasta.py file.
```python
from Bio import Entrez
Entrez.email = ""YOU@SOMEWHERE.com"" # your email address is required
handle = Entrez.esearch(db=""protein"", term = [""Homo sapiens[Orgn] AND pax6[Gene]""], usehistory=""y"")
record = Entrez.read(handle)
handle.close()
iterate over items
for k,v in record.items():
    print (k,v)
```

Output

```bash
Count 88
RetMax 20
RetStart 0
QueryKey 1
WebEnv MCID_5f87b7433f6876111801786c
IdList ['1587062735', '1587062729', '1587062727', '1587062721', '1587062719', '1587062717', '1587062715', '1587062713', '1587062710', '1587062708', '1587062706', '1587062703', '1587062701', '1587062699', '1587062697', '1587062695', '1587062690', '1587062688', '1587062686', '1587062684']
TranslationSet [{'From': 'Homo sapiens[Orgn]', 'To': '""Homo sapiens""[Organism]'}]
TranslationStack [{'Term': '""Homo sapiens""[Organism]', 'Field': 'Organism', 'Count': '1423829', 'Explode': 'Y'}, {'Term': 'pax6[Gene]', 'Field': 'Gene', 'Count': '2621', 'Explode': 'N'}, 'AND']
QueryTranslation ""Homo sapiens""[Organism] AND pax6[Gene]
```


The search results are returned as a dictionary and we can retrieve the list of unique IDs that match our query via record[""IdList""]. 
Note: The IdList returned by esearch is limited to the top 20 hits by default (defined by retmax). There are two workarounds:
1. Use the retmax=<number> keyword argument to increase the maximum number of retrieved records. The problem is you need to know what a reasonable number is. 
2. Or better, use the usehistory='y' keyword argument. This will save the search results on the remote server and provide WebEnv and QueryKey entries that can be used with the efetch function (see next section) to retrieve all search records (beyond the top 20).
By default the returned IDs reflect the GI numbers. The accession.version numbers can be retrieved instead by passing idtype='acc' as an optional keyword argument to the esearch function. See the detailed documentation of the esearch function here.
Download and save sequences as FASTA file:
With the ID list in hand, we can now download the sequence records using just a few lines of code and save them in a single multi-sequence FASTA file. The efetch function is used when you want to retrieve a full record from Entrez. 
```python
fetch records using id list
handle = Entrez.efetch(db=""protein"", rettype=""fasta"", retmode=""text"", id=record[""IdList""])
result = handle.read()  # return type is simple string
handle.close()
remove empty lines
fastaseq = result.replace(""\n\n"",""\n"")
with open('HsPax6-protein.fasta', 'w') as f:
   f.write(fastaseq)
```
Alternatively, we can pull individual sequences one at a time and save each sequence into a separate file. To do this we implement a for loop that iterates over this list and use the Entrez.efetch function to retrieve the FASTA sequence record associated with each id.  We wrap this for loop in an open file operation block to save the retrieved FASTA records into a single .fasta text file.
Let's retrieve the nucleotide sequences of our previous top 5 ID hits as GenBank files. We specify the database with the db=""nucleotide"" and format with the rettype=""gb"" keyword arguments. 
The code is provided in the entrez-genbank.py file.
```python
from Bio import Entrez
Entrez.email = ""YOU@SOMEWHERE.com"" # provide your email address 
handle = Entrez.esearch(db=""nucleotide"", term = [""Homo sapiens[Orgn] AND pax6[Gene]""], retmax=5, usehistory=""y"")
record = Entrez.read(handle)
handle.close()
for k,v in record.items():
    print (k,v)
iterate over ids in list
for seq_id in record[""IdList""]:
    # get entry from Entrez
    print (seq_id)
    handle = Entrez.efetch(db=""nucleotide"", id=seq_id, rettype=""gb"", retmode=""text"")
    result = handle.read()
    handle.close()
    # save
    filename = f""HsPax6-{seq_id}-nucleotide.gb""
    print (""Saving to file:"", filename)
    with open(filename, 'w') as gbfile:
        # append fasta entry
        gbfile.write(result.rstrip()+""\n"")
```

Output:

```bash
Count 106
RetMax 5
RetStart 0
QueryKey 1
WebEnv MCID_5f87bb652dd862297e1727ea
IdList ['1844161464', '1844139642', '1844139635', '1844139631', '1844139629']
TranslationSet [{'From': 'Homo sapiens[Orgn]', 'To': '""Homo sapiens""[Organism]'}]
TranslationStack [{'Term': '""Homo sapiens""[Organism]', 'Field': 'Organism', 'Count': '27682287', 'Explode': 'Y'}, {'Term': 'pax6[Gene]', 'Field': 'Gene', 'Count': '3232', 'Explode': 'N'}, 'AND']
QueryTranslation ""Homo sapiens""[Organism] AND pax6[Gene]
1844161464
Saving to file: HsPax6-1844161464-nucleotide.gb
1844139642
Saving to file: HsPax6-1844139642-nucleotide.gb
1844139635
Saving to file: HsPax6-1844139635-nucleotide.gb
1844139631
Saving to file: HsPax6-1844139631-nucleotide.gb
1844139629
Saving to file: HsPax6-1844139629-nucleotide.gb
Done
```


Note that the record['IdList'] may not represent all the records. Remember that the  record['WebEnv'] and record['QueryKey'] entries provide access to the search history on the remote server.  So we can use these instead of the record['IdList'] to get all records.
```
Alternative: fetch all records using search history
handle = Entrez.efetch(db=""protein"", rettype=""fasta"", retmode=""text"", webenv=record[""WebEnv""], query_key=record[""QueryKey""])
```
Exercise: Find and download the top 10 FASTA EST nucleotide sequences for the mouse (Mus Musculus) TP53 tumor suppressor. Hint: look up the EST database descriptor in this table.

Solution:

```python
handle = Entrez.esearch(db=""nucest"", term = [""Mus musculus[Orgn] AND tp53[Gene]""], retmax=10, usehistory=""y"")
record = Entrez.read(handle)
handle.close()
for k,v in record.items():
    print (k,v)

# iterate over ids in list
for seq_id in record[""IdList""]:
    # get entry from Entrez
    print (seq_id)
    handle = Entrez.efetch(db=""nucest"", id=seq_id, rettype=""fasta"", retmode=""text"")
    result = handle.read()
    handle.close()
    # save
    filename = f""MmP53-{seq_id}-est.fasta""
    print (""Saving to file:"", filename)
    with open(filename, 'w') as fastafile:
        # append fasta entry
        fastafile.write(result.rstrip()+""\n"")
```        


Retrieve Protein Records from the ExPASy Database
Here are a few examples demonstrating how to access the ExPASy databases Swissport and Prosite. The Biopython documentation provides  more details.
Swiss-Prot
```python
from Bio import ExPASy
from Bio import SwissProt
get single protein record
accession_no = ""O23729""
handle = ExPASy.get_sprot_raw(accession_no)
record = SwissProt.read(handle)
print (record.entry_name)
print (record.sequence_length)
print (record.data_class)
print (record.accessions)
print (record.organism)
print first 10 aa
print (record.sequence[:10]) # string
``` 

Output:

```bash
CHS3_BROFI
394
Reviewed
['O23729']
Bromheadia finlaysoniana (Orchid).
MAPAMEEIRQ
```


The return type of SwissProt.read() is a Bio.SwissProt.Record object. In the above example we're printing only a subset of its fields.  The record.sequence field is a string, but it can easily be converted into a Bio.Seq object.
Tip: Use dir(record) to get a list of all record attribute names.
Prosite
```python
from Bio import ExPASy
from Bio.ExPASy import Prosite
handle = ExPASy.get_prosite_raw(""PS00001"")
record = Prosite.read(handle)
print (record.name)
print (record.type) # e.g. PATTERN, MATRIX, or RULE
print (record.pattern) 
print (record.rules)
print (record.matrix)
```

Output:

```bash
ASN_GLYCOSYLATION
PATTERN
N-{P}-[ST]-{P}.
[]
[]
```


The return type of Prosite.read() is a Bio.ExPASy.Prosite.Record object. 
Note: Use the Bio.ExPASy.Prosite.parse() function to parse files containing multiple records.
Prosite Documentation
```
from Bio import ExPASy
from Bio.ExPASy import Prodoc
handle = ExPASy.get_prosite_raw(""PDOC00001"")
record = Prodoc.read(handle)
```
Exercise: Retrieve the SwissProt records for proteins with the following IDs: ""O23729"", ""O23730"", ""O23731"". Try to use list comprehension to create a list containing the records for all retrieved proteins.

Solution

```python
from Bio import ExPASy,SwissProt

accession_nos = [""O23729"", ""O23730"", ""O23731""]
handles  = [ExPASy.get_sprot_raw(a) for a in accession_nos]
records = [SwissProt.read(h) for h in handles]
for record in records:
    print(record.entry_name)
    print("","".join(record.accessions))
    print(record.keywords)
    print(repr(record.organism))
    print(record.sequence[:20],""\n"")
```


Learn more about SwissProt.
ScanProsite
We can query the Prosite database with protein sequences or motifs to find proteins with corresponding matches, see ScanProsite for details.
Option 1: Submit protein sequence (use the seq= keyword argument)
* UniProtKB accessions e.g. P98073
* Identifiers e.g. ENTK_HUMAN
* PDB identifiers e.g. 4DGJ
* Sequences in FASTA format. 
Option 2: Submit motif sequence (use the sig= keyword argument)
* PROSITE accession e.g. PS50240
* Identifier e.g. TRYPSIN_DOM
* Your own pattern e.g. P-x(2)-G-E-S-G(2)-[AS]. 
* Combinations of motifs can also be used.
```python
from Bio.ExPASy import ScanProsite
uniprot_id = ""P26367"" # human Pax-6
handle = ScanProsite.scan(seq=uniprot_id)
results = ScanProsite.read(handle)
``
By executinghandle.read(), you can obtain the search results in raw XML format. Here we useBio.ExPASy.ScanProsite.readto parse the raw XML into aBio.ExPASy.ScanProsite.Record` object which represents a specialized list.
We can now access the found matches like this:
print (""Number of matches:"", results.n_match)
for r in results:
    print (r)

Output:

```bash
Number of matches: 4
{'sequence_ac': 'P26367', 'sequence_id': 'PAX6_HUMAN', 'sequence_db': 'sp', 'start': 4, 'stop': 130, 'signature_ac': 'PS51057', 'signature_id': 'PAIRED_2', 'score': '64.941', 'level': '0'}
{'sequence_ac': 'P26367', 'sequence_id': 'PAX6_HUMAN', 'sequence_db': 'sp', 'start': 38, 'stop': 54, 'signature_ac': 'PS00034', 'signature_id': 'PAIRED_1', 'level_tag': '(0)'}
{'sequence_ac': 'P26367', 'sequence_id': 'PAX6_HUMAN', 'sequence_db': 'sp', 'start': 208, 'stop': 268, 'signature_ac': 'PS50071', 'signature_id': 'HOMEOBOX_2', 'score': '20.164', 'level': '0'}
{'sequence_ac': 'P26367', 'sequence_id': 'PAX6_HUMAN', 'sequence_db': 'sp', 'start': 243, 'stop': 266, 'signature_ac': 'PS00027', 'signature_id': 'HOMEOBOX_1', 'level_tag': '(0)'}
```


You see that each item r represents a dictionary describing a specific match.

Working with Sequence Files
Sequence Objects
The Seq object is similar to a string object augmented with methods for nucleotide sequence operations including 
* find(), count()
* complement(), reverse_complement()
* transcribe(), back_transcribe()
* translate()
The following code examples are in the seq.py script.
```python
from Bio.Seq import Seq
my_dna = Seq(""ATGAGTACACTATAGA"")
print (my_dna)
find position of first subsequence
print (my_dna.find(""TAC""))
print (my_dna.find(""AC""))
print (my_dna.find(""CTC""))
count
print (""A count:"", my_dna.count(""A""))
print (""AC count:"", my_dna.count(""AC""))
print (""AA count:"", Seq(""AAAA"").count(""AA"")) # non-overlapping
```

Output:

```bash
ATGAGTACACTATAGA
5
6
-1
A count: 7
AC count: 2
AA count: 2
```


Note the return of -1 if no sequence match was found.
```python
complement and reverse complement
compl = my_dna.complement()
rev_compl = my_dna.reverse_complement()
print (""original: \t"", my_dna)
print (""complement:\t"", compl)
print (""rev complement:\t"", rev_compl)
transcription
my_rna = my_dna.transcribe()
print (""RNA:"", my_rna)
translation
my_peptide = my_dna.translate()
print (""Peptide:"", my_peptide)
```

Output:

```bash
original:        ATGAGTACACTATAGA
complement:      TACTCATGTGATATCT
rev complement:  TCTATAGTGTACTCAT
RNA: AUGAGUACACUAUAGA
Peptide: MSTL*
```


Like Strings, Seq objects are immutable; this means that the sequence is read-only and cannot be modified in place. However, you can convert a Seq object into a MutableSeq object that allows you to manipulate the sequence after object initialization.
python
my_dna[2] = 'A' # error, immutable Seq object
mutable_dna = my_dna.tomutable()
mutable_dna[2] = 'A'
print (my_dna)
print (mutable_dna)

Output:

```bash
ATGAGTACACTATAGA
ATAAGTACACTATAGA
```


Note that the sequence is zero-indexed: the first nucleotide has index 0, the second has index 1, and so forth. So in this example we're changing the third nucleotide (index 2, G->A).  
Exercise: Create a Seq object with a DNA nucleotide sequence of your choice. Find the first putative start codon (ATG), replace each ""C"" with a ""G"", and transcribe and translate the original as well as the modified sequence. Hint: As an intermediary step, convert Seq object to a string and use a string method for replacement.

Solution:

```python
from Bio.Seq import Seq

seq = Seq(""CTGACTGGATGACCATTGGGCAACTACCCATGACTAGTTAAGTAATTTTTAAAAA"")
atg_pos = seq.find(""ATG"")

cds = seq[atg_pos:]
peptide = cds.translate(to_stop=True)

mod_seq = Seq(str(seq).replace(""C"",""G""))
mod_cds = mod_seq[atg_pos:]
mod_peptide = mod_cds.translate(to_stop=True)

print (""DNA (original):"", seq)
print (""DNA (modified):"", mod_seq)
print (""Peptide (original):"", peptide)
print (""Peptide (modified):"", mod_peptide)
```


Handling Sequence Records
The SeqRecord class provides the following fields:
* .seq: a sequence (as a Seq object)
* .id: the identifier, e.g. an accession number (String)
* .name: can be just the accession number or the locus name (String)
* .description: self-explanatory (String)
* .annotations: dictionary of additional often unstructured info (optional)
* .letter_annotations: often used for quality scores or secondary structure info
* .features: list of SeqFeature objects; more structured than annotations, e.g. gene position in a genome, or domain position in a protein
* .dbxref: list of database cross-references
So it is used to wrap around a Seq object with richer information. We can manually create a SeqRecord object like this:
```python
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
record = SeqRecord(
    Seq(""MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF""),
    id=""YP_025292.1"",
    name=""HokC"",
    description=""toxic membrane protein, small"")
print(record)
```
The above code example is in the seqrecord.py script.

Output:

```bash
ID: YP_025292.1
Name: HokC
Description: toxic membrane protein, small
Number of features: 0
Seq('MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF')
```


Sequence File Operations
The Bio.SeqIO class provides simple tools to read and write a variety of sequence file formats (including multiple sequence alignments). It operates exclusively on SeqRecord objects. 
Read Fasta File
```python
from Bio import SeqIO
file = open('HsPax6-protein.fasta') 
fastarecords = SeqIO.parse(file, ""fasta"")
create a list of SeqRecords
fastalist = [entry for entry in fastarecords]
iterate over fasta entries
for entry in fastalist:
    print (f""ID={entry.id}"")
    print (f""Name={entry.name}"")
    print (f""Description={entry.description}"")
    print (f""Seq length={len(entry.seq)}"")
    print (f""Features={entry.features}"")  # empty for Fasta format
    print (f""Sequence={entry.seq}\n"")
```
Exercise: Filter the list of records to only include sequences with less than 300 amino acids.

Solution

```python
# filter list of records
sublist = [e for e in fastalist if len(e.seq) < 300]
print (f""Total number of sequences: {len(fastalist)}"")
print (f""Number of sequences (<300 aa): {len(sublist)}"")
```


Convert Genbank to Fasta File
```python
from Bio import SeqIO
gb_file = 'HsPax6-208879460-nucleotide.gb'
with open(gb_file) as f:
    gb_generator = SeqIO.parse(f, format='gb')
    for entry in gb_generator:
        with open(f'Hs-pax6-{entry.id}-nucleotide.fasta', 'w') as fileout:
            SeqIO.write(entry, fileout, 'fasta')
```
The later versions of Biopython also include a Bio.SeqIO.convert() function.
```
convert GenBank to Fasta
count = Bio.SeqIO.convert(""my_file.gb"", ""genbank"", ""my_file.fasta"", ""fasta"")
```

AlignIO: Reading Sequence Alignment Files
The Bio.AlignIO class provides functions to handle paired or multiple sequence alignment files. It does not perform the alignment but provides tools to read/write alignment files and manipulate alignment objects. Bio.AlignIO uses the same set of functions for input and output as in Bio.SeqIO, and the same names are supported for the file formats.
The key functions are:
* Bio.AlignIO.read(): For a file that contains one and only one alignment. The return type is a Bio.Align.MultipleSeqAlignment object.
* Bio.AlignIO.parse(): A more general function when the file may contain multiple separate alignments. The return type is a generator that can be converted into a list of Bio.Align.MultipleSeqAlignment objects. 
Example: 
Let's create a Fasta file with Pax6 orthologs from human, mouse, xenopus, pufferfish, zebrafish (2), and fruitfly.  The following code example is in the createPax6_fasta.py script.
```python
from Bio import Entrez
get human, mouse, xenopus, pufferfish, zebrafish 1, zebrafish 2, Drosophila 1, Drosophila 2
ids = ['1587062735','AAH36957.1','NP_001006763.1','XP_029701655.1','NP_571379.1','NP_571716.1','NP_524628','NP_524638']
handle = Entrez.efetch(db=""protein"", rettype=""fasta"", retmode=""text"", id=ids)
result = handle.read()
handle.close()
fastaseq = result.replace(""\n\n"",""\n"")
with open('Pax6-multispec-protein.fasta', 'w') as f:
   f.write(fastaseq)
```
This will create the Pax6-multispec-protein.fasta Fasta file with 8 sequences.  The alignment was performed using Clustal Omega and you can download the Pax6-multispec-protein.aln alignment file and move it to your Python script folder that you use for this workshop.
Alternatively, create the alignment yourself:
1. Visit the Clustal Omega website and upload the Pax6-multispec-protein.fasta file as input. 
2. Under Step 1, click the Choose File button and upload the Pax6-multispec-protein.fasta file as input.
3. Under Step 3, click Submit.
4. When the alignment is done, click the Alignments tab, select the entire alignment output in the window and paste it into a text editor. Do not use Microsoft Word for this but programs like Text Edit, Notepad++, Emacs or vim instead.
5. Save the alignment in the text editor as Pax6-multispec-protein.aln in your Python script folder that you use for this workshop.
The following code examples are in the alignio-parse_clustal.py script.
Parse the alignment file
```
from Bio import AlignIO
inputfile = open(""Pax6-multispec-protein.aln"", ""r"")
assuming single alignment in file; use AlignIO.parse for multiple alignments
alignment = AlignIO.read(inputfile, ""clustal"")
inputfile.close()
print (""Alignment length:"", alignment.get_alignment_length())
print (alignment,""\n"")
```

Output:

```bash
Alignment with 8 rows and 867 columns
MFTLQPTPTAIGTVVPPWSAGTLIERLPSLEDMAHKGHSGVNQL...PWV NP_524628.2
---MMLTTEHIMHGHPH-----SSVGQSTLFGCSTAGHSGINQL...--- NP_524638.3
---------------------------------MQNSHSGVNQL...--- NP_001006763.1
--------------------------------------------...--- NP_001355831.1
---------------------------------MQNSHSGVNQL...--- AAH36957.1
--------------------------------MMQNSHSGVNQL...--- XP_029701655.1
----MPQKEY-Y----N-----RATWESGVASMMQNSHSGVNQL...--- NP_571379.1
----MPQKEY-H----N-----QPTWESGVASMMQNSHSGVNQL...--- NP_571716.1
```


Update identifier
python
species = ['H.sapiens', 'M.musculus', 'X.tropicalis', 'T.rubripes', 'D.rerio', 'D.rerio', 'D.melanogaster', 'D.melanogaster']
for idx,line in enumerate(alignment):
    line.id = f""{species[idx]}:{line.id}""
print (alignment)

Output:

```bash
Alignment with 8 rows and 867 columns
MFTLQPTPTAIGTVVPPWSAGTLIERLPSLEDMAHKGHSGVNQL...PWV H.sapiens:NP_524628.2
---MMLTTEHIMHGHPH-----SSVGQSTLFGCSTAGHSGINQL...--- M.musculus:NP_524638.3
---------------------------------MQNSHSGVNQL...--- X.tropicalis:NP_001006763.1
--------------------------------------------...--- T.rubripes:NP_001355831.1
---------------------------------MQNSHSGVNQL...--- D.rerio:AAH36957.1
--------------------------------MMQNSHSGVNQL...--- D.rerio:XP_029701655.1
----MPQKEY-Y----N-----RATWESGVASMMQNSHSGVNQL...--- D.melanogaster:NP_571379.1
----MPQKEY-H----N-----QPTWESGVASMMQNSHSGVNQL...--- D.melanogaster:NP_571716.1
```


Slicing and joining
```python
slice: first axis defines line, second axis defines column index (zero-indexed)
get lines 1-6, first 50 columns
subset = alignment[:6,:50]
print (subset)
```

Output:

```bash
Alignment with 6 rows and 50 columns
MFTLQPTPTAIGTVVPPWSAGTLIERLPSLEDMAHKGHSGVNQLGGVFVG H.sapiens:NP_524628.2
---MMLTTEHIMHGHPH-----SSVGQSTLFGCSTAGHSGINQLGGVYVN M.musculus:NP_524638.3
---------------------------------MQNSHSGVNQLGGVFVN X.tropicalis:NP_001006763.1
-------------------------------------------------- T.rubripes:NP_001355831.1
---------------------------------MQNSHSGVNQLGGVFVN D.rerio:AAH36957.1
--------------------------------MMQNSHSGVNQLGGVFVN D.rerio:XP_029701655.1
```


Let's join two alignment blocks:
python
edited = alignment[:,:50] + alignment[:,500:]
print (edited)

Output:

```bash
Alignment with 8 rows and 417 columns
MFTLQPTPTAIGTVVPPWSAGTLIERLPSLEDMAHKGHSGVNQL...PWV H.sapiens:NP_524628.2
---MMLTTEHIMHGHPH-----SSVGQSTLFGCSTAGHSGINQL...--- M.musculus:NP_524638.3
---------------------------------MQNSHSGVNQL...--- X.tropicalis:NP_001006763.1
--------------------------------------------...--- T.rubripes:NP_001355831.1
---------------------------------MQNSHSGVNQL...--- D.rerio:AAH36957.1
--------------------------------MMQNSHSGVNQL...--- D.rerio:XP_029701655.1
----MPQKEY-Y----N-----RATWESGVASMMQNSHSGVNQL...--- D.melanogaster:NP_571379.1
----MPQKEY-H----N-----QPTWESGVASMMQNSHSGVNQL...--- D.melanogaster:NP_571716.1
```


Exporting to other alignment file formats
```python
save as Stockholm
with open(""Pax6-multispec-protein.sth"", ""w"") as outputfile:
    AlignIO.write(alignment, outputfile, ""stockholm"")
get alignment as formatted string
print (""Formatted Alignment:"")
print (format(alignment, ""clustal""))
```

Output:

```bash
Formatted Alignment:
CLUSTAL X (1.81) multiple sequence alignment

H.sapiens:NP_524628.2               MFTLQPTPTAIGTVVPPWSAGTLIERLPSLEDMAHKGHSGVNQLGGVFVG
M.musculus:NP_524638.3              ---MMLTTEHIMHGHPH-----SSVGQSTLFGCSTAGHSGINQLGGVYVN
X.tropicalis:NP_001006763.1         ---------------------------------MQNSHSGVNQLGGVFVN
T.rubripes:NP_001355831.1           --------------------------------------------------
D.rerio:AAH36957.1                  ---------------------------------MQNSHSGVNQLGGVFVN
D.rerio:XP_029701655.1              --------------------------------MMQNSHSGVNQLGGVFVN
D.melanogaster:NP_571379.1          ----MPQKEY-Y----N-----RATWESGVASMMQNSHSGVNQLGGVFVN
D.melanogaster:NP_571716.1          ----MPQKEY-H----N-----QPTWESGVASMMQNSHSGVNQLGGVFVN
```


Exercise:
Find the first alignment block that shows no gaps across all 8 aligned sequences. 
1. Print the block.
2. Save the block as a new clustal formatted text file.
3. From that block, extract the D. rerio (zebrafish) sequences and print the two sequences 

Solution:

```python
from Bio import AlignIO

inputfile = open(""Pax6-multispec-protein.aln"", ""r"")
# assuming single alignment in file; use AlignIO.parse for multiple alignments 
alignment = AlignIO.read(inputfile, ""clustal"")
inputfile.close()

length = alignment.get_alignment_length()
# create boolean list to indicate if all lines in column are wiithout gap
pattern = [all([a.seq[i] != ""-"" for a in alignment]) for i in range(length)]
print (""No gap in column:"")
print (pattern,""\n"")
# create list of column indices without gaps
full_indices = [i for i in range(length) if pattern[i]]
print (""Full column indices:"")
print (full_indices,""\n"")

# scan consecutive columns for completeness
firstblock_i = []
j=0
start=0
for i in full_indices[start:]:
    if j==0 or i==full_indices[start]+j:
        firstblock_i.append(i)
    else:
        break
    j+=1

# slice alignment to show first block without gaps
block1 = alignment[:,firstblock_i[0]:firstblock_i[-1]+1]
print (block1)

# save
with open(""Pax6-multispec-block1.aln"", ""w"") as outputfile:
    AlignIO.write(block1, outputfile, ""clustal"")

# get zebrafish sequence lines
zebrafish_lines = block1[4:6,:]
print (zebrafish_lines)
```


Resources

Biopython Tutorial and Cookbook
UVA Research Computing
"
rc-learning-fork/content/notes/databases-intro/index.md,"
  There are two main families of databases: Relational and NoSQL.


Relational databases store information in an orderly, column, row, and table schema. They “relate” the tables together to present different views of the data.
  NoSQL databases are much less structured. This means they can store different data alongside each other – which makes things both easier to store but harder to query across.


  There are additional types of databases, such as ledger, time-series and others. Those are beyond the scope of this introduction.

Relational Databases (RDBMS)
Most users have at least heard of relational databases such as:

MySQL / MariaDB
PostgreSQL
Microsoft SQL Server
Oracle

Relational databases operate on the concepts of tables, relations, schemas, data types, indices, SQL, joins and basic ""CRUD"" operations.
C = Create   (Insert)
R = Read     (Select)
U = Update   (Update)
D = Delete   (Delete)

Take the example of an online store, where data revolves around the ideas of items, orders and customers. When a customer makes a purchase in our store, the data from the transaction is actually broken apart into tables of related data. Here’s one way of seeing that process:

The database for such an online store might have a handful of related tables:

Orders
Customers
Credit Cards
Items

Relational database tables use unique keys as a way to relate one table with another, and so the ""orders"" table 
might simply aggregate keys drawn from other tables for each order. This allows each table to have a clear definition 
of what data fields, and their data types, are expected with every transaction. Data coming in must be broken apart 
to conform to this data structure, and data going out must be drawn back together from across the tables.
But this “breaking apart” process is actually an intensive, time-consuming process. Data being sent off to
any particular table has to be validated by data type (strings, integers, dates, decimals, binary, etc.), length,
and NULL before it can be inserted into a particular data table. This happens across multiple tables at
the same time, and ensures that the entire transaction completes successfully or is rolled back.
Impedance Mismatch - a set of conceptual and technical difficulties that are often encountered when interacting with a relational database management system.
SQL, ""structured query language"" is the language spoken by most relational databases. While there are slight variations
in SQL syntax between RDBMS platforms (a semicolon here, a percent sign there), they all generally read the same to
anyone familiar with general SQL queries.
Create a table:
sql
  create table researchers
  (researcherID int NOT NULL AUTO_INCREMENT,
    first varchar(15),
    last varchar(20),
    email varchar(30),
    age int,
    PRIMARY KEY (ID)
  );
Insert an item into a table:
sql
  insert into researchers
    (first, last, email, age)
    values ('Jane', 'Doe', 'jdoe@georgia.edu', 34);
Select (read) all items from a table:
sql
  select * from researchers;
Select (read) a single item from a table:
sql
  select * from researchers where researcherID = 147;
  select * from researchers where first = 'Jane';
  select first, last from researchers where age = 34;

NoSQL Databases
NoSQL databases come in at least two main groupings: Aggregate oriented or Node-Arc/Graph.
1. Aggregate-Oriented Databases

Key-Value - Redis, Memcached
Document - DynamoDB, MongoDB
Column-Family - Cassandra, BigTable

NoSQL databases share very few common characteristics. Perhaps the only one is that they are schema-less. Typical aggregate-oriented NoSQL databases will store an aggregation in the form of strings or entire documents. That is usually in plain text, often in a specific format or notation, such as JSON or XML.
Here are some sample entries from a simple Key-Value datastore:




Key
Value




access_key
ABCDEfghijklmnop123456789xyzabc


secret_key
23481283852384128328a


current_count
472


jobs_remaining
13


last-winner
Darla Johnson


last-winner-date
08/17/2014 08:42:13.015 UTC





In the case of document NoSQL databases, the “value” portion of the entry can get much larger.
Here is an example of an entry in JSON. Note that the entire entry (or “document”) breaks down into a hierarchy of data: fields and their values, and dictionaries of multiple values,
{
  ""success"": {
    ""total"": 1
  },
  ""contents"": {
    ""quotes"": [
      {
        ""quote"": ""Remove the temptation to settle for anything short of what you deserve."",
        ""length"": ""71"",
        ""author"": ""Lorii Myers"",
        ""tags"": [
          ""expectation"",
          ""inspire"",
          ""perfection""
        ],
        ""category"": ""inspire"",
        ""date"": ""2017-09-08"",
        ""permalink"": ""https://theysaidso.com/quote/ZWrV624xU_q6_KYYlrQpYgeF/lorii-myers-remove-the-temptation-to-settle-for-anything-short-of-what-you-deser"",
        ""title"": ""Inspiring Quote of the day"",
        ""background"": ""https://theysaidso.com/img/bgs/man_on_the_mountain.jpg"",
        ""id"": ""ZWrV624xU_q6_KYYlrQpYgeF""
      }
    ],
    ""copyright"": ""2017-19 theysaidso.com""
  }
}
Also consider that subsequent entries into this table may or may not contain a background image, or the same number of tags, or the precise data structure of this
entry. NoSQL evolved out of the need to quickly collect varied data at very high rates and so it does not suffer from impedance mismatch. Rather, it suffers from
its difficulty to aggregate or join.
2. Node-Arc / Graph Databases
Graph, or Node-arc databases are entirely different, in that they try to store and represent connectivity between nodes in a constellation, and their relationships. So a “query” of a graph database might inform you about the networks of other nodes related to the node you are interested in, and the types and strengths of those relationships, among other uses. Some examples of Graph DBs are:

Neo4j
TinkerPop
Infinite



Using Databases in Your Research
We are frequently asked by researchers how to incorporate databases into their work. Here are four suggestions:



» Track Results

        Track the status of your completed work by adding a record to a table upon completion. This lets you
        know what work remains open and information about its processing.
      




» Queue Up Your Work

        Collect and store data about future work you need to complete, the steps required, and the expected lifecycle
        of each step. While this might be easy to do in Excel, you could grow this into a database that orchestrates
        some of these steps for you.
      






» Index Everything

        Maintain a searchable history of source data, result sets, and code used to process them.
        This could include links to related data, articles published, GitHub code repositories, and more.
      




» Automate

        If you are awash in source data or have a backlog of files to process, consider automating it by using a database.
        Your code, instead of handling a single file at a time, could read each row in the database and process files
        indexed in a table. A single HPC job could process thousands of files!
      




Note: Research Computing may be able to provide support for your database needs. Please schedule a consultation request on our website by filling out this form. 
Other Resources
Here is a great overview of databases and their histories:
{{< youtube qI_g07C_Q5I >}}
Martin Fowler - NoSQL - YouTube"
rc-learning-fork/content/notes/deep-learning-distributed/distributed-training.md,"Most models can be trained in a reasonable amount of time using a single GPU. Minimizing the ""time to finish"" is minimizing both the time the job spends running on the compute node and the time spend waiting in the queue. 
Distributed training is imperative for larger and more complex models/datasets. Data parallelism is a relatively simple and effective way to accelerate training.

However, if you are effectively using the GPU then you may consider running on multiple GPUs. For more, look into how to conduct a scaling analysis.

Data vs Model Parallelism
There are two ways to distribution computation across multiple devices.
{{< rawhtml >}}


{{< figure src=/notes/deep-learning-distributed/img/data_parallelism.png width=60% height=60% >}} 
{{< figure src=/notes/deep-learning-distributed/img/model_parallelism.png width=60% height=60% >}} 


{{< /rawhtml >}}
Data parallelism: A single model gets replicated on multiple devices. Each processes different batches of data, then they merge their results. 
* The single-program, multiple data (SPMD) paradigm is used. That is, the model is copied to each of the GPUs. The input data is divided between the GPUs evenly. After the gradients have been computed they are averaged across all the GPUs. This is done in a way that all replicas have numerically identical values for the average gradients. The weights are then updated and once again they are identical by construction. The process then repeats with new mini-batches sent to the GPUs. 
  * For additional information: https://www.telesens.co/wp-content/uploads/2019/04/img_5ca570946ee1c.png
* There exists many variants, and they differ in how the different model replicas merge results, in whether they stay in sync at every batch or whether they are more loosely coupled, etc. 
Model parallelism: Different parts of a single model run on different devices, processing a single batch of data together. 
* This works best with models that have a naturally-parallel architecture, such as models that feature multiple branches.
{{< table >}}
| Data Parallelism | Model Parallelism |
| ----------- | ----------- |
|Allows to speed up training | All workers train on different data |
|All workers have the same copy of the model | Neural network gradients (weight changes) are exchanged|
|Allows for a bigger model | All workers train on the same data|
| Parts of the model are distributed across GPUs | Neural network activations are exchanged |
{{< /table >}}
{{< figure src=/notes/deep-learning-distributed/img/dp_pcie.png caption=""Datal loading and gradient averaging share communication resources → congestion"" width=60% height=60% >}}
{{< figure src=/notes/deep-learning-distributed/img/dp_nvlink.png caption= ""Datal loading on PCIe,  gradient averaging on NVLINK → no congestion"" width=60% height=60% >}}
Tensorflow Example: Synchronous Data Parallelism
This guide focuses on data parallelism, in particular synchronous data parallelism, where the different replicas of the model stay in sync after each batch they process. Synchronicity keeps the model convergence behavior identical to what you would see for single-device training.
In TensorFlow we use the tf.distribute API to train Keras models on multiple GPUs. There are two setups:


Single host, multi-device training. This is the most common setup for researchers and small-scale industry workflows.


Multi-worker distributed training. This is a setup for large-scale industry workflows, e.g. training high-resolution image classification models on tens of millions of images using 20-100 GPUs.


Single Host, Multi GPUs
Each device will run a copy of your model (called a  replica ).
At each step of training:
* The current batch of data (called  global batch ) is split into e.g., 4 different sub-batches (called  local batches ).
* Each of the 4 replicas independently processes a local batch;  forward pass, backward pass, outputting the gradient of the weights.
* The weight updates from local gradients are merged across the 4 replicas.

In practice, the process of synchronously updating the weights of the model replicas is handled at the level of each individual weight variable. This is done through a mirrored variable object.

Coding: General Steps

Design the Model
Read in the data (recommended to use tf.data)
Create a Mirrored Strategy
Open a Strategy Scope
Train the Model
Evaluate the Model
Display the results

Activity:  Distributed TensorFlow Program
Make sure that you can run the TF code:
* TF_CNN_MultiGPU.ipynb
Pytorch Example: Data Parallel
https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
Multi-GPU, Distributed Data Parallel (DDP)
{{< figure src=/notes/deep-learning-distributed/img/dpp.png caption=""Source: https://www.telesens.co/wp-content/uploads/2019/04/img_5ca570946ee1c.png"" width=75% height=75% >}}

Do not use DataParallel (increased overhead, runs on threads) in PyTorch for anything since it gives poor performance relative to DistributedDataParallel (runs on processes).
Pytorch's Model Parallel Best Practices: https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html
Distributed Data Parallel video in pytorch: https://www.youtube.com/watch?v=TibQO_xv1zc
DDP is only applied when training, no effect during validation or evaluation
DataParallel is very easy to use and handles * everything for you, but not optimized
DDP involves more coding and adjustments, but more code optimized and hence significant speedup, more salable to multiple machines and flexibility.

Coding: General Steps

Design the Model
Set up the Ranks
Read in the Data
Train the Model
Evaluate the Model

Activity:  Distributed PT Program
Make sure that you can run the PT code:

PT_CNN_MultiGPU.py
PT_CNN_MultiGPU.slurm

Pytorch Lightning Example: Data parallel
https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
PyTorch Lightning
PyTorch Lightning wraps PyTorch to provide easy, distributed training done in your choice of numerical precision.
To convert from PT to PTL:
* Restructure the code by moving the network definition, optimizer and other code to a subclass of L.LightningModule.
* Remove .cuda() and .to() calls since Lightning code is hardware agnostic
Once these changes have been made one can simply choose how many nodes or GPUs to use and Lightning will take care of the rest. One can also use different numerical precisions (fp16, bf16). There is tensorboard support and model-parallel training.
Activity:  Distributed PT-Lightning Program
Make sure that you can run the PyTorch-Lightning code:

PTL_multiGPU.slurm
PTL_multiGPU.py
"
rc-learning-fork/content/notes/deep-learning-distributed/pytorch.md,"Pytorch is another widely-used deep-learning platform known for its flexibility and speed. It is a popular library used by academics and researchers.

It is a software library, developed by Facebook and maintained by Mega AI.
Torch is an open-source project for Deep Learning written in C and generally used via the Lua interface. Torch is no longer actively developed but libraries are used in Pytorch.
Because PyTorch and Tensorflow use some common underlying codes, many of the required functions (e.g., activation, loss, optimizer) will be the same.

Installation

Conda
bash
conda create --name torch-env pytorch torchvision pytorch-cuda=12.1 -c pytorch –c nvidia
Container (NGC)
https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch
Building from source
https://github.com/pytorch/pytorch#from-source

Performance and Utilization:

Use DataLoader and try increasing the value of cpus-per-task in tandem with num_workers  to prepare the data and keep the GPU busy. This was shown to dramatically increase the GPU utilization.
Writing a custom dataloader: https://www.youtube.com/watch?v=PXOzkkB5eH0
External datal loading libraries: https://github.com/libffcv/ffcv, https://developer.nvidia.com/dali


Mixed precision training requires either the V100 or A100 GPU and is included in PyTorch as torch.cuda.amp. PyTorch will perform FP32 matrix multiplications using TF32 by default.
Automatic Mixed Precision: https://pytorch.org/docs/stable/amp.html


gpustat, Line_profiler, nvprof or nsys (if on GPU)
For example, ./nvprof python mnist_classify.py --epochs=3


TensorBoard is a useful tool for tracking the training process of a PyTorch model. Available through conda and container version.

Performance Tuning
* Pytorch Performance Tuning Guide
* Performance Tuning Guide Slides
Torch Tensors
A tensor is a multidimensional array (like a numpy ndarray) which can be used on GPUs
```python
import torch
x = torch.rand(5,3, dtype=torch.long, device='cuda') # if not specified, uses cpu
y = torch.zeros(5,3)
z = torch.add(x+y) # or z=x+y
w = z.numpy()  # convert to numpy array, same memory location
t = torch.from_numpy(w) # numpy to torch tensor (on cpu)
```
CUDA tensors
Tensors can be moved onto any device using the .to method.
python
if torch.cuda.is_available():
    device = torch.device(""cuda"")
    y = torch.rand(3,5, device=device)
    x = torch.rand(3,5).to(device)
    z = x + y
    print(z)
    print(z.to(""cpu"", torch.double)) # ``.to`` can also change dtype together!
Coding a Pytorch Model: General Steps
1. Import the torch package 
2. Read in the data 
3. Preprocess the data 
      3a. Scale the data 
      3b. Split the data 
      3c. Convert data to tensors 
      3d. Load the tensors 
4. Design the Network Model 
5. Define the Learning Process 
6. Train the model 
7. Apply the model to the test data 
8. Display the results
Make sure that you can run the PyTorch code:

PT_CNN_SingleGPU.ipynb
"
rc-learning-fork/content/notes/deep-learning-distributed/deep-learning.md,"Deep learning is a branch of artificial intelligence, where programs use multiple layers of neural networks to transform a set of input values to output values.
{{< figure src=/notes/deep-learning-distributed/img/dnn_overview.png width=65% height=65% >}}
Nodes
Each ""node"" in the neural network performs a set of computations.
{{< figure src=/notes/deep-learning-distributed/img/node_weights.png width=50% height=50% >}}
The weights, $𝑤_𝑖$, and the bias, $b$, are not known.
Each node will have its own set of unknown values. 
During training, the “best” set of weights are determined that will generate a value close to $y$ for the collection of inputs $𝑥_𝑖$.
Network of Neurons
{{< figure src=/notes/deep-learning-distributed/img/multi_perceptron.png caption=""Multi-layer Perceptron"" width=55% height=55% >}}

Different computations with different weights can be performed  to produce different outputs.
This is called a feedforward network – all values progress from the input to the output.
A neural network has a single hidden layer
A network with two or more hidden layers is called a “deep neural network”.

Deep Learning Neural Network
{{< figure src=/notes/deep-learning-distributed/img/neural_network.png caption=""Multilayer Perceptron/ Fully Connected NN; Image borrowed from: http://www.kdnuggets.com/2017/05/deep-learning-big-deal.html"" width=75% height=75% >}}
DL Algorithm
During the training or “fitting” process, the Deep Learning algorithm is fed a set of measurements/features and the expected outcome (e.g., a label or classification). 
{{< figure src=/notes/deep-learning-distributed/img/dl_algorithm.png width=60% height=60% >}}
The algorithm determines the best weights and biases for the data.
How Does the Machine Learn?

Start with a random guess for the weights and biases.
The output values or “predicted” values of the network can be compared with the expected results/categories/labels.
Another function, called a “loss” or “cost” function can be used to determine the overall error of the model.  
That error can be used to work backwards through the network and tweak the weights/biases.
This step is called backward propagation.



Overview of the Learning Process
{{< video src=""/notes/deep-learning-distributed/video/learning_process.mp4"" controls=""yes"" >}}
Deep Neural Network
{{< figure src=/notes/deep-learning-distributed/img/dnn_ex.gif caption=""An example of a Deep Neural Network (DNN)"" width=70% height=70% >}}
DNN Examples

Feed-Forward NN
Consist of an input layer, an output layer and many hidden layers that are fully connected, and can be used to build speech-recognition, image-recognition, and machine-translation software.
Recurrent NN
RNNs are commonly used for image captioning, time-series analysis, natural-language processing, machine translation, etc.
Convolution NN
Consist of multiple layers and are mainly used for image processing and object detection.
And many more such as RBFNs, GANs, Modular NN, etc.

Activation Function
{{< figure src=/notes/deep-learning-distributed/img/activation_function.png caption="""" width=40% height=40% >}}
* The activation function introduces nonlinearity into the model.
* Can choose different activation functions for each layer.
* Examples include ReLU, Sigmoid(binary classification), and Softmax(multiclass classification).
* A complete list is available at
  * https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity and https://pytorch.org/docs/stable/nn.html#non-linear-activations-other
Loss Function
{{< figure src=/notes/deep-learning-distributed/img/loss_function.png caption="""" width=50% height=50% >}}
* The loss function tells us how good our model is at relating the input to the output.
* A function that will be optimized to improve the performance of the model.
* The cost value is the difference between the neural nets predicted output and the actual output from a set of labeled training data.
* The choice of loss function is based on the task.
* Examples include
  * Classification: BCELoss (Binary Cross Entropy) and Cross Entropy Loss.
  * Regression: Mean Squared Error (MSE)
    * A complete list is available at https://pytorch.org/docs/stable/nn.html#loss-functions and https://www.tensorflow.org/api_docs/python/tf/keras/losses
Optimizer Function

The optimizer function is a function for tweaking/adjusting the parameters during training so that the best weights and biases are efficiently reached.
Examples include SGD, Adam, and RMSprop.
A complete list is available at  https://pytorch.org/docs/stable/optim.html?highlight=optimizer#torch.optim.Optimizer and https://www.tensorflow.org/api_docs/python/tf/keras/optimizers

"
rc-learning-fork/content/notes/deep-learning-distributed/gpu.md,"Graphics Processing Units (GPUs), originally developed for accelerating graphics rendering, can dramatically speed up any simple but highly parallel computational processes. GPGPU is a shorthand often used for ""General-Purpose Computing on Graphics Processing Devices.""
CPU versus GPU
{{< table >}}
| CPU                       | GPU         |
| -----------               | ----------- |
| Several Cores (100-1)  | Many Cores (103-4) |
| Generic Workload (Complex & Serial Processing) | Specific Workload (Simple & Highly Parallel) |
| Up to 1.5 TB / node on UVA HPC   | Up to 80 GB /device on UVA HPC     |
{{< /table >}}

Integrated GPU vs Discrete GPUs:
Integrated GPUs are used mostly for graphics rendering and gaming
Dedicated GPUs are designed for intensive computations

{{< figure src=/notes/deep-learning-distributed/img/cpu_gpu.png caption=""CPU Optimized for Serial Tasks and GPU Accelerator optimized for Parallel Tasks; Credit: NVIDIA"" width=50% height=50% >}}
Like a CPU, a GPU has a hierarchical structure with respect to both the execution units and memory. A warp is a unit of 32 threads. NVIDIA GPUs impose a limit of 1024 threads per block. Some integral number of warps are grouped into a streaming multiprocessor (SM) and there are tens of SMs per GPU. Each thread has its own memory and there is limited shared memory between a block of threads. Finally, there is also the global memory which is accessible to each grid or collection of blocks.
Vendors and Types

NVIDIA, AMD, Intel
Datacenter : K80, P100, V100, A100, H100 (NVIDIA); MI300A, MI300X (AMD)
Workstations: A6000, Quadro (NVIDIA)
Gaming: GeForce RTX 20xx, 30xx, 40xx (NVIDA), Radeon (AMD)
Laptops and desktops: GeForce (NVIDIA), Radeon (AMD), Iris (Intel)

Programming GPUs
Several libraries and programming models have been developed to program GPUs.

CUDA 
CUDA is a parallel computation platform, developed by NVIDIA, for general-purpose programming on NVIDIA hardware.
HIP
HIP is a programming interface from AMD that allows developers to target either NVIDIA or AMD hardware.
OpenCL 
OpenCL is a more general parallel computing platform, developed by Apple. It allows software to access CPUs, GPUs, FPGAs, and other devices. 
SYCL
SYCL started as an outgrowth of OpenCL but is now independent of it.
Kokkos
Kokkos is another programming model that attempts to be device-independent. It can target multicore programming (OpenMP), CUDA, HIP, and SYCL.

Most of these programming paradigms can be used from Python, but nearly all machine learning/deep learning packages are based on CUDA and  will only work with NVIDIA GPUs.
GPU Computing

For certain workloads like image processing, training artificial neural networks and solving differential equations, a GPU-enabled code can vastly outperform a CPU code. Algorithms that require lots of logic such as ""if"" statements tend to perform better on the CPU.
Steps required to execute a function (kernel) on a GPU:
copy the input data from the CPU memory to the GPU memory
load and execute the GPU kernel on the GPU
copy the results from the GPU memory to CPU memory
Depending on the DL framework, some of these steps may be automatically done.
Recently, manufacturers have incorporated specialized units on the GPU called Tensor Cores (NVIDIA) or Matrix Cores (AMD) to perform certain operations in less than single precision.

Note: This is particularly beneficial to researchers training artificial neural networks or, more generally, cases where matrix-matrix multiplications and related operations dominate the computation time. Modern GPUs, with or without these specialized units, can be used in conjunction with a CPU to accelerate scientific codes.
https://github.com/PrincetonUniversity/gpu_programming_intro/
GPU accelerated libraries. https://developer.nvidia.com/gpu-accelerated-libraries
As with the CPU, a GPU can perform calculations in single precision (32-bit) faster than in double precision (64-bit)
Computational Graphs
Computational graphs help to break down computations.  For example, the graph for $y=(x1+x2) \times (x2 - 5)$  is
{{< figure src=/notes/deep-learning-distributed/img/computational_graph.png caption=""The beauty of computational graphs is that they show where computations can be done in parallel."" width=50% height=50% >}}
Why Use GPUs in DL?
Data flows in neural networks can be efficiently implemented by  computational graphs . This is the case with popular frameworks like TensorFlow and PyTorch.

With deep learning models, you can have hundreds of thousands of computational graphs.
A GPU can perform a thousand or more of the computational graphs simultaneously.  This will speed up your program significantly.
New GPUs have been developed and optimized specifically for deep learning.
All the major deep learning Python libraries (Tensorflow, PyTorch, Keras, Caffe,...)  support the use of GPUs and allow users to distribute their code over multiple GPUs.

GPUs in DL

Scikit-learn does not support GPU processing.
Deep learning acceleration is furthered with Tensor Cores in NVIDIA GPUs.
Tensor Cores accelerate large matrix operations by performing mixed-precision computing.
Tensor Cores accelerate math and reduce the memory traffic and consumption.
If you're  not  using a neural network as your machine learning model you may find that a GPU doesn't improve the computation time.
If you are using a neural network but it is very small then a GPU will not be any faster than a CPU - in fact, it might even be slower.

GPU Profiling

jupyterlab-nvdashboard (GPU Dashboards)
nvidia-smi
only a measure of the fraction of the time that a GPU kernel is running on the GPU. Nothing about how many CUDA cores are being used or how efficiently the GPU kernels have been written.
Line_profiler for any python code (PyTorch and TensorFlow).
Nsight Systems(nsys) for profiling GPU codes. It produces a timeline and can handle MPI but produces a different set of profiling data for each MPI process.
Nsight Compute(ncu) to look closely at the behavior of specific GPU kernels.
Nsys and ncu are more accurate measures of GPU utilization.
Additional information can be found: https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing


For codes used by large communities, one can generally associate GPU utilization with overall GPU efficiency. 

GPU Utilization

If there is zero GPU utilization...
Is code GPU enabled?
Is software environment properly configured?
Does code spend a long time transferring data between CPU and GPU?(e.g. during interactive jobs)
If there is low GPU utilization...
There may be misconfigured application scripts.
You may be using high end GPU for codes that do not have enough work to keep the GPU busy. Using an A100 GPU for a job that could be handled sufficiently by a P100 GPU might lead to underutilization.
You may be training deep learning models while only using a single CPU core. Frameworks like PyTorch and TensorFlow often show significant performance benefits when multiple CPU cores are utilized for data loading.
You may be using too many GPUs for a job. You can find the optimal number of GPUs and CPU-cores by performing a scaling analysis.
You may be running a code written to work for a single GPU on multiple GPUs.
Writing output files to slow storage systems instead of scratch or GPFS, can also reduce GPU utilization.


Make sure the software environment is configured properly. For hyperparameter tuning, consider using a job array. This will allow you to run multiple jobs with one sbatch command. Each job within the array trains the network using a different set of parameters.

UVA-NVIDIA DGX BasePOD

10 DGX A100 nodes
8 NVIDIA A100 GPUs.
80 GB GPU memory options.
Dual AMD EPYC:tm:; nodes: Series 7742 CPUs, 128 total cores, 2.25 GHz (base), 3.4 GHz (max boost).
2 TB of system memory.
Two 1.92 TB M.2 NVMe drives for DGX OS, eight 3.84 TB U.2 NVMe drives forstorage/cache.
Advanced Features:
NVLink for fast multi-GPU communication
GPUDirect RDMA Peer Memory for fast multi-node multi-GPU communication
GPUDirect Storage with 200 TB IBM ESS3200 (NVMe) SpectrumScale storage array
Ideal Scenarios:
Job needs multiple GPUs on a single node or multi node
Job (single or multi-GPU) is I/O intensive
Job (single or multi-GPU) requires more than 40GB of GPU memory


Always try to use the CUDA Toolkit 11.x and cuDNN 8.x since they are needed to take full advantage of the A100.

GPU Access on UVA HPC

General
GPUs available in both  interactive  and  gpu  partition, GPU type and number can be specified.
POD nodes
POD nodes are contained in the gpu partition with a specific Slurm constraint.
Slurm script:
```bash

SBATCH -p gpu
SBATCH --gres=gpu:a100:X   # X number of GPUs
SBATCH -C gpupod
* Open OnDemandnohighlight
--constraint=gpupod
```"
rc-learning-fork/content/notes/deep-learning-distributed/effective-use.md,"
Optimize single-node/single-GPU performance?
Use performance analysis tools
Tune and optimize data pipeline
Make effective use of the hardware (e.g. mixed precision)
Before running on multiple nodes, make sure the job can scale well to 8 GPUs on a single node. Never do one GPU per node for multinode jobs.
Use multiple CPU cores for data loading.
For hyperparameter tuning consider using a job array. This will allow you to run multiple jobs with one sbatch command.


You will likely want the code loading data to look like this:
python
train_dataset = datasets['train'].map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)
   .cache()
   .shuffle(SHUFFLE_SIZE)
   .batch(BATCH_SIZE)
   .prefetch(tf.data.AUTOTUNE)

Questions or Need more help?
Office Hours via Zoom:

Tuesdays: 3 pm - 5 pm
Thursdays: 10 am - noon

Zoom Links are available at https://www.rc.virginia.edu/support/#office-hours
* Website: https://rc.virginia.edu"
rc-learning-fork/content/notes/deep-learning-distributed/metrics.md,"Metrics are a formula for measuring the accuracy of the model.
* Examples include Accuracy and MeanSquaredError.
  * A complete list is available at https://keras.io/api/metrics.
Epochs and Batch Size

An epoch is one pass through the training loop over all of the training data.
During each epoch, training generally occurs on batches of data at a time instead of on the full dataset all at once. The number of samples in each batch is called the batch size.
Smaller batch sizes
guarantee that a batch can fit on the GPU
allow the model to generalize well, but increase training time

Training, Validation, and Test Sets

Training set: inputs/outputs used during NN training.
Validation set:
used during training to see how well the network is generalizing to unseen data, but the NN parameters are not affected by these inputs/outputs.
Inputs are sent to the NN during each epoch and the loss/metrics are calculated on this set of inputs/outputs.
If NN performance on the validation set is poor, this is a sign that the NN hyperparametes (e.g. number of layers, number of neurons, etc.) need to be adjusted.
Test set: Unseen data used to measure the performance of the NN.

Callbacks

Generally, we set the number of epochs for a model to train.
However, if model performance on the validation set is not improving, it would be nice if training could end early to avoid overfitting.
Callback: inputs to the model.fit function, dictate actions to take during training or inference
EarlyStopping: callback that stops the training process once model performance on the validation set no longer improves.
ModelCheckpoint: callback that saves the best model, e.g., the model with the lowest loss on the validation set. This is probably not the model produced at the last training epoch.

Dropout Regularization

Can be used on input or hidden layers
During each training epoch a percentage of random neurons in the layer are dropped out.
Prevents overfitting
Neurons are only dropped out during training, not inference.
"
rc-learning-fork/content/notes/deep-learning-distributed/tensorflow.md,"Tensorflow is a software library, developed by the Google Brain Team.
TensorFlow already has the code to assign the data to the GPUs and do the heavy computational work; we simply have to give it the specifics for our data and model.

Tensorflow is an example of deep learning (a neural network that has many layers).
Keras is an open-source deep-learning library in Python that provides an easy-to-use interface to TensorFlow.
tf.keras is the Keras API integrated into TensorFlow 2
More information can be found: https://www.tensorflow.org/guide/mixed_precision

Teminology:  Tensors

Tensor: A multi-dimensional array
Example: A sequence of images can be represented as a 4-D array: [image_num, row, col, color_channel]
Tensors can be used on a GPU

{{< figure src=/notes/deep-learning-distributed/img/tensors.png caption="""" width=60% height=60% >}}
Install Tensorflow

Conda
conda create -n tf2-gpu tensorflow-gpu -c conda-forge
Container (available as module and OOD)
apptainer exec --nv $CONTAINERDIR/tensorflow-2.13.0.sif python mycode.py
Build from source
https://www.tensorflow.org/install/source
TensorFlow will run GPU-enabled operations on the GPU by default. However,  more than one GPU requires appropriate changes within the TensorFlow script.

Performance and Profiling


Multithreading should give a substantial speed-up for large input pipelines via tf.data where the ETL takes place on the (multicore) CPU only. Use multiple CPU-cores to prepare the data and keep the GPU busy.
python
 train_dataset = datasets['train'].map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\
 .cache()\
 .shuffle(SHUFFLE_SIZE)\
 .batch(BATCH_SIZE)\
 .prefetch(tf.data.AUTOTUNE)


TensorBoard:

It can be used to view your graph, monitor training progress and more. Included in a Conda installation of TensorFlow.
Instructions available on the RC site: Tensorflow and UVA HPC

Profiling

line_profiler
An excellent starting point for profiling any Python script is line_profiler. This will provide high-level profiling data such as the amount of time spent on each line of your script.
Tensorflow Profiler and debugger embedded within tensorboard (GPU required).
Additional information can be found: https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras
Nsys and ncu
TensorRT is an SDK for high-performance deep learning inference. You can either use the container from NVIDIA with Singularity or build from source.


A Guide to Tensorflow Performance Optimization

Coding a Tensorflow Model: General Steps

Import Modules
Read in the data
Divide the data into a training set and a test set.
Preprocess the data
Design the Network Model
Train the model: Compile, Checkpointing, EarlyStopping and Fitting
Apply the model to the test data and display the results
Loading a checkpointed model

Make sure that you can run the CNN code:
  * TF_CNN_SingleGPU.ipynb
"
rc-learning-fork/content/notes/deep-learning-distributed/cnn.md,"What are Convolutional Neural Networks?

Originally, convolutional neural networks (CNNs) were a technique for analyzing images.
Applications have expanded to include analysis of text, video, and audio.
CNNs apply multiple neural networks to subsets of a whole image in order to identify parts of the image.

The idea behind CNN
{{< figure src=/notes/deep-learning-distributed/img/cnn_idea.jpg width=55% height=55% >}}
* Recall the old joke about the blind-folded scientists trying to identify an elephant.
* A CNN works in a similar way. It breaks an image down into smaller parts and tests whether these parts match known parts.
* It also needs to check if specific parts are within certain proximities.
   For example, the tusks are near the trunk and not near the tail.
Is the image on the left most like an X or an O?
{{< figure src=/notes/deep-learning-distributed/img/most_alike.png caption=""Images borrowed from http://brohrer.github.io/how_convolutional_neural_networks_work.html"" width=40% height=40% >}}
What features are in common?
{{< figure src=/notes/deep-learning-distributed/img/features_in_common.png width=65% height=65% caption="""" >}}
Building blocks of CNN
CNN performs a combination of layers
* Convolution Layer
  * This layer compares a feature with all subsets of the image
  * It creates a map showing where the comparable features occur
* Rectified Linear Units (ReLU) Layer
  * This layer goes through the features maps and replaces negative values with $0$
* Pooling Layer
  * This layer reduces the size of the rectified feature maps by taking the maximum value of a subset
The CNN ends with a final layer
* Classification (Fully-connected layer) layer
  * This combines the specific features to determine the classification of the image
{{< figure src=/notes/deep-learning-distributed/img/cnn_steps.png caption=""Convolution → Rectified Linear → Pooling"" width=70% height=70% >}}
These layers can be repeated multiple times. The final layer converts the final feature map to the classification.
{{< figure src=/notes/deep-learning-distributed/img/final_layer.png width=60% height=60% >}}
Example: MNIST Data
{{< figure src=/notes/deep-learning-distributed/img/mnist.jpg caption=""Image borrowed from Getting Started with TensorFlow by Giancarlo Zaccone"" width=40% height=40% >}}

The MNIST data set is a collection of hand-written digits (e.g., 0-9).
Each digit is captured as an image with 28x28 pixels.
The data set is already partitioned into a training set (60,000 images) and a test set (10,000 images).
The tensorflow packages have tools for reading in the MNIST datasets.
More details on the data are available at http://yann.lecun.com/exdb/mnist/

Why Use GPUs?
{{< figure src=/notes/deep-learning-distributed/img/params_overtime.png width=60% height=60% >}}
Over time, bigger models have been developed to handle more complex tasks, and consequently, to handle more computations. The training process involves hundreds of thousands of computations, and we need a form of parallelization to speed up the process. 
HPC systems can help meet this demand through specialized hardware, like GPUs which can provide the needed parallelization, and other hardware. 
"
rc-learning-fork/content/notes/deep-learning-distributed/machine-learning.md,"{{< figure src=/notes/deep-learning-distributed/img/dl_ml_ai.png caption=""Image borrowed from: https://www.edureka.co/blog/ai-vs-machine-learning-vs-deep-learning/"" width=75% height=75% >}}
Machine Learning Overview
Machine learning (ML) is a branch of artificial intelligence where computers learn from data and adapt the computational models to enhance performance. It is a method of analysis that allows computers to reveal information within data.
* The “learning” is not the type of learning that you and I do.
* It is a systematic approach to finding an appropriate data transformation from inputs to output.
Why ML?
* Computers can sort through data faster than humans can.
* Computers can identify patterns quickly and use these patterns for predictions or classifications.
* Machine learning can handle noisy data – it doesn’t find a perfect answer, but rather a “really good” answer.
Applications of ML

Regression techniques
Determines a mathematical model for the relationship among features or attributes so that an outcome can be predicted.
Results can be any value within a possible range  (e.g., what will the average Earth temperature be in 2050?)


Classification problem
Identifies a combination of attributes that best fits a class or category so that an object can be classified.
Results can be from a list of known possibilities  (e.g., is the tumor benign or malignant?)



Note: Examples included in this tutorial are all of classification type problems.
Types of ML

Supervised Learning:
A data set exists where the samples can be categorized into two or more classifications.
The computer uses the data set to learn how to predict the classification of an unknown sample.
Examples include Decision Trees and Deep Learning
Unsupervised Learning:
The collected data has no known classification or pattern.
The computer must identify the groups or hidden structures within the data.
Examples include Dendograms, K-means clustering, Self-organizing Maps
Reinforcement Learning:
Computer learns from positive or negative feedback
Example includes Swarm intelligence

Note: Examples included in this tutorial are all instances of supervised learning.
Data for ML

For many Machine Learning algorithms, the data is expected to be in a table format, where:
each row represents an object, and
each column has the measurements for a specific attribute or feature of the object


For supervised learning, the classifications of the objects must be known.
The data with known classifications are divided into a training set and a testing set.
The data is then used to develop a model.
The training data are submitted to an algorithm that will fit a model to the data.
The test data are submitted to the model to produce predicted classifications and determine the accuracy of the model.


Finally, the model can be used to predict classifications for “unknown” data.

ML Algorithm
The algorithm determines the best mathematical model for the code. However, you still need to provide a “framework” for the algorithm.
The framework provides the algorithm with tools for performing the learning.
{{< figure src=/notes/deep-learning-distributed/img/ml_overview.png caption="""" width=75% height=75% >}}
Deep Learning vs Machine Learning

Deep Learning is a subset of Machine Learning that differentiates itself from ML algorithms based on the methods (neural networks) it uses to solve problems.
Any deep learning algorithm would reiterate and perform a task repeatedly, improving a bit every time, in order to improve the outcome.
A deep learning program builds the feature set by itself without supervision and domain expertise. 
Unsupervised learning is not only faster, but it is usually more accurate.
Unsupervised learning needs access to immense amounts of data and compute power and takes much longer to train, but it is much faster to run tests.


ML algorithms have superior interpretability and are favorable for small amounts of data.
ML works only with sets of structured and semi-structured data, while deep learning works with both structured and unstructured data

Limitations and Challenges

DL models learn through observations. Outcome is not ”generalizable” if data was small or if its scope was limited.
If a model trains on data that contains biases, the model will reproduce those biases in its predictions.
If the learning rate is too high, then the model will converge too quickly, producing a less-than-optimal solution. If the rate is too low, then the process may get stuck, and it will be even harder to reach a solution.
There are a large amount of data and parameters.
"
rc-learning-fork/content/notes/deep-learning-distributed/_index.md,"Deep Learning (DL) is a powerful tool transforming scientific workflows. Because DL training is computationally intensive, it is well suited to HPC systems. Effective use of UVA HPC resources can greatly accelerate your DL workflows. In this tutorial, we will discuss:
* When is it appropriate to use a GPU?
* How to optimize single-GPU code?
* How to convert single-GPU code to Multi-GPU code in different frameworks and run it on UVA HPC?
The following outline presents topics that will be discussed as well as when code will be provided:
* Overview of Deep Learning
    * Example: Convolutional Neural Network
* Introduction to GPUs and GPU Computing
* Tensorflow/Keras
    * Single-GPU Code Example
* PyTorch
    * Single-GPU Code Example
* Distributed Training
    * TF Multi-GPU Code Example
    * PT Multi-GPU Code Example
    * PT Lightning Multi-GPU Code Example
* Effective Use/Remarks
Prior experience with the Python programming language and some familiarity with machine learning concepts are helpful for this tutorial. 
If you have not already done so, please download the example code here:
{{< file-download file=""/notes/deep-learning-distributed/code/distributed_dl.zip"" text=""distributed_dl.zip"" >}}"
rc-learning-fork/content/notes/building-running-c-cpp-fortran/index.md,"What is a Compiler
A compiler is a program that converts human-written source code directly into a standalone program called an executable (or binary).  This is in contrast to an interpreter, which 
is a program that executes source code, often called a script in this case, line by line.  
Compilers go through a multi-stage process to convert source code to an executable. The result is machine language and cannot be read by (most) humans.
Binaries/executables are specific to a platform, a combination of machine architecture and operating system. You cannot run a Windows binary on a Linux system, and vice versa.
When running an interpreter, the executable is the interpreter itself. Your script cannot be run directly.  Some scripts can invoke their own interpreters and run standalone, but they are not themselves binaries.
Building an Executable
The compiler first produces an object file for each source file. In Unix these end in .o 
Object files are binary (machine language) but cannot be executed. They must be linked into an executable.
Libraries are special archives of compiled code that can be invoked through their application programming interface, or API.  Like the object files, they must be linked into an executable in order to be utilized.  
The program that generates the executable from object files and any external libraries is the linker (also called a loader) is nearly always invoked through the compiler, not separately. The linker joins all object files as specified, and if run through the appropriate compiler it also links the compiler\'s runtime libraries for the source language. These are libraries used to carry out procedures intrinsic to the language. This happens automatically if the compiler matches the language of the main program; it is not necessary to add the runtime libraries explicitly.  However, in mixed-language programming it may be necessary to add them to the linking instructions.
Compilers on Rivanna
Rivanna provides multiple compilers and compiler versions, which we manage through modules. 
Gnu Compiler Collection (gcc)
The GNU Compiler Collection includes front ends for C, C++, and Fortran, as well as libraries for these languages (libc, libstdc++, libgfortran). 

gcc is the C compiler 
g++ is the C++ compiler 
gfortran is the Fortran compiler 

To see a list of available versions, type
module spider gcc
This will result in output similar to (versions will vary over time):
```
gcc:
Description:
  The GNU Compiler Collection includes front ends for C, C++,
  Objective-C, Fortran, Java, and Ada, as well as libraries for these
  languages (libstdc++, libgcj,...).

 Versions:
    gcc/system
    gcc/6.5.0
    gcc/7.1.0
    gcc/8.3.0
    gcc/9.2.0
 Other possible modules matches:
    gcccuda

```
Intel
Intel compilers frequently produce the fastest binaries for Intel architectures and are usually recommended for users who want the best performance.

icc is the C compiler
icpc is the C++ compiler 
ifort is the Fortran compiler 

module spider intel
Special note for Fortran users: When using the Intel compiler, nearly all Fortran codes must add a flag -heap-arrays to the compile line, or your executable is likely to end with a segmentation violation.
NVIDIA HPC SDK
We also offer the NVIDIA HPC SDK (software development kit) compilers.  This suite is particularly strong at programming for general-purpose GPUs, mainly of NVIDIA architecture. They provide tools such as OpenACC and OpenMP for programming for GPGPUs, but also support interfaces to CUDA through the higher-level languages, in particular C++ and Fortran.

nvcc 
nvc++
nvfortran 

bash
module spider nvhpc
Compiling a Single-File Program
Suppose your program is short and can be contained within a single file. 
If not told otherwise a compiler will attempt to compile and link the source file(s) it is instructed to compile in one step.  With only one file, no separate invocation of the linker is required.
The default name for the executable is a.out.
The option -o is used to name the binary something else.
Examples for each compiler: 


Build a C program

gcc -o myprog mycode.c
icc -o myprog mycode.c 
pgcc -o myprog mycode.c 



Build a C++ program

g++ -o myprog mycode.cxx
icpc --o myprog mycode.cxx
pgc++ -o myprog mycode.cxx



Build a Fortran program

gfortran --o myprog mycode.f90
ifort --o myprog mycode.f90 
pgfortran --o myprog mycode.f90 



Exercise 
On Rivanna, load a compiler module (your choice).  Copy one of the following 
files from /share/resources/tutorials/compilers to your home directory, or to 
a directory you create for the examples. 

mycode.c
mycode.cxx
mycode.f90 

Use the appropriate compiler to build your executable. Choose any name you wish for the executable.
Compiling and Linking Multiple Files
Most programs consist of multiple files. 
For Unix compilers the -c option suppresses linking. The compiler must then 
be run again to build the executable from the object files. 
g++ -c mymain.cxx
g++ -c mysub1.cxx
g++ -c mysub2.cxx 
g++ -o myprog mymain.o mysub1.o mysub2.o
These compiler flags are similar for all three compiler suites.
ifort -c mymain.f90 
ifort -c mysub1.f90 
ifort -c mysub2.f90
ifort -o myprog mymain.o mysub1.o mysub2.o
Linkers and Libraries
When the executable is created any external libraries must also be linked.
The compiler will search a standard path for libraries. On Unix this is typically /usr/lib, /usr/lib64, /usr/local/lib, /lib
There are two kinds of library, static and dynamic. If the library name ends
in .a it is a static library; in this case, the machine code is bundled into 
the executable at compile time.  If then name ends in .so it is a dynamic 
library; the machine code is loaded at runtime.  For dynamic libraries, the
compiler must know the path to the library at compile time, and the executable
must be able to find the library at runtime.
If you must use paths other than the defaults, you must specify those paths to the compiler. 
-L followed by a path is the option for Unix compilers; then the library must be named libfoo.a or libfoo.so.  Another option provides the shortened name of the library; it is referenced as -lfoo in either case.  
Example
g++ -o mycode -L/usr/lib64/foolib mymain.o mysub1.o mysub2.o -lfoo
As a general rule, libraries must be compiled by the same compiler that you use for your program.
For most Unix compilers, order matters! The object file containing the code being called must be linked after the object file that uses it.  Libraries come last, and must also be in the correct order.
Headers and Modules
Many codes need headers (C/C++) or modules (Fortran) for procedure prototypes, etc.  This is especially common if it is using an external library.
As was true for libraries, there is a system default search path for these ""include files.""
The compiler flag is -I/path/to/includes
The path to Fortran module files is also specified by the -I flag.
Library and Header Paths on Rivanna
Many external libraries on Rivanna are accessed through modules.
The modules system on Rivanna is hierarchical. You must first load your choice of compiler. From there you can type 
module avail
The modules at the top will be for libraries built with that compiler. 
You will generally need to provide paths for these libraries by -L and -I 
flags. You can find the paths with
printenv | grep ROOT
Most recent modules have an environment variable NAME_ROOT, e.g.
HDF5_ROOT
Example
A number of scientific and engineering codes need the input/output libraries 
HDF5 or NetCDF. We will use NetCDF for this example. 
```
module load intel
module load netcdf
printenv | grep ROOT 
```
The following should be typed on a single line:
gfortran -o myexec -L${NETCDF_ROOT}/lib -I${NETCDF_ROOT}/include mycode.f90 -lnetcdff -lnetcdf
Exercises
Exercise 1. Multiple files (no C example here)
Copy from /share/resources/tutorials/compilers your choice of files
- Copy main.cxx or main.f90
- Copy sub1.cxx or sub1.f90 
- Copy sub2.cxx or sub2.f90 
- C++ copy sub1.h and sub2.h as well
Use these three files to create an executable. 
Exercise 2. External Libraries 

Load the netcdf module for your choice of compiler suite as indicated above.

Copy simple_xy_wr.c or simple_xy_wr.cpp or simple_xy_wr.f90 (note the C++ suffix is cpp here).


Compile and link the examples. You will need to add
    -I${NETCDF_ROOT}/include and -L${NETCDF_ROOT}/lib to your link command. For C use -lnetcdf for the library. For Fortran use -lnetcdff -lnetcdf in that order. For C++ use -lnetcdf_c++ -lnetcdf in that order. 


Running Your Executable
On a Unix system, an executable must have the right permission. Unless something goes very wrong, the compiler/linker will set the ""execute bit"" for your executable; you should not need to do that yourself.
In your directory that contains at least one executable run
ls -l
Make sure you are in the same directory as the executable you want to run. Type
./myprog
Use the name you assigned your executable. ./ means current directory and is necessary because that is not in your default path.
Build Tools
This can all become very difficult to manage when codes have a lot of files. 
Various build tools have been developed. 
Make
The oldest and still most common is make. Even many newer tools like cmake generate a makefile, at least on Unix. 
Make is a tool to manage builds. It has a rigid and peculiar syntax. 
It will look for a file named makefile first, followed by Makefile (on case-sensitive systems).
The makefile defines one or more targets. The target is the product of one or more rules. 
The target is defined with a colon following its name. If there are dependencies those follow the colon.
Dependencies are other files that are required to create the current target.
Targets and Rules
Example
myexec: main.o module.o 
<tab>gfortran -o myexec main.o module.o
The tab is required in the rule. Don't ask why. The angle brackets are to indicate the character and are not typed.  
Macros (automatic targets) for rules:
- $@ represents the file name of the current target
- $< represents the name of the first prerequisite
Variables, Comments, and Continuations: 
We can define variables in makefiles 
F90=gfortran
CC=gcc
CXX=icpc
We then refer to them as $(F90), $(CC), etc. 
Common variables: F90, CC, CXX, FFLAGS, F90FLAGS, CFLAGS, CXXFLAGS, CPPFLAGS (for the preprocessor), LDFLAGS.
Comments may be inserted into a Makefile.  Anything from a # onward is ignored, unless it is a backslash \.  
The backslash is the line-continuation marker.  Be sure it is the last character on the line.  If it appears as the last character in a comment line, it allows the comment to be 
extended over multiple lines.
Suffix Rules:
If all files with a given suffix (.c, .cxx, .f90, etc.) are to be compiled the same way, we can write a suffix rule to handle them. 
This uses a phony target called .SUFFIXES.
The rule must begin with a tab as usual. 
```
.SUFFIXES: .f90 .o
$(F90) $(F90FLAGS) -c $< 
.SUFFIXES: .cxx .cpp .o 
$(CXX) -c $(CXXFLAGS) -c <$
```
Pattern Rules:
This is an extension by Gnu make (gmake), but nearly every make, especially on Linux, is gmake now.  They are similar to suffix rules. 
Gmake contains built-in pattern rules so it can handle common cases if you do not write your own rules.  For example, to compile a C code it will by default use
%.o : %.c
        $(CC) -c $(CFLAGS) $(CPPFLAGS) $< -o $@
A pattern rule that is particularly useful for Fortran 90+:
%.mod: %.o
This can be helpful because make's default pattern rule for .mod is for a programming language called Modula, but Fortran uses .mod for compiled module interface files.  Adding this pattern rule overrides the built-in rule for that suffix.
Make Options
Files with names other than makefile or Makefile can be used with the -f option
make -f Make.linux
Make creates the first target in the file unless a specific target is specified.(Some Makefiles are written to require a target.). To generate a different target, provide its name on the command line.
make pw.x
makemake
On Rivanna we provide a script that will create a skeleton Makefile. Users will have to edit it to provide names, paths as needed, etc. but it is far easier than starting from scratch. 
Create a directory for the source files you wish to build and type
makemake
The makemake script cannot distinguish files to be included in the build versus other files you may have in the directory. It will pick up all files ending in .c, .cxx, .cpp, .f, or .f90.  First edit the Makefile to remove any irrelevant file names.
At minimum you must name your program through the PROG variable. You should also specify the name of your compiler explicitly; e.g. gcc not cc.
The ""phony"" target clean is common in Makefiles.  It makes it easy to remove all compiled files and start over.  At minimum, this is necessary when compiler
options are changed.
Makemake Skeleton: 
```make
PROG = 
SRCS =  code.cxx file1.cxx file2.cxx 
OBJS =  code.o file1.o file2.o
LIBS = 
CC = cc
CXX = c++
CFLAGS = -O 
CXXFLAGS = -O
FC = f77 
FFLAGS = -O 
F90 = f90 
F90FLAGS = -O
LDFLAGS = 
all: $(PROG) 
$(PROG): $(OBJS)
        $(CXX) $(LDFLAGS) -o $@ $(OBJS) $(LIBS) 
.PHONY: clean
clean: 
        rm -f $(PROG) $(OBJS) *.mod 
.SUFFIXES: $(SUFFIXES) .f .f90 .F90 .f95
.SUFFIXES: $(SUFFIXES) .c .cpp .cxx 
.f90.o .f95.o .F90.o: 
        $(F90) $(F90FLAGS) -c $< 
.f.o: 
        $(FC) $(FFLAGS) -c $< 
.c.o: 
        $(CC) $(CFLAGS) -c $< 
.cpp.o .cxx.o:
        $(CXX) $(CXXFLAGS) -c $< 
main.o: code.cxx file1.h file2.h 
```
Exercise
Copy or move your multi-file program into its own directory. 
Run makemake in the directory.
Edit the resulting Makefile appropriately. You may remove any lines pertaining to languages you aren't using.
Run make.
Check that your executable works. 
Configure
Configure is a way to generate a Makefile automatically.
We will not discuss creating configure scripts, since they can be quite complex. But much software is distributed with them.
Configure usually takes many options and they vary by package. To see them, run from its directory 
make
./configure --help
Most configure scripts assume the software will be installed into /usr or /usr/local. You do not have permission to do this on Rivanna so you will nearly always need to use the -prefix option. 
make
./configure -prefix=/home/mst3k/udunits
Configure should produce a Makefile. 
When it has completed you can run make as usual. This is usually followed by
make
make install
Refer to the documentation for your package for more details.
Exercise 
Copy udunits.tar.gz from /share/resources/tutorials/compilers
Untar it (tar xf udunits.tar.gz). Cd into its directory. Run
configure -help
After examining the options, run it with at least the prefix option. You may wish to create a new directory for installing the package.
Build the library.
Install the library into the directory prepared for it.
CMake
CMake is a platform-independent build system.
Unlike configure, it can be used on Windows natively. 
On Rivanna, the system cmake is very old. Newer versions are available through modules. 
module spider cmake
The default module version should be sufficiently recent to build most cmake projects. 
module load cmake
Running CMake
CMake usually requires the creation of a separate build directory below the top-level directory of the project. 
CMake uses -D flags as the equivalents of many options to configure.
CMake caches your configuration into a file named CMakeCache.txt -- if you make changes you must remove this file or your changes will be ignored. It will be in the build directory if you created one.
Useful CMake Flags
You will usually need to add 
-DCMAKE_INSTALL_PREFIX=/path/to/install/location
This is the equivalent of the -prefix option to configure.
CMake does not pay (much) attention to your paths. It has its own means of finding compilers and libraries.
You will probably need to add
-DCMAKE_C_COMPILER=gcc
-DCMAKE_CXX_COMPILER=g++
-DCMAKE_FC_COMPILER=gfortran
Cmake Example
We will use an example prepared by Thom Troy (ttroy50 on GitHub)
Copy cmake-example.tar.gz from /share/resources/tutorials/compilers
Extract the tar-file and cd to the directory it creates.
Make an installation directory.
Make a build directory. Cd into it.
Load a compiler if you haven't already, for example gcc.
Load the cmake module.
Run (type cmake command all on one line) 
```
cmake .. -DCMAKE_INSTALL_PREFIX=/home/yourid/yourdir -DCMAKE_CXX_COMPILER=g++
make
```
This simple example doesn't create an installation target, so move the binary to the intended location. 
Building and Running Parallel Codes
Shared Memory
Programs using shared-memory parallelism must run on a single node. Threads are started by a master process. OpenMP is one popular library for this type of application.
OpenMP
OpenMP is a library that is built in to the compiler. It is invoked via compiler and linker flags. 


gcc 
      -fopenmp


Intel
      -qopenmp


PGI
      -mp


Generally the default for OpenMP is to create one thread per core in parallel regions.
This is not permitted on shared, resource-managed systems like Rivanna. The thread number must equal the number of cores requested.
For OpenMP we use the OMP_NUM_THREADS environment variable. 
In a SLURM script set
```plaintext
SBATCH -c 
where `N` is the number of cores you wish to use on the node.  Then in your commands, before you run your code, setbash
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
```
OpenMP Example 
Copy omphello.c or omphello.f90 from /share/resources/compilers
Using your choice of compiler, build an executable 

gcc --fopenmp omphello.c 
icc --qopenmp omphello.c 
pgcc --mp omphello.c

Or

gfortran --fopenmp omphello.f90
ifort --qopenmp omphello.f90 
pgfortran --mp omphello.c 

Run The OpenMP Program 
Add a -o <name> flag to your build step if you want a name other than a.out.
```
export OMP_NUM_THREADS=4 
./a.out 
```
Distributed memory
Programs using distributed-memory parallelism can run on multiple nodes. Independent processes that communicate through a library, usually MPI.
Building and Running MPI Programs
MPI is an external library. It must be built for the compiler with which it will be used.
We provided compiled versions of MPI for GCC and PGI. 
For Intel we recommend using the vendor\'s IntelMPI.
OpenMPI with GCC and PGI
module load gcc
Or
module load pgi
Follow this by
module load openmpi
Compile and link your code with the wrappers

mpicc for C
mpicxx for C++
mpif90 for Fortran

The wrappers insert the appropriate header/module paths and library paths, and also the correct libraries to link. You do not add them explicitly.
IntelMPI with Intel
module load intel 
module load intelmpi
Use the wrappers 

mpiicc for C
mpiicpc for C++
mpiifort for Fortran

Running MPI Programs 
MPI programs must be run under the control of an executor program. Usually this is called mpirun or mpiexec, but when submitting to SLURM we must use srun.
The mpirun and mpiexec must be told how many processes to start, and if on more than one host the list of host IDs must be provided.
The SLURM executor srun obtains the number of processes and the hostlist directly from the job assignments. Do not specify them on the command line. 
plaintext
srun mympicode
On our system, mpirun/mpiexec do not communicate with SLURM. Always use srun as the executor in SLURM scripts.  You may use mpirun on the frontend with a small number of processes for very short test runs.
plaintext
mpirun -np 4 mympicode
Exercise
Copy the file mpihello.c or mpihello.f90 from /share/resources/tutorials/compilers.
C++ programmers please note: the C++ bindings are deprecated in MPI, we just use the C routines.
Load the appropriate module for the compiler you are using.
Build the code with the appropriate wrapper. 
You can run a quick test on the frontend with 4 cores. You do not need a hostfile if all processes are on one node.
Write a SLURM script using srun to run the program. Submit to the devel partition for testing.
Integrated Development Environments
IDEs are a graphical user interface for code development. 
Examples include Microsoft Visual Studio, XCode, and  Eclipse.
IDEs installed on Rivanna:
- Geany
- Code::Blocks
- Eclipse
IDEs require access to X11, the Unix graphical windowing system. One way to accommodate this is through FastX Web.
https://rivanna-desktop.hpc.virginia.edu
Geany
Geany is a programmer\'s editor that also has some lightweight building capabilities.
Geany is easy to use and excellent for less complicated projects. It provides syntax coloring for dozens of languages.
It runs on Linux, Windows, and Mac so you can easily install it onto your personal computers. 
By default it uses gcc/g++/gfortran versions that are in the path (so load the desired gcc module before loading the geany module). 
You can alter the compiler commands from the Build menu, such as for using the intel compilers.
plaintext
Build->Set Build Commands
Building with Geany:
For a single-file program, Build->Build or the brick icon.
For a multi-file program, or more than one project, each file must be individually compiled.
plaintext
Build->Compile 
or the dropdown next to the brick.
You can use Make but must write the Makefile yourself (you can edit it with Geany). 
When complete, you can run the program with Execute if it\'s a short test.
Code::Blocks
Code::Blocks is a somewhat more conventional IDE for C/C++/Fortran.
The module is codeblocks (no colon)
module load codeblocks
It is based on the notion of ""projects"" rather than individual files.
It will need to add ""project"" files to any folders you want to use for your code. It is simplest to start each project from a new folder. 
Code can be built in two modes: Debug, or Production.  Debug adds a debugging flag -g whereas Production adds the optimization flag -O.
Eclipse
Usage of Eclipse for C/C++/Fortran is more complicated.  Please see the documentation."
rc-learning-fork/content/notes/unix-tutorial/unix_tutorial_4.md,"Wildcards
The Characters * and ?
The character * is called a wildcard, and will match against none or more character(s) in a file (or directory) name. For example, in your unixstuff directory, type
bash
% ls list*
This will list all files in the current directory starting with list. Try typing
% ls *list
This will list all files in the current directory ending with list. The character ? will match exactly one character. So ls ?ouse will match files like house and mouse, but not grouse. Try typing
bash
% ls ?list
Filename Conventions
Unix regards a directory as merely a special type of file. So the rules and conventions for naming files apply also to directories. In naming files, characters with special meanings, such as / * & %, should be avoided. Also avoid using spaces within names. The safest way to name a file is to use only alphanumeric characters, that is, letters and numbers, together with _ (underscore) and . (dot). File names conventionally start with a lower-case letter, and may end with a dot followed by a group of letters indicating the contents of the file. For example, all files consisting of C code may be named with the ending .c, for example, prog1.c. Then in order to list all files containing C code in your home directory, you need only type ls *.c in that directory.
Beware: some applications give the same name to all the output files they generate. For example, some compilers, unless given the appropriate option, produce compiled files named a.out. Should you forget to use that option, you are advised to rename the compiled file immediately, otherwise the next such file will overwrite it and the original file will be lost.
The Unix system itself ignores file suffixes.  You could name your C++ program mycode.py and Linux will not care.  However, many applications do care about file extensions.  A C++ compiler will expect the file to end in .cpp or .cxx. It will not recognize a file ending in .py, but a Python interpreter would.
Filesystem Security (Access Rights)
In your unixstuff directory, type
% ls -l (l for long listing!)
You will see that you now get lots of details about the contents of your directory, similar to the example below.
{{< code file=""/notes/unix-tutorial/snippets/ls-l.txt"" lang=""bash"" >}}
Each file (and directory) has associated access rights, which may be found by typing ls -l.
In the left-hand column is a 10-symbol string consisting of the symbols d, r, w, x, -, and, occasionally, s or S. If d is present, it will be at the left hand end of the string and indicates a directory; otherwise - will be the starting symbol of the string. The nine remaining symbols indicate the permissions, or access rights, and are taken as three groups of three.

the leftmost group of 3 gives the file permissions for the user that owns the file (or directory) (mst3k in the above example);
the middle group gives the permissions for the group of people to whom the file (or directory) belongs (users in the above example);
the rightmost group gives the permissions for all others.
The symbols r, w, etc., have slightly different meanings depending on whether they refer to a simple file or to a directory.

Access Rights on Files

r (or -), indicates read permission, that is, the presence or absence of permission to read and copy the file
w (or -), indicates write permission, that is, the permission (or otherwise) to change a file
x (or -), indicates execution permission, that is, the permission to execute a file, where appropriate

Access Rights on Directories

r allows users to list files in the directory
w means that users may delete files from the directory or move files into it
x means the right to access files in the directory or to cd into it.

So, in order to read a file, you must have execute permission on the directory containing that file, and hence on any directory containing that directory as a subdirectory, and so on, up the tree.
Some Examples
| Permissions | Meaning |
|---|---|
| -rwxrwxrwx | a file that everyone can read, write and execute (and delete) |
| -rw------- | a file that only the owner can read and write: no one else can read or write and no one has execution rights (e.g., yourmailbox file) |
Changing Access Rights
chmod (changing a file mode)
Only the owner of a file can use chmod to change the permissions of a file. The options of chmod are as follows:
| Symbol | Meaning |
|---|---|
| u | user |
| g | group |
| o | other |
| a | all |
| r | read |
| w | write (and delete) |
| x | execute (and access directory) |
| + | add permission |
| - | take away permission |
For example, to remove read write and execute permissions on the file biglist for the group and others, type
% chmod go-rwx biglist
This will leave the other permissions unaffected. To give read and write permissions on the file biglist to all,
% chmod a+rw biglist
Exercise 4A
Try changing access permissions on the file science.txt and on the directory backups. Use ls -l to check that the permissions have changed.
Shared Systems
Some multiuser systems, such as high-performance clusters, may not permit chmod on certain directories, or may automatically revert to the default set of permissions (called the umask).  This is for data protection and privacy for users.
Summary
| Command | Operation |
|---|---|
| * | match any number of characters |
| ? | match one character |
| chmod | change file or directory permissions |"
rc-learning-fork/content/notes/unix-tutorial/unix_tutorial_5.md,"A process is an executing program identified by a unique PID (process identifier). To see information about your processes, with their associated PID and status, type
bash
% ps -u mst3k
A process may be in the foreground, in the background, or be suspended. In general the shell does not return the Unix prompt until the current process has finished executing. Some processes take a long time to run and hold up the terminal. Backgrounding a long process has the effect that the Unix prompt is returned immediately, and other tasks can be carried out while the original process continues executing.
Running Background Processes
To background a process, type an & at the end of the command line. For example, the command sleep waits a given number of seconds before continuing. Type
bash
% sleep 10
This will wait 10 seconds before returning the command prompt %. Until the command prompt is returned, you can do nothing in that terminal except wait. To run sleep in the background, type
```bash
% sleep 10 &
[1] 6259
``
The&` runs the process in the background and returns the prompt immediately, allowing you to run other programs while waiting for that one to finish. The first line in the above example is typed in by the user; the next line, indicating job number and PID, is returned by the machine. The user is notified of a job number (numbered from 1) enclosed in square brackets, together with a PID and is notified when a background process is finished.  Backgrounding is useful for jobs which will take a long time to complete.
To the system, a ""job"" represents a group of processes (often just one) to which signals should be sent. Signals include backgrounding, foregrounding, and cancelling. This meaning of ""job"" must be distinguished from ""jobs"" submitted to a queueing system such as Slurm.  To a resource manager (queueing system) a ""job"" is a set of instructions to be executed.
Backgrounding a Current Foreground Process
At the prompt, type
bash
% sleep 100
You can suspend the process running in the foreground by holding down the [CTRL] key and typing [z] (written as ^Z) Then to put it in the background, type
bash
% bg
Note: do not background programs that require user interaction.
Listing Suspended and Background Processes
When a process is running, backgrounded or suspended, it will be entered onto a list along with a job number. To examine this list, type
bash
% jobs
An example of a job list could be

Suspended sleep 100
Running firefox
Running vi

To restart (foreground) a suspended processes, type
bash
% fg %jobnumber
For example, to restart sleep 100, type
bash
% fg %1
Typing fg with no job number foregrounds the last suspended process.
Killing a Process
kill (terminate or signal a process)
It is sometimes necessary to kill a process (for example, when an executing program is in an infinite loop). To kill a job running in the foreground, type ^C ([CTRL] + [c]). For example, run
bash
% sleep 100 ^C
To kill a suspended or background process, type
bash
% kill %jobnumber
For example, run
```bash
% sleep 100 &
% jobs
```
If it is job number 4, type
bash
% kill %4
To check whether this has worked, examine the job list again to see if the process has been removed.
ps (process status)
Alternatively, processes can be killed by finding their process numbers (PIDs) and using kill PID_number:
```
% sleep 100 &
% ps
PID   TTY       TIME CMD
20077 pts/5 S   0:05 sleep 100
21563 pts/5 T   0:00 firefox
21873 pts/5 S   0:25 vi
To kill off the process `sleep 100`, type
% kill 20077
and then type ps again to see if it has been removed from the list. If a process refuses to be killed, uses the `-9` option, i.e., type
% kill -9 20077
```
Linux systems support a command killall which takes the name of the process rather than its PID.
bash
killall -9 sleep
Note: It is not possible to kill off other users' processes!  Unless, of course, it is a system you control and for which you have root privileges.  The root account is the system administrator, and on Linux is all-powerful.
Summary
| Command | Operation |
|---|---|
| ls -lag | list access rights for all files |
| chmod [options] file | change access rights for named file |
| command & | run command in background |
| ^C | kill the job running in the foreground |
| ^Z | suspend the job running in the foreground |
| bg | background the suspended job |
| jobs | list current jobs |
| fg %1 | foreground job number 1 |
| kill %1 | kill job number 1 |
| ps | list current processes |
| kill 26152 | kill process number 26152 |
| killall name | kill process name |"
rc-learning-fork/content/notes/unix-tutorial/introduction.md,"Typographical Conventions
In what follows, we shall use the following typographical conventions:

Characters written in color-coded typewriter font are commands to be typed into the computer as they stand.
Characters written in standard typewriter font indicate non-specific file or directory names.
Words inserted within square brackets, e.g. [Ctrl], indicate keys to be pressed.

So, for example:
% ls anydirectory [Enter]
means ""at the Unix prompt %, type ls followed by the name of some directory, then press the key marked [Enter]."" Don't forget to press the [Enter] key: commands are not sent to the computer until this is done.
Note: Unix is case-sensitive, so ""LS"" is not the same as ls. The same applies to filenames, so myfile.txt, MyFile.txt and MYFILE.TXT are three separate files. Beware if copying files to a PC, since DOS and Windows do not make this distinction.
Introduction to the Unix Operating System
An operating system is the suite of programs that make the computer work. Historically, Unix had two ""flavors,"" System V from AT&T's Bell Labs, and BSD (for Berkeley Software Distribution).  Today that distinction is less relevant but the dominant versions of Unix still have different roots.  Linux, used for some workstations and many high-performance computing clusters, arose from System V.  Mac OS was derived from a version of BSD.  Android is a heavily-modified Linux.
The ""windowing"" (graphical) system on Linux is called X11.  Modern Linux systems all provide at least one ""desktop"" environment, similar to Windows or the Mac OS desktop. However, working at the command line is still often the most efficient use of teh system, so users can benefit by learning their way around a terminal.
A Unix operating system is made up of three parts: the kernel, the shell and the programs (applications or ""apps"").
The Kernel
The kernel of Unix is the hub of the operating system: it allocates time and memory to programs and handles the filesystem and communications in response to system calls. 
The Shell
The shell acts as an interface between the user and the kernel. When a user logs in, the login program checks the username and password, and then starts another program called the shell. The shell is a command line interpreter (CLI). It interprets the commands the user types in and arranges for them to be carried out. The commands are themselves programs: when they terminate, the shell gives the user another prompt. The adept user can customise his/her own shell, and users can use different shells on the same machine. The bash shell is the default on Linux.  Mac OS formerly used bash, but on newer releases the zsh shell is the default.  Most of the basic commands are the same.
As an illustration of the way that the shell and the kernel work together, suppose a user types rm myfile to delete the file myfile. The shell searches the filesystem, first for the file containing the program rm, and directs the kernel, through system calls, to execute the program rm on myfile. When the process rm myfile has finished, the shell then returns the prompt to the user, indicating that it is ready for further commands.
Some Features


Filename Tab Completion: By typing part of the name of a command, filename or directory and pressing the [Tab] key, the shell will complete the rest of the name automatically. If the shell finds more than one name beginning with those letters you have typed, it will halt, prompting you to type a few more letters before pressing the [Tab] key again.

Both bash and zsh support tab completion, but zsh has a somewhat more sophisticated set of features.  



History: The shell keeps a list of the commands you have typed in. If you need to repeat a command, use the cursor keys to scroll up and down the list or typehistory for a list of previous commands.


Several discussions of the differences between bash and zsh are available online, such as here.  The rest of these tutorials are targeted to bash.


Prompt
The prompt is a character or string of characters that indicates the shell is ready for a command.  It varies by shell and system, and can be customized by more advanced users.  Throughout these tutorials we will us the percent sign % as  the prompt."
rc-learning-fork/content/notes/unix-tutorial/unix_tutorial_1.md,"Files and Processes
Everything in Unix is either a file or a process. A process is an executing program identified by a unique PID (process identifier). A file is a collection of data. They are created by users using text editors, running compilers etc. Examples of files:

a document (report, essay etc.)
the text of a program written in some high-level programming language
instructions comprehensible directly to the machine and incomprehensible to a casual user, for example, a collection of binary digits (an executable or binary file)
a directory, containing information about its contents, which may be a mixture of other directories (subdirectories) and ordinary files

The Directory Structure
In Unix, folders are generally called directories.  Directories are arranged in a hierarchical structure, like an inverted tree. The top of the hierarchy is traditionally called root.
Listing Files and Directories
ls (list)
When you first log in, your current working directory is your home directory. Your home directory has the same name as your username, for example, mst3k, and it is where your personal files and subdirectories are saved. To find out what is in your home directory, type
% ls 
This is short for ""list.""  Most Unix commands are two to four letters, sometimes with unintuitive meanings.
The ls command lists the contents of your current working directory. There may be no files visible in your home directory, in which case the prompt will be returned. Alternatively, there may already be some files or folders created when your account was set up. 
In most Unix systems, ls does not report hidden files by default.  Files or directories with names beginning with a dot (.) are hidden and usually contain important program configuration information. They are hidden because you should not change them unless you understand what you are doing.  To list all files in your home directory including those whose names begin with a dot, type
% ls -a
The -a is an example of a command-line option. The options change the behavior of the command. There are online manual pages that tell you which options a particular command can take, and how each option modifies the behavior of the command. 
Making Directories
mkdir (make directory)
We will now make a subdirectory in your home directory to hold the files you will be creating and using in the course of this tutorial. To make a subdirectory called unixstuff in your current working directory type
% mkdir unixstuff
To see the directory you have just created, type
% ls
Changing to a Different Directory
cd (change directory)
The command cd _directory_ means change the current working directory to ""directory"". The current working directory may be thought of as the directory you are in, i.e. your current position in the directory tree. To change to the directory you have just made, type
% cd unixstuff
Type ls to see the directory's contents (it should be empty).
Exercise 1A
Make another directory inside the unixstuff directory called backups.
The Directories . and ..
While still in the unixstuff directory, type
% ls -a
The unixstuff directory (and in all other directories) contains two special directories called (.) and (..). In Unix, (.) means the current directory, so typing
% cd .
means stay where you are (the unixstuff directory). This may not seem very useful at first, but using (.) as the name of the current directory will save a lot of typing, as we shall see later in the tutorial. (..) means the parent of the current directory, so typing
% cd ..
will take you one directory up the hierarchy. Try it now.
Note: there is a space between cd and the dot or double dot. Also note: typing cd with no argument always returns you to your home directory. This is very useful if you are lost in the file system.
Pathnames
pwd (print working directory)
Pathnames enable you to work out where you are in relation to the whole filesystem. For example, to find out the absolute pathname of your home directory, type cd to get back to your home directory and then type
bash
% pwd
Most of the time you should see /home/mst3k.  On a multiuser central system like Rivanna, /home may be a symbolic link, i.e. an alias, for something else.  To see it, type
bash
% pwd -P
The full pathname may look something like this: /sfs/qumulo/qhome/mst3k. 
Exercise 1B
Use the commands ls, pwd and cd to explore the file system. (Remember, if you get lost, type cd with no argument to return to your home directory.)
More About Home Directories and Pathnames
Understanding Paths
First type cd to get back to your home directory, then type
bash
% ls unixstuff
to list the contents of your unixstuff directory. Now type
bash
% ls backups
You will get a message like this:
backups: No such file or directory
The reason is that ""backups"" is not in your current working directory. To use a command on a file (or directory) not in the current working directory (the directory you are currently in), you must either cd to the correct directory, or specify its full pathname. To list the contents of your backups directory, you must type
bash
% ls unixstuff/backups
This path starts from the current location.  A path may be absolute or relative.  An absolute path starts from the root location.  The absolute path is
bash
ls /home/mst3k/unixstuff/backups
A relative path starts from the current working directory.  The special symbols . and .. are often used for relative paths.    
~ (your home directory)
Home directories can also be referenced by the tilde ~ character. It can be used to specify paths starting at your home directory. So typing
bash
% ls ~/unixstuff
will list the contents of your unixstuff directory, no matter where you currently are in the file system. What do you think
bash 
% ls ~
would list? What do you think
% ls ~/..
would list?
Getting Help
Online Manuals
There are online manuals which give information about most commands. The manual pages tell you which options a particular command can take, and how each option modifies the behaviour of the command. Type man command to read the manual page for a particular command. For example, to find out more about the wc (word count) command, type
bash
% man wc
In newer Linux systems, most commands have a --help option
bash
% wc --help
However, man sends output through a pager, whereas --help prints directly to the console.
Another useful command,
% whatis wc
gives a one-line description of the command, but omits any information about options, etc.
Summary
| Command | Operation |
|---|---|
| ls | list files and directories |
| ls -a | list all files and directories |
| mkdir | make a directory |
| cd directory | change to directory |
| cd | change to home directory |
| cd ~ | change to home directory |
| cd .. | change to parent directory |
| pwd | display the path of the current directory |
| man | display the manual pages for the specified command |
| whatis | display a description of the specified command |"
rc-learning-fork/content/notes/unix-tutorial/unix_tutorial_6.md,"exit
Exit the current shell. If it is the login shell, this command logs the user off.
which
The which command indicates the path to the executable specified.
bash
% which myexec
The which command returns the location of the executable according to the rules used to search paths. The shell always searches from left to right in the list contained in the PATH environment variable; the first executable of the specified name is the one that is used.
wc
The wc command returns the number of lines, words, and characters in an ASCII file. A word is defined as a non-zero length string surrounded by whitespace.
% wc myfile.txt
To print only the number of lines, use
bash
% wc –l  myfile.txt
diff
diff shows the differences between two ASCII files on a per-line basis.
bash
% diff file1 file2
find
find is an extremely powerful command with many options. The simplest and most common use of it is to search for a file of a given name or with a name containing a pattern.
bash
% find . -name myscript.sh
This starts from current directory (.) and searches for myscript.sh. The period is optional under Linux (but not under Mac OSX). To search for a name with a pattern it must typically be enclosed in quotes
bash
% find . -name ""*.sh""
See the manpage or examples online for more usage patterns of this command.
du
The du command outputs the number of kilobytes used by each subdirectory. Useful if you have gone over quota and you want to find out which directory has the most files. Some options make it more useful; in particular, -s summarizes directories and -h prints it in human-readable format. In your home directory, type
bash
% du -s -h *
gzip and zip
This reduces the size of a file, thus freeing valuable disk space. For example, type
bash
% ls -l science.txt
and note the size of the file. Then to compress science.txt, type
bash
% gzip science.txt
This will compress the file and place it in a file called science.txt.gz. To see the change in size, type ls -l again. To uncompress the file, use the gunzip command.
% gunzip science.txt.gz
Most Linux systems also provide the standard zip and unzip commands.
bash
% zip science.txt.zip
% unzip science.txt.zip
tar (tape archive)
The standard archive format in Linux is tar.  Tar is usually used to bundle directories.  (Zip can also be used for this purpose.)  Typically the output file is compressed with gzip.
bash
tar czf mydir.tar.gz mydir
The c option creates the tarfile (also called a ""tarball"").  The f option is for file, and this form of the command must be followed by the name of the file to contain the archive.  The z option gzips the file. 
To extract the files
bash
tar xf mydir.tar.gz
Newer versions of tar can detect that the archive is zipped, so a z is not necessary for extraction.  The x option extracts.  This will create the directory mydir if it does not exist.  If it does, the contents will be replaced by the contents of mydir.tar.gz.
file
file classifies the named files according to the type of data they contain, for example ASCII (text), pictures, compressed data, etc. To report on all files in your home directory, type
bash
% file *
cut
The cut command extracts selected portions of a line, based on fields separated by a delimiter
% cut­?d delim ­?fC1,C2,C3
Examples:
bash
% cut -d ' ' ?f1 /etc/resolve.conf
% cat myfile | cut -c 80
sort
This command sorts lines of a text file, based on command-­line options. The default is to sort alphabetically, based on lexicographical ordering (in which e.g. 100 comes before 2).
bash
% sort mylist.txt
uniq
Removes duplicate lines (file must be sorted first since it only compares lines pairwise).
bash
% uniq mylist.txt
A frequent pattern is to pipe the output of sort into uniq
bash
% sort animals | uniq
history
The bash shell keeps an ordered list of all the commands that you have entered. Each command is given a number according to the order it was entered.
bash
% history (show command history list)
If you are using the bash or tcsh shell, you can use the exclamation character (!) to recall commands easily.
bash
% !! (recall last command)
% !-3 (recall third most recent command)
% !5 (recall 5th command in list)
% !grep (recall last command starting with grep)
You can increase the size of the history buffer by typing
% set history=100"
rc-learning-fork/content/notes/unix-tutorial/unix_tutorial_2.md,"Copying Files
cp (copy)
cp file1 file2 is the command which makes a copy of file1 in the current working directory and calls it file2. 
For our example we will create a file science.txt.  Click the down-arrow icon to download the file.  Use whatever method you know to place this file into your home directory.  
{{< code-download file=""/notes/unix-tutorial/snippets/science.txt"" lang=""no-highlight"" >}}
bash
% cd ~/unixstuff
Then at the Unix prompt, type,
bash
% cp ../science.txt .
Note: Don't forget the dot (.) at the end. Remember, in UNIX, the dot means the current directory. 
The above command means copy the file science.txt from the parent directory to the current directory, keeping the name the same.  To change the name, use
bash
% cp ../science.txt ./newname
Exercise 2A
Create a backup of your science.txt file by copying it to a file called science.bak.
Moving Files
mv (move)
mv file1 file2 moves (or renames) file1 to file2. To move a file from one place to another, use the mv command. This has the effect of moving rather than copying the file, so you end up with only one file rather than two. It can also be used to rename a file, by moving the file to the same directory, but giving it a different name. We are now going to move the file science.bak to your backup directory. First, change directories to your unixstuff directory (can you remember how?). Then, inside the unixstuff directory, type
bash
% mv science.bak backups/.
Type ls and ls backups to see if it has worked.
Removing Files and Directories
rm (remove), rmdir (remove directory)
To delete (remove) a file, use the rm command. As an example, we are going to create a copy of the science.txt file then delete it. Inside your unixstuff directory, type
bash 
% cp science.txt tempfile.txt
Confirm the file was created:
bash
% ls
Now delete it:
bash
% rm tempfile.txt
% ls
You can use the rmdir command to remove a directory, but only if it is empty. Try to remove the backups directory. You will not be able to since Unix will not let you remove a non-empty directory.
To remove a non-empty directory use 
bash
rm -rf directory
{{< warning >}}
The above command will remove the directory without confirming anything!  Be extremely careful with it!
{{< /warning >}}
You can request confirmation with
bash
% rm -if directory
though this may be tedious.  The -i option (inquire) also works for rm
bash
% rm -i myfile
Exercise 2B
Create a directory called tempstuff using mkdir, then remove it using the rmdir command.
Displaying the Contents of a File on the Screen
cat (concatenate)
The cat command can show a text file's contents
bash
% cat science.txt
Be sure to use the correct path to the file. Cat can also join two text files, hence its name.
bash
% cat file1 file2 > file3
THe > sign is a redirection, which we will discuss later.
clear (clear screen)
Clear the screen.
more
The command more prints the contents of a file onto the screen a page at a time. Type
bash
% more science.txt
Press the [space bar] if you want to see another page. Type [q] if you want to quit reading. 
The more command is an example of a pager, a program that ""pages"" through a text file.
head
The head command writes the first ten lines of a file to the screen.
bash
% head science.txt
Now type
bash
% head -5 science.txt
What difference did the -5 make to the head command?
tail
The tail command writes the last ten lines of a file to the screen. Clear the screen and type
bash
% tail science.txt
How can you view the last 15 lines of the file?
Searching the Contents of a File
Simple Searching Using more
Using more, you can search through a text file for a keyword (pattern). For example, to search through science.txt for the word 'science', type
bash
% more science.txt
then, still in more (i.e. don't press [q] to quit), type a forward slash [/] followed by the word to search
no-highlight
/science
The more command finds and highlights the keyword. Type [n] to search for the next occurrence of the word.
grep (don't ask why it is called grep)
grep is one of many standard Unix utilities. It searches files for specified words or patterns. First clear the screen, then type
bash
% grep science science.txt
As you can see, grep has printed out each line containing the word science... or has it? Try typing
bash
% grep Science science.txt
The grep command is case-sensitive; it distinguishes between Science and science. To ignore upper/lower case distinctions, use the -i option, i.e. type
bash
% grep -i science science.txt
To search for a phrase or pattern, you must enclose it in single quotes (the apostrophe symbol). For example to search for spinning top, type
bash
% grep -i 'spinning top' science.txt
Some other options for grep are: -v (display those lines that do NOT match); -n (precede each matching line with the line number); and -c (print only the total count of matched lines). Try some of them and see the different results. Don't forget, you can use more than one option at a time, for example, the number of lines without the words science or Science is
bash
% grep -ivc science science.txt
wc (word count)
A handy little utility is the wc command, short for word count. To do a word count on science.txt, type
bash
% wc -w science.txt
To find out how many lines the file has, type
bash
% wc -l science.txt
Summary
| Command | Operation |
|---|---|
| cp file1 file2 | copy file1 and call it file2 |
| mv file1 file2 | move or rename file1 to file2 |
| rm file | remove file |
| rmdir directory | remove directory |
| cat file | display file |
| more file | display file a page at a time |
| head file | display the first few lines of a file |
| tail file | display the last few lines of file |
| grep 'keyword' file | search file for keywords |
| wc file` | count number of lines/words/characters in file |"
rc-learning-fork/content/notes/unix-tutorial/unix_tutorial_3.md,"Redirection
Most processes initiated by Unix commands write to the standard output (that is, they write to the terminal screen), and many take their input from the standard input (that is, they read it from the keyboard). There is also the standard error, where processes write their error messages, by default to the terminal screen. Type cat without specifying a file to read
bash
% cat
Then type a few words on the keyboard and press the [Return] key. Finally hold the [CTRL] key down and press [d] (written as ^D for short) to end the input. What has happened? If you run the cat command without specifying a file to read, it reads the standard input (the keyboard), and on receiving the 'end of file' (^D), copies it to the standard output (the screen). In UNIX, we can redirect both the input and the output of commands.
Redirecting the Output
We use the > symbol to redirect the output of a command. For example, to create a file called list1 containing a list of fruit, type
bash
% cat > list1
Then type in the names of some fruit as follows. Press [Return] after each one. Terminate with the end-of-file marker control-d (^D).
no-highlight
pear 
banana 
apple 
^D
The cat command reads the standard input (the keyboard) and the > redirects the output, which normally goes to the screen, into a file called list1. To read the contents of the file, type
bash
% cat list1
Exercise 3A
Using the above method, create another file called list2 containing the following fruit: orange, plum, mango, grapefruit. The form >> appends standard output to a file. So to add more items to the file list1, type
bash
% cat >> list1
Then type in the names of more fruit
no-highlight
peach 
grape 
strawberry
^D
To read the contents of the file, type
bash
% cat list1
You should now have two files. One contains six fruit names, the other contains four fruits. We will now use the cat command to join (concatenate) list1 and list2 into a new file called biglist. Type
bash
% cat list1 list2 > biglist
This reads the contents of list1 and list2 in turn, then outputs the text to the file biglist. To read the contents of the new file, type
bash
% cat biglist
Redirecting the Input
We use the < symbol to redirect the input of a command. The command sort alphabetically or numerically sorts a list. Type
bash
% sort
Using < you can redirect the input to come from a file rather than the keyboard. For example, to sort the list of fruit, type
bash
% sort < biglist
and the sorted list will be output to the screen. To output the sorted list to a file, type
bash
% sort < biglist > slist
Use cat to read the contents of the file slist.
Pipes
To see who is on the system with you, type
bash
% who
One method to get a sorted list of names is to type
bash
% who > names.txt
% sort < names.txt
This is a bit slow and you have to remember to remove the temporary file called names.txt when you have finished. What you really want to do is connect the output of the who command directly to the input of the sort command. This is exactly what pipes do. The symbol for a pipe is the vertical bar | which, on a US keyboard, is above Enter on the right, with the backslash. For example, typing
bash
% who | sort
will give the same result as above, but quicker and cleaner. To find out how many users are logged on, use wc (word count) with the option -l (ell) for number of lines only:
bash
% who | wc -l
Exercise 3B
Using pipes, print all lines of list1 and list2 containing the letter 'p', sort the result, and print to a file sorted_plist.txt.  Hint: from grep --help find an option to print only the line, omitting the file name.
{{< spoiler text=""Solution"" >}}
{{< code file=""/notes/unix-tutorial/snippets/ex3b.txt"" lang=""bash"" >}}
{{< /spoiler >}}
Summary
| Command | Operation |
|---|---|
| command > file | redirect standard output to a file |
| command >> file | append standard output to a file |
| command < file | redirect standard input from a file |
| command1 | command2 | pipe the output of command1 to the input of command2 |
| cat file1 file2 > file0 | concatenate file1 and file2 to file0 |
| sort | sort data |
| who | list users currently logged in |"
rc-learning-fork/content/notes/unix-tutorial/unix_tutorial_7.md,"Variables are a way of passing information from the shell to programs when you run them. Programs look ""in the environment"" for particular variables and if they are found will use the values stored. Some are set by the system, others by you, yet others by the shell, or any program that loads another program. Standard Unix variables are split into two categories, environment variables and shell variables. In broad terms, shell variables apply only to the current instance of the shell and are used to set short-term working conditions. Environment variables are exported and have a farther reaching significance; those set at login are valid for the duration of the session. By convention, environment variables are written in UPPERCASE while shell variables usually have lowercase names.
Environment Variables
An example of an environment variable is the $SHELL variable. The value of this is the current shell you are using. Type
bash
% echo $SHELL
More examples of environment variables are
bash
$USER (your login name)
$HOME (the path name of your home directory)
$PWD (current working directory)
$DISPLAY (the name of the computer screen to display X windows; only set if X is enabled)
$PATH (the directories the shell should search to find a command)
Using and Setting Variables
Environment variables are set using the export command (bash or zsh) or the setenv command (tcsh or csh), displayed using the printenv (bash, tcsh) or env (bash, zsh) commands, and unset using the unsetenv command. To show all values of these variables, type
bash
% printenv | more
To set a value of an environment variable, type (for bash)
bash
% export VAR=value
Sourcing
A group of shell commands can be placed into a file and then sourced.  When a file is sourced, the commands are executed as if they had been typed at the command line in the current shell.  For example, if several environment variables needed to be set over and over again, they could be collected into a file such as this simple script called envs.sh:
{{< code-download file=""/notes/unix-tutorial/snippets/envs.sh"" lang=""bash"" >}}
Exercise 6A
Download the envs.sh file to the Unix system you are using.  Run the command
bash
source envs.sh
Print the values of the environment variables in the file.  To print the value of the shell variable ncpus, type
bash
echo $ncpus
Dotfiles
Each time you log in to a Unix host, the system looks in your home directory for initialization files. Information in these files is used to set up your working environment. The first initialization file sourced is the login initialization. It is sourced only in the login shell.  Note: modern ""desktop"" user interfaces tend to ""swallow"" the login setup file, and it may be difficult to determine what is happening in these cases if there is an error.
At login the bash shell first sources .bash_profile or .profile (if .bash_profile exists .profile will be ignored). Child shells source .bashrc.  The zsh sources .zprofile and child shells source .zshrc. Two older shells, csh (C shell) and tcsh, read .login for login shells and .cshrc or .tcshrc for all other shells. 
Note that all these file names begin with periods or ""dots""; hence they are called dotfiles.  As we have learned, dotfiles are hidden and will only be visible with ls -a. 
The .bash_profile, .profile, or .login is to set conditions which will apply to the whole session and/or to perform actions that are relevant only at login.  The .bashrc, .zshrc, or .tcshrc file is used to set conditions and perform actions specific to the shell and to each invocation of it. The rc stands for resource; many Unix dotfiles use this convention to set resources.
If you wish for your login shell to source the .bashrc also, add the lines
bash
if [ -f ~/.bashrc ];
  then . ~/.bashrc
fi
to the .bash_profile script. 
Warning: NEVER put commands that run graphical displays (e.g. web browsers) in your dotfiles. If you change your .bashrc you can force the shell to reread it by using the shell source command.
% source ~/.bashrc
Setting the Path
When you type a command, your path (or $PATH) variable defines in which directories the shell will look to find the command you typed. If the system returns a message saying ""command: Command not found"", this indicates that either the command doesn't exist at all on the system or it is simply not in your path. 
For example, suppose you have installed a program called ""units"" into your home directory in a folder called units174.  Units is a simple utility that can convert Imperial to metric and vice versa, from SI to cgi, and so forth.  This folder contains a bin subdirectory in which the executable is located.  To run units, you must either directly specify the units path (~/units174/bin/units), or you need to have the directory ~/units174/bin in your path. You can add it to the end of your existing path (the $PATH represents this) by issuing the command:
bash
% export PATH=$PATH:$HOME/units174/bin
If you have units installed you can test that this worked by trying to run units in any directory other than where units is actually located.
% cd; units
Hint: You can run multiple commands on one line by separating them with a semicolon. 
To add this path permanently, add the export line to your .bashrc file after the list of other commands. Make sure that you include the $PATH when you reset it, or you will lose access to basic system commands!"
rc-learning-fork/content/notes/unix-tutorial/_index.md,"These tutorials are derived from the excellent tutorials from the University of Surrey, UK, with some minor modifications for our site. The originals can be found here."
rc-learning-fork/content/notes/deep-learning-hpc/gpu_resource_usage.md,"NVIDIA GPU Resource Usage
nvidia-smi  will report GPU utilization and memory usage for NVIDIA GPUs.
* GPU Utilization refers to the percentage of time that at least one kernel was running on the GPU.
watch -n 1 nvidia-smi will update the display every second.
{{< figure src=/notes/deep-learning-hpc/img/nvidia.png caption=""Source: https://medium.com/analytics-vidhya/explained-output-of-nvidia-smi-utility-fc4fbee3b124 and https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf"" width=80% height=80% >}}
GPU Resource Usage
gpustat  will report GPU utilization and memory usage.
```bash

module load gpustat
gpustat
```

{{< figure src=/notes/deep-learning-hpc/img/gpustat.png caption=""Source: https://github.com/wookayin/gpustat"" width=90% height=90% >}}
PyTorch

Correct GPU memory usage will be reported by the previous tools.

TensorFlow/Keras

By default, TensorFlow automatically allocates ALL of the GPU memory so the previous tools will show that all (or almost all) of the GPU memory is being used.
To track the amount of GPU memory actually used, you can add these lines to your python script:
python
import os
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

Visit the Tensorflow website for additional information.
Check Your Knowledge

Find the name of the GPU that you have access to.
"
rc-learning-fork/content/notes/deep-learning-hpc/resource_allocation_tools.md,"Computations on the CPU or GPU
{{< table >}}
| Task  | CPU or GPU |
| ---   | --- |
| Data Preprocessing    | Either, but probably CPU |
| Deep Learning Training   | GPU |
| Deep Learning Inference  | Either, but probably GPU |
{{< /table >}}
When you request memory for Rivanna, that is CPU memory. If you request a GPU, you will receive all of the GPU memory."
rc-learning-fork/content/notes/deep-learning-hpc/gpu_hpc.md,"{{< table >}}
| GPU | Full Name | Year Launched | Memory | # of Tensor Cores |
| --- | --- | --- | --- | ---- |
| A100 | NVIDIA A100 | 2020 | 40GB or 80GB | 432 (3rd gen) |
| A6000 | NVIDIA RTX A6000 | 2020 | 48GB | 336 (3rd gen) |
| A40 | NVIDIA A40 | 2020 | 48GB | 336 (3rd gen) |
| RTX3090 | NVIDIA GeForce RTX 3090 | 2020 | 24GB | 328 (3rd gen) |
| RTX2080Ti | NVIDIA GeForce RTX 2080 Ti | 2018 | 11GB | 544 (2nd gen) |
| V100 | NVIDIA V100 | 2018 | 32GB | 640 (1st gen) |
{{< /table >}}
UVA-NVIDIA DGX BasePOD

10 DGX A100 nodes
8 NVIDIA A100 GPUs.
80 GB GPU memory options.
Dual AMD EPYC:tm:; nodes: Series 7742 CPUs, 128 total cores, 2.25 GHz (base), 3.4 GHz (max boost).
2 TB of system memory.
Two 1.92 TB M.2 NVMe drives for DGX OS, eight 3.84 TB U.2 NVMe drives forstorage/cache.
Advanced Features:
NVLink for fast multi-GPU communication
GPUDirect RDMA Peer Memory for fast multi-node multi-GPU communication
GPUDirect Storage with 200 TB IBM ESS3200 (NVMe) SpectrumScale storage array
Ideal Scenarios:
Job needs multiple GPUs on a single node or multi node
Job (single or multi-GPU) is I/O intensive
Job (single or multi-GPU) requires more than 40GB of GPU memory

Note: The POD is good if you need multiple GPUs and very fast computation.
GPU access on UVA HPC
When you request memory for UVA HPC, that is CPU memory.
If you request a GPU, you will receive all of the GPU memory.

Choose ""GPU"" or ""Interactive"" as the HPC Partition in OOD
Optionally, choose GPU type and number of GPUs
POD nodes are contained in the gpu partition with a specific Slurm constraint.
Slurm script:
```bash

SBATCH -p gpu
SBATCH --gres=gpu:a100:X   # X number of GPUs
SBATCH -C gpupod
* Open OnDemandnohighlight
--constraint=gpupod
```
Note: Only one person can be using a GPU at a time."
rc-learning-fork/content/notes/deep-learning-hpc/access_dl_containers.md,"Software on HPC is accessed via environment modules or containers.

Software Module:
Examples: R, Rstudio, JupyterLab, TensorFlow, PyTorch
Full list of software available on HPC: https://www.rc.virginia.edu/userinfo/hpc/software/complete-list/
Container:
Containers bundle an application, the libraries and other executables it may need, and even the data used with the application into portable, self-contained files called images.
Containers simplify installation and management of software with complex dependencies and can also be used to package workflows.

Visit our documentation on HPC software modules and containers for more information.
Access through Open OnDemand
{{< figure src=/notes/deep-learning-hpc/img/OOD_jypnb.png width=70% height=70% >}}

Click on the kernel to open a Jupyter Notebook.
Packages from the selected kernel will be available for use in the notebook.

Run DL Script on the Command Line
Use the PyTorch container:
bash
module load apptainer pytorch 
apptainer run --nv $CONTAINERDIR/pytorch-2.0.1.sif file_name.py
Use the TensorFlow/Keras container:
bash
module load apptainer tensorflow 
apptainer run --nv $CONTAINERDIR/tensorflow-2.13.0.sif file_name.py 

The --nv flag tells the command to use the GPU
The default command defined in each container is python so using run basically executes python file_name.py

Check Your Knowledge

Log in to Rivanna using the Interactive partition using the following parameters.
2 hours
8 cores
Allocation: hpc_training
GPU: yes, 1
Copy the folder project/hpc_training/dl_with_hpc to your home or scratch account using one of the following:
```bash
cp -r /project/hpc_training/dl_with_hpc ~/<...>

OR
cp -r /project/hpc_training/dl_with_hpc /scratch//<...>
3. Convert __example1.ipynb__ file to __example1.py__ file (make sure you are in the `dl_with_hpc` folder)bash
jupyter nbconvert --to python example1.ipynb
``
4. Runexample1.py` using the Pytorch container."
rc-learning-fork/content/notes/deep-learning-hpc/code_profiling.md,"A code profiler provides information about how much time each line of a program takes to run.
* This information is helpful for effectively speeding up code execution time.
* There are various profilers available for Python.  A recommendation is line_profiler (https://github.com/pyutils/line_profiler).
Install line_profiler
Use the PyTorch container:
bash
module load apptainer pytorch 
apptainer exec $CONTAINERDIR/pytorch-2.0.1.sif pip install line_profiler 
Use the TensorFlow container:
bash
module load apptainer tensorflow 
apptainer exec $CONTAINERDIR/tensorflow-2.13.0.sif pip install line_profiler 
Outside of a container:
bash
pip install --user line_profiler 
Run line_profiler


In the code file, include the import statement as below and use @profile to decorate the functions you would like to profile:
python
from line_profiler import profile
...
@profile
def fcn_to_profile(arg1, arg2, ...):
...


Run line_profiler, use the --nv flag for GPU use


Pytorch:
bash
LINE_PROFILE=1 apptainer run --nv $CONTAINERDIR/pytorch-2.0.1.sif file_name.py


Tensorflow:
bash
LINE_PROFILE=1 apptainer run --nv $CONTAINERDIR/tensorflow-2.13.0.sif file_name.py 


Profiler results will be printed out into two text files (the files are the same): profile_output.txt and profile_output_[TIMESTAMP].txt.


Visit the Line Profiler documentation for more information.
Notes:
- Running the command without LINE_PROFILE=1 will just run file_name.py but not profile it.
- line_profiler has a very slight overhead (for code run on GPU).  Some notive more of a slow down on strictly CPU code (~40 more seconds for code that should run in ~160 seconds).
Check Your Knowledge

Install line_profiler.
Use line_profiler to profile the function ""train"" in example1.py.
While the code is running, open a terminal and watch the nvidia-smi output.
How was the GPU utilization?
Which line takes the longest to run?  Does this surprise you?
What would you suggest we do to increase the efficiency of the code?


"
rc-learning-fork/content/notes/deep-learning-hpc/cpu_resource_usage.md,"CPU Resource Usage For Running Jobs
For a  running job, the sstat command will report on CPU and memory usage.  
In this example, the job has been running on 20 cores for about 4 days.
{{< figure src=/notes/deep-learning-hpc/img/sstat.png caption="""" width=70% height=70% >}}
CPU Resource Usage For Completed Jobs
For a  completed job , the seff command will return an efficiency report.
{{< figure src=/notes/deep-learning-hpc/img/CPU_Resource_Usage.png width=60% height=60% >}}

CPU Efficiency: This is good usage for the number of cores.
Memory Efficiency: Only about 4 GB of (CPU) memory was needed.

Visit our documentation about CPU and Memory Usage for more information. 
CPU Efficiency
It may be the case that even if CPU Efficiency is a low percentage, you need all of the requested CPU cores for a specific part of the code, e.g., data preprocessing. In this case, request the number of CPU cores that you need for the compute intensive part of the code.
Previous Job Numbers
The sacct command will return a list of previous Slurm jobs on the current day.
* To get jobs run on previous days, use the -S flag and provide a start date
* Ex: to display all Slurm jobs submitted after 2/1/2024
bash
$ sacct -X -S2024-02-01
(the -X flag is optional, but tells sacct to skip the output of intermediate steps):
Check Your Knowledge

Find your most recent job number using sacct.
Print out the resource usage using seff for the job number you found in step 2.
How efficient was your CPU and memory usage?
"
rc-learning-fork/content/notes/deep-learning-hpc/hardware_overview.md,"HPC Overview
{{< figure src=/notes/deep-learning-hpc/img/RC_HPC.png caption=""Source: https://www.rc.virginia.edu/userinfo/hpc/#hardware-configuration"" width=55% height=55% >}}
Partitions:

Standard (no GPUs)
Largemem (no GPUs)
GPU (compute node with NVIDIA GPU(s))
Interactive  (compute node with NVIDIA GPU(s))

Visit our documentation on HPC hardware configuration for more information.
GPU General Overview

Graphics Processing Units (GPUs), originally developed for accelerating graphics rendering, can dramatically speed up any simple but highly parallel computational processes (General Purpose GPU).

CPU vs. GPU:
{{< table >}}
| CPU | GPU |
| --- | --- |
| Several Cores (100-1)  | Several Cores (103-4) |
| Low Latency | High Throughput |
| Generic Workload (Complex & Serial Processing) | Specific Workload (Simple & Highly Parallel) |
| Up to 1.5 TB / node on Rivanna   | Up to 80 GB /device on Rivanna      |
{{< /table >}}

Integrated GPU vs Discrete GPUs:
Integrated GPUs are used mostly for graphics rendering and gaming
Dedicated GPUs are designed for intensive computations

Vendors and Types:

NVIDIA, AMD, Intel
Datacenter : K80, P100, V100, A100, H100 (NVIDIA); MI300A, MI300X (AMD)
Workstations: A6000, Quadro (NVIDIA)
Gaming: GeForce RTX 20xx, 30xx, 40xx (NVIDA), Radeon (AMD)
Laptops and desktops: GeForce (NVIDIA), Radeon (AMD), Iris (Intel)

Programming GPUs
Several libraries and programming models have been developed to program GPUs.

CUDA 
CUDA is a parallel computation platform, developed by NVIDIA, for general-purpose programming on NVIDIA hardware.
HIP
HIP is a programming interface from AMD that allows developers to target either NVIDIA or AMD hardware.
OpenCL 
OpenCL is a more general parallel computing platform, developed by Apple. It allows software to access CPUs, GPUs, FPGAs, and other devices. 
SYCL
SYCL started as an outgrowth of OpenCL but is now independent of it.
Kokkos
Kokkos is another programming model that attempts to be device-independent. It can target multicore programming (OpenMP), CUDA, HIP, and SYCL.

Most of these programming paradigms can be used from Python, but nearly all machine learning/deep learning packages are based on CUDA and  will only work with NVIDIA GPUs."
rc-learning-fork/content/notes/deep-learning-hpc/choose_gpu.md,"GPUs on HPC
{{< table >}}
| GPU | Full Name | Year Launched | Memory | # of Tensor Cores |
| --- | --- | --- | --- | --- |
| A100 | NVIDIA A100 | 2020 | 40GB or 80GB | 432 (3rd gen) |
| A6000 | NVIDIA RTX A6000 | 2020 | 48GB | 336 (3rd gen) |
| A40 | NVIDIA A40 | 2020 | 48GB | 336 (3rd gen) |
| RTX3090 | NVIDIA GeForce RTX 3090 | 2020 | 24GB | 328 (3rd gen) |
| RTX2080Ti | NVIDIA GeForce RTX 2080 Ti | 2018 | 11GB | 544 (2nd gen) |
| V100 | NVIDIA V100 | 2018 | 32GB | 640 (1st gen) |
{{< /table >}}
Wait Time in the Queue

You may not need to request an A100 GPU!
Requesting an A100 may mean you wait in the queue for a much longer time than using another GPU,
This could give you a slower overall time (wait time + execution time) than if you had used another GPU.

{{< figure src=/notes/deep-learning-hpc/img/queue_wait_graph.png caption=""Photo Source: https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis"" width=80% height=80% >}}
Memory Required to Train a DL Model
Generally, you will choose a GPU based on how much GPU memory you need. But, it is a hard problem to determine how much GPU memory a DL model will need for training  before  training the model. 
* In addition to storing the DL model, training also requires additional storage space such as:
  * Optimizer states
  * Gradients
  * Data (how much is determined by the batch size)
* Training can also use automatic mixed precision which lowers the amount of memory needed
Visit https://blog.eleuther.ai/transformer-math/ for more information on math related to computation and memory usage for transformers.
General Advice

If you are learning about DL and doing tutorials, the GPUs in the Interactive partition are probably fine.
You can leave the GPU choice as default on the GPU partition and work on whichever GPU you get or choose a GPU with a smaller amount of memory first.
Train your model for one epoch and monitor the GPU memory usage.
Use this information to choose a GPU to do the complete training on.
You can calculate the size of your DL model (the number of parameters) to compute the memory needed to store the model. See here for details.
There is a tool on Hugging Face that can calculate memory needs for a transformers or timm model (using a batch size of 1): https://huggingface.co/spaces/hf-accelerate/model-memory-usage
Providing more information to users on how to choose a GPU for DL is currently being worked on.
Information will be updated on our website as it becomes available.
"
rc-learning-fork/content/notes/deep-learning-hpc/need_help.md,"Check out the website: https://www.rc.virginia.edu/
Office Hours via Zoom

Tuesdays:         3 pm - 5 pm
Thursdays:        10 am - noon

Zoom Links are available at https://www.rc.virginia.edu/support/
Visit the Research Computing Data Analytics Center
{{< figure src=/notes/deep-learning-hpc/img/RCDA_Center.png caption=""https://www.rc.virginia.edu/service/dac/"">}}"
rc-learning-fork/content/notes/deep-learning-hpc/dl_slurm_scripts.md,"Introduction to Slurm Scripts
HPC environments are generally shared resources among a group of users. In order to manage user jobs, we use Slurm, a resource manager for Linux clusters. This includes deciding which jobs run, when those jobs run, and which node(s) they run on.
* A Slurm script gives Slurm the information it needs to run a job (i.e computational resources, necessary software, and command(s) to execute the code file)
Jobs are submitted to the Slurm controller, which queues them until the system is ready to run them. The controller selects which jobs to run, when to run them, and how to place them on the compute node or nodes, according to a predetermined site policy meant to balance competing user needs and to maximize efficient use of cluster resources
More information about UVA's Slurm Job Manager can be found here.
Example Pytorch Slurm Script
```bash
!/bin/bash
Set Up Resources
SBATCH -A mygroup                # -A: allocation
SBATCH -p gpu                    # -p: partition
SBATCH --gres=gpu:1              # --gres=gpu:1 :use 1 gpu
SBATCH -c 1                      # -c: number of cores
SBATCH -t 00:01:00               # -t: time limit
SBATCH -J pytorchtest            # -J: job name
SBATCH -o pytorchtest-%A.out     # -o: standard output file (%A is the job #)
SBATCH -e pytorchtest-%A.err     # -e: standard error file (%A is the job #)
module purge                          # Load Software
module load apptainer pytorch/2.0.1  
apptainer run --nv $CONTAINERDIR/pytorch-2.0.1.sif pytorch_example.py  # Run Code
```
Example TensorFlow/Keras Slurm Script
```bash
!/bin/bash
SBATCH -A mygroup
SBATCH -p gpu
SBATCH --gres=gpu:1
SBATCH -c 1
SBATCH -t 01:00:00
SBATCH -J tftest
SBATCH -o tftest-%A.out
SBATCH -e tftest-%A.err
module purge
module load apptainer tensorflow/2.13.0
apptainer run --nv $CONTAINERDIR/tensorflow-2.13.0.sif tf_example.py
```
More Slurm Options

To request a specific amount of memory per node:
For example, --mem=64G
Units are given with a suffix (K, M, G, or T).  If no unit is given, megabytes is assumed.
Other options available at https://slurm.schedmd.com/sbatch.html
"
rc-learning-fork/content/notes/deep-learning-hpc/cpu_memory_cores.md,"Request CPU Memory

Standard Partition (no GPU): you will get 9GB RAM per core
Interactive Partition: you will get 6GB RAM per core
GPU Partition: you can specify how much RAM you want

You should have enough RAM to comfortably work with your GPU. In other words, request at least as much RAM as the GPU you select.
* If you select multiple GPUs, request as much RAM as the GPU you selected with the largest memory.
* If you are using a large dataset and/or want to do extensive preprocessing, more RAM is probably helpful.
    * How much more?  Depends!  You can experiment and check your memory efficiency.
Visit this Deep Learning Hardware Guide for more information.
Request CPU Cores
It depends how many CPU Cores to request!  Generally, make your best guess to start.  Then check the  CPU and GPU efficiency of your script and adjust from there.
* Are you are doing any data preprocessing on the CPU prior to training the network on the GPU?
  * Is the preprocessing code serial or parallel?
  * NOTE: Even if your code is written as a serial program, NumPy automatically uses multiple cores for linear algebra operations!
* Are you using a single core or multiple cores for the data loading from the CPU to the GPU for the training process?
  * Use enough CPU cores to keep the GPU busy
PyTorch

PyTorch's DataLoader has a num_workers parameter, which is the number of CPU cores to use for the data loading.
The default is num_workers=1, but this may not load data fast enough to keep the GPU busy.
Try increasing num_workers to improve GPU efficiency and speed up DL code.

Keras

Keras will use multiple cores for data loading automatically
"
rc-learning-fork/content/notes/deep-learning-hpc/_index.md,"In this tutorial, we will be discussing the following topics:
* Accessing Deep Learning containers
* Hardware Overview
* Resource allocation and helpful tools
* How to choose a GPU
* DL Slurm scripts"
rc-learning-fork/content/notes/deep-learning-hpc/gpu_dl.md,"Because the training process involves hundreds of thousands of computations, we need a form of parallelization to speed up the process.

For instance, ChatGPT's free version (based on GPT-3.5) uses a model with 175 billion parameters,
whereas ChatGPT's paid version (based on GPT-4) uses a model with over 1 trillion parameters
Although Neural Network calculations are very simple, there are a lot of them!

GPUs (graphics processing units) provide the needed parallelization and speed up.
Deep Learning using GPUs

All the major deep learning Python libraries (Tensorflow, PyTorch, Keras, etc.) support the use of GPUs and allow users to distribute their code over multiple GPUs.
New GPUs have been developed and optimized specifically for deep learning.
Scikit-learn does not support GPU processing.
Deep learning acceleration is furthered with Tensor Cores in NVIDIA GPUs.
Tensor Cores accelerate large matrix operations by performing mixed-precision computing. It accelerates math and reduces the memory traffic and consumption.

Neural Networks

If you're  not  using a neural network as your machine learning model you may find that a GPU doesn't improve the computation time.
If you are using a neural network but it is very small then a GPU will not be any faster than a CPU - in fact, it might even be slower.

General GPU Workflow
The following is a high-level summary of the general GPU workflow, skipping memory allocations: 

Create data on the CPU
Send data from the CPU to the GPU (for DL this is done in batches)
Compute result on the GPU
Send the result back to the CPU

Depending on the Deep Learning framework you are using, some of these steps may be automatically done for you."
rc-learning-fork/content/notes/llms-hpc/_index.md,"In this tutorial, we will be discussing the following topics:

LLM overview
Setup/Installation
HPC Resources for LLMs
Model selection
Inference and fine-tuning
Slurm scripts
"
rc-learning-fork/content/notes/gpu-applications-rivanna/index.md,"In this workshop participants are introduced to the gpu computing resources on Rivanna.

Introduction to GPU
The graphics processing unit was invented specifically for graphics rendering. Nowadays they are also used as accelerators for parallel computing; you may also hear the term ""general-purpose GPU"" (GPGPU).
{{< table >}}
|Property|CPU|GPU|
|---|---|---|
|Number of cores|$10^{0-1}$ | $10^{3-4}$ |
|Throughput | Low | High |
|Per-core performance | High | Low |
|Workload type| Generic | Specific (e.g. rendering, deep learning)|
|Memory on Rivanna| up to 1.5 TB per node | up to 80 GB per device |
{{< /table >}}
Integrated vs discrete GPU
Integrated GPUs are mostly for graphics rendering and light gaming. They are integrated on the CPU motherboard to achieve more compact systems.
Discrete (or dedicated) GPUs are designed for resource-intensive computations.
GPU vendors and types
NVIDIA, AMD, Intel

Datacenter: H100, A100, V100, P100, K80
Workstation: A6000, Quadro
Gaming: GeForce RTX 40xx, 30xx, 20xx

(bold means available on Rivanna)
Myths

GPUs are better than CPUs and will eventually replace them.
    CPU and GPU complement each other. GPU will not replace CPU.
If I run my CPU code on a GPU, it'll be way faster.
    This depends on whether your code can run on a GPU at all. Even so, if the computation is not resource-intensive enough, there will be no acceleration. In fact, your code may even be slower on a GPU.
Running a GPU program on two GPU devices will be twice as fast as running it on one.
    Again, this depends on whether your program can run on multiple GPU devices and the computation intensity.
GPU acceleration only applies to data science and machine/deep learning.
    Many scientific codes are making use of GPU acceleration: VASP, QuantumEspresso, GROMACS, ... See here for a list compiled in 2018.

GPUs on Rivanna
Go to this page. GPUs are indicated by ""GPU"" under the specialty hardware column. 
Command to check the current status of GPU nodes:
```bash
$ qlist -p gpu
STATE    NODE           CPUS(A/I/O/T) TOTALMEM(MB)  ALLOCMEM(MB)  AVAILMEM(MB)  GRES(M:T:A)               JOBS
mix      udc-an28-1     8/120/0/128   1000000       40960         959040        gpu:a100:8(S:0-7):1         1
mix      udc-an28-7     28/100/0/128  1000000       680960        319040        gpu:a100:8(S:0-7):6         6
mix      udc-an33-37    12/24/0/36    384000        384000        0             gpu:v100:4(S:0-1):3         3
...
```
Important things to note:

CPU memory is not GPU memory
Each GPU node contains multiple GPU devices
Different GPU types have different specs (GPU memory, CPU cores, etc.)
In descending order of performance: A100, V100, P100, K80

GPU-Enabled Applications on Rivanna
Popular GPU applications on Rivanna at a glance
{{< table >}}
|nvhpc|gcc/goolf|nvompic|singularity|Jupyter kernels|
|---|---|---|---|---|
|(User code)|gromacs |quantumespresso|pytorch   |PyTorch|
|           |gpunufft|berkeleygw     |tensorflow|TensorFlow|
|           |mumax3  |yambo          |rapidsai  |RAPIDS|
|           |          |                 |amptorch  |AMPTorch|
|           |          |                 |alphafold ||
|           |          |                 |deeplabcut||
|           |          |                 |isaacgym  ||
{{< /table >}}
Modules
The nvhpc module (NVIDIA HPC SDK) provides these libraries and tools:
- Compilers (nvc, nvc++, nvfortran)
- CUDA
- Mathematical libraries: cuBLAS, cuRAND, cuFFT, cuSPARSE, cuTENSOR, cuSOLVER
- Communication libraries: NVSHMEM, NCCL
- Tools: CUDA-GDB, Nsight System
In addition, applications are installed under three toolchains goolfc, nvompic (compiled languages), and singularity (container).
goolfc
Stands for:

GCC compilers (g)
OpenMPI (o)
OpenBLAS (o)
ScaLAPACK (l)
FFTW (f)
CUDA (c)

```bash
goolfc: goolfc/9.2.0_3.1.6_11.0.228
Description:
  GNU Compiler Collection (GCC) based compiler toolchain along with CUDA
  toolkit, including OpenMPI for MPI support with CUDA features enabled,
  OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK with CUDA features
  enabled.


This module can be loaded directly: module load goolfc/9.2.0_3.1.6_11.0.228

```
The toolchain version consists of three subversions joined by _, corresponding to the version of gcc, openmpi, and cuda, respectively.
```bash
$ module load goolfc
$ module avail
------- /apps/modulefiles/standard/mpi/gcc-cuda/9.2.0-11.0.228/openmpi/3.1.6 -------
   fftw/3.3.8     (L,D)    hoomd/2.9.6     python/3.8.8    (D)
   gromacs/2021.2          python/3.7.7    scalapack/2.1.0 (L)
----------- /apps/modulefiles/standard/compiler/gcc-cuda/9.2.0-11.0.228 ------------
   gpunufft/2.1.0    mumax3/3.10    nccl/2.7.8    openmpi/3.1.6 (L,D)
```
Usage instructions

GROMACS

nvompic
Stands for:

NVIDIA compilers (nv)
OpenMPI (ompi)
CUDA (c)

```bash
$ module spider nvompic

nvompic: nvompic/21.9_3.1.6_11.4.2
Description:
  NVHPC Compiler including OpenMPI for MPI support.


This module can be loaded directly: module load nvompic/21.9_3.1.6_11.4.2

```
The toolchain version consists of three subversions joined by _, corresponding to the version of nvhpc, openmpi, and cuda, respectively.
```bash
$ module load nvompic
$ module avail
------------- /apps/modulefiles/standard/mpi/nvhpc/21.9/openmpi/3.1.6 -------------
   berkeleygw/3.0.1    fftw/3.3.10 (D)    quantumespresso/7.0    yambo/5.0.4
   elpa/2021.05.001    hdf5/1.12.1 (D)    scalapack/2.1.0
----------------- /apps/modulefiles/standard/compiler/nvhpc/21.9 ------------------
   hdf5/1.12.1    openblas/0.3.17 (D)    openmpi/3.1.6 (L,D)
```
Usage Instructions

BerkeleyGW
QuantumEspresso

singularity
The popular deep learning frameworks, TensorFlow and PyTorch, are backed by containers. (To learn more about containers, see Using Containers on Rivanna.)
bash
module load singularity tensorflow
On JupyterLab, you may conveniently select the kernel of the desired framework and version.
Usage instructions

TensorFlow
PyTorch
RAPIDS (also see workshop)

Jupyter kernels

TensorFlow
PyTorch
RAPIDS

Requesting a GPU
Open OnDemand
Select the gpu partition. If you need a specific GPU type, select from the dropdown menu. Default will assign you the first available GPU.
Slurm script
Your Slurm script must contain these lines:
```bash
SBATCH -p gpu
SBATCH --gres=gpu
```
See here for further information.
Demo (Python & Matlab)

Congratulations - you have completed this tutorial!

References

CPU vs GPU: What's the Difference
NVIDIA HPC SDK documentation
""That Was Fast: GPUs Now Accelerate Almost 600 HPC Apps""
"
rc-learning-fork/content/notes/rio-intro/index.md,"{{< slideshow folder=""slides/rio-intro"" ext=""jpg"" >}}"
rc-learning-fork/content/notes/bioinfo-tools-riv/index.md,"{{< slideshow folder=""slides/bioinfomatics-tools-riv"" ext=""png"" >}}"
rc-learning-fork/content/notes/python-machine-learning/pytorch_coding.md,"General Steps

Import the torch package
Read in the data
Preprocess the data
  3a. Scale the data
  3b. Split the data
  3c. Convert data to tensors
  3d. Load the tensors
Design the Network Model
Define the Learning Process
Train the model
Apply the model to the test data
Display the results

1. Import torch Package
```python
import torch
if torch.cuda.is_available():
  device_type = ""cuda:"" + str(torch.cuda.current_device())
else:
  device_type = ""cpu""
device = torch.device(device_type)
```
2. Read in the Data
python
import numpy as np
data_file = 'Data/cancer_data.csv'
target_file = 'Data/cancer_target.csv'
x=np.loadtxt(data_file,dtype=float,delimiter=',')
y=np.loadtxt(target_file, dtype=float, delimiter=',')
print(""shape of x: {}\nshape of y: {}"".format(x.shape,y.shape))
3a. Scale the data
```python
feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x = sc.fit_transform(x)
```
3b. Split the Data
python
from sklearn import model_selection
test_size = 0.30
seed = 7
train_data, test_data, train_target, test_target = model_selection.train_test_split(x, y, test_size=test_size, random_state=seed)
3c. Convert data to tensors
```python
defining dataset class
from torch.utils.data import Dataset
class dataset(Dataset):
  def init(self,x,y):
    self.x = torch.tensor(x,dtype=torch.float32)
    self.y = torch.tensor(y,dtype=torch.float32)
    self.length = self.x.shape[0]
def getitem(self,idx):
    return self.x[idx],self.y[idx]
def len(self):
    return self.length
trainset = dataset(train_data,train_target)
```
3d. Load the tensors
```python
DataLoader
from torch.utils.data import DataLoader
trainloader = DataLoader(trainset,batch_size=64,shuffle=False)
```
4. Design the Network Model
```python
from torch import nn
class Net(nn.Module):
  def init(self,input_shape):
    super(Net,self).init()
    self.fc1 = nn.Linear(input_shape,32)
    self.fc2 = nn.Linear(32,64)
    self.fc3 = nn.Linear(64,1)
def forward(self,x):
    x = torch.relu(self.fc1(x))
    x = torch.relu(self.fc2(x))
    x = torch.sigmoid(self.fc3(x))
    return x
model = Net(input_shape=x.shape[1])
```
5. Define the Learning Process
```python
learning_rate = 0.01
epochs = 700
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
loss_fn = nn.BCELoss()
```
6. Fit the Model
```python
losses = []
accur = []
for i in range(epochs):
  for j,(x_train,y_train) in enumerate(trainloader):
#calculate output
output = model(x_train)

#calculate loss
loss = loss_fn(output,y_train.reshape(-1,1))

#accuracy
predicted = model(torch.tensor(x,dtype=torch.float32))
acc = (predicted.reshape(-1).detach().numpy().round() == y).mean()
#backprop
optimizer.zero_grad()
loss.backward()
optimizer.step()

if i%50 == 0:
  losses.append(loss)
  accur.append(acc)
  print(""epoch {}\tloss : {}\t accuracy : {}"".format(i,loss,acc))
```
7. Apply the Model to Test Data
python
testset = dataset(test_data,test_target)
trainloader = DataLoader(testset,batch_size=64,shuffle=False)
predicted = model(torch.tensor(test_data,dtype=torch.float32))
8. Evaluate the Results
```python
acc = (predicted.reshape(-1).detach().numpy().round() == test_target).mean()
print('\nAccuracy:  %.3f' % acc)
from sklearn.metrics import confusion_matrix
predicted = predicted.reshape(-1).detach().numpy().round()
print(confusion_matrix(test_target, predicted))
```
Activity:  PyTorch Program
Make sure that you can run the PyTorch code: 06_PyTorch.ipynb"
rc-learning-fork/content/notes/python-machine-learning/neural_networks.md,"Neural networks are a computational model used in machine learning which is based on the biology of the human brain.
The building blocks of a neural network are neurons, also known as nodes. Within each node is a very basic algebraic formula that transforms the data.
Simulation of a Neuron
{{< figure src=/notes/python-machine-learning/img/neuron_simulation.png caption="""" width=60% height=60% >}}
The ""incoming signals"" would be values from a data set.
A simple computation (like a weighted sum) is performed by the ""nucleus"".
Then, an ""activation"" function is used to determine if the output is ""on"" or ""off"".
The weights, $w_i$, and the bias $b$, are not known at first. Random guesses are chosen. During training, the ""best"" set of weights are determined that will generate a value close to $y$ for the collection of inputs $x_i$.
Network of Nodes
A single node does not provide much information (often times, a 0 or 1 value), but creating a network or layer of nodes will provide more information.
{{< figure src=/notes/python-machine-learning/img/node_network.png caption="""" width=60% height=60% >}}
Different computations with different weights can be performed  to produce different outputs. This is called a feedforward network – all values progress from the input to the output.
The Layers of a Network
{{< figure src=/notes/python-machine-learning/img/neuron_network_layers.png caption="""" width=60% height=60% >}}
A neural network has a single hidden layer. A neural network with two or more hidden layers is called a ""deep neural network"".
How does the machine learn?
The output values or ""predicted"" values of the network can be compared with the expected results/categories/labels.

Start with a random guess for the weights and biases.
Another function, called a ""loss"" or ""cost"" function can be used to determine the overall error of the model.
That error can be used to work backwards through the network and tweak the weights/biases.
This step is called  backward propagation .



Overview of the Learning Process
{{< video src=""/notes/python-machine-learning/video/learning_process.mp4"" controls=""yes"" >}}
Activation Function

An activation function will determine if a node should ""fire"".
Examples include relu, sigmoid, and softmax.
A complete list is available at https://keras.io/api/layers/activations/ .

Loss Function

A loss function is a function that will be optimized to improve the performance of the model.
Examples include BinaryCrossEntropy and CategoricalCrossEntropy.
A complete list is available at https://keras.io/api/losses/ .

Metrics: A formula for measuring the accuracy of the model.
* Examples include Accuracy and MeanSquaredError.
* A complete list is available at https://keras.io/api/metrics.
Optimizer functions

The function for tweaking the weights.
Examples include SGD, Adam, and RMSprop.
A complete list is available at https://keras.io/api/optimizers/ .

Epochs and Batch Size
Epochs:   Number of loops – how many times the forward/backward process should be performed.
Batch Size:  Within an epoch, the training data are divided into small batches and sent through the network.  All training data are processed in an epoch."
rc-learning-fork/content/notes/python-machine-learning/pytorch.md,"PyTorch is another interface for example of TensorFlow. It is a software library, developed by Facebook and maintained by Mega AI.
Because PyTorch uses Tensorflow as the underlying code, many of the required functions (e.g., activation, loss, optimizer) will be the same.
Activation Function

The activation function will be determine if a node should ""fire"".
Examples include nn.ReLU, nn.Sigmoid, and nn.Softmax.
A complete list is available at
https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity
https://pytorch.org/docs/stable/nn.html#non-linear-activations-other

Loss Function

The loss function will be optimized to improve the performance of the model.
Examples include nn.BCELoss (Binary CrossEntropy) and nn.CrossEntropyLoss.
A complete list is available at https://pytorch.org/docs/stable/nn.html#loss-functions

Optimizer functions

The optimizer function is used for tweaking the weights.
Examples include SGD, Adam, and RMSprop.
A complete list is available at https://pytorch.org/docs/stable/optim.html?highlight=optimizer#torch.optim.Optimizer
"
rc-learning-fork/content/notes/python-machine-learning/machine_learning_overview.md,"Machine learning is a branch of artificial intelligence where computers learn from data, and adapt the computational models to enhance performance. It is a method of analysis that allows computers to reveal information within data. The ""learning"" is not the type of learning that you and I do. Instead, it is a systematic approach to finding an appropriate data transformation from inputs to output.
Why Machine Learning?

Computers can sort through data faster than humans can. 
Computers can identify patterns quickly and use these patterns for predictions or classifications. 
Machine learning can handle noisy data – it doesn't find a perfect answer, but rather a ""really good"" answer.

Problems that ML can solve

Regression
Regression models determine a mathematical model for the relationship among features or attributes so that an outcome can be predicted.
Results can be any value within a possible range  (e.g., what will the average Earth temperature be in 2050?)


Classification
Classification techniques identify a combination of attributes that best fits a class or category so that an object can be classified.
Results can be from a list of known possibilities  (e.g., is the tumor benign or malignant?)



Types of Machine Learning

Supervised Learning:
A data set exists where the samples can be categorized into two or more classifications.
The computer uses the data set to learn how to predict the classification of an unknown sample.
Examples include Decision Trees and Deep Learning
Unsupervised Learning:
The collected data has no known classification or pattern.
The computer must identify the groups or hidden structures within the data.
Examples include Dendograms, K-means clustering, Self-organizing Maps
Reinforcement Learning:
Computer learns from positive or negative feedback
Example includes Swarm intelligence

Data for Machine Learning

Data are plural, and the singular is datum
For many Machine Learning algorithms, the data are expected to be in a table format, where:
each row represents an object, and
each column has the measurements for a specific attribute or feature of the object


For supervised learning, the classifications of the objects must be known.
The data with known classifications are divided into a training set and a testing set.
The data are used to develop a model.
The training data are submitted to an algorithm that will fit a model to the data.
The test data are submitted to the model to produce predicted classifications and determine the accuracy of the model.


Finally, the model can be used to predict classifications for ""unknown"" data.

Ideas behind Machine Learning
The algorithm determines the best mathematical model for the code. However, you still need to provide a ""framework"" for the algorithm.The framework provides the algorithm with tools for performing the learning:
* Machine Learning Algorithm
* Data Measurements
* Model defining the relationship between the input and the output
* Label or Classification
{{< figure src=/notes/python-machine-learning/img/ml_overview.png caption="""" width=75% height=75% >}}"
rc-learning-fork/content/notes/python-machine-learning/random_forest.md,"Random forest is a classification algorithm within supervised learning. It uses an Ensemble Technique.
  * Ensemble techniques combine a group of ""weaker"" learning techniques to build a stronger technique.
  * Random Forest combines the results of multiple decision trees to create a more robust result.
Random Forest:  How does it work?
{{< figure src=/notes/python-machine-learning/img/random_forest_dt.png caption=""Image borrowed from https://ai-pool.com/a/s/random-forests-understanding"" width=70% height=70% >}}
Different decision tree algorithms can produce different results.The random forest aggregates the decisions from the trees to determine an overall solution.
Suppose that the data fall into one of two categories (blue or orange) depending on two values, x and y, as shown in this figure:
{{< figure src=/notes/python-machine-learning/img/linear_decision.png caption="""" width=25% height=25% >}}
A decision tree could choose a relationship between x, y, and the categories that matches one of the following figures:
{{< figure src=/notes/python-machine-learning/img/decision_tree_choices.png caption="""" width=50% height=50% >}}
By combining the many, many outcomes, the random forest can approach the desired mapping.
{{< figure src=/notes/python-machine-learning/img/random_forest.png caption="""" width=60% height=60% >}}
Random Forests can use different techniques for selecting features for computing each decision value. This can lead to the choice of different features.
{{< figure src=/notes/python-machine-learning/img/random_forest_tree.png caption="""" width=70% height=70% >}}
Random Forest:  Feature Importance
{{< figure src=/notes/python-machine-learning/img/feature_importance.png caption="""" width=50% height=50% >}}

We would like to know the ""importance"" of the features (e.g., which features are the most important for making decisions).
Different algorithms use various metrics to determine the importance of the features.

The value of the measurements are not as important as the order of the features."
rc-learning-fork/content/notes/python-machine-learning/decision_trees_coding.md,"The Data

For our first example, we will be using a set of measurements taken on various red wines.
The data set is from
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.
The data is located at
https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv
There are 12 measurements, taken on 1599 different red wines.

Attribute Summary
{{< figure src=/notes/python-machine-learning/img/attribute_summary.png caption="""" width=50% height=50% >}}
Question: Can we predict the quality of the wine from the attributes?
Coding Decision Trees:  General Steps

Load the decision tree packages
Read in the data
Identify the target feature
Divide the data into a training set and a test set.
Fit the decision tree model
Apply the model to the test data
Display the confusion matrix

1. Load Decision Tree Package
python
from sklearn import tree
2. Read in the data
python
import pandas as pd
data_url = ""https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv""
wine = pd.read_csv(data_url, delimiter=';')
print(wine.info())
3. Identify the target feature
```python
Split the quality column out of the data
wine_target = wine['quality']
wine_data = wine.drop('quality', axis=1)
```
For the functions that we will be using, the target values (e.g., quality) must be a separate object.
4. Divide the Data
python
from sklearn import model_selection
test_size = 0.30
seed = 7
train_data, test_data, train_target, test_target = model_selection.train_test_split(wine_data,
wine_target, test_size=test_size,
random_state=seed)
5. Fit the Decision Tree Model
python
model = tree.DecisionTreeClassifier()
model = model.fit(train_data, train_target)
6. Apply the Model to the Test Data
python
prediction = model.predict(test_data)
7. Display Confusion Matrix
python
row_name =""Quality""
cm = pd.crosstab(test_target, prediction,
rownames=[row_name], colnames=[''])
print(' '*(len(row_name)+3),""Predicted "", row_name)
print(cm)
Activity:  Decision Tree Program
Make sure that you can run the decisionTree code: 01_Decision_Tree.ipynb"
rc-learning-fork/content/notes/python-machine-learning/multi_gpu.md,"Activity:  Multi-GPU Program
Before running this next notebook, you will need to create a new JupyterLab session and request 2 GPUs rather than 1.
Make sure that you can run the PyTorch code: 07_Tensorflow_Parallel.ipynb
Supplemental Information
Check out our material on Convolutional Neural Networks: https://learning.rc.virginia.edu/notes/deep-learning-distributed/cnn/
Activity: Convolutional Neural Networks:
Make sure that you can run the CNN code: 05_CNN.ipynb
Questions or Need More help?
Office Hours via Zoom:

Tuesdays: 3 pm - 5 pm
Thursdays: 10 am - noon

Zoom Links are available at https://www.rc.virginia.edu/support/#office-hours
* Website: https://rc.virginia.edu"
rc-learning-fork/content/notes/python-machine-learning/neural_networks_coding.md,"Example: Breast Cancer Data

The Cancer data set originally was captured at UCI Repository (https://archive.ics.uci.edu/ml/datasets.html)
Look at the data, so that you understand what is in each file.

{{< table >}} 
| Filename | Brief Description |
| :-: | :-: |
| cancer_data.csv | The table of measurementscancer_DESCR.csv – an overview of the data |
| cancer_feature_names.csv | The names of the columns in cancer_data.csv |
| cancer_target.csv | The classification (0 or 1) of each row in cancer_data.csv |
| cancer_target_names.csv | The names of the classifications (malignant or benign) |
{{< /table >}}
Coding a Neural Network:  General Steps

Load the neural network packages
Read in the data
Divide the data into a training set and a test set.
Preprocess the data
Design the Network Model
Train the model
Apply the model to the test data
Display the results

1. Load Neural Networks Package
python
from tensorflow import keras
from tensorflow.keras import layers
2. Read in the Data
python
import numpy as np
data_file = 'Data/cancer_data.csv'
target_file = 'Data/cancer_target.csv'
cancer_data=np.loadtxt(data_file,dtype=float,delimiter=',')
cancer_target=np.loadtxt(target_file, dtype=float, delimiter=',')
3. Divide Data
```python
from sklearn import model_selection
test_size = 0.30
seed = 7
train_data, test_data, train_target, test_target = model_selection.train_test_split(cancer_data, cancer_target, test_size=test_size, random_state=seed)
```
4. Preprocess the Data
```python
Convert the classes to 'one-hot' vector
from keras.utils import to_categorical
y_train = to_categorical(train_target, num_classes=2)
y_test = to_categorical(test_target, num_classes=2)
```
5. Design the Network Model
```python
def define_model():
    model = keras.Sequential([ 
       layers.Dense(30, activation=""relu""), 
       layers.Dense(2, activation=""softmax"") 
    ])
    model.compile(optimizer=""rmsprop"", 
              loss=""binary_crossentropy"",
              metrics=[""accuracy""]
    )
    return(model)
model = define_model()
```
6. Train the Model
python
num_epochs = 10
batch_size = 32
model.fit(train_data, y_train, epochs=num_epochs, batch_size=batch_size)
7. Apply the Model to the Test Data
python
predictions = np.argmax(model.predict(test_data), axis=-1)
8. Display the Results
python
score = model.evaluate(test_data, y_test)
print('\nAccuracy:  %.3f' % score[1])
from sklearn.metrics import confusion_matrix
print(confusion_matrix(test_target, predictions))
Activity:  Neural Network Program
Make sure that you can run the Neural Network codes: 03_Neural_Network.ipynb"
rc-learning-fork/content/notes/python-machine-learning/tensorflow.md,"TensorFlow is an example of deep learning, a neural network that has many layers. It is a software library developed by the Google Brain Team.
Deep Learning Neural Network
{{< figure src=/notes/python-machine-learning/img/deep_neural_network.jpg caption=""Image borrowed from: http://www.kdnuggets.com/2017/05/deep-learning-big-deal.html"" width=50% height=50% >}}
Terminology: Tensors
A Tensor is a multi-dimensional array.
Example:  A sequence of images can be represented as a 4-D array: [image_num, row, col, color_channel]
{{< figure src=/notes/python-machine-learning/img/tensors.png caption="""" width=60% height=60% >}}
Terminology:  Computational Graphs

Computational graphs help to break down computations.
For example, the graph for $y=(x1+x2)*(x2 - 5)$:

{{< figure src=/notes/python-machine-learning/img/computational_graph.png caption=""The beauty of computational graphs is that they show where computations can be done in parallel."" width=40% height=40% >}}
The Need for GPUs
With deep learning models, you can have hundreds of thousands of computational graphs. A GPU has the ability to perform a thousand or more of the computational graphs simultaneously.  This will speed up your program significantly.
Note:  Most algorithms can run without GPUs, but they will be slower."
rc-learning-fork/content/notes/python-machine-learning/decision_trees.md,"Decision trees are a classification algorithm within supervised learning. The algorithm determines a set of questions or tests that will guide it toward a classification of an observation and it organizes a series of attribute tests into a tree-structure to help determine classification of the unlabeled data.

Motivating Question:
Given a set of data, can we determine which attributes should be tested first to predict a category or outcome (i.e., which attributes lead to ""high information gain"")?

Simple Scenario
Suppose we have:
* a group of people, each one with a tumor, and
* two measurements (x, y) for each tumor.
Plotting the data, and coloring the points red for malignant tumors and blue for benign tumors, we might see a plot as follows:
{{< figure src=/notes/python-machine-learning/img/pre_decision_plot.png caption="""" width=60% height=60% >}}
Clearly, something happens near x=3.
{{< figure src=/notes/python-machine-learning/img/decision_plot.png caption="""" width=60% height=60% >}}
With very few errors, we can use x=3 as our ""decision"" to categorize the tumor as malignant versus benign.
Resulting decision tree:
{{< figure src=/notes/python-machine-learning/img/result_decision_tree.png caption="""" width=30% height=30% >}}
Unfortunately, it is not always this easy, especially if we have much more complex data. More layers of questions can be added with more attributes.
Example: What should you do this weekend?
{{< table >}} 
| Weather | Parents Visiting | Have extra cash | Weekend Activity |
| :-: | :-: | :-: | :-: |
| Sunny | Yes | Yes | Cinema |
| Sunny | No | Yes | Tennis |
| Windy | Yes | Yes | Cinema |
| Rainy | Yes | No | Cinema |
| Rainy | No | Yes | Stay In |
| Rainy | Yes | No | Cinema |
| Windy | No | No | Cinema |
| Windy | No | Yes | Shopping |
| Windy | Yes | Yes | Cinema |
| Sunny | No | Yes | Tennis |
{{< /table >}}
This table can be represented as a tree. 
{{< figure src=/notes/python-machine-learning/img/tree_first.png caption="""" width=65% height=65% >}}
This tree can be made more efficient. 
{{< figure src=/notes/python-machine-learning/img/tree_second.png caption="""" width=50% height=50% >}}
Also with complex data, it is possible that not all features are needed in the Decision Tree.
Decision Tree Algorithms
There are many existing Decision Tree algorithms. If written correctly, the algorithm will determine the best question/test for the tree.

How do we know how accurate our decision tree is?

Decision Tree Evaluation

A confusion matrix is often used to show how well the model matched the actual classifications.
The matrix is not confusing – it simply illustrates how ""confused"" the model is!
It is generated based on test data.

{{< figure src=/notes/python-machine-learning/img/decision_tree_chart.png caption="""" width=70% height=70% >}}"
rc-learning-fork/content/notes/python-machine-learning/tensorflow_coding.md,"General Steps

Load the neural network packages
Read in the data
Divide the data into a training set and a test set.
Preprocess the data
Design the Network Model
Train the model
Apply the model to the test data
Display the results

1. Load Keras Packages
python
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
2. Read in the Data
python
import numpy as np
data_file = 'Data/cancer_data.csv'
target_file = 'Data/cancer_target.csv'
cancer_data=np.loadtxt(data_file,dtype=float, delimiter=',')
cancer_target=np.loadtxt(target_file, dtype=float, delimiter=',')
3. Split the Data
python
from sklearn import model_selection
test_size = 0.30
seed = 7
train_data, test_data, train_target, test_target = model_selection.train_test_split(cancer_data,cancer_target, test_size=test_size, random_state=seed)
4. Pre-process the Data
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
Fit only to the training data
scaler.fit(train_data)
Now apply the transformations to the data:
x_train = scaler.transform(train_data)
x_test = scaler.transform(test_data)
Convert the classes to 'one-hot' vector
y_train = to_categorical(train_target, num_classes=2)
y_test = to_categorical(test_target, num_classes=2)
```
5. Define the Model
```python
def define_model():
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Dense, Dropout
  from tensorflow.keras.optimizers import SGD
model = keras.Sequential([ 
    layers.Dense(30, activation=""relu""),
    layers.Dropout(0.5),
    layers.Dense(60, activation=""relu""),
    layers.Dropout(0.5),
    layers.Dense(2, activation=""softmax"")
  ])
model.compile(optimizer=""rmsprop"", loss=""binary_crossentropy"", metrics=[""accuracy""])
return(model)
model = define_model()
```
7.Fit the Model
python
b_size = int(.8*x_train.shape[0])
num_epochs = 20
model.fit(x_train, y_train, epochs=num_epochs, batch_size=b_size)
8.Apply the Model to Test Data
python
predictions = np.argmax(model.predict(x_test), axis=-1)
9.Evaluate the Results
python
score = model.evaluate(x_test, y_test, batch_size=b_size)
print('\nAccuracy:  %.3f' % score[1])
from sklearn.metrics import confusion_matrix
print(confusion_matrix(test_target, predictions))
Activity:  TensorFlow Program
Make sure that you can run the TensorFlow code: 04_TensorFlow.ipynb"
rc-learning-fork/content/notes/python-machine-learning/random_forest_coding.md,"The Data
For the Random Forest example, we will reuse the winequality_red data set.
Coding Random Forest:  General Steps

Load the random forest packages
Read in the data
Identify the target feature
Divide the data into a training set and a test set.
  a. Choose the sample size
  b. Randomly select rows
  c. Separate the data
Fit the random forest model
Apply the model to the test data
Display the feature importance

1. Load Random Forest Package
python
from sklearn.ensemble import RandomForestClassifier
2, 3, 4.
```python
Repeat steps 2-4 from the Decision Tree example
import pandas as pd
. . .
train_data, test_data, train_target, test_target = model_selection.train_test_split(wine_data, wine_target, test_size=test_size, random_state=seed)
```
5. Fit the Random Forest Model
python
model = RandomForestClassifier()
model.fit(train_data, train_target)
6. Apply the Model to the Test Data
python
forest_results = model.predict(test_data)
7. Compute Feature Importance
python
importances = model.feature_importances_
8. List Feature Importance
python
import numpy as np
indices = np.argsort(importances)[::-1]
print(""Feature ranking:"")
col_names = list(train_data.columns.values)
for f in range(len(indices)):
feature = col_names[indices[f]]
space = ' '*(20 - len(feature))
print(""%d.\t %s %s (%f)"" % \
(f + 1, feature, space, importances[indices[f]]))
Activity:  Random Forest Program
Make sure that you can run the Random Forest code: 02_Random_Forest.ipynb"
rc-learning-fork/content/notes/python-machine-learning/_index.md,"In this tutorial we will be covering the following topics:
* Overview of Machine Learning
* Decision Trees
    * Coding Decision Trees
* Random Forest
    * Coding Random Forest
* Overview of Neural Networks
    * Coding Neural Networks
* Tensorflow/Keras
    * Coding Tensorflow
* PyTorch
    * Coding PyTorch
* Overview of Parallelizing Deep Learning
    * Coding 
As mentioned above, example codes will be provided for respective topics. Prior experience with the Python programming language and some familiarity with machine learning concepts are helpful for this tutorial. Please download and unzip the following file to follow along on code activities. 
{{< file-download file=""notes/python-machine-learning/code/ML_with_Python.zip"" text=""ML_with_Python.zip"" >}}"
rc-learning-fork/content/notes/git-intro/index.md,"Introduction
Version control software provides a systematic way to keep track of changes made to files. There are a number of version control software (VCS) systems ... Git is one of them. It's a powerful tool for tracking and reconciling changes to text files from individual or multiple contributors. The basic unit of Git is the repository. Unlike some other VCS, Git tracks changes by storing snapshots of entire repository at different points in time. This is internally different from ""delta-based"" system that just keeps track of changes to the files. While Git can be used as a standalone piece of software, many people leverage web hosting platforms that expand the VCS functionality and have project management and collaborative features built in. Several examples of these services include Bitbucket, GitLab and GitHub.
GitHub
As we mentioned in the introduction, GitHub is a web-based platform for hosting Git repositories. The platform includes a web interface to explore files and perform version control operations, as well as a number of collaboration tools for commenting, opening requests for new features, using project management methods, social networking, creating versions of software releases, and many more. GitHub is an extremely popular service, particularly among software developers and scientists who want to share code as part of ""open source"" projects.
1. Log into github.com
To begin with we'll log into github.com
If you haven't already created an account, make sure you follow the steps create one.
2. Click the + icon and select ""New repository""

3. Give the repository a name
Like the prompt suggests, repository names should be short and memorable. And they must be unique to your account ... i.e. you can't have two repositories in your account with the same name.

4. Check the box to ""Initialize this repository with a README""
New repositories on GitHub require contents to initialize. To get started, we can initialize with a README file, which are typically included in repositories to provide a description of content, usage and / or any necessary software setup.

5. Create a new file
GitHub provides a file editor in the browser. We're going to make use of that here to demonstrate some basic concepts of version control ... but nb that editing files this way is not a typical workflow, especially if you're storing versions of your code locally (on your computer) and remotely (on GitHub). More on that later ...
```
smpl <- rnorm(1000)
xbar <- mean(x)
s <- sd(x)
hist(smpl)
```
6. Edit the file
```
smpl <- rnorm(1000)
xbar <- mean(x)
s <- sd(x)
hist(smpl)
abline(v = xbar, lwd = 2, col = ""red"")
abline(v = xbar + 2s, lwd = 2, col = ""red"", lty = 3)
abline(v = xbar - 2s, lwd = 2, col = ""red"", lty = 3)
```
7. Take a look at the commit history and branch explorer

Each change to the repository (or commit) is recorded and tracked separately via a unique combination of characters. This hash is abbreviated in the commit history view, which provides an interface to explore the file(s) and line(s) that were changed as part of the commit.
GitHub also provides a view of branches, which you can think of as a collection of commits that can represent an entirely different version of the repository. Ultimately, you can perform a merge operation to combine changes across branches. This can be particularly helpful for collaborations between multiple individuals or for a single developer who would like to keep the ""experimental"" features separate from stable code. 

As mentioned above, the fundamental unit of Git is the repository. The steps we've completed up until now have introduced the basics of creating, committing and tracking changes within a single repository. However, GitHub allows its users to have multiple repositories. In some cases, rather than creating a repository from scratch you might need to fork another user's repository. This workflow can be useful for collaborative projects, as it essentially copies the contents and complete version control history at a single point in time.
8. Fork a repository
To illustrate the idea of forking, we'll need to start with an existing repository. For this exercise, we've created a repository that will include a comma separated value (.csv) file with data on our favorite foods. 
Each of us will fork this original repository ... and in doing so create new repositories with all the files and previous changes in our accounts. 
Navigate to https://github.com/uvasomrc/foods and click the Fork button in the upper right-hand corner of the page.

9. Make a unique change to an existing file
Find the line with your initials in foods.csv, and after the comma enter your favorite food. If you don't find your initials in the list, feel free to add a new line with your initials and favorite food separated by a comma.
10. Submit a Pull Request
The pull request mechanism allows contributors to propose changes to the owner of the upstream repository. That owner can review these changes, and conditionally accept or reject them. This process may involve ongoing dialogue and review, during which time the proposed changes can be updated by editing the forked repository.
Git (Command Line Interface)
Thus far we've managed our version control activities using the GitHub platform, which has a Graphical User Interface (GUI). Git as a program also has a command line interface (CLI), which can be extremely useful whether you're managing repositories locally or remotely. In this part of the workshop we'll cover some common workflows using the Git CLI. 
1. Fork the quality/ repository
To motivate the Git CLI material, we've created a repository with an example python script that creates diagnostic plots of read quality scores for sequence data.
Start by forking this repository on GitHub:
https://github.com/uvasomrc/quality
2. Confirm that git is installed and configured on your computer
Now that you have the repository forked to your GitHub account, we'll clone it locally. 
To work with Git on your computer, you'll need it installed:
https://git-scm.com/downloads
Mac has the Terminal app, and Windows has Git BASH.
From the command line, confirm that Git is installed by calling it by name followed by the --version flag.
git --version
If the command above returns the version without error, then you have Git successfully installed. 
There's one more step to configure the program post-installation. Each commit is associated with an author and email address. To check if you have a globally configured username and email address use the following command:
git config -l
If you don't see anything returned, then you'll need to do the configuration:
git config --global user.name ""{YOUR_NAME_HERE}""
git config --global user.email {YOUR_EMAIL_ADDRESS_HERE}
3. Clone the repository
With Git installed and configured you are now ready to clone the repository contents and commit history to your computer. 
Cloning the repository will create a new folder as a subdirectory of your current working directory. If you're not sure where that is, in your terminal you can print the working directory:
pwd
Navigate back to your repository on GitHub and find the Clone or download button, and click it to expand. Copy and paste the Clone with HTTPS link:
https://github.com/{YOUR_ACCOUNT_NAME_HERE}/quality

Use git clone followed by the link (above):
git clone {CLONE_WITH_HTTPS_LINK_HERE}
4. View the log of commits
After cloning to your computer, you now have a new folder with all the files and version control history from the remote repository. You can navigate to this directory and list all the contents:
cd quality
ls -la
As you can see, on your computer you now have all the files from the remote and something called .git, which is a hidden folder that includes the information that Git uses to track versions of the code.
To access the history of commits (starting at the point in time when you cloned the repository) use the following:
git log
5. Check the status
As you work with the code it's good practice to keep an eye on the status of your code base:
git status
6. Run qcheck.py
As described above this repository contains code written in python (qcheck.py) ... this script loads sequence data stored in .fastq format and produces some simple diagnostic plots. The data/ folder includes some example files so you can run the script. Keep in mind you'll need python, as well as the biopython and matplotlib modules installed (see README of the quality).
With those requirements satisfied you can execute the script as follows:
python qcheck.py data/SRR622461_2.fastq data/SRR622461_2.fastq
7. Edit the script and re-run it
Now let's edit the script. One parameter we might want to adjust is the number of reads plotted (see line 19 of qcheck.py) ... let's try changing that from 50 to 60:
vim qcheck.py
Re-run the script:
python qcheck.py data/SRR622461_2.fastq data/SRR622461_2.fastq
8. Check the status and view the diff
The status of the repo will show us that we have made changes:
git status
The diff command is very useful in seeing the actual changes we've made:
git diff
The output from diff includes line-by-line additions and subtractions. For more information, refer to the resources
9. Stage and commit the change made to qcheck.py
Let's presume we want to keep track of the edits to qcheck.py. Before we commit the changes, we need to first add (or stage) them to be committed:
git add qcheck.py
Check the status again and confirm that the edits are staged:
git status
Now try using git commit followed by the file name:
git commit qcheck.py
Oops. That didn't work ... why not?
Every commit requires a message specifying what and why a change was made. You must include a message passed in quotes after the -m flag. While this is technically just a ""subject line"" for the commit (you can write more descriptive message ""bodies""), in most cases this is sufficient to annotate the changes:
git commit -m ""increased number of reads to be plotted and changed plot file name""
It is worth noting here that Git users have varied philosophies and practices regarding writing commit messages. For more information, refer to the resources.
The commit we just made is associated with a unique hash, which is an alphanumeric reference for the code at the exact point time when we committed the change(s). You can refer to this hash (or an abbreviation)  of it with other Git commands, and you'll see it in the log:
git log
10. Synchronize these changes with the repo on GitHub.com
Given that the repository we are working with was originally cloned from GitHub, there is a remote URL associated with the repository:
git remote -v
We can send our changes to the remote repository with a push command followed by the name of the remote (default is origin) and name of branch (default is master):
git push origin master
Note that sending the commit(s) we've made up to the remote GitHub repository requires:

An internet connection
Authentication to GitHub (you need to essentially ""log in"" through the command line)
Verification that the GitHub account you are using has permissions to update the remote repository

If you have a SSH (Secure Shell) key set up, you'll see a message saying that the changes have been synchronized on GitHub. If not you'll need to enter your GitHub username and password.
You can persistently associate your computer with your GitHub account (so you don't have to enter your username and password every time) you can set up an SSH key:
https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/
If you visit the remote URL, you should see the changes you made to qcheck.py, along with the hash / message associated with your commit.
You may have noticed that not all the files in your quality/ folder on your computer appear on GitHub. For example, the if you look in the plots/ folder on GitHub you won't see the .png files ... and that's intentional.
One of the files we've included in our repository is a .gitignore file, which serves as a relay to Git regarding what not to track. You can use wildcards (*) to exclude all files that exist in a certain subdirectory, end in a certain suffix, start with a certain prefix, etc.

Let's change gear a bit ... we'll still be working with the Git CLI, but rather than starting by cloning a remote repository we're going to instead initialize a new one locally.
11. Create and initialize a new Git repository
To create a Git repository we first need a new directory (or folder):
cd ..
mkdir clock
cd clock
Once we've created the clock/ folder and have changed down into it, we can run git init to initialize the folder as a Git repository:
git init
From now on, Git will know to look for changes to files inside of clock/.
12. Create, add and commit files for the clock
With the repository initialized, we will create two files (README.md and clock.sh) ... start by making empty files using touch:
touch README.md
touch clock.sh
Is Git keeping track?
git status
Now stage and commit both at once (the . stages all the files that have been changed):
git add .
git commit -m ""adding skeleton of scripts""
git status
13. Edit, add and commit the README file
We have two empty files that we've committed ... now let's add some content. 
Start with the README:
vim README.md
```
Clock
This repository contains a script that returns the time when executed.
Usage
bash clock.sh
```
Now stage the changes to README.md:
git add README.md
And commit them:
git commit -m ""adding README with description and instructions""
14. Edit, add and commit the clock script
clock.sh will have the actual code for our BASH program:
vim clock.sh
```
!/bin/bash
d=$(date '+%H:%M:%S');
printf ""the time is ...\n$d\n""
```
git add clock.sh
git commit -m ""added the bash clock script""
15. Sync the local repository with a GitHub remote
The local Git repository we've just created is entirely independent of GitHub:
git remote -v
However, we can associate our local repository with GitHub. 
First, we'll need to create a new repository on GitHub. It's probably a good idea to use the same name as your local repo for this. 
Make sure none of the boxes to ""initialize"" are  checked on GitHub

Now from the command line, we can add connect the remote we just created to our local Git repository:
git remote add origin https://github.com/{YOURREPONAMEHERE}
git push -u origin master

Whether your on a remote or local ... a fork or upstream ... you can further organize and track code in Git with a branch structure.
16. Create a new branch
A branch is essentially a detour from the code base at a particular commit in history. As it turns out, we've been actually using a branch already, albeit the default master:
git branch -v
To create a new branch use git checkout with the -b flag followed by the name for the branch:
git checkout -b feature
Now if we look at the branches with the -v option, we see that there is now a branch called feature:
git branch -v
The * indicates that we are ""on"" the feature branch, and all of our subsequent commits will be applied until we switch branches again.
17. Edit, add and commit the file on the new branch
Let's modify our clock program, and add and commit those changes to this branch:
vim clock.sh
```
!/bin/bash
d=$(date '+%H:%M:%S');
printf ""the time is ...\n$d\nhave a nice day\n""
```
git add clock.sh
git commit -m ""adding new feature to the clock script""
18. Checkoutmaster again
Change back to your original branch (master) with the following:
git checkout master
19. Make a new edit, then add and commit that change
Try modifying the same line on master as we did on feature:
vim clock.sh
```
!/bin/bash
d=$(date '+%H:%M:%S');
printf ""the time is ...\n$d\nhave a nice day!\n""
```
Add and commit the changes:
git add clock.sh
git commit -m ""adding 'have a nice day!' to the clock""
20. Attempt to merge the changes from the feature branch
Branching is helpful in that ultimately you can merge changes from multiple branches into one.
Because we are currently on the master branch, the following will attempt to merge changes from feature into master:
git merge feature
However, as you see there is a conflict between the two branches. Using a text editor we can view and resolve this conflict:
vim clock.sh
Exercises
The exercises are intended to give you a chance to explore and use the tools discussed in the morning lecture. These exercises introduce new concepts as well, and in doing so point towards various tools and documentation. Feel free to spend as much or as little time with each prompt.

1. Learn Git Branching
Git branching is an important and sometimes difficult concept to learn. A group of developers has created a very helpful tool for exploring how branches behave.
Go through the Learn Git Branching exercises:
https://learngitbranching.js.org/
Alternatively, visit the sandbox to interactively make commits, create branches, merge branches, etc.:
https://learngitbranching.js.org/?NODEMO

2. Good Commit Messages
Every git commit requires an accompanying message. At minimum this should be a single ""subject line"" briefly describing the changes made. However, the commit message can include a ""body"" with more thorough and descriptive notes about how / why the edits were implemented. These comments are particularly useful for a future maintainer or contributor ... and that person might be you! So you can do yourself a huge favor by creating good commit messages. 
Take some time to read a blog post by developer Chris Beams titled ""How to Write a Git Commit Message"":
https://chris.beams.io/posts/git-commit/
Try making a new commit locally to your quality/ repository. Use a commit message with a body.

3. Syncing a Fork
When you fork a repository, you bring along all the files, commits and associated version control information ... starting from the point in time when it was forked.
As you continue to work on your fork, the upstream repository (from which you originally forked) may or may not be static. The owner or other contributors might modify the contents, creating commits that depart from the tree structure that you are tracking.
To keep up with these changes you must sync the fork:
https://help.github.com/articles/syncing-a-fork/
Use the documentation above to sync your fork of the foods/ repository with the upstream:
https://github.com/uvasomrc/foods

4. Create a conflict
The basic unit of Git is the repository. However, GitHub slightly extends to this concept in its Gist service, which essentially allows users to upload snippets of code without having to track an entire repository.
For this exercise, you'll be working from code hosted in a GitHub Gist:
https://gist.github.com/JonathanMH/397fc427842614dd4803
Start by cloning the Gist:
https://help.github.com/articles/forking-and-cloning-gists/
create_conflict.sh is a shell script that demonstrates what happens when there is a conflict in git commits. Try running it on your computer ... and if you're not sure how to do that, Google it!
Make sure you are running this script relative to a directory location (folder) that you are comfortable making a new folder called git-repo/ ... feel free to delete this folder after the exercise.

5. GitHub Pages
In addition to hosting free public repositories, GitHub also provides a service to host static websites called GitHub Pages:
https://pages.github.com/
Follow the 5 steps on the GitHub Pages website to get started.
If you would like a more advanced example, try forking the following repo:
https://github.com/onlywei/explain-git-with-d3
Forking remotely copies an upstream repository from one account to another. Because it is operating on the repository level, the fork inherits all commits and branches. In this case the explain-git-with-d3 repo includes branch named gh-pages. As a service, GitHub.com will host anything that is stored in a gh-pages branch. To access the hosted version of the contents, you can go to https://{USERNAME}.github.io/{REPOSITORYNAME}"
rc-learning-fork/content/notes/matlab-parallel-programming/matlab-parallel-programming_6.md,"Why MATLAB with NVIDIA GPUs

Easy access to NVIDIA GPUs with 990+ GPU-enabled functions  
MATLAB users can work with NVIDIA GPUs without CUDA programming  
NVIDIA GPUs accelerate many applications like AI/Deep Learning  

Graphics Processing Units (GPUs)

For graphics acceleration and scientific computing  
Many parallel processors  
Dedicated high-speed memory  

{{< figure src=""/notes/matlab-parallel-programming/img/gpu-cores.png"" width=""300px"" >}}
GPU Requirements
Note: Parallel Computing Toolbox requires NVIDIA GPUs. Learn more here.
| MATLAB Release | Required Compute Capability |
| :-: | :-: |
| MATLAB R2018a and later releases | 3.0 or greater |
| MATLAB R2014b - MATLAB R2017b | 2.0 or greater |
| MATLAB R2014a and earlier releases | 1.3 or greater |
GPU Computing Paradigm
NVIDIA CUDA-enabled GPUs
{{< figure src=/notes/matlab-parallel-programming/img/parallel-toolbox-to-gpus.png >}}

Our tools can both be used to speed up your calculation using multiple CPUs and by using GPUs.  
Although GPUs have hundreds of cores, we treat the GPU as a single  unit, and access directly from a MATLAB computation engine. For example, any single Worker can access the entire GPU.       
Expected speed-up varies with problem specifics as well as the avaialable hardware.
Programming with GPUs

Easiest to use: Parallel-enabled toolboxes
Moderate ease of use and moderate control: Common programming constructs (gpuArray, gather)
Greatest control: Advanced programming constructs (spmd, arrayfun, CUDAKernel, mex))

Demo: Wave Equation
Accelerating scientific computing in MATLAB with GPUs

Objective: Solve 2nd order wave equation with spectral methods
Approach:
Develop code for CPU
Modify the code to use GPUcomputing using gpuArray
Compare performance ofthe code using CPU and GPU

{{< figure src=/notes/matlab-parallel-programming/img/second-order-wave-equation-example.png >}}

Speed-up using NVIDIA GPUs
{{< figure src=/notes/matlab-parallel-programming/img/speed-up-gpu.png >}}

Ideal Problems
Massively Parallel and/or Vectorized operations
Computationally Intensive
500+ GPU-enabled MATLAB functions
Simple programming constructs
gpuArray, gather


In the case of GPUs, it's slightly different.
Ideal problems for GPU computing :

Massively parallel—The computations can be broken down into hundreds or thousands of independent units of work.  You will see the best performance all of the cores are kept busy, exploiting the inherent parallel nature of the GPU. 
Computationally intensive—The time spent on computation significantly exceeds the time spent on transferring data to and from GPU memory. Because a GPU is attached to the host CPU via the PCI Express bus, the memory access is slower than with a traditional CPU. This means that your overall computational speedup is limited by the amount of data transfer that occurs in your algorithm. 
Algorithm consists of supported functions

Our developers have written CUDA versions of key MATLAB and toolbox functions and presented them as overloaded  functions - We have over 500 GPU-enabled functions in MATLAB and a growing number of GPU-enabled functions in additional toolboxes as well.
The diagram pretty much sums up the easiest way to do GPU computing in MATLAB - Transfer/create data on the GPU using the “gpuArray”, run your function as you would normally - if the inputs are available on the GPU, we do the right thing and run on the GPU and then “gather” the data back to the CPU. This seamless support allows you to run the same code on both the CPU and the GPU.
GPU Computing with Matlab
Useful Links
GPU Computing
GPU Computing in MATLAB
MATLAB GPU Computing Support for NVIDIA CUDA-Enabled GPUs
Run MATLAB Functions on Multiple GPUs
Boost MATLAB algorithms using NVIDIA GPUs"
rc-learning-fork/content/notes/matlab-parallel-programming/matlab-parallel-programming_2.md,"Parallel Computing Paradigm Multicore Desktops
{{< figure src=/notes/matlab-parallel-programming/img/parallel-computing-toolbox.png >}}

MathWorks offers two parallel computing tools. The first is Parallel Computing Toolbox which is our desktop solution that allows users to be more productive with their multicore processors by utilizing headless MATLAB sessions called workers to use the full potential of their hardware and speed up their workflow. The Toolbox provides easy to use syntax that allows users to stay in their familiar desktop environment while our solution takes care of the details of using multiple sessions in the background. We make the tough aspects of parallelism easy for users: GPU, Deep Learning, Big Data, Clusters, Clouds, etc.
Parallel ComputingToolbox also provides support for NVIDIA CUDA-based GPUs, and provides the syntax used by our second tool, MATLAB Parallel Server.
Under the hood the parallel computing products divide the tasks/computations/simulations and assigns them to these workers in the background - enabling the computations to execute in parallel.
A key concept for parallel computing with MATLAB is the ability to run multiple MATLAB computational engines that are controlled by a single MATLAB session and that are licensed as workers. A collection of these workers with inter-process communication is what we call a parallel pool. 
The parallel computing products provide features that can divide tasks/computations/simulations and assigns them to MATLAB computational engines in the background - enabling execution in parallel.
In general you should not run more MATLAB computational engines than the number of physical cores available.   Otherwise, you are likely to have resource contention.
Note that Parallel Computing Toolbox provides licensing for enough workers to cover all of the physical cores in a single machine.
See also:  https://www.mathworks.com/discovery/matlab-multicore.html 
Accelerating MATLAB and Simulink Applications
{{< figure src=/notes/matlab-parallel-programming/img/explicit-parallelism.png >}}

Demo: Classification Learner App
Analyze sensor data for human activity classification
{{< figure src=""/notes/matlab-parallel-programming/img/classification-demo.png"" height=""200px"" >}}

Objective: visualize and classify cellphone sensor data of human activity
Approach:
Load human activity dataset
Leverage multicore resources to train multiple classifiers in parallel


Check out more here: https://insidelabs-git.mathworks.com/ltc-ae/demos/HumanActivityRecognition
Let's open MATLAB and try out a demonstration of using built in parallel functionality invoked by setting a toggle to invoke parallel. 
In this demo, we are loading IOT sensor data from a mobile phone and then using this sensor data to classify the data into activity-standing, sitting, running, etc. 
The demo will utilize the multiple cores in my parallel pool to train multiple classifiers simultaneously. 
Demo: Cell Phone Tower OptimizationUsing Parallel-Enabled Functions

Parallel-enabled functions in Optimization Toolbox
Set flags to run optimization in parallel
Use pool of MATLAB workers to enable parallelism

{{< figure src=""/notes/matlab-parallel-programming/img/cellphone-optimization-demo.png"" height=""200px"" >}}
Parallel-enabled Toolboxes (MATLAB® Product Family)
Enable parallel computing support by setting a flag or preference

{{< figure src=/notes/matlab-parallel-programming/img/matlab-toolboxes.png >}}

If you want a bit more control, then Parallel Computing Toolbox adds some parallel keywords into the MATLAB language. An example of this is Parfor or batch commands.
Explicit Parallelism: Independent Tasks or Iterations
Simple programming constructs: parfor
Examples of this include parameter sweeps, and Monte Carlo simulations. There are no dependencies or communications between tasks.
{{< figure src=/notes/matlab-parallel-programming/img/explicit-parallelism-diagram.png >}}

How do we express parallelism in code?
If you are dealing with problems that are computationally intensive and are just taking too long because there are multiple tasks/iterations/simulations you need to execute to gain deeper insight   these are ideal problems for parallel computing and the easiest way to address these challenges is to use parallel for loops.
Real-world examples of such problems are parameter sweeps or Monte Carlo simulations.
For example, lets say you have 5 tasks to be completed - If you run these in a FOR loop, they run serially one after the other - you wait for one to get done, then start the next iteration - However if they're all independent tasks with no dependencies or communication between individual iterations - you can distribute these tasks to the MATLAB workers we spoke about - multiple tasks can execute simultaneously  you'll maximize the utilization of the cores on your desktop machine and save up on a lot of time!
Requirements for parfor loops:
- Task independent
- Order independent
Constraints on the loop body
- Cannot “introduce” variables (e.g. load, etc.)
- Cannot contain break or return statements
- Cannot contain another parfor loop
Explicit Parallelism: Independent Tasks or Iterations
for i = 1:5
  y(i) = myFunc(myVar(i));
end
parfor i = 1:5
  y(i) = myFunc(myVar(i));
end

For example, lets say you have 5 tasks to be completed - If you run these in a FOR loop, they run serially one after the other - you wait for one to get done, then start the next iteration- However if they're all independent tasks with no dependencies or communication between individual iterations - you can distribute these tasks to the MATLAB workers we spoke about - multiple tasks can execute simultaneously  you'll maximize the utilization of the cores on your desktop machine and save up on a lot of time !
Mechanics of parfor Loops
{{< figure src=/notes/matlab-parallel-programming/img/parforloop-mechanics.png >}}
Tips for Leveraging parfor

Consider creating smaller arrays on each worker versus one large array prior to the parfor loop
Take advantage of parallel.pool.Constant to establish variables on pool workers prior to the loop
Encapsulate blocks as functions when needed


Finally, if you need lots of control in your program, then we have some advanced programming constructs that will let you do things like send messages between workers."
rc-learning-fork/content/notes/matlab-parallel-programming/matlab-parallel-programming_3.md,"To optimize the performance of your MATLAB® code, it’s important to first analyze and address potential bottlenecks before considering approaches like parallel computing or code generation. One effective way to accelerate your MATLAB code is by optimizing your serial code through techniques like preallocation and vectorization.

Preallocation involves initializing an array with its final size before use, which helps prevent the dynamic resizing of arrays, especially in loops (e.g., for and while loops).  
Vectorization replaces loops with matrix and vector operations, enabling MATLAB to process data more efficiently.

Additionally, replacing sections of your code with MEX-functions—MATLAB executable files—can yield significant performance improvements. Using MATLAB Coder™, you can generate readable and portable C code, which is then compiled into a MEX-function to replace the equivalent MATLAB code.
Before modifying your code, it's important to focus on the most critical areas. The Code Analyzer and MATLAB Profiler are two essential tools to help identify where optimizations are most needed:

The Code Analyzer works in the MATLAB Editor, checking your code as you write it. It flags potential issues and suggests modifications to improve performance.
The MATLAB Profiler provides detailed timing information about your code’s execution. It shows where your code spends the most time, which functions are called the most, and which lines of code are the most time-consuming. This information helps pinpoint bottlenecks and enables you to streamline your serial code.

Once your serial code is optimized, you can further improve performance by leveraging additional computing resources. MATLAB parallel computing tools allow you to tap into the power of multicore processors, computer clusters, and GPUs to accelerate your workflows, ensuring that your code runs more efficiently even as your computational needs scale.

Run MATLAB on multicore machines
{{< figure src=/notes/matlab-parallel-programming/img/matlab-multicore-machines.jpg >}}
MATLAB provides two main approaches to parallel computing: implicit multithreading and explicit parallel computing.
Built-in Multithreading (Implicit)
MATLAB automatically enables implicit multithreading, where multiple threads operate within a single MATLAB computation engine. Functions such as fft, eig, svd, and sort are multithreaded, meaning they can perform operations using multiple cores without needing any special configuration. This feature leverages underlying multi-threaded libraries and is automatically applied in MATLAB, as well as in many toolboxes like the Image Processing Toolbox, which benefits from core MATLAB function support. However, this implicit support has limitations—some MATLAB functions do not support multithreading, and any performance gains are confined to the local workstation.
Parallel Computing Using Explicit Techniques
For more control, MATLAB offers explicit parallel computing, which involves using multiple computation engines (workers) controlled by a single session. This allows you to explicitly distribute computations across multiple cores or even scale your applications to clusters and clouds. With tools like the Parallel Computing Toolbox, you can use syntax such as parfor to control how portions of your workflow are distributed to different cores. This explicit parallelism provides more flexibility and can be extended beyond a single machine to larger computing resources using MATLAB Parallel Server. Additionally, MATLAB enables parallel computing on GPUs, allowing for even greater computational power when needed.
In summary, implicit multithreading in MATLAB is automatically enabled for many core functions, while explicit parallel computing provides greater flexibility and scalability for large-scale applications across multiple cores, clusters, or clouds."
rc-learning-fork/content/notes/matlab-parallel-programming/matlab-parallel-programming_7.md,"Summary

Easily develop parallel MATLAB applications without being a parallel programming expert
Run many Simulink simulations at the same time on multiple CPU cores.
Speed up the execution of your applications using additional hardware including GPUs, clusters and clouds
Develop parallel applications on your desktop and easily scale to a cluster when needed


In this presentation, I've shown you how easy it can be to use cluster hardware with your MATLAB workflow
I've also shown you the specific steps needed.   MathWorks provides tools and infrastructure to let you prototype on the desktop, easily start the Amazon resources you need, and extend your workflow to additional hardware.
Some Other Valuable Resources

MATLAB Documentation
MATLAB  Advanced Software Development  Performance and  Memory
Parallel Computing Toolbox
Parallel and GPU Computing Tutorials
https://www.mathworks.com/videos/series/parallel-and-gpu-computing-tutorials-97719.html
Parallel Computing on the Cloud with MATLAB
http://www.mathworks.com/products/parallel-computing/parallel-computing-on-the-cloud/

MATLAB Central Community
Every month, over  2 million MATLAB & Simulink users visit MATLAB Central to get questions answered, download code and improve programming skills.
{{< figure src=""/notes/matlab-parallel-programming/img/matlab-central-logo.png"" height=""200px"" >}}
MATLAB Answers : Q&A forum; most questions get answered in only  60 minutes
File Exchange : Download code from a huge repository of free code including  tens of thousands of open source community files
Cody : Sharpen programming skills while having fun
Blogs : Get the inside view from Engineers who build and support MATLAB & Simulink
ThingSpeak : Explore IoT Data
And more for you to explore…
Get Help
{{< figure src=/notes/matlab-parallel-programming/img/get-help-example.png >}}
Quick access to:
- Self-serve tools
- Community knowledge base
- Support engineers
Part II: Matlab Parallel Computing On Rivanna

Upload the file Rivanna.zip to your Rivanna account using any of the methods outlined in the following link
Log into Rivanna using the FastX web interface and launch the Mate Desktop
If you are off-grounds, you will have to run the UVA Anywhere VPN before logging in through FastX
In the Mate Desktop, open two terminal windows, one to start the Matlab Desktop and one to work from the Rivanna command line.

In the second terminal window, go to the location where you uploaded the Rivanna.zip file and unzip it with the command unzip Rivanna.zip to create a folder Rivanna.


In the first terminal window, load and launch the Matlab Desktop with the commands,
module load matlab;
matlab

Make the current directory for Matlab the Rivanna folder and open example1.slurm file in the Matlab editor.
Modify the slurm script with your own id and allocation, save it. Do this for each of the slurm example files.
In the second terminal window, submit the slurm script to run on Rivanna with the command, e.g. sbatch example1.slurm
Check that the job is in the queue with the command: squeue -u <your id>

See more on displaying job status here


Once the job is running, it should finish in a few minutes. The error file (.err) should not contain anything and the output file (.out) should contain what Matlab would normally send to the Matlab command window.

If for some reason you need to cancel your job, use the scancel command with the job id. See further documentation here
For the examples of running Matlab using multiple compute nodes, those jobs are submitted from within Matlab rather than with an external slurm script. Matlab creates its own slurm script to submit the job. See further documentation here

Using multiple cores on one node:
```
!/bin/bash
This slurm script file runs
a multi-core parallel Matlab job (on one compute node)
SBATCH -p standard
SBATCH -A hpc_build
SBATCH -t time=1:00:00
SBATCH -mail-type=END
SBATCH --mail-user=teh1m@virginia.edu
SBATCH --job-name=runParallelTest
SBATCH -o output=runParallelTest_%A.out
SBATCH -e error=runParallelTest_%A.err
SBATCH --nodes=1 # Number of nodes
SBATCH --ntasks-per-node=8 # Number of cores per node
Load Matlab environment
module load matlab
Create and export variable for slurm job id
export slurm_ID=""${SLURM_JOB_ID}""
Set workers to one less that number of tasks (leave 1 for master process)
export numWorkers=$((SLURM_NTASKS-1))
Input parameters
nLoops=400; # number of iterations to perform
nDim=400; # Dimension of matrix to create
Run Matlab parallel program
matlab -nodisplay -r \
""setPool1; pcalc2(${nLoops}, ${nDim}, '${slurm_ID}'); exit;""
```
SLURM Script end-of-job email
{{< figure src=""/notes/matlab-parallel-programming/img/slurm-email-example.png"" width=""500px"" >}}
When the job finishes, the end-of-job email sent by SLURM will contain the output of the SLURM seff command.
GPU Computations
The gpu queue provides access to compute nodes equipped with RTX2080Ti, RTX3090, A6000, V100, and A100 NVIDIA GPU device
```
!/bin/bash
SBATCH -A mygroup
SBATCH --partition=gpu
SBATCH --gres=gpu:1
SBATCH --ntasks=1
SBATCH --time=12:00:00
module load singularity tensorflow/2.10.0
singularity run --nv $CONTAINERDIR/tensorflow-2.10.0.sif myAI.py
```
The second argument to gres can be rtx2080, rtx3090, v100, or a100 for the different GPU architectures. The third argument to gres specifies the number of devices to be requested. If unspecified, the job will run on the first available GPU node with a single GPU device regardless of architecture.
NVIDIA GPU BasePOD™
As artificial intelligence (AI) and machine learning (ML) continue to change how academic research is conducted, the NVIDIA DGX BasePOD, or BasePOD, brings new AI and ML functionality to Rivanna, UVA's High-Performance Computing (HPC) system. The BasePOD is a cluster of high-performance GPUs that allows large deep-learning models to be created and utilized at UVA.
The NVIDIA DGX BasePOD™ on Rivanna and Afton, hereafter referred to as the POD, is comprised of:
* 10 DGX A100 nodes with
  * 2TB of RAM memory per node
  * 80 GB GPU memory per GPU device
  * Compared to the regular GPU nodes, the POD contains  advanced features  such as:
    - NVLink for fast multi-GPU communication* GPUDirect RDMA Peer Memory for fast multi-node multi-GPU communication
    - GPUDirect Storage with 200 TB IBM ESS3200 (NVMe) SpectrumScale storage array, which makes it ideal for the following types of jobs:
      - The job needs multiple GPUs on a single node or even multiple nodes.
      - The job (can be single- or multi-GPU) is I/O intensive.
      - The job (can be single- or multi-GPU) requires more than 40 GB GPU memory. (The non-POD nodes with the highest GPU memory are the regular A100 nodes with 40 GB GPU memory.)
Learn more about basepod
Slurm script additional constraint
```
SBATCH -p gpu
SBATCH --gres=gpu:a100:X # replace X with the number of GPUs per node
SBATCH -C gpupod
```
Remarks
- Before running on multiple nodes, please make sure the job can scale well to 8 GPUs on a single node.
- Multi-node jobs on the POD should request all GPUs on the nodes, i.e. --gres=gpu:a100:8.
- You may have already used the POD by simply requesting an A100 node without the constraint, since 18 out of the total 20 A100 nodes are POD nodes.
- As we expand our infrastructure, there could be changes to the Slurm directives and job resource limitations in the future. Please keep an eye out for our announcements and documentation.
Constructing Resource-Efficient SLURM scripts

Strategy for submitting jobs to busy queues?
Request fewer cores, less time, or less memory (and corresponding cores).
Important to know exactly what compute/memory resources you job needs, as detailed in the seff output.

Constructing Resource-Efficient SLURM scripts
Shell script to monitor and record cpu/memory usage using top
```
!/bin/bash
This script takes four command line arguments, samples the output of the top command
and stores the output of the sampling in the file named Top.out.
$1 is the user ID of the owner of the processes to sample from the top output
$2 is the name to include in the top output filename
$3 is the top sampling interval
$4 is the name of the code to be tracked (as shown in top)
Example of line to include in slurm script submission before executable invoked
./sampleTop.sh   10  &
```
Constructing Resource-Efficient SLURM scripts
Shell script to monitor job resource usage output file
{{< figure src=""/notes/matlab-parallel-programming/img/slurm-output-file.png"" width=""500px"" >}}
Parallel/GPU Computing
Running parallel applications
So far, we've covered the basics concepts of parallel computing - hardware, threads, processes, hybrid applications, implementing parallelization (MPI and OpenMP), Amdahl's law and other factors that affect scalability.
Theory and background are great, but how do we know how many CPUs/GPUs to use when running our parallel application?
The only way to definitively answer this question is to perform a scaling study where a representative problem is run on different number of processors.
A representative problem is one with the same size (grid dimensions; number of particles, images, genomes, etc.) and complexity (e.g., level of theory, type of analysis, physics, etc.) as the research problems you want to solve.
Presenting scaling results (the right way)
Plotting the same data on log axes gives a lot more insight. Note the different scales for the left axes on the two plots. Including a line showing linear scaling and plotting the parallel efficiency on the right axis adds even more value.
{{< figure src=/notes/matlab-parallel-programming/img/scaling-logs-graphs.jpg >}}
Where should I be on the scaling curve?
If your work is not particularly sensitive to the time to complete a single run, consider using a CPU/GPU count at or very close to 100% efficiency, even if that means running on a single core.
This specially makes sense for parameter sweep workloads where the same calculation is run many times with a different set of inputs.
{{< figure src=/notes/matlab-parallel-programming/img/second-scaling-law.png >}}
Go a little further out on the scaling curve if the job would take an unreasonably long time at lower core counts or if a shorter time to solution helps you make progress in your research.
If code does not have checkpoint-restart capabilities and the run time would exceed queue limits, you'll have no choice but to run at higher core counts.
{{< figure src=""/notes/matlab-parallel-programming/img/scaling-comparison-1.png"" width=""400px"" >}}
{{< figure src=""/notes/matlab-parallel-programming/img/scaling-comparison-2.png"" width=""400px"" >}}
If the time to solution is absolutely critical, it's okay to run at lower efficiency.
Examples might include calculations that need to run on a regular schedule (data collected during day must be processed overnight) or severe weather forecasting."
rc-learning-fork/content/notes/matlab-parallel-programming/matlab-parallel-programming_4.md,"Take Advantage of Cluster Hardware
{{< figure src=/notes/matlab-parallel-programming/img/hardware-cluser-diagram.png >}}
There are several compelling reasons to offload your Parallel Computing Toolbox workflow from a desktop computer to a cluster. By doing so, you can free up your desktop for other tasks and take advantage of more powerful computing resources available on the cluster. For instance, you can submit jobs to the cluster and retrieve the results once they’re completed, allowing you to shut down your local computer while the job runs.
One key benefit of using a cluster is the ability to scale speed-up. By utilizing more cores, you can reduce computation time significantly—what would normally take hours can be reduced to just minutes. This enables faster iterations, allowing you to make updates to your code and run multiple experiments in a single day.
Additionally, clusters offer the ability to scale memory. If your array is too large to fit into your local computer's memory, you can use distributed arrays to split the data across multiple computers. Each computer stores only a portion of the array, making it possible to work with much larger datasets. Many MATLAB matrix operations and functions are enhanced to work with these distributed arrays, enabling you to operate on the entire array as a single entity within your desktop session, without needing to recode your algorithms. For more details on which functions have been enhanced for distributed arrays, refer to the Release Notes for recent updates to the Parallel Computing Toolbox.
Parallel Computing Paradigm
Clusters and Clouds
{{< figure src=/notes/matlab-parallel-programming/img/matlab-parallel-clusters.png >}}

The problems or challenges you work on might need additional computational resources or memory than what is available on a single multicore desktop computer.
You can also scale up and access additional computational power or memory of multiple computers in a cluster in your organization or on the cloud. When we say scale up it essentially means that your pool or workers are now located on the cluster computers instead of the cores of your desktop computer. Irrespective of where your pool of workers are, your interface remains in the MATLAB Desktop. We've separated the algorithm from the infrastructure so you can write your code as you always do. 
You might want to perform computations on a cluster to free up your desktop computer for other work.   You can submit jobs to a cluster and retrieve the results when they're done.   You can even shut down your local computer while you wait.   
You can also use a cluster to scale an application that you've developed on your desktop.   Getting computations to take minutes rather than hours allows you to make updates to code and execute multiple runs all in the same day. 

batch Simplifies Offloading Serial Computations
Submit jobs from MATLAB, free up MATLAB for other work, access results later
To offload work from your MATLAB session and run in the background in another session, use the batch command inside a script.
Jobs will run in the background, allowing you to access results later.  
Example Commands:

Basic usage:
matlab
   job = batch('myfunc'); 
With a parallel pool:
matlab
   job = batch('myfunc', 'Pool', 3); 

{{< figure src=""/notes/matlab-parallel-programming/img/parallel-pool.png"" width=""500px"" >}}

Why parallel computing matters
Scaling with a compute cluster
{{< figure src=/notes/matlab-parallel-programming/img/cluster-scaling-example.png >}}

In this example, you see a parameter sweep in which we run up to 160,000 different configurations.
If my problem is well-sized, I can get more than a 90x speed-up with 100 workers.
Just adding more workers is not a guarantee of more speed-up, though.  Every application has its limits, and eventually overhead will dominate.
When choosing how many workers you should use, it's best to think about your overall needs,  in this case, even with 64 workers, I can go from 4 hours to just 4 minutes, which might be good enough.  
As mentioned previously, the sweet spot is ultimate execution  on the order of a few minutes.
Parallel Computing with Matlab
Useful Links:
Scale Up from Desktop to Cluster
Choose a Parallel Computing Solution
Parallel Computing Toolbox Documentation
Benchmarks for Matlab Parallel Code
Parallel Computing Toolbox Release Notes
Key functionality
|  | Description | Functionality | Ease of use | Control |
| :-: | :-: | :-: | :-: | :-: |
| Parallel Enabled Toolboxes | Ready to use parallelized functions in MathWorks tools | MATLAB and Simulink parallel enabled functionsToolbox integration | Turnkey-automatic | Minimal (presets) |
| Common programming constructs | Constructs that enable you to easily parallelize your code | parforgpuArraybatchdistributed/tallparsimparfeval | Simple | Some |
| Advanced programming constructs | Advanced parallelization techniques | spmdarrayfun/pagefunCUDAKernelMapReduceMATLAB Spark API | Advanced | Extensive |
Migrate to Cluster / Cloud
-Use MATLAB Parallel Server
- Change hardware without changing algorithm
{{< figure src=/notes/matlab-parallel-programming/img/change-cluser-matlab.png >}}
Use Multiple Nodes to Quickly Find the Optimal Network

Experiment Manager App
Manage experiments and reduce manual coding
Sweep through a range of hyperparameter values
Compare the results of using different data sets
Test different neural network architectures
MATLAB Parallel Server
Enables multiple nodes to train networks in parallel -> greatly reduce testing time
Running many experiments to train networks and compare the results in parallel

{{< figure src=/notes/matlab-parallel-programming/img/matlab-parallel-server-sweep.png >}}
Broad Range of Needs and Cloud Environments Supported
{{< figure src=/notes/matlab-parallel-programming/img/matlab-hardware-diagram.png >}}
| Access requirements | Desktop in the cloud | Cluster in the cloud(Client can be any cloud on on-premise desktop) |
| :-: | :-: | :-: |
| Any user could set up | NVIDIA GPU Cloud  | MathWorks Cloud Center |
| Customizable template-based set up |  MathWorks Cloud Reference Architecture | MathWorks Cloud Reference Architecture |
| Full set-up in custom environment | Custom installation - DIY | Custom installation - DIY |
Learn More: Parallel Computing on the Cloud

Parallel Computing Toolbox and MATLAB Parallel Server allow you to easily extend your execution of  MATLAB and Simulink to more resources.
In the parallel computing workflow, you start with MATLAB on the desktop, and incorporate parallel features from Parallel Computing Toolbox to use more resources.   
You can use Parallel Computing Toolbox in a range of environments, from those which can be set up by users on their own laptop to cloud installations for enterprise deployment.
When you need to scale beyond the desktop, you will need access to a cluster that has MATLAB Parallel Server installed.  This might be a cluster managed by your IT department, or it could be a cloud cluster that you set up on your own with Cloud Center or a MathWorks reference architecture.
Again, there are a range of environments and ways you can access MATLAB Parallel Server, from self-serve options like Cloud Center and the customizable reference architectures to integration with your existing cluster and cloud infrastructure.
Once you have access to MATLAB Parallel Server, you can use parallel resources on the cluster in the same way you did on the desktop, without needing to re-code algorithms.
Note: NVIDIA GPU Cloud is actually a container, and can be run in the cloud or on-premise.
For MATLAB Parallel Server, see: https://www.mathworks.com/products/matlab-parallel-server/get-started.html"
rc-learning-fork/content/notes/matlab-parallel-programming/out.md,"Parallel Matlab on Rivanna
Research Computing Workshop
By Ed Hall
Why parallel computing?

Save time and tackle increasingly complex problems
Reduce computation time by using available compute cores and GPUs
Why parallel computing with MATLAB and Simulink?
Accelerate workflows with minimal to no code changes to your original code
Scale computations to clusters and clouds
Focus on your engineering and research, not the computation



What's the need for Parallel Computing you might ask or what's the motivation for it? 
The size of the problems we need to solve is increasing and there's a growing need to gain faster helping bring products to market quickly. And this need exists for engineers and researchers across multiple industries and applications!
Another motivation is simply the fact that hardware is becoming powerful - modern computers, even laptops, are parallel in architecture with multiple processors/cores. Access to GPUs, cluster of computers or even cloud computing infrastructure is becoming common.
However the challenge lies in the fact that you need software to utilize the power of this hardware and hence parallel computing expertise is a requirement
How do MathWorks Parallel Computing Tools help ?
Enables Engineers, scientists and researchers like yourself leverage the computational power of available hardware without the need to be a parallel computing expert
Accelerate your workflows with minimal changes to your existing code 
Allow you to seamlessly scale your applications and models from your Desktop to clusters in your organization or on the cloud if you need access to more computational power/memory. 
Let's look at some examples of customer across multiple industries who have been using MathWorks parallel computing tools and why they chose parallel computing 
Benefits of parallel computing

Here are some examples of where people are successfully using our parallel computing tools.  All of these people are scaling up their problems to run on additional hardware and seeing significant benefits from it.  
Here are examples from some of our user stories of what can be done with our parallel computing tools.  In many of these cases, they wrote the code once and could run in it many different environments.  Some of these are simple Monte Carlo analyses, some are parameter sweeps, there are also people doing optimization.  
Our parallel computing tools are all about enabling you to scale to more hardware without needing to be an expert in parallel computing.  

Automotive Test Analysis and Visualization
* 3-4 months of development time saved
* Validation time sped up 2X

Heart Transplant Studies
* 4 weeks reduced to 5 days
* Process time sped up 6X

Discrete-Event Model of Fleet Performance
* Simulation time reduced from months to hours
* Simulation time sped up 20X

Calculating Derived Market Data
* Implementation time reduced by months
* Updates sped up 8X
Accelerating and Parallelizing MATLAB Code

Optimize your serial code for performance
Analyze your code for bottlenecks andaddress most critical items
Include compiled languages
Leverage parallel computing tools to take advantage of additionalcomputing resources


Let's discuss techniques that you can use to accelerate your MATLAB® algorithms and applications
It is often a good practice to optimize your serial code for performance before considering parallel computing, code generation, or other approaches. Two effective programming techniques to accelerate your MATLAB code are preallocation and vectorization.
With preallocation, you initialize an array using the final size required for that array. Preallocation helps you avoid dynamically resizing arrays, particularly when code contains for and while loops.
Vectorization is the process of converting code from using loops to using matrix and vector operations
Replacing parts of your MATLAB code with an automatically generated MATLAB executable (MEX-function) may yield speedups. Using MATLAB Coder™, you can generate readable and portable C code and compile it into a MEX-function that replaces the equivalent section of your MATLAB algorithm 
Before modifying your code, you need to determine where to focus your efforts. Two critical tools to support this process are the Code Analyzer and the MATLAB Profiler. The Code Analyzer in the MATLAB Editor checks your code while you are writing it. The Code Analyzer identifies potential problems and recommends modifications to maximize performance. 
The Profiler shows where your code is spending its time. It provides a report summarizing the execution of your code, including the list of all functions called, the number of times each function was called, and the total time spent within each function. The Profiler also provides timing information about each function, such as which lines of code use the most processing time.
This lets you find bottlenecks and streamline your serial code. 
The techniques described so far have focused on ways to optimize serial MATLAB code. You can also gain performance improvements by using additional computing power. MATLAB parallel computing products provide computing techniques that let you take advantage of multicore processors, computer clusters, and GPUs
Run MATLAB on multicore machines


Built-in multithreading (implicit)
Automatically enabled in MATLAB
Multiple threads in a single MATLAB computation engine
Functions such as fft,eig, svd, and sort are multithreaded in MATLAB
Parallel computing using explicit techniques
Multiple computation engines (workers)  controlled by a single session
Perform MATLAB computations on GPUs
High-level constructs to let you parallelize MATLAB applications
Scale parallel applications beyond a single machine to clusters and clouds


It's important to note that MATLAB does offer implicit multi-core support with its built-in multithreading.   A growing number of core MATLAB functions, as well as  some specific functions in the Image Processing Toolbox take advantage of underlying multi-threaded library support.   Other toolboxes get the benefit of implicit multi-threading through their use of core MATLAB function.  This implicit support  has been part of  our products since 2008.   The disadvantage of implicit multithreading is that not every MATLAB function is able to be multi-threading, and any advantage is limited to your local workstation.
MathWorks parallel computing tools provide an explicit way to support multi-core computers.  With Parallel Computing Toolbox syntax, like parfor, you control which portions of your workflow are distributed to multiple cores.   Not only is the explicit support controllable, but it is extensible to clusters using MATLAB Parallel Server
Utilizing multiple cores on a desktop computer
Parallel Computing Paradigm Multicore Desktops


MathWorks offers two parallel computing tools.   The first is Parallel Computing Toolbox which is our desktop solution that allows users to be more productive with their multicore processors by utilizing headless MATLAB sessions called workers to use the full potential of their hardware and speed up their workflow.  The Toolbox provides easy to use syntax that allows users to stay in their familiar desktop environment while our solution takes care of the details of using multiple sessions in the background. We make the tough aspects of parallelism easy for users:  GPU, Deep Learning, Big Data, Clusters, Clouds
Parallel ComputingToolbox also provides support for NVIDIA CUDA-based GPUs, and provides the syntax used by our second tool, MATLAB Parallel Server.
Under the hood the parallel computing products divide the tasks/computations/simulations and assigns them to these workers in the background - enabling the computations to execute in parallel.
A key concept for parallel computing with MATLAB is the ability to run multiple MATLAB computational engines that are controlled by a single MATLAB session and that are licensed as workers. A collection of these workers with inter-process communication is what we call a parallel pool. 
The parallel computing products provide features that can divide tasks/computations/simulations and assigns them to MATLAB computational engines in the background - enabling execution in parallel.
In general you should not run more MATLAB computational engines than the number of physical cores available.   Otherwise, you are likely to have resource contention.
Note that Parallel Computing Toolbox provides licensing for enough workers to cover all of the physical cores in a single machine.
See also:  https://www.mathworks.com/discovery/matlab-multicore.html 
Accelerating MATLAB and Simulink Applications
Easiest to use: Parallel-enabled toolboxes ('UseParallel', true)
Moderate ease of use, moderate control: Common programming constructs
Greatest control: Advanced programming constructs

Let's focus first on built in support that can be enabled simply in MATLAB toolboxes. In these toolboxes, all you have to do is indicate that you wish to go parallel using the UseParallel flag.
Demo: Classification Learner AppAnalyze sensor data for human activity classification


Objective: visualize and classify cellphone sensor data of human activity
Approach:
Load human activity dataset
Leverage multicore resources to train multiple classifiers in parallel


https://insidelabs-git.mathworks.com/ltc-ae/demos/HumanActivityRecognition
Let's open MATLAB and try out a demonstration of using built in parallel functionality invoked by setting a toggle to invoke parallel. 
In this demo, we are loading IOT sensor data from a mobile phone and then using this sensor data to classify the data into activity-standing, sitting, running, etc. 
The demo will utilize the multiple cores in my parallel pool to train multiple classifiers simultaneously. 
Demo: Cell Phone Tower OptimizationUsing Parallel-Enabled Functions

Parallel-enabled functions in Optimization Toolbox
Set flags to run optimization in parallel
Use pool of MATLAB workers to enable parallelism


Parallel-enabled Toolboxes (MATLAB® Product Family)
Enable parallel computing support by setting a flag or preference
Image Processing


Batch Image Processor, Block Processing, GPU-enabled functions

Across a wide range of MATLAB toolboxes, all you have to do is indicate that you wish to use parallel using the UseParallel flag or via a toggle switch preference.
Statistics and Machine Learning
Resampling Methods, k-Means clustering, GPU-enabled functions

Deep Learning, Neural Network training and simulation


**Signal Processing and Communications **
GPU-enabled FFT filtering, cross correlation, BER simulations



Bag-of-words workflow, object detectors
Optimization & Global Optimization
Estimation of gradients, parallel search


Accelerating MATLAB and Simulink Applications
Easiest to use: Parallel-enabled toolboxes
Moderate ease of use, moderate control: Common programming constructs (parfor, batch)
Greatest control: Advanced programming constructs

If you want a bit more control, then Parallel Computing Toolbox adds some parallel keywords into the MATLAB language.  An example of this is Parfor or batch commands.
Explicit Parallelism: Independent Tasks or IterationsSimple programming constructs: parfor
Examples: parameter sweeps, Monte Carlo simulations
No dependencies or communications between tasks


How do we express parallelism in code?
If you are dealing with problems that are computationally intensive and are just taking too long because there are multiple tasks/iterations/simulations you need to execute to gain deeper insight   these are ideal problems for parallel computing and the easiest way to address these challenges is to use parallel for loops.
Real-world examples of such problems are parameter sweeps or Monte Carlo simulations.
For example, lets say you have 5 tasks to be completed - If you run these in a FOR loop, they run serially one after the other - you wait for one to get done, then start the next iteration - However if they're all independent tasks with no dependencies or communication between individual iterations - you can distribute these tasks to the MATLAB workers we spoke about - multiple tasks can execute simultaneously  you'll maximize the utilization of the cores on your desktop machine and save up on a lot of time !
Requirements for parfor loops 
Task independent
Order independent
Constraints on the loop body
Cannot “introduce” variables (e.g. load, etc.)
Cannot contain break or return statements
Cannot contain another parfor loop
Explicit Parallelism: Independent Tasks or Iterations
for i = 1:5
  y(i) = myFunc(myVar(i));
end
parfor i = 1:5
  y(i) = myFunc(myVar(i));
end


For example, lets say you have 5 tasks to be completed - If you run these in a FOR loop, they run serially one after the other - you wait for one to get done, then start the next iteration- However if they're all independent tasks with no dependencies or communication between individual iterations - you can distribute these tasks to the MATLAB workers we spoke about - multiple tasks can execute simultaneously  you'll maximize the utilization of the cores on your desktop machine and save up on a lot of time !
Mechanics of parfor Loops

Tips for Leveraging parfor
Consider creating smaller arrays on each worker versus one large array prior to the parfor loop
Take advantage of parallel.pool.Constant to establish variables on pool workers prior to the loop
Encapsulate blocks as functions when needed
Accelerating MATLAB and Simulink Applications
|  | Easiest to use: Parallel-enabled toolboxes |  |
| :-: | :-: | :-: |
|  | Moderate ease of use, moderate control: Common programming constructs |  |
|  | Greatest control: Advanced programming constructs(spmd, createJob, labSend,..) |  |

Finally, if you need lots of control in your program, then we have some advanced programming constructs that will let you do things like send messages between workers.
Spmd, arrayfun, CUDAKernel,labsend, 
Scaling up to cluster and cloud resources
Take Advantage of Cluster Hardware


Offload computation:
Free up desktop
Access better computers
Scale speed-up:
Use more cores
Go from hours to minutes
Scale memory:
Utilize tall arrays and distributed arrays
Solve larger problems without re-coding algorithms


There are a few primary reasons to move your Parallel Computing Toolbox workflow from a Desktop computer to a cluster:
You might want to perform computations on a cluster to free up your desktop computer for other work.   You can submit jobs to a cluster and retrieve the results when they're done.   You can even shut down your local computer while you wait.
You can also use a cluster to scale an application that you've developed on your desktop.   Getting computations to take minutes rather than hours allows you to make updates to code and execute multiple runs all in the same day. 
If you have an array that is too large for your computer's memory, you can use distributed arrays to store the data across multiple computers, so that each computer contains only a part of the array.   A growing number of MATLAB matrix operations and functions have been enhanced to work with distributed arrays using the Parallel Computing Toolbox.   These enhanced functions allow you to use your desktop session to operate on the entire distributed array as a single entity.   [See the Release Notes for recent releases of Parallel Computing Toolbox for more details regarding which functions have been enhanced]
Parallel Computing ParadigmClusters and Clouds


The problems or challenges you work on might need additional computational resources or memory than what is available on a single multicore desktop computer
You can also scale up and access additional computational power or memory of multiple computers in a cluster in your organization or on the cloud. When we say scale up it essentially means that your pool or workers are now located on the cluster computers instead of the cores of your desktop computer. Irrespective of where your pool of workers are, your interface remains in the MATLAB Desktop. We've separated the algorithm from the infrastructure so you can write your code as you always do. 
You might want to perform computations on a cluster to free up your desktop computer for other work.   You can submit jobs to a cluster and retrieve the results when they're done.   You can even shut down your local computer while you wait.   
You can also use a cluster to scale an application that you've developed on your desktop.   Getting computations to take minutes rather than hours allows you to make updates to code and execute multiple runs all in the same day. 
batch Simplifies Offloading Serial ComputationsSubmit jobs from MATLAB and free up MATLAB for other work
job = batch('myfunc');


To offload work from your MATLAB session to run in the background in another session, you can use the batch command inside a script.
Just submit your jobs and they will run in the background, you can then access results later
batch Simplifies Offloading Serial ComputationsSubmit jobs from MATLAB, free up MATLAB for other work, access results later
job = batch('myfunc','Pool',3);

Why parallel computing mattersScaling with a compute cluster



In this example, you see a parameter sweep in which we run up to 160,000 different configurations
Doc example https://www.mathworks.com/help/distcomp/examples/plot-progress-during-parallel-computations-using-parfor-and-dataqueue.html?searchHighlight=dataqueue&s_tid=doc_srchtitle
If my problem is well-sized, I can get more than a 90x speed-up with 100 workers.
Just adding more workers is not a guarantee of more speed-up, though.  Every application has its limits, and eventually overhead will dominate.
When choosing how many workers you should use, it's best to think about your overall needs,  in this case, even with 64 workers, I can go from 4 hours to just 4 minutes, which might be good enough.  
As mentioned previously, the sweet spot is ultimate execution  on the order of a few minutes.
Notes:
Computation time is from tic/toc around just the parfor
Ran this on Amazon EC2 (no need to advertise that, but it is true!)
XB=frameworkSubmit(50,200,1,3,'R2014a_c3_192_2hr');
% Add links to blackjack and a\b
% might want to mention partictoc, or pull it from code
 % all demos need html version
Parallel Computing with MatlabUseful Links
Scale Up from Desktop to Cluster
Choose a Parallel Computing Solution
Parallel Computing Toolbox Documentation
Benchmarks for Matlab Parallel Code
Parallel Computing Toolbox Release Notes
Key functionality
|  | Description | Functionality | Ease of use | Control |
| :-: | :-: | :-: | :-: | :-: |
| Parallel Enabled Toolboxes | Ready to use parallelized functions in MathWorks tools | MATLAB and Simulink parallel enabled functionsToolbox integration | Turnkey-automatic | Minimal (presets) |
| Common programming constructs | Constructs that enable you to easily parallelize your code | parforgpuArraybatchdistributed/tallparsimparfeval | Simple | Some |
| Advanced programming constructs | Advanced parallelization techniques | spmdarrayfun/pagefunCUDAKernelMapReduceMATLAB Spark API | Advanced | Extensive |
Migrate to Cluster / Cloud
Use MATLAB Parallel Server
Change hardware without changing algorithm

Use Multiple Nodes to Quickly Find the Optimal Network

Experiment Manager App
Manage experiments and reduce manual coding
Sweep through a range of hyperparameter values
Compare the results of using different data sets
Test different neural network architectures
MATLAB Parallel Server
Enables multiple nodes to train networks in parallel -> greatly reduce testing time
Running many experiments to train networks and compare the results in parallel



Broad Range of Needs and Cloud Environments Supported

| Access requirements | Desktop in the cloud | Cluster in the cloud(Client can be any cloud on on-premise desktop) |
| :-: | :-: | :-: |
| Any user could set up | NVIDIA GPU Cloud  | MathWorks Cloud Center |
| Customizable template-based set up |  MathWorks Cloud Reference Architecture |  |
| Full set-up in custom environment | Custom installation - DIY |  |
Learn More: Parallel Computing on the Cloud

Parallel Computing Toolbox and MATLAB Parallel Server allow you to easily extend your execution of  MATLAB and Simulink to more resources.
In the parallel computing workflow, you start with MATLAB on the desktop, and incorporate parallel features from Parallel Computing Toolbox to use more resources.   
You can use Parallel Computing Toolbox in a range of environments, from those which can be set up by users on their own laptop to cloud installations for enterprise deployment.
When you need to scale beyond the desktop, you will need access to a cluster that has MATLAB Parallel Server installed.  This might be a cluster managed by your IT department, or it could be a cloud cluster that you set up on your own with Cloud Center or a MathWorks reference architecture.
Again, there are a range of environments and ways you can access MATLAB Parallel Server, from self-serve options like Cloud Center and the customizable reference architectures to integration with your existing cluster and cloud infrastructure.
…and once you have access to MATLAB Parallel Server, you can use parallel resources on the cluster in the same way you did on the desktop, without needing to re-code algorithms.
Note: NVIDIA GPU Cloud is actually a container, and can be run in the cloud or on-premise.
For MATLAB Parallel Server, see: https://www.mathworks.com/products/matlab-parallel-server/get-started.html
Tackling data-intensive problems on desktops and clusters
Extend Big Data Capabilities in MATLAB with Parallel Computing


MATLAB provides a single, high-performance environment for working with big data. You can: 
Access data that does not fit in memory from standard big data sources Use data from a Hadoop Distributed File System (HDFS) in MATLAB 
Create repositories of large amounts of images, spreadsheets, and custom files (datastore) 
Use capabilities customized for beginners and power users of big data applications Use tall arrays for working with columnar data with millions or billions of rows 
Partition data, too big for a single computer, across computers in a cluster using distributed arrays 
Use hundreds of MATLAB and toolbox functions supported with tall, distributed, and sparse arrays 
Create your own big data algorithms using MATLAB MapReduce or MATLAB API for Spark 
Program once, and scale to many execution environments: desktop machines, compute clusters, and Spark clusters 
Easily access data however it is stored
Prototype algorithms quickly using small data sets
Scale up to big data sets running on large clusters
Using the same intuitive MATLAB syntax you are used to
Parallel Computing Toolbox extends the tall array and MapReduce capabilities built into MATLAB so that you can run on local workers for improved performance. You can then scale Tall array and MapReduce up to additional resources with MATLAB Parallel Server, on traditional clusters or Apache Spark™ and Hadoop® clusters
Overcoming Single Machine Memory Limitations Distributed Arrays

Distributed Data
Large matrices using the combined memory of a cluster
Common Actions
Matrix Manipulation
Linear Algebra and Signal Processing
A large number of standard MATLAB functions work with distributed arrays just as they do for normal variables



Parallel Computing Toolbox provides something called a “distributed” array. On your desktop this looks just like a normal MATLAB variable but it's data is distributed across MATLAB workers on the cluster. When you perform an operation on a distributed array the work happens out in the cluster but nothing much else changes from your normal workflow.
You can prototype distributed array on the desktop and then scale up to additional resources with MATLAB Parallel Server.
A large number of standard MATLAB functions work with distributed arrays just as they do for normal variables. That means you can program a distributed-array algorithm just how you would program a normal in-memory algorithm.
Just like we do for the GPU, we have overloaded functions that work transparently with variables that are stored across the memory of multiple physical computers.   With the overloaded functions, you can write one application that can work with local data or distributed data without needing to be an expert in message passing.   Our goal is to make difficult things easy, so you can focus on your algorithms and research
Tall Arrays


Applicable when:
Data is  columnar  - with  many  rows
Overall data size is  too big to fit into memory
Operations are mathematical/statistical in nature
Statistical and machine learning applications
Hundreds of functions supported in MATLAB andStatistics and Machine Learning Toolbox


Automatically breaks data up into small “chunks” that fit in memory
Tall arrays scan through the dataset one “chunk” at a time
Processing code for tall arrays is the same as ordinary arrays

Tall arrays wrap around one of these datastores and treat the entire set of data as one continuous table or array. When we need to run calculations, the underlying datastore let's us work through the array one piece at a time. However, that's all under the hood, and when you're using a tall array you just write normal MATLAB code. 
In this example we started with CSV files, which contain tabular data. The resulting tall array is actually a tall table, so we can use standard table functions like summary or dot references to get to the columns and then use max, min, plus, minus just as we would if it wasn't a tall table.
Demo: Predicting Cost of Taxi Ride in NYCWorking with tall arrays in MATLAB

Objective: Create a model to predict the cost of a taxi ride in New York City
Inputs:
Monthly taxi ride log files
The local data set contains > 2 million rows
Approach:
Preprocess and explore data
Work with subset of data for prototyping
Fit linear model
Predict fare and validate model


Accelerating applications with NVIDIA GPUs
Why MATLAB with NVIDIA GPUs
Easy access to NVIDA GPUs with 990+ GPU-enabled functions
Matlab users can work with NVIDIA GPUs without CUDA programming
NVIDIA GPUs accelerate many applications like AI/Deep Learning
Graphics Processing Units (GPUs)
For graphics acceleration and scientific computing
Many parallel processors
Dedicated high speed memory

GPU Requirements
Parallel Computing Toolbox requires NVIDIA GPUs
nvidia.com/object/cuda_gpus.html
| MATLAB Release | Required Compute Capability |
| :-: | :-: |
| MATLAB R2018a and later releases | 3.0 or greater |
| MATLAB R2014b - MATLAB R2017b | 2.0 or greater |
| MATLAB R2014a and earlier releases | 1.3 or greater |
GPU Computing ParadigmNVIDIA CUDA-enabled GPUs


Our tools can both be used to speed up your calculation using multiple CPUs and by using GPUs.  
Although GPUs have hundreds of cores, we treat the GPU as a single  unit, and access directly from a  MATLAB computation engine.   For example,  any single Worker can access the entire GPU.       
Expected speed-up varies with problem specifics as well as the avaialable hardware.
Programming with GPUs
|  | Easiest to use: Parallel-enabled toolboxes |  |
| :-: | :-: | :-: |
|  | Moderate ease of use and moderate control: Common programming constructs(gpuArray, gather) |  |
|  | Greatest control: Advanced programming constructs |  |
Demo: Wave EquationAccelerating scientific computing in MATLAB with GPUs

Objective: Solve 2nd order wave equation with spectral methods
Approach:
Develop code for CPU
Modify the code to use GPUcomputing using gpuArray
Compare performance ofthe code using CPU and GPU



Speed-up using NVIDIA GPUs


Ideal Problems
Massively Parallel and/or Vectorized operations
Computationally Intensive
500+ GPU-enabled MATLAB functions
Simple programming constructs
gpuArray, gather


In the case of GPUs, it's slightly different.
Ideal problems for GPU computing :

Massively parallel—The computations can be broken down into hundreds or thousands of independent units of work.  You will see the best performance all of the cores are kept busy, exploiting the inherent parallel nature of the GPU. 
Computationally intensive—The time spent on computation significantly exceeds the time spent on transferring data to and from GPU memory. Because a GPU is attached to the host CPU via the PCI Express bus, the memory access is slower than with a traditional CPU. This means that your overall computational speedup is limited by the amount of data transfer that occurs in your algorithm. 
Algorithm consists of supported functions

Our developers have written CUDA versions of key MATLAB and toolbox functions and presented them as overloaded  functions - We have over 500 GPU-enabled functions in MATLAB and a growing number of GPU-enabled functions in additional toolboxes as well.
The diagram pretty much sums up the easiest way to do GPU computing in MATLAB - Transfer/create data on the GPU using the “gpuArray”, run your function as you would normally - if the inputs are available on the GPU, we do the right thing and run on the GPU and then “gather” the data back to the CPU. This seamless support allows you to run the same code on both the CPU and the GPU.
GPU Computing with Matlab
Useful Links
GPU Computing
GPU Computing in MATLAB
MATLAB GPU Computing Support for NVIDIA CUDA-Enabled GPUs
Run MATLAB Functions on Multiple GPUs
Boost MATLAB algorithms using NVIDIA GPUs
Programming with GPUs
|  | Easiest to use: Parallel-enabled toolboxes |  |
| :-: | :-: | :-: |
|  | Moderate ease of use, moderate control: Common programming constructs |  |
|  | Greatest control: Advanced programming constructs(spmd,arrayfun,CUDAKernel,mex) |  |
Summary and resources
Summary

Easily develop parallel MATLAB applications without being a parallel programming expert
Run many Simulink simulations at the same time on multiple CPU cores.
Speed up the execution of your applications using additional hardware including GPUs, clusters and clouds
Develop parallel applications on your desktop and easily scale to a cluster when needed


In this presentation, I've shown you how easy it can be to use cluster hardware with your MATLAB workflow
I've also shown you the specific steps needed.   MathWorks provides tools and infrastructure to let you prototype on the desktop, easily start the Amazon resources you need, and extend your workflow to additional hardware.
Some Other Valuable Resources

MATLAB Documentation
MATLAB  Advanced Software Development  Performance and  Memory
Parallel Computing Toolbox
Parallel and GPU Computing Tutorials
https://www.mathworks.com/videos/series/parallel-and-gpu-computing-tutorials-97719.html
Parallel Computing on the Cloud with MATLAB
http://www.mathworks.com/products/parallel-computing/parallel-computing-on-the-cloud/

MATLAB Central Community
Every month, over  **2 million ** MATLAB & Simulink users visit MATLAB Central to get questions answered, download code and improve programming skills.

MATLAB Answers : Q&A forum; most questions get answered in only  60 minutes
File Exchange : Download code from a huge repository of free code including  **tens of thousands ** of open source community files
Cody : Sharpen programming skills while having fun
Blogs : Get the inside view from Engineers who build and support MATLAB & Simulink
ThingSpeak : ** ** Explore IoT Data
And more for you to explore…
Get Help

Quick access to:
Self-serve tools
Community knowledge base
Support engineers
Part II: Matlab Parallel Computing On Rivanna

Upload the file Rivanna.zip to your Rivanna account using any of the methods outlined in the following URL.
https://www.rc.virginia.edu/userinfo/data-transfer/#data-transfer-methods
Log into Rivanna using the FastX web interface and launch the Mate Desktop.
http://rivanna-desktop.hpc.virginia.edu
If you are off-grounds, you will have to run the UVA Anywhere VPN before logging in through FastX.
https://www.rc.virginia.edu/userinfo/linux/uva-anywhere-vpn-linux/
In the Mate Desktop, open two terminal windows, one to start the Matlab Desktop and one to work from the Rivanna command line.

In the second terminal window, go to the location where you uploaded the Rivanna.zip file and unzip it with the command 'unzip Rivanna.zip' to create a folder Rivanna.


In the first terminal window, load and launch the Matlab Desktop with the commands,

module load matlab;
matlab
Make the current directory for Matlab the Rivanna folder and open example1.slurm file in tthe Matlab editor.
Modify the slurm script with your own id and allocation, save it. Do this for each of the slurm example files.
In the second terminal window, submit the slurm script to run on Rivanna with the command, e.g.
sbatch example1.slurm
Check that the job is in the queue with the command
squeue -u 

https://www.rc.virginia.edu/userinfo/rivanna/slurm/#displaying-job-status


Once the job is running, it should finish in a few minutes. The error file (.err) should not contain anything and the output file (.out) should contain what Matlab would normally send to the Matlab command window.

If for some reason you need to cancel your job, use the scancel command with the job id
https://www.rc.virginia.edu/userinfo/rivanna/slurm/#canceling-a-job
For the examples of running Matlab using multiple compute nodes, those jobs are submitted from within Matlab rather than with an external slurm script. Matlab creates its own slurm script to submit the job.
The reference for this material is at the URL
https://www.rc.virginia.edu/userinfo/rivanna/software/matlab/

Using multiple cores on one node:
```
!/bin/bash
This slurm script file runs
a multi-core parallel Matlab job (on one compute node)
SBATCH -p standard
SBATCH -A hpc_build
SBATCH -t time=1:00:00
SBATCH -mail-type=END
SBATCH --mail-user=teh1m@virginia.edu
SBATCH --job-name=runParallelTest
SBATCH -o output=runParallelTest_%A.out
SBATCH -e error=runParallelTest_%A.err
SBATCH --nodes=1 # Number of nodes
SBATCH --ntasks-per-node=8 # Number of cores per node
Load Matlab environment
module load matlab
Create and export variable for slurm job id
export slurm_ID=""${SLURM_JOB_ID}""
Set workers to one less that number of tasks (leave 1 for master process)
export numWorkers=$((SLURM_NTASKS-1))
Input parameters
nLoops=400; # number of iterations to perform
nDim=400; # Dimension of matrix to create
Run Matlab parallel program
matlab -nodisplay -r \
""setPool1; pcalc2(${nLoops}, ${nDim}, '${slurm_ID}'); exit;""
```
SLURM Script end-of-job email


When the job finishes, the end-of-job email sent by SLURM will contain the output of the SLURM seff command.

GPU Computations
The gpu queue provides access to compute nodes equipped with RTX2080Ti, RTX3090, A6000, V100, and A100 NVIDIA GPU device
```
!/bin/bash
SBATCH -A mygroup
SBATCH --partition=gpu
SBATCH --gres=gpu:1
SBATCH --ntasks=1
SBATCH --time=12:00:00
module load singularity tensorflow/2.10.0
singularity run --nv $CONTAINERDIR/tensorflow-2.10.0.sif myAI.py
```
The second argument to gres can be rtx2080, rtx3090, v100, or a100 for the different GPU architectures. The third argument to gres specifies the number of devices to be requested. If unspecified, the job will run on the first available GPU node with a single GPU device regardless of architecture.
NVIDIA GPU BasePOD™
As artificial intelligence (AI) and machine learning (ML) continue to change how academic research is conducted, the NVIDIA DGX BasePOD, or BasePOD, brings new AI and ML functionality to Rivanna, UVA's High-Performance Computing (HPC) system. The BasePOD is a cluster of high-performance GPUs that allows large deep-learning models to be created and utilized at UVA.

The NVIDIA DGX BasePOD™ on Rivanna and Afton, hereafter referred to as the POD, is comprised of:
10 DGX A100 nodes with
2TB of RAM memory per node
80 GB GPU memory per GPU device
Compared to the regular GPU nodes, the POD contains  advanced features  such as:
NVLink for fast multi-GPU communication
GPUDirect RDMA Peer Memory for fast multi-node multi-GPU communication
GPUDirect Storage with 200 TB IBM ESS3200 (NVMe) SpectrumScale storage array
which makes it ideal for the following types of jobs:
The job needs multiple GPUs on a single node or even multiple nodes.
The job (can be single- or multi-GPU) is I/O intensive.
The job (can be single- or multi-GPU) requires more than 40 GB GPU memory. (The non-POD nodes with the highest GPU memory are the regular A100 nodes with 40 GB GPU memory.)

https://www.rc.virginia.edu/userinfo/rivanna/basepod/
Slurm script additional constraint
```
SBATCH -p gpu
SBATCH --gres=gpu:a100:X # replace X with the number of GPUs per node
SBATCH -C gpupod
```
Remarks
Before running on multiple nodes, please make sure the job can scale well to 8 GPUs on a single node.
Multi-node jobs on the POD should request all GPUs on the nodes, i.e. --gres=gpu:a100:8.
You may have already used the POD by simply requesting an A100 node without the constraint, since 18 out of the total 20 A100 nodes are POD nodes.
As we expand our infrastructure, there could be changes to the Slurm directives and job resource limitations in the future. Please keep an eye out for our announcements and documentation.
Constructing Resource-Efficient SLURM scripts
Strategy for submitting jobs to busy queues?
Request fewer cores, less time, or less memory (and corresponding cores).
Important to know exactly what compute/memory resources you job needs, as detailed in the seff output.
Constructing Resource-Efficient SLURM scripts
Shell script to monitor and record cpu/memory usage using top
```
!/bin/bash
This script takes four command line arguments, samples the output of the top command
and stores the output of the sampling in the file named Top.out.
$1 is the user ID of the owner of the processes to sample from the top output
$2 is the name to include in the top output filename
$3 is the top sampling interval
$4 is the name of the code to be tracked (as shown in top)
Example of line to include in slurm script submission before executable invoked
./sampleTop.sh   10  &
```
Constructing Resource-Efficient SLURM scripts
Shell script to monitor job resource usage output file

Parallel/GPU Computing
Running parallel applications
So far, we've covered the basics concepts of parallel computing - hardware, threads, processes, hybrid applications, implementing parallelization (MPI and OpenMP), Amdahl's law and other factors that affect scalability.
Theory and background are great, but how do we know how many CPUs/GPUs to use when running our parallel application?
The only way to definitively answer this question is to perform a scaling study where a representative problem is run on different number of processors.
A representative problem is one with the same size (grid dimensions; number of particles, images, genomes, etc.) and complexity (e.g., level of theory, type of analysis, physics, etc.) as the research problems you want to solve.
Presenting scaling results (the right way)
Plotting the same data on log axes gives a lot more insight. Note the different scales for the left axes on the two plots. Including a line showing linear scaling and plotting the parallel efficiency on the right axis adds even more value

Where should I be on the scaling curve?
If your work is not particularly sensitive to the time to complete a single run, consider using a CPU/GPU count at or very close to 100% efficiency, even if that means running on a single core.
This specially makes sense for parameter sweep workloads where the same calculation is run many times with a different set of inputs

Go a little further out on the scaling curve if the job would take an unreasonably long time at lower core counts or if a shorter time to solution helps you make progress in your research.
If code does not have checkpoint-restart capabilities and the run time would exceed queue limits, you'll have no choice but to run at higher core counts.


If the time to solution is absolutely critical, it's okay to run at lower efficiency.
Examples might include calculations that need to run on a regular schedule (data collected during day must be processed overnight) or severe weather forecasting."
rc-learning-fork/content/notes/matlab-parallel-programming/matlab-parallel-programming_5.md,"Extend Big Data Capabilities in MATLAB with Parallel Computing
{{< figure src=/notes/matlab-parallel-programming/img/matlab-parallel-diagram.png >}}

MATLAB provides a single, high-performance environment for working with big data. You can:

Access data that does not fit in memory from standard big data sources and use data from a Hadoop Distributed File System (HDFS) in MATLAB.
Create repositories of large amounts of images, spreadsheets, and custom files using a datastore.
Utilize capabilities tailored for both beginners and power users of big data applications.
Use tall arrays for working with columnar data containing millions or billions of rows.
Partition data too large for a single computer across multiple computers in a cluster using distributed arrays.
Take advantage of hundreds of MATLAB and toolbox functions supported with tall, distributed, and sparse arrays.
Create your own big data algorithms using MATLAB MapReduce or the MATLAB API for Spark.
Program once and scale to many execution environments, including desktop machines, compute clusters, and Spark clusters.
Easily access data, regardless of how it is stored.
Prototype algorithms quickly using small datasets.
Scale up to big datasets running on large clusters, all while using the same intuitive MATLAB syntax you're accustomed to.

The Parallel Computing Toolbox extends the capabilities of tall arrays and MapReduce built into MATLAB, allowing you to run on local workers for improved performance. You can then scale tall arrays and MapReduce up to additional resources with MATLAB Parallel Server, on traditional clusters or Apache Spark™ and Hadoop® clusters.

Overcoming Single Machine Memory Limitations Distributed Arrays
{{< figure src=/notes/matlab-parallel-programming/img/distributed-arrays.png >}}



Distributed Data: MATLAB offers a tool for working with large datasets through distributed arrays. These arrays enable you to manipulate large matrices using the combined memory of a cluster. On your desktop, distributed arrays appear just like normal MATLAB variables, but their data is distributed across MATLAB workers on the cluster. When you perform an operation on a distributed array, the work happens out in the cluster, but the workflow remains unchanged from your normal MATLAB experience. You can prototype distributed arrays on your desktop and then scale up to additional resources with MATLAB Parallel Server.


Common Actions:

Matrix Manipulation
Linear Algebra and Signal Processing

A large number of standard MATLAB functions work with distributed arrays just as they do for normal variables. This means you can program a distributed-array algorithm in the same way as you would program a normal in-memory algorithm. 
MATLAB also provides overloaded functions that work transparently with variables stored across the memory of multiple physical computers. These functions allow you to write one application that can work with both local data and distributed data, without needing to be an expert in message passing. The goal is to make complex tasks easier, allowing you to focus on your algorithms and research, rather than on the underlying details of parallel computing.
Tall Arrays
{{< figure src=""/notes/matlab-parallel-programming/img/tall-data.png"" height=""200"" >}}

Applicable when:
Data is columnar with many rows.
The overall data size is too big to fit into memory.

Operations are mathematical or statistical in nature.


Statistical and Machine Learning Applications:

Hundreds of functions are supported in MATLAB and the Statistics and Machine Learning Toolbox.

Tall arrays are designed to handle large datasets that do not fit into memory by automatically breaking the data into small ""chunks"" that fit into memory. These arrays process the data one ""chunk"" at a time, allowing you to work with data that would otherwise be too large for your system's memory.
The processing code for tall arrays is the same as it would be for ordinary arrays, making it easy to integrate tall arrays into your existing MATLAB workflows. Tall arrays wrap around a datastore and treat the entire dataset as a single, continuous table or array. When you need to perform calculations, the datastore allows you to work through the array one piece at a time, without requiring you to manage the data chunks manually.
For example, when working with CSV files containing tabular data, the resulting tall array is actually a tall table. You can then use standard table functions, such as summary or dot references to access columns, and apply operations like max, min, plus, and minus just as you would with a non-tall table.
{{< figure src=""/notes/matlab-parallel-programming/img/tall-arrays-diagram.png"" height=""300px"" >}}

Demo: Predicting Cost of Taxi Ride in NYC Working with tall arrays in MATLAB

Objective: Create a model to predict the cost of a taxi ride in New York City
Inputs:
Monthly taxi ride log files
The local data set contains > 2 million rows
Approach:
Preprocess and explore data
Work with subset of data for prototyping
Fit linear model
Predict fare and validate model

{{< figure src=/notes/matlab-parallel-programming/img/taxi-example.png >}}"
rc-learning-fork/content/notes/matlab-parallel-programming/_index.md,By Ed Hall
rc-learning-fork/content/notes/matlab-parallel-programming/matlab-parallel-programming_1.md,"Why parallel computing?
Parallel computing offers a powerful solution to tackle increasingly complex problems while saving valuable time. By utilizing available compute cores and GPUs, parallel computing reduces computation time significantly.
Why parallel computing with MATLAB and Simulink?

Accelerated workflows with minimal to no code changes to your original code  
Scalable computations to clusters and cloud infrastructures  
Focus on engineering and research, not the computation  

Parallel computing is essential because the size of the problems we need to solve is increasing, and there’s a growing demand to get products to market faster. This need spans industries and applications, from engineering to research. As hardware capabilities expand, modern computers—including laptops—are increasingly designed with parallel architectures, featuring multiple processors and cores. Additionally, access to GPUs, computer clusters, and cloud computing infrastructure is becoming common.
However, the challenge lies in effectively utilizing this powerful hardware, which is where parallel computing expertise becomes essential.
How do MathWorks Parallel Computing Tools help?

Leverage available hardware without needing to be a parallel computing expert  
Accelerate workflows with minimal changes to existing code  
Scale applications seamlessly from desktop to clusters or cloud for more computational power and memory  

{{< figure src=/notes/matlab-parallel-programming/img/matlab-gpu-multicore-cpu.png >}}
Let's look at some examples of customer across multiple industries who have been using MathWorks parallel computing tools and why they chose parallel computing 
Benefits of parallel computing

| {{< figure src=/notes/matlab-parallel-programming/img/automotive-test-analysis.png width=""300px"" >}} | {{< figure src=/notes/matlab-parallel-programming/img/discrete-model-fleet-performance.png width=""300px"" >}} |
|:-----------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------:|
| Automotive Test Analysis and Visualization                                                                   | Discrete-Event Model of Fleet Performance                                                                    |
| - 3–4 months of development time saved                                                                           | - Simulation time reduced from months to hours                                                                   |
| - Validation time sped up 2X                                                                                     | - Simulation time sped up 20X                                                                                    |
| {{< figure src=/notes/matlab-parallel-programming/img/heart-transplant-studies.png width=""300px"" >}} | {{< figure src=/notes/matlab-parallel-programming/img/derived-market-data.jpg width=""300px"" >}} |
|:-----------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------:|
| Heart Transplant Studies                                                                                     | Calculating Derived Market Data                                                                              |
| - 4 weeks reduced to 5 days                                                                                      | - Implementation time reduced by months                                                                          |
| - Process time sped up 6X                                                                                        | - Updates sped up 8X                                                                                             |
"
rc-learning-fork/content/notes/containers/basics.md,"To run and build containers, you will need Docker Desktop installed on your local machine. Instructions and installation files can be found here: https://docs.docker.com/engine/install/.
Terminology
Image: The layers of libraries, code, and configuration that make up the environment that you need to run your application. 
Container: A running instance of an image. You can have many containers of a single image run simultaneously.
DockerHub: An online registry for Docker images (similar to GitHub)
Commonly Used Docker Commands


docker pull: Fetches an image from a container registry to your local machine


docker images: List all locally available images (kind of like ls)


docker run: Run a container based on a particular image

"
rc-learning-fork/content/notes/containers/overview.md,"{{}}
Shipping containers are used to transport cargo around the world. In computing, containers allow you to transport and share entire filesystems, processes, scripts, and more!"
rc-learning-fork/content/notes/containers/basics-webapps.md,"Just like with the whalesay/cowsay program, we can run web apps in a container. Let's try with this static website example.
1. Pull the nginx image
```

docker pull nginx
```

2. Run the nginx container
We can use -p to specify a custom port to connect to our container. In this case we are using port 8080.
```

docker run -p 8080:80 nginx
```

We can see that the container is running when we go to http://localhost:8080 in the browser.
When you're done, you can use Ctrl + C to stop the container."
rc-learning-fork/content/notes/containers/shinyapp-debugging.md,"Running the Container
Before we do anything else with our new Docker image, we want to make sure it runs correctly locally.
To run the container, we will use the following command.
docker run -p 8080:80 <dockerhub-username>/<image-name>:<tag>
With -p 8080:80, we are mapping port 80 of the container to port 8080 of our local machine. When we want to preview our app, we will go to localhost:8080 in our web browser. You don't have to use 8080, you can use another number if you want.
{{}}

Debugging
What if there's something wrong with the image for our Shiny app? How will we know? If there's an error, we'll see this message: 
{{}}
How do we go about debugging this? We'll need to take a peek into our app. We will use docker run like before, but this time we will run the container interactively.
docker run -it -p 8888:80 <dockerhub-username>/<image-name>:<tag> /bin/bash


-it allows us to run the container interactively


With /bin/bash, we are entering the container in a bash shell.


We use port 8888 instead of 8080, because we need to restart Docker Desktop in order to use 8080 again.



When we try going to localhost:8888, the app isn't running yet. We had a command at the end of our Dockerfile to start up Shiny server and run our app, but we just replaced that with /bin/bash. So within our new container, let's run /usr/bin/shiny-server.sh.
Now we see that error message again when we reload the page in the browser. Hit Ctrl + C to stop running the app.
Navigate to /var/log/shiny-server. Here you will see log files--use the cat command to take a look at them. In this example, we are missing the fresh package from our Docker image. We need to add that to our Dockerfile and rebuild the image."
rc-learning-fork/content/notes/containers/github.md,"Tools like Git and GitHub are great keeping track of changes in our app. They also allow for easy collaboration and sharing of code.
Another great functionality of GitHub is GitHub Actions (GHA). GHA detect updates pushed to your repository and can kick off workflows automatically. For example, if we make updates to our Shiny code and push them to our repository, GHA can automatically rebuild our Docker image and push the new image to GHCR. This means we don't have to manually go through the whole rebuild+push process ourselves!
To take advantage of this, we will need to:


Create a new GitHub repository for our Shiny App


Add the Dockerfile to the GitHub repo.


Set up a workflow for rebuilding the container whenever there is a new commit.

"
rc-learning-fork/content/notes/containers/basics-dockerfile.md,"In order to serve our own webapp in a container, we will need to build an image for it. There are many ""blank"" images like the nginx container we pulled earlier that we can use as a starting point. This is called a base image.
To create our image, we will first write a Dockerfile. A Dockerfile is a text file containing the commands we will use to build the environment we need for our app. These are very similar to commands we would use to install libraries and packages that we need locally.
Once complete, we will use the docker build and docker push commands to build our image and upload it to DockerHub."
rc-learning-fork/content/notes/containers/basics-example.md,"Cowsay is a Linux game that prints ASCII art of a cow and a speech bubble containing a string input by the user. The Whalesay image modifies that game by replacing the cow with a whale.
Let's try pulling the whalesay image from DockerHub and running a container on our machine.
1. Pull the Image
```

docker pull docker/whalesay

Using default tag: latest
latest: Pulling from docker/whalesay
Image docker.io/docker/whalesay:latest uses outdated schema1 manifest format. Please upgrade to a schema2 image for better future compatibility. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/
e190868d63f8: Pull complete 
909cd34c6fd7: Pull complete 
0b9bfabab7c1: Pull complete 
a3ed95caeb02: Pull complete 
00bf65475aba: Pull complete 
c57b6bcc83e3: Pull complete 
8978f6879e2f: Pull complete 
8eed3712d2cf: Pull complete 
Digest: sha256:178598e51a26abbc958b8a2e48825c90bc22e641de3d31e18aaf55f3258ba93b
Status: Downloaded newer image for docker/whalesay:latest
docker.io/docker/whalesay:latest
```
You'll notice that there are several lines with a hash/SHA ID followed by ""Pull complete"". These correspond to the different layers of the image, or the different components that will make up the compute environment when we run the container. We will talk more about layers when we cover building our own images.
2. List all pulled images
By listing all images, we are confirming that we successfully pulled the whalesay image.
```

docker images

REPOSITORY        TAG       IMAGE ID       CREATED       SIZE
docker/whalesay   latest    6b362a9f73eb   8 years ago   247MB
```
3. Run the Container
```

docker run docker/whalesay
```

Just running whalesay looks like it didn't do anything since we didn't provide a command. What happened behind-the-scenes is that Docker spun up an instance of the whalesay image, ran an empty command, and then exited once that was complete.
Let's try running a container again, this time adding the cowsay command at the end.
```

docker run docker/whalesay cowsay ""hi there""


< hi there >

\
 \
  \     
                ##        .            
          ## ## ##       ==            
       ## ## ## ##      ===            
   /""""""""""""""""""""""""""""""""___/ ===

~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~ 
       __ o          /          
        \    \        /           
          __/ 
```"
rc-learning-fork/content/notes/containers/github-newrepo.md,"There are several ways to create a new GitHub repository. Here is one way I like to do it:


Create a new repository with the ""New repository"" button.


Fill out the ""Repository name"".


Select ""Add a README file"".


Click ""Create repository"".


In your local terminal, clone the new repository with the following command: 
git clone https://github.com/<username>/<repo-name>


Copy or move the files you want to the  folder.


Run the following code to add the new files to your GitHub repository:
```
cd /path/to/
git add *
git commit -m ""adding files""
git push
```

"
rc-learning-fork/content/notes/containers/overview-services.md,"Container-based architecture, also known as ""microservices,"" is an approach to designing and running applications as a distributed set of components or layers. Such applications are typically run within containers, made popular in the last few years by Docker.
Containers are portable, efficient, reusable, and contain code and any dependencies in a single package. Containerized services typically run a single process, rather than an entire stack within the same environment. This allows developers to replace, scale, or troubleshoot portions of their entire application at a time.
 
Research Computing runs microservices in a clustered orchestration environment that automates the deployment and management of many containers easy and scalable. This cluster has >1000 cores and ~1TB of memory allocated to running containerized services. It also has over 300TB of cluster storage and can attach to project and value storage."
rc-learning-fork/content/notes/containers/shinyapp-dockerfile.md,"Below is the Dockerfile that we will use to build the container for our Chick Weight app. Each line is a command for building our environment and corresponds to a different layer of our image. We will cover each section below.
1. Choose a base image
```
Install R version 4.1.2
FROM r-base:4.1.2
```
In this section we are specifying that we are starting with the r-base Docker image. The r-base container uses Ubuntu and already has R installed, so we don't need to worry about installing that ourselves. There are many other base containers out there that you can use depending on what kind of app you're developing.
2. Install Ubuntu packages and libraries
The following packages and libraries will allow us to install Shiny server and various R packages that we need for our app. This list will cover most of your bases for most Shiny apps. If you find you need additional libraries, you can just add them to this list.
How do you know if you're missing a library? You'll get an error message, and we will cover how to debug in a later section.
```
Install Ubuntu packages
RUN apt-get update && apt-get install -y \
    sudo \
    gdebi-core \
    pandoc \
    libcurl4-gnutls-dev \
    libcairo2-dev/unstable \
    libxt-dev \
    libssl-dev \
    libxml2-dev \
    libnlopt-dev \
    libudunits2-dev \
    libgeos-dev \
    libfreetype6-dev \
    libpng-dev \
    libtiff5-dev \
    libjpeg-dev \
    libgdal-dev \
    git           
```
3. Install Shiny server
This just installs Shiny server in your image. If you're not developing a Shiny app, no need to include it. If you are developing a Shiny app, no need to change it!
```
Install Shiny server
RUN wget --no-verbose https://s3.amazonaws.com/rstudio-shiny-server-os-build/ubuntu-12.04/x86_64/VERSION -O ""version.txt"" && \
    VERSION=$(cat version.txt) && \
    wget ""https://s3.amazonaws.com/rstudio-shiny-server-os-build/ubuntu-12.04/x86_64/shiny-server-$VERSION-amd64.deb"" -O ss-latest.deb && \
    gdebi -n ss-latest.deb && \
    rm -f version.txt ss-latest.deb
```
4. Install R Packages
Here we are installing all the packages that we need for our Shiny app. Again, if you're not developing a Shiny app, you can skip this part. 
```
Install R packages that are required
CRAN packages
RUN R -e ""install.packages(c('shiny','shinydashboard','dplyr','ggplot2','fresh'))""
```
5. Copy configuration files to the Docker image
These are just some files that make our Shiny app run. These will be in the directory that your app is in. These will be the same for all Shiny apps.
```
Copy configuration files into the Docker image
COPY shiny-server.conf /etc/shiny-server/shiny-server.conf
COPY shiny-server.sh /usr/bin/shiny-server.sh
RUN rm -rf /srv/shiny-server/*
```
6. Copy your code to your app
Ideally your code will be maintained within a GitHub repository (we will cover how to do this in a later section). Here we are cloning the GitHub repo and copying the contents to the shiny-server folder.
```
Get the app code
RUN git clone https://github.com/uvarc/chickweight.git
COPY chickweight/* /srv/shiny-server/
RUN rm -rf chickweight
```
7. Some R and Shiny Stuff
This is just some stuff for setting R paths, etc.
```
Make the ShinyApp available at port 80
ENV R_HOME=/usr/lib/R
ENV PATH=/usr/lib/R:/usr/lib/R/bin:$PATH
EXPOSE 80
WORKDIR /srv/shiny-server
RUN chown shiny.shiny /usr/bin/shiny-server.sh && chmod 755 /usr/bin/shiny-server.sh
```
8. Run the Shiny app!
The CMD just tells the container what to run once it starts. This line starts up the Shiny server and app.
```
Run the server setup script
CMD [""/usr/bin/shiny-server.sh""]
```
Once we're done writing the Dockerfile, we save it as ""Dockerfile"" (no file extension).
```"
rc-learning-fork/content/notes/containers/github-commands.md,"As we make changes to our Shiny app or Dockerfile, we will need to push the updates to our GitHub repository. Typically this is done with a git add + git commit + git push.


git add <file(s)>: git add is similar to packing a box. We are adding the files that we want to send up to our repository.


git commit -m <message>: With git commit we are creating a ""packing slip"" for our box of updated files. We can add a message that tells us and others why we're pushing these updates and what they do. Your message can be vague or informative--the choice is up to you.


git push: This command is doing the actual ""shipping"" of our files.



If we are working with collaborators, another useful command is git pull. This allows us to update our local repository with any updates our friends have pushed."
rc-learning-fork/content/notes/containers/shinyapp-dockerbuild.md,"Now that we've written the Dockerfile, it's time to build our image! To do that, we will use the docker build command.
In the same directory that your Dockerfile is in, you can run the following:
docker build -t <dockerhub-username>/<image-name>:<tag> .


-t lets us specify the tag name for this image.


dockerhub-username is pretty self-explanatory.


image-name is the name for our image. It will also be the name of the image repository in DockerHub.


tag can be a version number or other identifying label to help you differentiate this image from previous iterations. If you leave this blank, the tag name will default to latest.


I will be calling this image cagancayco/chickweight:latest. Simply replace cagancayco with your own username."
rc-learning-fork/content/notes/containers/overview-vms.md,"You may be familiar with the virtual machines (VMs), which accomplish the same goal as containers: to create a reproducible and shareable compute environment. The big difference between VMs and containers is that VMs provide their own guest OS, whereas containers don't require an OS and run on the Docker Engine and share the host's OS kernel.
The size of Docker images is usually on the order of tens of MBs, while VMs can be several tens of GBs large.
"
rc-learning-fork/content/notes/containers/github-actions.md,"GitHub Actions allow us to incorporate Continuous Integration/Continuous Deployment in our repository. We can automatically rebuild and redeploy our app whenever any changes are committed to the repo.
To add a workflow to your repo:


Create a .github/workflows directory.


Add the following .YAML file to the new directory, replacing the IMAGE_NAME and SVC_NAME with your image's name and the name that you want for your app on the Kubernetes cluster, respectively.


```
name: Container Build CICD
on:
  push:
    branches:
    - 'main'
env:
  # The preferred container registry
  REGISTRY: ghcr.io
  # The base org/image-name for the container
  IMAGE_NAME: uvarc/chickweight
  # A service name if mapping this to a k8s deployment
  SVC_NAME: chickWeight
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set ENV
        run: echo ""IMAGE_TAG=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV
      -
        name: Set up QEMU
        uses: docker/setup-qemu-action@v1
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      -
        # GHCR require a GitHub username and a Personal Access Token with the right permissions.
        # These can be stored as repository secrets in the repo settings.
        name: Login to GHCR
        uses: docker/login-action@v1
        with:
          registry: ghcr.io
          username: ${{ secrets.GHCR_USERNAME }}
          password: ${{ secrets.GHCR_PAT }}
      -
        name: Build and push
        id: docker_build
        uses: docker/build-push-action@v2
        with:
          push: ${{ github.event_name != 'pull_request' }}
          tags: ghcr.io/${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}
          labels: ${{ steps.meta.outputs.labels }}
      -
        name: Image digest
        run: echo ${{ steps.docker_build.outputs.digest }}
  # Now update another repo so that ArgoCD can deploy the new version.
  # Note that the dispatch call simply curls a POST payload to another repository with JSON that you define.
  - name: Remote Dispatch
    run: |
      curl -X POST https://api.github.com/repos/uvarc/uvarc-services/dispatches \
        -H 'Accept: application/vnd.github.everest-preview+json' \
        -H ""Authorization: token ${{ secrets.GHCR_PAT }}"" \
        --data '{""event_type"": ""${{ env.IMAGE_NAME }} update to ${{ env.IMAGE_TAG }}"", ""client_payload"": { ""service"": ""${{ env.SVC_NAME }}"", ""version"": ""${{ env.IMAGE_TAG }}"" }}'

```"
rc-learning-fork/content/notes/containers/overview-purpose.md,"Have you ever tried using new code or software from an exciting paper you just read, only to end up spending hours figuring out which versions of the dependencies work on your own machine? Containers eliminate that issue altogether!
A container is a single unit of software that contains all the packages and code you need to run an application. Sometimes that application is as small as a single function (like printing 'Hello World!'), and sometimes that application is an entire web app. A container will always run the same, regardless of the host system it runs on--making it the perfect solution for sharing reproducible code.
There are several container technologies out there, but the big ones are Docker and Singularity. Docker is what you will encounter most often in the wild. Singularity (now called Apptainer) is used on HPC systems where most users don't have root access.

"
rc-learning-fork/content/notes/containers/github-updates.md,"Perhaps there are some new updates to our code
git branch"
rc-learning-fork/content/notes/containers/shinyapp-dockerpush.md,"Once our container runs successfully, we can push it to a container registry so that others can pull the container. ""Others"" includes our Kubernetes cluster at UVA.
We have a few options for container registries. The two main options are DockerHub and GitHub Container Registry (ghcr.io).
For this workshop, we're going to use GHCR. The main benefit of using GHCR over DockerHub is that it is less restrictive with how many times your image can be pulled in an hour.
Re-tagging our image
To use GHCR, we need to prepend our image name with ghcr.io/<github-username>. That means we will need to re-tag our image. We can do that with the docker tag command.
docker tag cagancayco/chickweight:latest ghcr.io/uvarc/chickweight
Allowing Docker to push to GHCR
If this is your first time using GHCR, it is likely that you need to authorize your local Docker installation to push to GitHub.


Go to GitHub -> Settings -> Developer Settings -> Personal Access Tokens, or click here.


Generate new token with read/write permissions for packages.


Copy the token to the clipboard.


In your terminal, run
docker login ghcr.io -u <github-username> -p <copied-token>


Now we can use docker push to push our container to GitHub!
docker push ghcr.io/uvarc/chickweight
Set the package to public
By default, GHCR packages (or images) are set to private. We need them to be public in order for the Kubernetes cluster to see it. To change the visibility, do the following:


Go to Packages -> chickweight -> Package Settings


Click ""change visibility"" and follow the on-screen instructions.

"
rc-learning-fork/content/notes/containers/_index.md,"You've developed an app and written the paper. Now it's time to deploy the app so the world (and the reviewers) can see how awesome it is. This is Part 1 of a two-part workshop that will cover how to deploy web apps for publication. In Part 1 we will go over how to containerize our apps with Docker and maintain them with GitHub.
 "
rc-learning-fork/content/notes/containers/shinyapp.md,"In this section of the workshop, we will take a completed ShinyApp and create a Docker image for it by writing a Dockerfile. We will then run the container to make sure that the ShinyApp is working as expected. Once we know the container is working, we will create a GitHub repository for our Shiny code and Dockerfile.
To complete this section on your own, you will need:


Docker Desktop installed on your machine
    The installers and installation instructions can be found here: https://docs.docker.com/engine/install/


A GitHub account
    If you don't have one already you can make one here: https://github.com/


A copy of the Shiny code
    You can clone a copy of the code to your local machine using git clone https://github.com/uvarc/chickweight

"
rc-learning-fork/content/notes/hpc-from-terminal/section4.md,"Wildcards
Wildcards are special characters that can stand for strings.  Wildcards enable you to work with groups of files without needing to type multiple files with similar names. 
The asterisk * can replace zero to unlimited characters except for a leading period.
The question mark ? replaces exactly one character.  
Examples
bash
$ls *.py
$cd array_test
$ls input?.dat
$ls input??.dat
$ls input*.dat
$rm list?.sh
{{< warning >}}
BE CAREFUL when using wildcards with rm! Gone is gone! On some systems there may be backups, or there may not be, and on your personal system you would need to set up backups and learn to retrieve files. It is advisable to first run an ls with the pattern you plan to use with rm.
{{< /warning >}}
Standard Streams
Each executable has associated with it three input/output streams:  standard input ,  standard error , and  standard output.  Normally these streams come from or go to your console (i.e. your terminal).
Most Unix commands read from standard input and/or write to standard output.
These I/O streams are often represented as  stdin,  stderr, and  stdout.
The Unix commands we have studied so far all write to standard output.
bash
$ls -l
produces output to the terminal.
Standard Stream Redirection
The standard streams are normally associated with the console. The command will print to the terminal, or will read input typed into the terminal.  Stdin and stout can also be redirected to write to or read from a file.
Redirect standard input with <
bash
$./mycode < params.txt
Redirect standard output with >
bash
$ls –l > filelist.txt
If the file exists, > will overwrite it.  Append with >>.
bash
$cat file1 >> bigfile.csv
Redirection of standard error depends on the shell and is needed for only a few commands.
For bash
bash
$make >& make.out
redirects both stdout and stderr from the make command to make.out.
Pipes
One of the most powerful properties of Unix is that you can  pipe  the standard output of one command into the standard input of another.
The pipe symbol | is above the backslash on most US keyboards.  Pipes can be chained indefinitely, though most uses need just one.
no-highlight
cmd1 | cmd2 | cmd3
Example
Commands such as ls have lengthy manpages that will tend to scroll off our terminal window.
bash 
$man ls | more
Now we can page through the listing.
Finding and Searching Files
Searching with grep
The grep command  is commonly used in UNIX to filter a file or input, line by line, against a pattern.  Patterns can be complex and use regular expressions, but most of the time wildcards are sufficient.
no-highlight
grep [OPTIONS] PATTERN FILENAME
Example
The -i option stands for ""ignore case.""
bash
$grep -i Unix intro_basic-unix.txt
Grep is frequently used with wildcards and pipes.
bash
$grep -i write *f90
$grep weight: *md | grep 100
Example 
How many sequences are in a FASTA-formatted file? Each sequence record in a FASTA file has one line of description that always starts with >, followed by multiple lines of the sequence itself. Each sequence record ends when the next line starts with >.
bash
$grep -c '>' sequences.fasta
The -c option returns the number of lines containing the pattern. Please be sure to include the quotes around the > or the shell will interpret it as a redirection.  
A Handy Trick
To find all occurrences of a pattern in all files in a directory, use grep -r.
bash
$grep -r ""print"" python_programs
Be careful with the pattern for a recursive search, or the output can be excessive.
Finding Files by Name
The find command can locate a file if you cannot remember its directory.  It can take wildcards, in which case it is best to use quotes around the name pattern.
bash
$find . -name 2col.txt
./shakespeare/2col.txt
$find . -name ""people*""
./data/people.txt
The period . tells find to start at the current working directory.
Find has many options to locate files by name, type, date, and others.  See here for examples.
Running Executables
Executables are often also called binaries. The terms are synonymous in most cases.
The shell has a predefined search path. It will look in a sequence of directories for an executable with the specified name, and will invoke the first one it encounters.  If the executable is in this search path, you can simply type its name at the prompt.
bash
$gedit hello_world.sh
here gedit is the name of the executable.  Its actual location is /usr/bin/gedit, but /usr/bin is in the default search path.
If the location of the binary is not in your search path, you must type the path to the executable (it can be absolute or relative)
bash
$./hello_world.sh
For security reasons the current directory is not in your default search path. Hence you must explicitly provide the ./ path to execute a binary in the current directory.  If you wish to add it, type (for bash)
bash
$export PATH=$PATH:.
PATH is called an environment variable. It holds the list of paths to be searched by the shell.  In this example it is essential to add the first $PATH or you will lose the default path set by the system.
If you are unsure of the path to the binary you wish to run, the which command will tell you the path to the binary the shell will start.
bash
$which g++
/apps/software/standard/core/gcc/11.4.0/bin/g++
Process Control
A running executable is a process to the Unix operating system. When it is run at a command line, a process can be running in the  foreground, which suppresses a prompt, or in the  background, which returns a prompt to the shell.  
To start in the background, add an ampersand & at the end of the command.
bash
$./myexec -o myopt myfile&
Managing Processes
The jobs command lists your running jobs (processes) with their job index.
The key combination control-z (ctrl-z or ^z) suspends the foreground job. To resume the job in the background, type bg.
This can be combined with output from jobs
bash
$bg %1  # place the job number 1 into the background
$fg %4  # place the job number 4 back to the foreground
For more general information about processes, use ps (process status) The -u option limits it to processes owned by user mst3k.
bash
$ps -u mst3k
   PID TTY          TIME CMD
498571 ?        00:00:00 systemd
498575 ?        00:00:00 (sd-pam)
498581 ?        00:00:00 pulseaudio
498582 ?        00:00:00 sshd
498593 pts/3    00:00:00 bash
498665 ?        00:00:00 dbus-daemon
498670 ?        00:00:00 dbus-daemon
498672 ?        00:00:00 dbus-kill-proce
498677 ?        00:00:00 gio
498685 ?        00:00:00 gvfsd
498691 ?        00:00:00 gvfsd-fuse
517189 pts/3    00:00:00 ps
The pid is the process id.  
Killing Processes
You have accidentally started a production job on a loginnode node.  What to do?
You can kill your foreground process with Crtl c. 
```bash
oops, I was supposed to run this through Slurm
$./myexe
^c
```
If you need to kill a background process, you can use jobs to locate and foreground it.  You may also have processes that don't appear with jobs.  Use ps to find the PID, then
bash
$kill -9 <pid>
Do not type the angle brackets, just the number. Many processes will ignore the kill command without the -9 option so we routinely include it.
To kill by executable name
bash
$killall -9 <executable name>
The kill command with -9 immediately kills the process without allowing the process to clean up or save data. The killall command can be used to kill all the processes that match a specific name or pattern.
If you find yourself in a jam and do not know what is wrong and you must start over, 
bash
$kill -9 -1
kills all your processes, including your login.
Dotfiles
“Dotfiles” are files that describe resources to programs that look for them.  They begin with a period or “dot” (hence the name).  In general, they are used for configuration of various software packages.  
Dotfiles are hidden from ls, but ls -a shows them.  Sometimes ls is aliased to ls –a.
Bash has configuration dotfiles: .bash_profile and .bashrc.
  * if no .bash_profile is present it will read .profile
  * .bash_profile is sourced only for login shell
  * the default .bash_profile on our HPC system incorporates the .bashrc; otherwise on a general Linux system, .bashrc is not run for a login shell.
Dot ""files"" may also be, and often are, directories.
bash
$ls -a
.   .bash_logout   .bashrc  .lesshst  shakespeare  .Xauthority
..  .bash_profile  data     .mozilla  .ssh"
rc-learning-fork/content/notes/hpc-from-terminal/section5.md,"Most high-performance computing clusters have commands that are specific to that environment, and are not part of Unix per se.
Home Directory
The default home directory provides 50GB of storage capacity, e.g., /home/mst3k.  Each user also has access to 10 TB of  temporary  storage in the ""scratch"" folder, e.g. /scratch/mst3k
The /home and /scratch directories are for personal use and not shareable with other users.
{{< warning >}}
/scratch is NOT permanent storage and files that have not been accessed for more than 90 days will be marked for deletion.
{{< /warning >}}
Research groups can lease permanent storage.
Checking Your Storage
To see how much disk space, you have used in your home and scratch directories, open a terminal window and type  hdquota  at the command-line prompt:
bash
$hdquota
Type    Location         Name                  Size Used Avail Use%
=======================================================================
home       /home        mst3k                   51G   12G   39G  24%
Scratch    /project     slurmtests             2.0P  1.9P  144T  93%
Project    /project     arcs                    16T   12T  3.8T  75%   Project    /project     rivanna_software       1.1T  4.2M  1.0T   1%   Project    /project     ds5559                  51G  3.7G   47G   8%   Value      /nv          vol174                 5.5T  1.2T  4.4T  21%
Note: only home, and other storage just for some
Checking Your Allocation
To see how many SUs you have available for running jobs, type at the command-line prompt allocations.
bash
$allocations
 Allocations available to Misty Tea (mst3k):
  ds5559: less than 25,000 service-units remaining
  ga_bioinfo-test: less than 100,000 service-units remaining
For more information about a specific allocation, please run
bash
$allocations -a <allocation name>
The Modules Environment
Environment modules are not strictly a part of Unix, but are widely used by many HPC sites, including ours.  Modules enable the user to set complex paths, environment variables, and so forth, simply by loading a module.
The commands are set up automatically when you log in.  Loaded modules only affect the shell in which the command is run.
Modules required for a job must be loaded in the batch job script.
Module Commands
{{< table >}}
|  Command   |  Result  |
|  -------   |  ------  |
|  module load \<name> |  load the default module for the  package |
|  module load \<name>/\<version> | load the specific version of a package |
|  module spider \<name> | view the available versions of a package | 
|  module spider \<name>/\<version> | view instructions for loading a version |
|  module purge | clear all modules |
|  module swap \<name>/\<version1> \<name>/\<version2> | switch from version 1 to version 2 |
{{< /table >}}
Examples
bash
$module load gcc
$module load matlab/R2023a
$module spider R/4.3.1
$module load gcc/11.4.0 openmpi/4.1.4 R/4.3.1
$module purge
Running Jobs
In an HPC environment, the tasks that users want to perform are generically called jobs.  These must not be confused with the ""jobs"" of the Unix jobs command, although the concepts are related. An HPC ""job"" is a combination of resource requests, any setup required such as loading modules, and the actual commands required to run the desired software.
HPC jobs are run on compute nodes, not on the interactive loginnodes.
In the Introduction we discussed using the OOD job composer to submit and monitor jobs. When working at the command line, we must use more detailed features of Slurm.  Please go through our tutorial to learn how to write and submit Slurm scripts from the command line.
Need Help?
Research Computing is ready to help you learn to use our systems efficiently.  You can submit a ticket.  For in-person help, please attend one of our weekly sessions of office hours."
rc-learning-fork/content/notes/hpc-from-terminal/section1.md,"UNIX
UNIX is a text-oriented operating system (OS) originally developed at Bell Labs during the 1960s. Two versions of this OS are dominant today, Linux and macOS. 
Strictly speaking, ""Linux"" refers just to the kernel, which is the part of an operating system that manages the hardware interfaces.  On top of the kernel sits a number of utilities that enable users and applications to interact with the kernel. 
Linux is the operating system most widely used at HPC facilities, internet servers, and the majority of financial trading system worldwide. A version of Linux powers Android systems.  
macOS is based on a slightly different version of Unix.
Shell
In all version of Unix, the shell is a program that interprets commands and acts as an interface between the user and the kernel.
Multiple shells are available. In Linux systems, the default is the bash shell.  MacOS formerly defaulted to bash as well, but has recently switched to zsh.
The shell displays a prompt, which indicates that it is ready to accept commands.  In this tutorial, we will utilize the dollar sign $ as the prompt; yours may be different, and later in this tutorial we will learn how to customize it to your preferences.
To determine your current shell, at the prompt type
bash
$echo $SHELL
It is important to keep in mind that Unix in general and the shell in particular is  case-sensitive.  The commands LS and ls would not be the same.
Logging In
Logging into a remote UNIX based system requires a program generally called a client. The options for the client depends on your OS.
SSH
Command line access through a terminal on your computer is based on the ssh or Secure Shell protocol.  It is
  * Encrypted
  * Available on most UNIX systems
Your ssh client communicates with the ssh server program running on the remote system.  Once established, the connection is secure even if the network is insecure.   
Example
If your computer is a macOS or Linux system, you log in with
bash
ssh -Y mst3k@login.hpc.virginia.edu
Throughout this tutorial we will use mst3k as our example user ID. You should substitute your own.  The option -Y allows access to graphical applications and requires that an X11 server application must be installed on your computer.  This should be the default for Linux, but macOS users must install XQuartz before this command-line option will be useful.
Graphical Applications
The command-line secure shell is not the only option for accessing the HPC system. Windows users in particular may wish to use other methods, since although ssh is available for it, Windows is not particularly friendly to command lines.
Open OnDemand (OOD)
Open OnDemand is a Web-based interface to the system. It provides a graphical file-management interface and access to several popular applications running on the compute nodes. A simple terminal that opens on a loginnode is also provided.
See the introduction in our basic tutorial.
FastX
FastX is a Web-based graphical interface to a loginnode. It is also covered in the introduction.
MobaXterm (Windows)
MobaXterm combines an ssh client, a sftp client for file transfers, and an X11 server into a single bundle. More details are available at our Website or in the introduction.
Running Shell Commands
The syntax of Unix commands is not completely standardized but in general follow the pattern of a two or three-letter abbreviation followed by command-line options, which are single-letter options preceded by one hyphen or multiple-letter options with two hyphens. Many commands also take arguments.
bash
$cmd -o  --option  argument
Example
Invoke the utility rm to delete a file.
bash
$rm myfile
In this example, the shell issues a request to the kernel to delete myfile.  The kernel then communicates with the software that manages file storage to execute the operation.
When it is complete the shell then returns the UNIX prompt to the user, indicating that it is waiting for further commands.
Running Our First Command
Let’s run our first command. Into a terminal type
bash
$pwd
/home/mst3k
This command stands for print working directory.  It prints the name of the folder in which the shell is currently working.
Navigating the Bash Shell
Modern shells provide useful ""hotkey"" commands that can save considerable typing. The ""control-"" notation means that the Ctrl (control) key and the following key should be depressed at the same time.
{{< table >}}
| Function |  Key |
|-------|-----|
|Tab completion: expand typed string to as far as it has a unique name. | tab |
|Search for earlier command | control-r  |
|Move to the beginning of the line | control-a |
|Move to the end of the line | control-e |
|Clear the screen | clear or control-l |
{{< /table >}}
Bash History
When using bash you may use its built-in history mechanism to save yourself some keystrokes.
{{< table >}}
| Function |  Key |
|-------|-----|
|scroll through the previous commands you have typed | up arrow|
|if already scrolled back, scroll to more recent commands | down arrow |
|edit text on a line | right or left arrow |
{{< /table >}}
Logging Out
To log out of a shell, type exit
bash
$exit
This logs you out of your current terminal window. If that is a login window, for example your ssh shell, it will log you out of the system entirely."
rc-learning-fork/content/notes/hpc-from-terminal/section2.md,"Files and Directories
Files store some sort of information: data, source code for scripts or programs, images, video, and so forth.
There are two basic types of files:
  * Text (documents, code)
  * Binary (images, executables)
The Unix shells ignore file extensions, but software might require certain extensions.  This includes compilers (.cxx, .f90 and so forth), interpreters (.py, .R, etc.), image and video tools (.png, .jpg, .mp4). Since the format of the file extension depends on the expectations of software or on user preference, there is no rule limiting the number of characters, but most consist of one to three characters.
Directories are collections of files and other directories. Often they are called folders, but directory is generally the preferred (and historical) term in Unix.
Directories that are stored within another directory are subdirectories of the parent.
Both files and directories have “metadata” associated with them such as name, timestamps, and permissions.
The names of Unix files and directories should contain no spaces. 
```bash
$ls 
'10. directories.md'
'15. streams.md'
```
The quotes indicate that a space is part of the file name.  While most modern Unix tools can handle spaces, the shell does not always do so, and special precautions must be taken to avoid surprises. For that reason, underscores or hyphens are preferred instead of spaces.
Paths
In most operating systems, all files and directories are located with a path.  The “path” is the ""full name"" of every file & directory.
In a Unix-based operating system, the files are organized into a tree structure (the method to store and organize the files is called filesystem, e.g. ext3,ext4, NTFS, etc).  The tree is ""upside down"" because the root directory is at the top, and directories branch off from there.  The root directory is indicated with a forward slash /.
{{< diagram >}}
graph TD
    C[""/""] --> etc[""etc""]
    C[""/""] --> opt[""opt""]
    C[""/""] --> lib[""lib""]
    C --> usr[""usr""]
    C --> home[""home""]
    C --> var[""var""]
home --> mst3k[""mst3k""]
home --> im4u[""im4u""]

mst3k-->myfile[""myfile""]
mst3k-->mydir[""mydir""]

mydir-->file1[""file1""]
mydir-->file2[""file2""]

usr--> bin[""bin""]
usr--> share[""share""]
usr--> local[""local""]

bin--> ls[""ls""]
bin--> pwd[""pwd""]
bin--> cd[""cd""]

{{< /diagram >}}
The home directory is the usual name on Linux and similar Unix-based operating systems for the folder that holds user directories and files.  On macOS it is called User.  On both, the separator between branches of the tree is the forward slash.
{{< hl >}}
/home/mst3k/myfile
{{< /hl >}}

{{< hl >}}
/Users/Misty Tea/Documents/Homework.pages
{{< /hl >}}
Windows files and folders also have paths.  In Windows, drive letters or volumes are the top-level folders, and usually there is more than one.  User files are in Users, similar to macOS.
{{< hl >}}
C:\Users\Misty Tea\Documents\Homework.docx
{{< /hl >}}
Absolute and Relative Paths
Paths may be absolute or relative.
An absolute path is path to a file or folder starting at the root. On Unix it will begin with /, to designate the root.
An absolute path is guaranteed to get you to the location you want.
A relative path is the path to a file starting at the current location.  We use the notation . (a single period) for the current working directory, and .. (two periods) for the parent of the current working directory.
Examples
Absolute paths:
no-highlight
/home/mst3k/file.txt 
/home/mst3k/files/file.txt
/home/mst3k/projects/project1
/home/mst3k/projects/project1/output.txt
Note that /home/mst3k/file.txt and /home/mst3k/files/file.txt are different files unless you explicitly link them.
Relative paths.  Suppose we are in the /home/mst3k/files folder.
no-highlight
./file.txt      
../files/file.txt
The relative path to file.txt in files from the project1 folder is
no-highlight
../../files/file.txt
Tilde Notation
In Unix the tilde ~ stands for the user's home directory, e.g. /home/mst3k.
no-hightlight
ls ~
ls ~/files
File Commands
ls
ls lists files in a directory.
bash
$ls
With no argument, listing the entire contents of the current working directory is assumed.
Like most Unix commands, it has many options.  They may be combined.
{{< table >}}
|  Option  |  Purpose |
|-------|-----|
|-l  | long listing, includes file date, size, and permissions |
|-a  | displays all files including hidden (dotfiles) |
|-h  | show file sizes in human-readable terms |
|-C  | lay out listing in columns |
|-1  | (digit one) list one file per line, no header |
|-t  | show the newest files first |
|-r  | reverse the order |
|-F  |append a symbol to indicate the type of file (ordinary, executable, directory) |
{{< /table >}}
bash
$ls -ltr ./projects
cp
cp to copy a file.
bash
$cp file1.txt file2.txt
$cp mymod.py ../projects/python_code
Commonly-used options:
{{< table >}}
|  Option  |  Purpose |
|-------|-----|
| -i    |  ask for confirmation before overwriting an existing file |
| -r    |  copy recursively all subdirectories|
| -n    |  ""noclobber""; do not overwrite an existing file|
| -f    |  force an overwrite of an existing file|
{{< /table >}}
bash
$cp -r /share/resources/tutorials/rivanna-cl ~
mv
mv to rename or move a file.
bash
$mv file1.txt file2.txt
$mv mymod.py ../projects/python_code
Options for mv are similar to cp.
rm
rm to remove a file or directory
bash
$rm file1.txt 
$rm file1.txt data.csv
Once the file is removed, it is gone and can no longer be accessed.
Options for rm are similar to cp.
{{< warning >}}
By default, the rm command does not ask for confirmation before deleting a file! Use the -i option if you are unsure.
{{< /warning >}}
Directory Commands
cd
cd change directory
bash
$cd
With no argument, cd moves to the user's home directory.  If you are hopelessly lost, you can always type cd and start over.
bash
$cd ../projects/project1
mkdir
mkdir to create a directory.  Works relative to current working directory.
bash
$mkdir new_project
or
bash
$mkdir new_project
$mkdir ~/projects/new_project/src
rmdir
rmdir to remove an empty directory.  Does not affect directories with content.
bash
$rmdir ~/projects/new_project/src
rm -rf
rm -rf to force the removal of a directory and all its contents, including subdirectories.
bash
$rm -rf ~/projects/project1/src
cp -r
Directories are copied with the cp -r command previously discussed.
Exercise
Start a terminal on a loginnode by whatever means you prefer. Type (the $ is the prompt and is not typed):
bash
$echo $SHELL
$pwd
$mkdir test_dir
$cd test_dir
$ls ..
$mkdir sub_dir
$ls
Q1: What is the full path of your home directory?
Solution

```bash
$cd ~
$pwd
```

Q2: Delete the sub_dir folder.
Solution

```bash
$rmdir ~/test_dir/sub_dir
```

Q3: Copy the directory
/share/resources/tutorials/rivanna-cl/shakespeare into your home directory.
Solution

```bash
$cp -r /share/resources/tutorials/rivanna-cl/shakespeare ~
```
"
rc-learning-fork/content/notes/hpc-from-terminal/section3.md,"File Permissions
Each file and directory has a set of permissions associated with it. Traditionally, Unix has assigned permissions to the owner (the user), the group (a set of users), others (users not the owner nor a member of the group that may have permissions on the file), and all (all users on the system). These are generally denoted by ugoa.
Within each category, a user can have permission to read, write, or execute the file.  These are denoted by rwx.
The command
bash
$ls -l
shows the permissions associated with each file.  Since in Unix a directory is a special type of file, the permissions are the same.
```bash
$ls -l
total 8
drwxr-xr-x. 2 mst3k mst3k 4096 Dec 18 10:39 data
drwxr-xr-x. 2 mst3k mst3k 4096 Dec 18 10:22 shakespeare
``
The output ofls -l` is
{{< table >}}
| Permissions | Number of ""Hard Links""  |  Owner |  Group | Size | Date and Time Last Modified | Name|
| ---- | ---- | -------- | ------ | ---- | ----------- | ---- |
| drwxr-xr-x | 2 | mst3k | mst3k | 4096 | Dec 18 10:39 | data |
{{< /table >}}
Permissions not granted to a user are indicated with a hyphen -.
The permissions layout in the first columns should be read as
{{< table >}}
| Type |  Owner   |  Group Owner |  Others |
| ---- | -------- | ------ | ---- |
|  d   |  rwx     |  r-x   |  r-x |
{{< /table >}}
In the above example, the d indicates the listing is a directory. A directory must have ""execute"" x permission in order for a user to enter it.
Listing the files in a directory will result in output such as
bash
$ls -l shakespeare
-rwxr-xr-x. 1 mst3k mst3k     91 Dec 18 10:22 2col.txt
-rwxr-xr-x. 1 mst3k mst3k 173940 Dec 18 10:22 Hamlet.txt
-rwxr-xr-x. 1 mst3k mst3k 162563 Dec 18 10:22 HamletWords.txt
-rwxr-xr-x. 1 mst3k mst3k     34 Dec 18 10:22 k2sort.txt
-rwxr-xr-x. 1 mst3k mst3k 180857 Dec 18 10:22 Lear.txt
-rwxr-xr-x. 1 mst3k mst3k 154520 Dec 18 10:22 Othello.txt
-rwxr-xr-x. 1 mst3k mst3k  25628 Dec 18 10:22 uniques
-rwxr-xr-x. 1 mst3k mst3k  25628 Dec 18 10:22 uniques.txt
-rwxr-xr-x. 1 mst3k mst3k     72 Dec 18 10:22 wcdemo.txt
-rwxr-xr-x. 1 mst3k mst3k     26 Dec 18 10:22 words_and_num.txt
The hyphen as Type indicates an ordinary file.
Changing Permissions
The owner of a file or directory may change the permissions with the chmod command.  Two formats are supported: a three- or four-digit code (in octal, i.e. base 8) indicating permissions, or the add/remove symbolic format.  The digit format is advanced so we will not discuss it; a reference is available here.  The symbolic format is more intuitive and mnemonic.  
Examples 
Add execute permission to a file for its owner. This is frequently used with shell scripts.
bash
$chmod u+x ascript.sh
Add execute permissions for the owner and group members
bash
$chmod ug+x ascript.sh
Allow others to read and write a file
bash
$chmod o+wr myfile.txt
Please note that on multiuser, central systems such as an HPC cluster, the administrators may not allow individual users to change the permissions on certain file sets such as home and scratch directories, for reasons of data security and privacy.
Creating and Editing Files
To create files we use a text editor.  Do not use a word processor such as LibreOffice.
Graphical Options
Graphical editors must be used from within a graphical environment.  On Linux the standard graphical windowing system is called X11.  Newer Linux versions provide some form of ""desktop"" environment similar to Windows or macOS on top of X11.  On our system we provide the MATE Desktop Environment.  It can be accessed from FastX on a loginnode.  It can also be started on a compute node from the Open OnDemand Desktop Interactive Application.
1. gedit/pluma
Modern Linux systems provide at least one graphical text editor. One option is gedit.  On the MATE Desktop, the equivalent is pluma, but the gedit command starts pluma.
Gedit/pluma is very similar to Notepad on Windows.
2. VS Code
This is accessed by a module.
bash
$module load code-server
Then open a browser (Firefox is the default on MATE) and go to the URL http://127.0.0.1:8080. First set up would need a password that can be extracted from the file ~/.config/code-server/config.yaml.
Text-Based File Editors
Regular Linux users are advised to learn at least one text-based editor, in case a graphical interface is not available or is too slow.
1. vi (vim)
The oldest and most widely available text-based editor on Unix is vi for ""visual."" Modern versions are nearly all based on vim (""vi improved).  On our system we generally alias the vi command to vim.
Vim/vi is used entirely through keyboard commands which must be memorized.  The mouse is not utilized.  Only a few commands are needed to be able to do basic editing and they are not difficult to learn.  A beginner tutorial is here. One stumbling block for new users is that vim has command mode and insert mode and it must be toggled between the two.
Basics:
  * To enter the insert mode press  i
  * To enter the command mode press  ESC
  * To save the file enter  :w
  * To save under a new name  :w filename
  * To exit :q
MATE also provides a graphical interface called gvim. It can be accessed through the Applications->Accessories menu as Vi IMproved.  GVim combines the keyboard-oriented commands of vim with some mouse-based capabilities.
2. nano
Nano is simple text editor that is fairly self-explanatory and simple to learn. It is always in insert mode. A tutorial similar to that for vim above is here.
Basics:
  * Immediately start typing
  * To exit:  control+X
  * Type the filename and press  Enter
3. Emacs
The Emacs editor is another long-time editor on Unix. Similar to gvim, it combines a graphical interface, when available, with a keyboard-based command system.  If the graphical interface is not available, such as from an Open OnDemand terminal, it will fall back to a text-only interface. Emacs is powerful and complex.  The documentation is here.
Viewing Files
Pagers
The most widely used way to view files quickly without editing is to use a pager.  A pager prints to the terminal window the amount of text that fits in that window.  The default pager on Linux is more (also called less because ""less is more"").
bash
$more filename
This displays the contents of a file on the screen with line scrolling. To scroll you can use ‘arrow’ keys. To advance one line, press the Enter key.  To advance a full page, press the space bar. Press q to exit.
bash
$more ~/rivanna-cl/shakespeare/Lear.txt
To page upward within the text, press b (back).
Searching.
You can search in the forward direction with /, where pattern is a combination of characters you wish to find.
bash
$more ~/rivanna-cl/shakespeare/Lear.text
 /serpent

 ...skipping
     Turn all her mother's pains and benefits
     To laughter and contempt, that she may feel
     How sharper than a serpent's tooth it is
     To have a thankless child! Away, away!                Exit.

Search stops at the first occurrence. To locate the next one, type n.
Printing a file to the console
The cat (concatenate) command prints the contents of a file to the screen.
bash
$cat myfile
It does not fit the output to the terminal windows size; it simply keeps printing to the end.
It can also be used to create or join files (hence its name). We will learn more about its behavior here when we look at standard streams.
```bash
create a file
$cat > newfile
Now I can enter
some text
It will be copied exactly into the file
^d
``
The^dnotation means hold downCtrlandd` together. It is the end-of-file marker for bash.
```bash
append a file to another
$cat otherfile >> newfile
``
{{< warning >}}
If used single>in place of the double>>in the above,catwill overwritenewfilewithotherfile`.
{{< /warning >}}
Displaying Parts of a File
head and tail
head filename
Displays only the starting lines of a file. The default is first ten lines. Use “-n” to specify the number of lines.
bash
$head ~/rivanna-cl/shakespeare/Lear.text

This Etext file is presented by Project Gutenberg, in
cooperation with World Library, Inc., from their Library of the
Future and Shakespeare CDROMS.  Project Gutenberg often releases
Etexts that are NOT placed in the Public Domain!!

*This Etext has certain copyright implications you should read!*

<&ltTHIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM
SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS
PROVIDED BY PROJECT GUTENBERG WITH PERMISSION.  ELECTRONIC AND

tail filename
Displays the last 10 lines.
bash
$tail 30 ~/rivanna-cl/shakespeare/Lear.text

     The cup of their deservings.- O, see, see!
  Lear. And my poor fool is hang'd! No, no, no life!
     Why should a dog, a horse, a rat, have life,
     And thou no breath at all? Thou'lt come no more,
     Never, never, never, never, never!
     Pray you undo this button. Thank you, sir.
     Do you see this? Look on her! look! her lips!
     Look there, look there!                            He dies.
  Edg. He faints! My lord, my lord!
  Kent. Break, heart; I prithee break!
  Edg. Look up, my lord.
  Kent. Vex not his ghost. O, let him pass! He hates him
     That would upon the rack of this tough world
     Stretch him out longer.
  Edg. He is gone indeed.
  Kent. The wonder is, he hath endur'd so long.
     He but usurp'd his life.
  Alb. Bear them from hence. Our present business
     Is general woe. [To Kent and Edgar] Friends of my soul, you
        twain
     Rule in this realm, and the gor'd state sustain.
  Kent. I have a journey, sir, shortly to go.
     My master calls me; I must not say no.
  Alb. The weight of this sad time we must obey,
     Speak what we feel, not what we ought to say.
     The oldest have borne most; we that are young
     Shall never see so much, nor live so long.
                                       Exeunt with a dead march.

THE END

Transferring Files Using the Command Line
Files can be transferred by graphical clients such as MobaXterm and Filezilla, or through Globus.  If you are using a terminal from your local computer, you can also use some command-line tools.
scp and sftp
The secure shell protocol includes two file-transfer command-line tools; scp and sftp.  Sftp is scp with a slightly different interface.
bash
$scp LOCAL_FILE mst3k@login.hpc.virginia.edu:REMOTE_PLACE
$scp mst3k@login.hpc.virginia.edu:REMOTE_FILE LOCAL_PLACE
REMOTE_PLACE and LOCAL_PLACE refer to the location on the appropriate host where you want the file to be written.  REMOTE_PLACE can be omitted and the file will be transferred to your home directory under the same name.  To change the name or specify a directory on the other system, use a different name or path for REMOTE_PLACE.
LOCAL_PLACE must be present but can be . for the directory where the scp was run.
The colon : is required.
To copy a directory, use scp -r similarly to cp -r.
Examples
bash
$scp myfile.txt mst3k@login.hpc.virginia.edu:
$scp myscript.py mst3k@login.hpc.virginia.edu:project1
$scp myscript.py mst3k@login.hpc.virginia.edu:script.py
$scp myfile.csv mst3k@login.hpc.virginia.edu:/scratch/mst3k
$scp mst3k@login.hpc.virginia.edu:/scratch/mst3k/run11/output.txt .
Scp resembles cp.  Sftp is an implementation over scp of the interface of a popular, but insecure, protocol widely used in the past called ftp (file transfer protocol).
bash
$sftp mst3k@login.hpc.virginia.edu
sftp> put local_file
sftp> get remote_file
sftp> quit
The sftp client permits other commands. ls lists files on the remote system.  lls lists local files.
rsync
The rsync command is used to synchronize files and folders.  It has many options and some attention must be paid to whether a trailing slash is needed or not.
bash
$rsync -av ldir/ mst3k@login.hpc.virginia.edu:rdir
$rsync my_file mst3k@login.hpc.virginia.edu:/scratch/$USER
By default rsync does not transfer files that are older than the equivalent on the target. This can increase the transfer speed significantly. Rsync also resumes a transfer that was interrupted.  Scp always starts again from the beginning.
Rsync is very powerful but has many options and can be confusing. For more details see our documentation. Several online resources with examples are also available, such as this.
Setting up Passwordless ssh
If you will frequently use ssh, scp, sftp, or rsync to a remote system, it becomes inconvenient to repeatedly enter your password.  Follow the instructions given here to generate a key to log in without a password.
Finding Information About a Command
Basic documentation for a command can be read from the shell with the  man  (manual) command.
bash
$man ls
Many commands also provide a --help option which usually prints the same information.
bash
$ls --help
The output of man is often called the manpage of that command.
Exercise
Q1: Create a directory called newdir.  Navigate to it.  Make the new file mynewfile and save it with ^d. Use nano or another editor of your preference and type a line or two of text.
Solution

```bash
$mkdir newdir
$cd newdir
$cat > mynewfile
nano mynefile
```

Q2: View the content of the mynewfile using more. Rename mynewfile to the_file. Copy the_file to old_file. List the files in long format.
Solution

```bash
$more mynewfile
$mv mynewfile the_file
$cp the_file old_file
$ls -l
```"
rc-learning-fork/content/notes/hpc-from-terminal/_index.md,"This tutorial is an introduction to the command-line interface of high-performance computing systems. We’ll learn how to use commands to perform basic operations in the terminal: creating and navigating directories, listing and displaying files, moving and copying files. We will also cover creating files, searching files, and managing file permissions. Working in the command line, we can combine existing programs and utilities, automate repetitive tasks, and connect to remote resources."
rc-learning-fork/content/notes/matlab-image-processing/index.md,"{{< figure library=""true"" src=""matlab-logo.png"" width=30% height=30% >}}
As our microscopes, cameras, and medical scanners become more powerful, many of us are acquiring images faster than we can analyze them. MATLAB's Image Processing Toolbox provides interactive tools for performing common preprocessing techniques, as well as a suite of functions for automated batch processing and analysis.
Reading and Writing Images
imread: Read image from a graphics file
imwrite: Write image to file
imfinfo: Retrieve image information
Supported file types: BMP, GIF, JPEG, PNG, TIFF, and more
Example 1: Image Read/Write
```
clear; clf
% Read in image
I = imread('peppers.png');
% Display image and add a title
imshow(I); title('RGB Image')
% Get info about the image
imfinfo('peppers.png')
% Convert RGB image to grayscale
gI = rgb2gray(I);
% Display new grayscale inage
imshow(gI); title('Grayscale Image')
% Write grayscale image to new file
imwrite(gI, 'peppers_gray.png')
```
Other Image Types
MATLAB can also read DICOM and Nifti files with the dicomread and niftiread functions.

Image Tool
The Image Tool imtool allows us to interactively view and adjust images.
Functionality:
* View image information


Inspect pixel values


Adjust contrast


Crop image


Measure pixel distance


Example 2: Playing with the Image Tool
```
clear; clf
I = imread('colon_cells_1.tif');
imtool(I)
```

Denoising and Filters
Example 3: Removing Salt-and-Pepper Noise
{{< figure src=""intro-fiji-16.png"" >}}
Mean Filter
```
clear; clf
% Load and display image
I = imread('coins.tif');
imshow(I)
% Apply a mean (or average) filter
H = fspecial('average', [3 3]); % H is our filter, we are creating a 3x3 mean filter
I_mean = imfilter(I, H); % apply the filter
imshow(I_mean)
```
Median Filter
I_median = medfilt2(I, [3 3]); % apply 3x3 median filter
imshowpair(I_mean, I_median, 'montage') % show images side-by-side
Example 4: Removing Gaussian Noise
{{< figure src=""wiener-filter.png"" >}}
```
clear; clf
% Load and display image
I = imread('planet.png');
imshow(I)
% Apply Wiener Filter
I_wiener = wiener2(I, [5 5]);
% Display the original and filtered images
imshowpair(I, I_wiener, 'montage')
```

Adjusting Image Contrast
Example 5: Adjusting Contrast Interactively
```
clear; clf
% Load the image
I = imread('colon_cells_1.tif');
% View the image in the Image Tool
imtool(I)
```
Using the Adjust Contrast Tool


Click the Adjust Contrast button in the toolbar (black and white circle). 


Click and drag to change the bounds of the histogram.


Once satisfied with the contrast, click the ""Adjust Data"" button and close the Adjust Contrast Tool.


Export the new image to the workspace. File > Export to Workspace > Enter new variable name (Optional) > OK



Example 6: Automated Contrast Enhancement
imadjust saturates the bottom 1% and top 1% of pixels.
```
I_adj2 = imadjust(I);
imshowpair(I, I_adj2, 'montage')
```
Example 7: Batch Contrast Enhancement
```
clear; clf
% A for loop allows us to perform the same operation iteratively
for i = 1:6
% Specify filename
filename = strcat('colon_cells_', num2str(i), '.tif');

% Load file
I = imread(filename);

% Adjust contrast
I_adj = imadjust(I);

% Save adjusted image as new file
new_filename = strcat('a_', filename);
imwrite(I_adj, new_filename)

end
```

Correcting Uneven Illumination
Example 8: Gaussian Blur and Image Math
```
clear; clf
% Load the image
I = imread('rice.png');
% Apply Gaussian blur to create background image
bg = imgaussfilt(I, 60);
% Show image and background
imshowpair(I, bg, 'montage')
% Subtract background from image
I_corr = I - bg;
% Show original and corrected images
imshowpair(I, I_corr, 'montage')
% Write out the corrected image (to be used later)
imwrite(I_corr, 'rice_corr.png')
```

Image Segmentation
Example 9: Detecting and Measuring Circular Objects
```
clear; clf
% Load the image
I = imread('colon_cells_1.tif');
% Adjust the contrast
I_adj = imadjust(I);
% Binarize the image
I_bw = imbinarize(I_adj);
% Determine radius range with imdistline tool
imshow(I_bw)
d = imdistline;
delete(d) % remove the imdistline tool
% Find circle centers and radii
[centers, radii] = imfindcircles(I_bw, [2 10], 'ObjectPolarity', 'bright');
% Display detected circles on image
imshow(I_adj)
h = viscircles(centers, radii);
% Try increasing sensitivity of circle detection
[centers, radii] = imfindcircles(I_bw, [2 10], 'ObjectPolarity', 'bright', 'Sensitivity', 0.9);
figure; % opens a new figure window
imshow(I_adj)
h = viscircles(centers, radii);
```

Example 10: Analyzing Foreground Objects
```
clear; clf
% Load the corrected rice image
I = imread('rice_corr.png');
% Adjust contrast
I_adj = imadjust(I);
% Binarize the image
I_bw = imbinarize(I_adj);
% Remove objects that are smaller than 50 pixels (not rice)
I_bw = bwareaopen(I_bw, 50);
imshow(I_bw)
% Fill in holes
I_bw = imfill(I_bw, 'holes');
imshow(I_bw)
% Identify objects (connected components) in the image
cc = bwconncomp(I_bw, 8);
% Extract areas of individual grains of rice
A = regionprops(cc, 'Area');
% Extract mean intensity values
meanInt = regionprops(cc, I, 'MeanIntensity');
```
Other properties in regionprops:


Area


Centroid


Major/Minor Axis Length


Perimeter


Max/Mean/Min Intensity



Image Registration
Example 11: Geometric Image Registration
```
clear; clf
% Load the images and convert to grayscale
im1 = rgb2gray(imread('arc-de-triomphe1.jpg'));
im2 = rgb2gray(imread('arc-de-triomphe2.jpg'));
% Display image differences
figure;
imshowpair(im1, im2, 'falsecolor')
% Detect features in both images
pts1 = detectSURFFeatures(im1);
pts2 = detectSURFFeatures(im2);
% Extract feature descriptors
[features1, validPts1] = extractFeatures(im1, pts1);
[features2, validPts2] = extractFeatures(im2, pts2);
% Match features using their descriptors
indexPairs = matchFeatures(features1, features2);
% Retrieve locations of matching points in each image
matched1 = validPts1(indexPairs(:,1));
matched2 = validPts2(indexPairs(:,2));
% Show point matches
figure;
showMatchedFeatures(im1, im2, matched1, matched2)
% Estimate the transformation that will move Image 2 to Image 1 space
[tform, inlier2, inlier1] = estimateGeometricTransform(matched2, matched1, 'similarity');
% Use the estimated transform to move Image 2 to Image 1 space
outputview = imref2d(size(im1)); % sets world coordinates given # of rows and columns
recovered = imwarp(im2, tform, 'OutputView', outputview);
figure;
imshowpair(im1, recovered, 'falsecolor')
```

Image Data Management with OMERO
With the advent of high-throughput screening, the need for efficient image management tools is greater than ever. From the microscope to publication, OMERO is a database solution that handles all your images in a secure central repository. You can view, organize, analyze and share your data from anywhere you have internet access. Work with your images from a desktop app (Windows, Mac or Linux), on UVA’s high performance computing platform (Rivanna), from the web, or through 3rd party software like Fiji and ImageJ, Python, and MATLAB. OMERO is able to read over 140 proprietary file formats, including all major microscope formats.
Example 12: Image Processing and Analysis with OMERO
```
%%%%%%%%%
% Sample MATLAB Image Processing pipeline with OMERO
%%%%%%%%%
%% Log into OMERO
myUsername = '';
myPassword = '';
%%%%% Don't change anything in this section! %%%%%
client = loadOmero('omero.hpc.virginia.edu',4064);
session = client.createSession(myUsername, myPassword);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Specify the Project and Dataset containing raw data
projectID = 107;
datasetID = 163;
%% Each image is processed and analyzed, then exported back to OMERO
%%%%% Don't change anything below this line! %%%%%
dataset = getDatasets(session, datasetID, true);
datasetName = char(dataset.getName().getValue());
imageList = dataset(1).linkedImageList;
imageList = imageList.toArray.cell;
newdataset = createDataset(session, 'binarized', ...
    getProjects(session,projectID));
for i = 1:length(imageList)
    pixels = imageList{i}.getPrimaryPixels();
    name = char(imageList{i}.getName().getValue());
    store = session.createRawPixelsStore();
    store.setPixelsId(pixels.getId().getValue(),false);
    plane = store.getPlane(0,0,0);
    img = uint8(toMatrix(plane,pixels));
type = 'uint8';

newimg = uint8(contrastIncrease(img));
newimg = uint8(binarizeImage(newimg));
newname = strcat('b_',name);

pixelsService = session.getPixelsService();
pixelTypes = toMatlabList(session.getTypesService().allEnumerations('omero.model.PixelsType'));
pixelTypeValues = arrayfun(@(x) char(x.getValue().getValue()),pixelTypes,'Unif',false);
pixelType = pixelTypes(strcmp(pixelTypeValues, type));

description = sprintf('Dimensions: 512 x 512 x 1 x 1 x 1');

idNew = pixelsService.createImage(512,512,1,1,toJavaList(0:0,'java.lang.Integer'),pixelType,newname,description);

imageNew = getImages(session, idNew.getValue());

link = omero.model.DatasetImageLinkI;
link.setChild(omero.model.ImageI(idNew,false));
link.setParent(omero.model.DatasetI(newdataset.getId().getValue(),false));
session.getUpdateService().saveAndReturnObject(link);

pixels = imageNew.getPrimaryPixels();
store = session.createRawPixelsStore();
store.setPixelsId(pixels.getId().getValue(),false);
byteArray = toByteArray(newimg, pixels);

store.setPlane(byteArray,0,0,0);
store.save();
store.close();

cc = bwconncomp(newimg,4);
numCells = num2str(cc.NumObjects);

mapAnnotation = writeMapAnnotation(session,'Count',numCells);

link = linkAnnotation(session, mapAnnotation, 'image', idNew.getValue());

end
client.closeSession();
function I3 = contrastIncrease(image)
I = image;
background = imopen(I,strel('disk',15));

I2 = I - background;

I3 = imadjust(I2);

end
function bw = binarizeImage(image)
bw = imbinarize(image);
bw = bwareaopen(bw, 10)*255;

end
```"
rc-learning-fork/content/notes/matlab-statistics/index.md,"Overview
MATLAB is an integrated technical computing environment from the MathWorks that combines array-based numeric computation, advanced graphics and visualization, and a high-level programming language. Separately licensed toolboxes provide additional domain-specific functionality.
Matlab Academy: Statistical Methods with Matlab
Documentation: Statistics and Machine Learning Toolbox (help page)
Documentation: Statistics and Machine Learning Toolbox (product page)


Course Overview
Video: Statistical Methods with Matlab
Exploring Data
Visualizing Data Sets
{{< figure src=""exploreData1.png""  >}}
{{< figure src=""exploreData2.png""  >}}
Documentation:   histogram          boxplot             scatter  

Measures of Centrality and Spread
{{< figure src=""centrality1.png""  >}}
{{< figure src=""centrality2.png""  >}}
{{< figure src=""centrality3.png""  >}}
  Documentation:   mean         median      
 mode        trimmean  

{{< figure src=""spread1.png""  >}}
{{< figure src=""spread2.png""  >}}
{{< figure src=""spread3.png""  >}}
 Documentation:   std          iqr 
     range        var  

Distributions
{{< figure src=""distribution3.png""  >}}
{{< figure src=""spread4.png""  >}}

{{< figure src=""distribution1.png""  >}}
{{< figure src=""distribution2.png""  >}}
Documentation:   normpdf         unifpdf        randn        rand  
Summary
{{< figure src=""summary1.png""  >}}

{{< figure src=""summary2.png""  >}}

{{< figure src=""summary3.png""  >}}

Fitting a Curve to Data
Linear Regression
{{< figure src=""regression1.png""  >}}
{{< figure src=""regression2.png""  >}}
{{< figure src=""regression4.png""  >}}
{{< figure src=""regression3.png""  >}}
  Documentation:   fit 

Evaluating Goodness of Fit
{{< figure src=""regression5.png""  >}}
{{< figure src=""regression6.png""  >}}
{{< figure src=""regression7.png""  >}}

Nonlinear Regression
{{< figure src=""regression8.png""  >}}
{{< figure src=""regression9.png""  >}}
{{< figure src=""regression10.png""  >}}
Summary
{{< figure src=""summary4.png""  >}}
{{< figure src=""summary5.png""  >}}
{{< figure src=""summary6.png""  >}}
Interpolating Data
Linear Interpolation
{{< figure src=""interp1.png""  >}}
{{< figure src=""interp2.png""  >}}
{{< figure src=""interp3.png""  >}}
{{< figure src=""interp4.png""  >}}
{{< figure src=""interp5.png""  >}}
{{< figure src=""interp6.png""  >}}
Nonlinear Interpolation
{{< figure src=""interp7.png""  >}}
{{< figure src=""interp8.png""  >}}
{{< figure src=""interp9.png""  >}}
  Documentation:   interp1  

Summary
{{< figure src=""summary7.png""  >}}
{{< figure src=""summary8.png""  >}}
Additional Resources
{{< figure src=""additionalRes1.png""  >}}

MATLAB Central       MathWorks Support   

Exercises
Visualizing Data sets
Exercise: Visualize Height and Weight Data
Measure of Centrality and Spread
Exercise: Find the Mean and Median
Exercise: Find the Standard Deviation and IQR
Distributions
Exercise: Fit and Plot a Normal Distribution
Exercise: Generating Random Numbers
Review: Exploring Data
Exercise: Earthquakes
Linear Regression
Exercise: Fit a Line to Data
Exercise: Fit a Polynomial to Data
Evaluating the Goodness of Fit
Exercise: Evaluate and Improve the Fit


Nonlinear Regression
Exercise: Fit a Nonlinear Model
Review: Fitting a Curve to Data
Exercise: Temperature Fluctuations
Linear Interpolation
Exercise: Fill in Missing Data
Exercise: Resample Data
Nonlinear Interpolation
Exercise: Resample Data with Different Interpolation Methods
Review: Interpolation
Exercise: Stock Prices"
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_exercise2.md,"Start an Open OnDemand File Explorer tab.  In your home directory, create a new file.  Use the Editor to enter the following text:

I came in with Halley's Comet in 1835. It is coming again next year, and I expect to go out with it. It will be the greatest disappointment of my life if I don't go out with Halley's Comet. The Almighty has said, no doubt: ""Now here are these two unaccountable freaks; they came in together, they must go out together."" 
-Mark Twain

Name the file whatever you wish.  Make a new folder ""Quotes.""  Move the file to this directory.  
Go to the FastX desktop and open Caja (the filing-cabinet icon, or from the System Tools menu).  Navigate to your new directory.  Change the name of the file.  Use whatever method you prefer (right-click or F2 key).  Still in Caja, copy the file.  Give the copy the original name you choose.  Move it to your Desktop.
Return to your Desktop in OOD and delete the file there.
On FastX, return to the Quotes directory.  Open the file with Pluma, and after ""Mark Twain"" add "", 1909""."
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_jobs_tab.md,"The ""Jobs"" tab on the menu bar allows you to submit and search for jobs on the cluster.
{{< figure src=""/notes/hpc-intro/img/OOD_jobs_tab.png"" caption=""OOD Jobs Menu"" >}}
The ""Active Jobs"" tab shows all jobs currently running or queues on all partitions for all users. The Filter search bar allows you to narrow jobs by either user, queue, job name, job ID, or job status (running, queued, completed, etc)."
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_useful_commands.md,"Sometimes it's useful to check how many SUs are still available on your allocation. The allocations command displays information on your allocations and how many SUs are associated with them:
{{< figure src=""/notes/hpc-intro/img/features_allocations.png"" caption=""Allocations"" >}}
running allocations -a <allocation_name> provides even more detail on when the allocation was last renewed and its members.
One way to check your storage utilization is with the hdquota command. This command will show you how much of your home, scratch, and project (if applicable) storage are being utilized. Below is the sample output for hdquota:
{{< figure src=""/notes/hpc-intro/img/features_hdquota.png"" caption=""Disk Usage"" >}}
This is a useful command to check whether you're running out of storage space or to see where files need to be cleaned up. For more detailed information on disk utilization you may also use the du command to investigate specific directories.
To gain information on the different queues you can type qlist on the command line:
{{< figure src=""/notes/hpc-intro/img/features_qlist.png"" caption=""Queues"" >}}
This will show the list of partitions, their occupancy, and the SU charge rate. You can type qlimits for information on each queue's limits:
{{< figure src=""/notes/hpc-intro/img/features_qlimits.png"" caption=""Queue Limits"" >}}
Finally, the sinfo command will provide some more detailed information on the health of each queue and the number of active nodes available. These commands can be useful in diagnosing why a job may not be running, or in better understanding queue usage for more efficient job throughput. More information on hardware specifications and queue information can be found on our website."
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_shell_access.md,"The ""Clusters"" tab on the menu bar open a new browser tab with a Linux command line interface for shell access:
{{< figure src=""/notes/hpc-intro/img/features_shell_access.png"" caption=""Command line Shell Access"" >}}
To get a feel of how to use the command line, type the command allocations after the $ prompt to display information on your allocations.
This is a suitable method to access the cluster through command line without connecting to the UVA VPN."
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_job_stats.md,"In the OOD Job Viewer, clicking on the right arrow for a particular job will show details of the job if it is queued or running.  Completed jobs will have no information available.  Be patient as it can take a few moments for the information to be loaded.  In this illustration the selected job is PENDING.  The reason given is Resources, which means that no resources are available yet for this job.
{{< figure src=""/notes/hpc-intro/img/OOD_job_status.png"" caption=""Viewing only my jobs."" >}}
Completed jobs will be visible only for a short while as they are wrapping up and exiting.
If an error occurs, try reloading the page."
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_files_tab.md,"The ""Files"" tab on the menu bar gives access to all files in home, scratch, standard or project (if applicable) directories.
{{< figure src=""/notes/hpc-intro/img/OOD_files_tab.png"" caption=""OOD Files Menu"" >}}
Here, you can upload and download small files to and from the cluster from your local computer. You can also create and delete new files and directories in addition to copying or renaming them. The Filter search bar searches for files or directories in the file system."
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_job_composer.md,"The job composer tab allows you to create and submit a job to run on the cluster.
{{< figure src=""/notes/hpc-intro/img/Features_job_composer.png"" caption=""OOD Job Composer"" >}}
Selecting the default template will automatically create a submission script called demo_hello_world.slurm located in /home/computingID/Rivanna/data/sys/myjobs/projects/default/1 on the file system:
{{< figure src=""/notes/hpc-intro/img/featues_template_job.png"" caption=""Default Template Job"" >}}
Before submitting the job, your_allocation on the #SBATCH --account=your_allocation line must be replaced with the name of the allocation you're a member of. We will review editing files later. Once the correct allocation name is edited in, you can click ""Submit"" to queue your job. It will be given a corresponding Job ID, and once it's completed, the Folder contents will now contain a corresponding output file that contains the instructions from the submission script:
{{< figure src=""/notes/hpc-intro/img/features_job_output.png"" caption=""Default Template Output"" >}}
There are several job templates that can be run in addition to the default hello world option under New Job > From Template:
{{< figure src=""/notes/hpc-intro/img/features_templates.png"" caption=""Template Options"" >}}"
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features.md,"Open OnDemand has many features accessible directly from the menu bar.
{{< figure src=""/notes/hpc-intro/img/OOD_File_Menu.png"" caption=""Open OnDemand Menu Options"" >}}"
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_job_viewer.md,"Open OnDemand allows you to check the status of your jobs easily.  Open the Jobs tab and go to Active Jobs.  The default view is All Jobs.
{{< figure src=""/notes/hpc-intro/img/OOD_squeue_viewer.png"" caption=""Job status viewer in OOD."" >}}
You can filter to select subsets of the jobs, for example you can view only jobs in the gpu partition.
{{< figure src=""/notes/hpc-intro/img/OOD_squeue_filter.png"" caption=""Viewing only the GPU partition."" >}}
You can also look at the status of only your own jobs by switching from All Jobs to My Jobs.
{{< figure src=""/notes/hpc-intro/img/OOD_squeue_myjobs.png"" caption=""Viewing only my jobs."" >}}"
rc-learning-fork/content/notes/hpc-intro/features_of_ood/features_interactive_sessions.md,"This tab will show you running, pending or completed interactive sessions from the OOD interface.
{{< figure src=""/notes/hpc-intro/img/features_interactive.png"" caption=""Interactive Sessions"" >}}"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_fastx_kill_restart.md,"You can also terminate -- or restart -- a session from the ""My Sessions"" tab.  To terminate, click the x in the upper right of the session, or use the menu.
To restart instead of terminating, click the arrow for ""run"".
{{< figure src=""/notes/hpc-intro/img/FastX_kill_session.png"" caption=""Terminating or restarting a session."" >}}"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_fastx_session_launch.md,"Most users should choose the MATE session.  
{{< figure src=""/notes/hpc-intro/img/FastX_session_launch.png"" caption=""Starting a MATE session"" >}}
Click the icon, then click the play button on the MATE session under ""Disconnected Sessions"" on the page. This will open a new window with two options: Browser Client, and Desktop Client.
{{< figure src=""/notes/hpc-intro/img/FastX_connect_options.png"" caption=""Browser or Desktop Client"" >}}
Select the Browser Client to connect to your session."
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_ssh.md,"SSH client provides direct access to the command line.
Use the command
ssh -Y mst3k@login.hpc.virginia.edu

On a Mac or Linux system, simply open a terminal application and type the above command.
On a Windows system, open the Command Prompt app and type the above command.
Replace the mst3k with your computing ID.
You must use the UVA VPN when off-grounds.
"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_fastx_logout.md,"If you simply close your session browser tab, FastX suspends your session rather than terminates it.  It is generally preferable to terminate rather than suspend so that you will not accidentally have multiple sessions running.
One way to terminate is to log out.  Go to the System menu in the top ribbon and select Log Out mst3k (with your ID).
{{< figure src=""/notes/hpc-intro/img/FastX_logout.png"" caption=""Logging out of the desktop."" >}}"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_fastx_desktop.md,"A MATE desktop looks a little like an older Windows desktop. In the ribbon at the top are Caja, a file manager; a Terminal application, and the Firefox Web browser.   
{{< figure src=""/notes/hpc-intro/img/FastX_desktop.png"" caption=""The MATE Desktop"" >}}"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_logging_on.md,"
There are three ways to connect to the HPC System:
Open OnDemand, a graphical user interface through a web browser
you can examine and manipulate files and submit jobs.
you can access applications such as Matlab, Jupyterlab, and R Studio Server.


FastX Web, direct access to a desktop 
ssh (Secure Shell) client, which provides direct access to the command line.
For Windows we recommend MobaXterm


"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_ssh 2.md,"SSH client provides direct access to the command line.
Use the command
ssh -Y mst3k@login.hpc.virginia.edu

On a Mac or Linux system, simply open a terminal application and type the above command.
On a Windows system, open the Command Prompt app and type the above command.
Replace the mst3k with your computing ID.
You must use the UVA VPN when off-grounds.
"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_fastx.md,"FastX is a Web-based desktop environment for HPC. It is accessible either through the Open OnDemand Interactive Apps menu, or directly at fastx.hpc.virginia.edu.
FastX requires the VPN.  If the VPN is not active, the start page will not load.
Always use either the OOD link or the fastx URL.  The underlying name of the host may change from time to time.
{{< figure src=""/notes/hpc-intro/img/FastX_splash_screen.png"" caption=""Logging in to FastX"" >}}"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_ood_dashboard.md,"The Open OnDemand home page is the Dashboard. 
{{< figure src=""/notes/hpc-intro/img/OOD_Dashboard.png"" caption=""OOD Dashboard"" >}}"
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_ood.md,"To connect to Open OnDemand, open your web browser and type
https://ood.hpc.virginia.edu
You will need to authenticate with Netbadge (“Netbadge in"")
You can connect to Open OnDemand from off-Grounds locations without a VPN connection.
Remember that Open OnDemand is a Web application.  If it freezes on you, click UVA Open OnDemand in the upper left.  It will log you out eventually so you may need to log in again.  You also may have to refresh pages to see changes.
It may also open many tabs.  It is safe to close them if you aren't using them; just make sure you first click ""Save"" for any files you are editing and want to save changes.  You can even close all the tabs and log in again."
rc-learning-fork/content/notes/hpc-intro/connecting_to_the_system/connecting_fastx_session_page.md,"FastX starts a session on a frontend. A new session is started by selecting either the MATE or Terminal icon under Applications on the right side of the page.
{{< figure src=""/notes/hpc-intro/img/FastX_session_page.png"" caption=""Starting a new session"" >}}"
rc-learning-fork/content/notes/hpc-intro/files/file_paths_linux.md,"UVA HPC runs the Linux operating system.  File paths start from root, denoted with a forward slash (/).  The layout of the folders/directories is like an upside-down tree.
{{< figure src=""/notes/hpc-intro/img/unix_tree.png"" caption=""Schematic of folders on Rivanna. Only some files and folders shown."" >}}"
rc-learning-fork/content/notes/hpc-intro/files/file_moba.md,"MobaXterm
Start an SFTP session in MobaXterm.  Use one of the specific hosts login1.hpc.virginia.edu, login2.hpc.virginia.edu, login3.hpc.virginia.edu
{{< figure src=""/notes/hpc-intro/img/Moba_sftp_session.png"" caption=""New SFTP session in MobaXTerm"" >}}
A double-paned window will open.  Drag and drop files between your local machine and the remote server.
{{< figure src=""/notes/hpc-intro/img/Moba_sftp_pane.png"" caption=""Drag and drop"" width=1000px >}}"
rc-learning-fork/content/notes/hpc-intro/files/file_exercise3.md,"Now that we have covered the basics of OOD interactive apps, OOD functionality, and how to work with files, we will now put everything together to create a unique job submission script and run it through the job composer on OOD. In this example we will write a simple 'Hello World!' python script and a submission script to be run with the OOD job composer.
You'll need to create two files in your Desktop (/home/computingID/Desktop): hello.py and hello.slurm. You can use any text editor of your choice: FastX editors (pluma, gedit, etc.) or the  OOD file editor. In hello.py add the following lines:
```
Write hello 10 times
for i in range(10):
print (""\n {}  Hello World!"".format(i+1))
print(""\n\n"")
```
Next, we will need a submission script to submit this code to run on a compute node. Open hello.slurm and add the following:
```
!/bin/bash
SBATCH --cpus-per-task=1
SBATCH --mem=6000
SBATCH --time=00:05:00
SBATCH --partition=standard
SBATCH --account=your_allocation
module purge
module load miniforge
python hello.py
```
Be sure to replace your_allocation with the name of the allocation you have access to.
Once these two files are created, you can use the job composer on OOD to submit hello.slurm to a compute node to run the python code.
Once the job has completed, you should see a slurm-jobID.out file in your Desktop. View the file and make sure its contents are what you expect."
rc-learning-fork/content/notes/hpc-intro/files/file.md,"Files are the foundation of working with an HPC cluster.  We need to be able to

Transfer files to and from the cluster
Edit text files
Create files through the software we run

Each user has a home location and a scratch location.  When you log in you will be in the home location.  For now we will assume you will work with files in your home folder.  We will discuss the scratch folder later."
rc-learning-fork/content/notes/hpc-intro/files/file_actions_mv_cp.md,"Renaming Files and Folders

In the Open OnDemand File Explorer, click on Files on the Dashboard and click the file or folder you want to rename.  Find the menu (three vertical dots) to the right of the file name.  Click Rename.
In the ""Caja"" file manager on FastX, select the file or folder.  The combination of clicking on the icon and hitting the F2 key, should work on most keyboards as it does for Windows.  You can also right-click and choose Rename. 

Moving Files and Folders

In the Open OnDemand File Explorer, use the Copy/Move button in the upper right.  Select the file or folder you wish to move.  A dialog will open.  In your navigation pane, go to the target folder.  Click Copy in the dialog on the left.   
In the ""Caja"" file manager on FastX, if moving within the same parent folder, just drag the file or folder to the new location.  If moving between folders that do not share a parent, open another Caja window.  Cut the file or folder and paste to its new location.

Copying Files and Folders

In the Open OnDemand File Explorer, use the Copy/Move button, but click on Copy rather than Move.
In the ""Caja"" file manager on FastX, open another Caja window and drag the icon of the file or folder between them.  Alternatively right-click and use the copy and paste menu items.
"
rc-learning-fork/content/notes/hpc-intro/files/file_actions_create_delete.md,"There are three quick ways to work with files.


The Open OnDemand File Explorer.  


If logged in through FastX, you can use the ""Caja"" file manager.  It can be accessed through the filing-cabinet icon in the ribbon at the top, or via the Applications->System Tools menu.  Caja works very similarly to Windows Explorer and the Mac Finder, but is somewhat more limited. It should be simple to use.  The Open OnDemand file manager shows only one location at a time, whereas Caja, like Explorer or Finder, can open multiple windows. Note: you will not be allowed to do anything as ""Administrator.""


{{< info >}}
In Open OnDemand and Caja, rather than trying to navigate to your /scratch directory, use Go To (OOD) or Go->Location (Caja) and type the path /scratch/mst3k, using your own ID rather than mst3k.
{{< /info >}}
Creating Files and Folders

Open OnDemand: click the New File (file) or New Dir (folder) button and provide the name. You may also provide a path.
In FastX with Caja: For a new file go to the File->Create Document menu. For a folder use File->Create Folder.
In FastX you can use an editor such as pluma, which is accessible through the Applications->Accessories menu, using its File->New menu item.  You can then use the editor to add content to the file.

Deleting Files and Folders

In the Open OnDemand File Explorer, select the file or folder, then click the red Delete button.  It will request confirmation. 
In the ""Caja"" file manager on FastX, right-click and Delete.  Since the space in your home directory is limited, we recommend not moving to Trash.
"
rc-learning-fork/content/notes/hpc-intro/files/file_editing.md,"Once we have our files on the system, we may need to edit them.  It is a good idea to edit your files directly on the system, rather than editing on your local computer and then transferring them back and forth.
You can create files by the same process as editing an existing one; just select New if there is a menu.
You can use:
* The built-in editor in Open OnDemand. Click on Files on the Dashboard, highlight the file that you want to edit. From the dropdown menu next to the file name, select Edit.  A simple editor will open. To create a file, navigate to the desired location, click the New File button, then edit that file.
* If logged in through FastX, you can use the pluma editor, which is accessible through the Applications->Accessories menu.  You can also start it from a terminal with either its name pluma or as gedit (those are the same program).
* The MATE desktop in FastX also provides the semi-graphical editors Emacs and GVim in the same menu.
* In FastX, you can also use a programmer's interface such as VSCode, Spyder, or Rstudio.  For extensive editing or running programs through environments such as VSCode, use the Open OnDemand interactive app."
rc-learning-fork/content/notes/hpc-intro/files/file_filezilla.md,"Filezilla
This illustration is from a Linux computer.  macOS is similar.
First click the Site Manager icon in the upper left.
{{< figure src=""/notes/hpc-intro/img/Filezilla_ribbon.png"" caption=""Site Manager"" >}}
Select New Site.  Rename it.  Fill in the text boxes and dropdown.  Be sure to select SFTP in the Protocol box.  As for MobaXTerm, we recommend using a specific host name such login.hpc.virginia.edu.  Click OK to save and Connect to initiate the connection.  A multiple-pane window similar to that of MobaXTerm will open.
{{< figure src=""/notes/hpc-intro/img/Filezilla_settings.png"" caption=""Site Manager"" >}}"
rc-learning-fork/content/notes/hpc-intro/files/file_up_down.md,"You have several options for transferring data onto your home or scratch directories.

Use a drag-and-drop option with MobaXterm (Windows) or Filezilla (Mac OS and Linux).
For small files, use the Upload and Download buttons in the Open OnDemand FileExplorer App.
Use the scp command from a Terminal. 
Use the web browser in the FastX desktop to download data from UVA Box or other cloud locations.
Use the git clone command to copy git repositories
Set up a Globus endpoint on your laptop and use the Globus web interface to transfer files.
See https://www.rc.virginia.edu/userinfo/globus/ for details.
"
rc-learning-fork/content/notes/hpc-intro/files/file_globus.md,"Globus
Globus is a non-profit service for secure, reliable research data management developed and operated by the University of Chicago and Argonne National Laboratory, supported by funding from the Department of Energy, NSF, and the NIH. With Globus, subscribers can move, share, & discover data via a single interface – whether your files live on a supercomputer, lab cluster, tape archive, public cloud or your laptop, you can manage this data from anywhere, using your existing identities, via just a web browser.
{{< figure src=""/notes/hpc-intro/img/globus.png"" >}}
Advantages of Using Globus
Globus provides a secure, unified interface to your research data. Use Globus to ""fire and forget"" high-performance data transfers between systems within and across organizations.
Installing Globus
To transfer data to and from your computer, you will first need to install Globus Personal Connect. The following links provide instructions for installing Globus Personal Connect based on your machine's operating system.
| Platform | Installation instructions |
| --- | --- |
| Mac | https://docs.globus.org/how-to/globus-connect-personal-mac |
| Linux | https://docs.globus.org/how-to/globus-connect-personal-linux |
| Windows | https://docs.globus.org/how-to/globus-connect-personal-windows |
Transferring Files
Files are transferred with the Globus File Manager Web App. There are three ways to get to the app:

Go straight to https://app.globus.org/file-manager
Go to https://www.globus.org/ > Log In (top right corner)
Click Globus icon in Toolbar > Web: Transfer Files

Once the app is open you can choose collections to transfer data between.
Sharing Data with Collaborators
Globus users are able to share data with anyone with a Globus account. All UVA HPC and Ivy users have Globus accounts (authenticate with Netbadge).
External collaborators don’t need to be affiliated with an institution using Globus in order to share data with them. Anyone can create a Globus account using @globusid.org
More information on using Globus can be found on our learning website and from our documentation."
rc-learning-fork/content/notes/hpc-intro/files/file_paths.md,"Every file has a full name called its path. The path provides the operating system with the exact location of the file, relative to some starting point.
Examples:
* Windows
    C:\Users\mst3k\Desktop\mystuff.txt
* Mac OS
    /Users/mst3k/Desktop/mystuff.txt
* Linux (usually)
    /home/mst3k/Desktop/mystuff.txt
These paths traverse through some folders, which in Linux are often called directories, to arrive at the file mystuff.txt"
rc-learning-fork/content/notes/hpc-intro/overview_of_uva_hpc/overview_allocations.md,"Time on the HPC cluster is allocated.

An allocation refers to a block of CPU time that you can use to run your computations.
Only faculty may request an allocation. Research staff may apply for an exception.
Students must be sponsored by a faculty or research staff.
All individuals on a given allocation share the service units.
Allocations may be requested at https://www.rc.virginia.edu/userinfo/rivanna/allocations/
Allocations are measured in service units (SUs), where 1 SU = 1 core-hour in most cases.  Nodes equipped with GPUs may charge more than one SU per core-hour.
"
rc-learning-fork/content/notes/hpc-intro/overview_of_uva_hpc/overview_terminology.md,"
Node
A node is the basic building block of a cluster.

Nodes are a type of computer called a server.  

They generally have more power than a typical computer.
They may have specialty hardware like graphical processing units.



Two types of nodes

Head Node  – a server used for logging in and submitting jobs.
Compute Node  --  a server that carries out the computational work.



Core – an individual processor on a computer


The cluster's nodes have many cores (typically 40 each)


Memory


The random-access memory on a node


Storage

Disk storage visible from a node
"
rc-learning-fork/content/notes/hpc-intro/overview_of_uva_hpc/overview_accounts.md,"
Allocations and Groups
An allocation is associated with a group.  Currently this is a Grouper group.
Members (but not administrators) of the allocation group automatically receive an account on the system.
RC staff do not manage allocation groups.  The PI is responsible for adding and removing group members.  
The PI may designate administrators and delegate the task of managing the group to them.


"
rc-learning-fork/content/notes/hpc-intro/overview_of_uva_hpc/overview_storage.md,"Active users can access free storage for active work.  Research groups that need more permanent storage, or wish to share storage space, can also lease storage.
No-Cost Storage
Each user has access to a home directory and a scratch directory.
Home Directory
When you log in, you will be in your home directory, e.g. /home/mst3k.  Each home directory on has 200GB of storage capacity.  The home directory is for individual use and is not shareable with other users.
Scratch Directory
You have access to 10 TB of   temporary  storage. It is located in a subdirectory under /scratch, followed by your userID,  e.g.,  /scratch/mst3k
The /scratch directory is for individual use and is not shareable with other users.
{{< warning  >}}
/scratch is NOT permanent storage and files that have not been accessed for more than 90 days will be marked for deletion. 
{{< /warning >}}
Leased Storage
We offer two tiers of leased storage. For rates and offerings see our website.
Research Standard Storage
Standard storage is inexpensive but is not backed up, and access can be slow.  
Research Project Storage
Research project storage provides snapshots.  Snapshots are not backups, but are a ""snapshot"" of the files over a time interval in the past, currently one week."
rc-learning-fork/content/notes/hpc-intro/overview_of_uva_hpc/overview_system_specs.md,"Currently the supercomputer is made up of two systems, Rivanna and Afton, and between the two has 626 nodes with over 20448 cores and 8PB of various storage.
Several queues (or “partitions”) are available to users for different types of jobs. One queue is restricted to single-node (serial or threaded) jobs; another for multi-node parallel programs, and others are for access to specialty hardware such as large-memory nodes or nodes offering GPUs.
More information on queueing policies and hardware configurations can be found on our website."
rc-learning-fork/content/notes/hpc-intro/overview_of_uva_hpc/overview_getting_allocation.md,"{{< figure src=""/notes/hpc-intro/img/allocation_schematic.png"" caption=""Requesting an Allocation"" >}}"
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_rstudio_session.md,"Once you launch/submit your request, your job will wait in the queue until resources are available. You'll then be able to connect to your session:
{{< figure src=""/notes/hpc-intro/img/OOD_Rstudio_session.png"" >}}
{{< figure src=""/notes/hpc-intro/img/OOD_Rstudio_session2.png"" caption=""Starting an Rstudio session."" >}}
Rstudio Server can continue running any active processes if your network is disconnected.  Simply log back in to Open OnDemand, go to the ""My Interactive Sessions tab"", and click Launch  again.  It will reconnect, not launch another session."
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive.md,"Open OnDemand's File Explorer, the FastX Web interface, and various command-line interfaces, can be used to prepare work for the cluster. This includes transferring and editing files, looking at output, and so forth. However, all production work must be run on the compute nodes, not on the frontends.
A large, multi-user system like UVA's HPC cluster must be managed by some form of resource manager to ensure equitable access for all users.  Research Computing uses the Slurm resource manager.  Resource managers are also often called queueing systems.  Users submit jobs to the queueing system. A process called a scheduler examines the resource requests in each job and assigns a priority. The job then waits in a queue, which Slurm calls a partition, until the requested resource becomes available.  A partition is a set of compute nodes with a particular set of resources and limits. There are partitions for single-node jobs, multiple-node jobs, GPU jobs, and some other dedicated partitions. A list of the different queues and resources are listed here.
Open OnDemand offers an easy way to run interactive jobs.  With an interactive job, you are logged in directly to a compute node and can work as if it were a frontend.  Please keep in mind that an interactive job terminates when the time limit you specify expires, unless you explicitly end the session."
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_apps.md,"To submit an interactive job, from the Open OnDemand dashboard click on the menu Interactive Apps for the dropdown list.
We will focus on JupyterLab, Rstudio Server, and the Desktop for now.
{{< figure src=""/notes/hpc-intro/img/OOD_Interactive_Apps.png"" caption=""OOD Interactive Apps menu"" >}}"
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_resources.md,"{{< table >}}
| Field | Description |
| :-: | :-: |
| Number of cores | Used in parallel processing.  Your code must be modified to take advantage of using multiple cores. |
| Memory Request in GB | When dealing with Big Data, you will need to increase the amount of memory.  A good rule of thumb is to request 2 to 3 times the size of data that you are reading in or generating. |
| Work Directory | Allows you to change the working directory of a Jupyter Notebook to your /scratch folder or a /standard or /project share if applicable. |
| Optional: Slurm Option | Allows you to provide advanced features, like requesting specific nodes or providing a reservation |
| Optional Group | Only needed if you are in more than 16 computing groups.  You may need to force the system to see your allocation. |
| Optional: GPU type for GPU partition &  Optional: Number of GPUs | Only needed if you are running on a GPU node.  The “default” for GPU type will put you on the first available GPU node. For now, the number of GPUS should be 1. |
{{< /table >}}
Some fields on the Web Forms are blank, while others are set to default values.
The most important request will usually be the Memory Request."
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_launch_jupyter.md,"When you submit a request for an interactive app, it will be placed into the partition you specified, where it will wait until resources become available.  Requests with higher resource requests (more cores, more memory, more time) may wait longer.  
Once the job session begins, the screen will ask you to connect.  In our example you will see a Connect to Jupyter button appear.
{{< figure src=""/notes/hpc-intro/img/OOD_Jupyter_connect.png"" caption=""Connecting to a session."" >}}
When you connect, you will see your files on the left sidebar and a collection of kernels from which to choose.  You may not see all of these ""tiles"" because some accounts have customized tiles set up.  
{{< figure src=""/notes/hpc-intro/img/OOD_Jupyter_kernels.png"" caption=""Start screen for JupyterLab"" >}}
If you have connected previously, it may start from your earlier status and you will not see the tiles."
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_run_jupyter.md,"If have not previously used the OOD JupyterLab interactive app, you must select a kernel before initiating the notebook.  Once JupyterLab is set up, you can also start another notebook with a different kernel by selecting File->New Notebook.  It will then show a dropdown with the kernels available to you.
{{< figure src=""/notes/hpc-intro/img/OOD_Jupyter_nb.png"" caption=""Starting a new notebook."" >}}
If you are accidentally disconnected, you can go back to the OOD ""My Interactive Sessions"" tab and reconnect.  However, anything left running in a cell may have been terminated.  This is due to a limitation of Jupyter, not OOD or the HPC cluster, and does not apply to all interactive apps.
Remember to delete your session if you finish early. Closing your browser tab does not end the session.
{{< figure src=""/notes/hpc-intro/img/OOD_delete_session.png"" caption=""Ending a session."" >}}"
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_jupyter.md,"From the Interactive Apps menu, select JupyterLab.
The Jupyter Web Form gathers information about the computing resources that you need to run your Jupyter Notebook.
{{< info >}}
After you fill in the form, it will remember settings the next time that you connect to it, so watch out if you wish to change something.
{{< /info >}}
{{< figure src=""/notes/hpc-intro/img/Interactive-Jupyter-form-2024.png"" caption=""Setting up a job in JupyterLab through OOD"" >}}
You must choose a partition from the dropdown list. The partition limitations are explained below the dropdown box. Most of the time, you will select the Standard partition.  If you are running a deep learning model, you will want to choose a GPU Partition. If you do not specify a GPU model, your job will be assigned to the first available.
The ""Number of hours"" is the time your job session will remain active.  
{{< warning >}}
If you exceed the requested time limit, your session will be terminated without warning.
{{< /warning >}}
The ""Allocation"" is the name of the allocation that should be charged.  Your advisor should have told you what to use.  You can be a member of more than one allocation.  In that case one of them, not chosen by you, will be the default.  It is best to always fill in the name of an allocation, but remember to change it if necessary.
Once you have completed the form, click on the Launch button to submit the request."
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_rstudio.md,"Rstudio Server is a standalone app like JupyterLab. Starting a session is very similar to JupyterLab, but the form differs slightly.  Instead of kernel tiles, you will select a version of R from a dropdown menu from those available.  In this example, the version is R 4.4.1.
{{< figure src=""/notes/hpc-intro/img/Interactive-RStudio-form-2024.png"" caption=""Starting an Rstudio session."" >}}"
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_jupyter_terminal.md,"You are also able to access the terminal through a JupyterLab session.
{{< figure src=""/notes/hpc-intro/img/OOD_JupyterLab_Terminal.png"" caption=""JupyterLab Terminal"" >}}
Here, you can execute Linux commands to create custom conda environments and JupyterLab kernels. Additionally, you can access and run singularity containers through this functionality.
Your JupyterLab sessions will be saved in your /home/computingID/ondemand/data/sys/dashboard/batch_connect/sys/jupyter_lab/output/ directory; however, you can navigate to any part of the filesystem in the JupyterLab terminal."
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_jupyter_exercise.md,"Start a JupyterLab interactive session.  Select the Python 3 kernel.  If you are familiar with Python, you may write any code you wish.  If you do not know Python, click on a cell and type the following
python
import numpy as np
import matplotlib.pyplot as plt
Hit [Shift][Enter] (hold both keys at once) to run the cell, or from the Run menu choose Run Selected Cell.
In the next cell type
python
x=np.linspace(-1.*np.pi,np.pi,100)
y=np.sin(x)
plt.plot(x,y)
Run this cell.  
Close the tab, return to My Interactive Sessions in the OOD Dashboard, and delete the interactive job."
rc-learning-fork/content/notes/hpc-intro/interactive_apps/interactive_ood_other.md,"Some other widely-used interactive apps are MATLAB and the Desktop.  
Desktop
The most general OOD interactive app is the Desktop.  It will start a desktop identical to the FastX desktop, but on a compute node rather than a frontend.  From the desktop you can open a variety of applications from the menu, such as the Caja file manager.  You can also open a terminal window and type any valid commands into it.  In this illustration, the user has loaded a module to build a program for running. The OOD interactive desktop is the preferred method for interactive jobs.
{{< figure src=""/notes/hpc-intro/img/OOD_Desktop_terminal.png"" caption=""The OOD Desktop."" >}}
MATLAB
The MATLAB interactive app starts a MATLAB Desktop environment on a Desktop (VNC). Similar to Rstudio Server, in the form you can choose a version of MATLAB from a dropdown menu.  Once there, you are in a less complete desktop environment.  Your files may not be visible on the Desktop, but you can access them from the Places menu or from the Caja (filing cabinet) icon in the ribbon at the top of the screen.  If you exit the MATLAB Desktop, it will also exit the session.
Reconnecting
Both MATLAB and the Desktop will persist if your network is disconnected. Log back in to Open OnDemand, find your session from the My Interactive Sessions tab, then click Launch again."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/uva-gpus.md,"{{< table >}}
| GPU | Full Name | Year Launched | Memory | # of Tensor Cores |
| --- | --- | --- | --- | --- |
| A100 | NVIDIA A100 | 2020 | 40GB or 80GB | 432 (3rd gen) |
| A6000 | NVIDIA RTX A6000 | 2020 | 48GB | 336 (3rd gen) |
| A40 | NVIDIA A40 | 2020 | 48GB | 336 (3rd gen) |
| RTX3090 | NVIDIA GeForce RTX 3090 | 2020 | 24GB | 328 (3rd gen) |
| RTX2080Ti | NVIDIA GeForce RTX 2080 Ti | 2018 | 11GB | 544 (2nd gen) |
| V100 | NVIDIA V100 | 2018 | 32GB | 640 (1st gen) |
{{< /table >}}"
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/gpu-dashboard.md,"{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_5.png width=50% height=50% >}}
The GPU Dashboard is included in OOD (Open On Demand). It checks CPU and GPU efficiency. This will be demoed during the exercises in this workshop.
It includes GPU and CPU memory and utilization tracking in real time. This is helpful for GPU selection in future OOD sessions."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/memory-usage-gpu.md,"PyTorch
Correct GPU memory usage will be reported by GPU Dashboard.
TensorFlow/Keras 
By default, TF automatically allocates ALL of the GPU memory so GPU Dashboard may show that all (or almost all) of the GPU memory is being used.
To track the amount of GPU memory actually used, you can add these lines to your python script:
```python
import os
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
```
More Info
Homework for Keras users: try out GPU dashboard and see if it reports all of the GPU memory as used.
Resource Allocation for LLMs
Resource needs will vary based on LLM use (inference, fine-tuning, etc.)
We will cover good starting choices in the Inference and Fine-Tuning sections of today’s workshop."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/rivanna-gpu.md,"General
Choose “GPU” or “Interactive” as the Rivanna Partition in OOD.
Optional: choose GPU type and number of GPUs.
POD nodes
POD nodes are contained in the gpu partition with a specific Slurm constraint.
Slurm script:
```bash
SBATCH -p gpu
SBATCH --gres=gpu:a100:X   # X number of GPUs
SBATCH -C gpupod
```
Open OnDemand:
bash
 --constraint=gpupod

Only one person can be using a GPU at a time."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/nvidia-basepod.md,"
10 DGX A100 nodes
8 NVIDIA A100 GPUs.
80 GB GPU memory options.
Dual AMD EPYC™ 7742 CPUs, 128 total cores, 2.25 GHz (base), 3.4 GHz (max boost).
2 TB of system memory.
Two 1.92 TB M.2 NVMe drives for DGX OS, eight 3.84 TB U.2 NVMe drives forstorage/cache.
Advanced Features:
NVLink for fast multi-GPU communication
GPUDirect RDMA Peer Memory for fast multi-node multi-GPU communication
GPUDirect Storage with 200 TB IBM ESS3200 (NVMe) SpectrumScale storage array
Ideal Scenarios:
Job needs multiple GPUs on a single node or multi node
Job (single or multi-GPU) is I/O intensive
Job (single or multi-GPU) requires more than 40GB of GPU memory


The POD is good if you need multiple GPUs and very fast computation."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/resources-needed.md,"HPC Resources:
  * CPU memory
  * CPU cores
  * GPU
The resources you choose will depend on your specific LLM use, whether it involves inference, fine-tuning, or training an LLM from scratch (which we are not covering today)."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/gpus-llms.md,"Because LLMs involve a huge number of computations, we need a form of parallelization to speed up the process.
For example, the free version of ChatGPT (based on GPT-3.5) has 175 billion parameters, while the paid version (based on GPT-4) has over 1 trillion parameters.
GPUs (graphics processing units) provide the needed parallelization and speed up.
All the major deep learning Python libraries (Tensorflow, PyTorch, Keras,…) support the use of GPUs and allow users to distribute their code over multiple GPUs.
New GPUs have been developed and optimized specifically for deep learning."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/computations-cpu-gpu.md,"{{< table >}}
| Task | CPU or GPU |
| --- | --- |
| Tokenization | CPU |
| LLM Training/Fine-tuning | GPU |
| LLM Inference | Either, but GPU recommended |
{{< /table >}}
When you request memory for HPC, that is CPU memory.
If you request a GPU, you will receive all of that GPU’s memory."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/queue-wait.md,"You may not need to request an A100 GPU!
Requesting an A100 may mean you wait in the queue for a much longer time than using another GPU. This could give you a slower overall time (wait time + execution time) than if you had used another GPU.
{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_4.png width=90% height=90% >}}
When you request memory for HPC, that is CPU memory.
If you request a GPU, you will receive all of the GPU memory."
rc-learning-fork/content/notes/llms-hpc/hpc-resources-llms/gpu-workflow.md,"

Create data on the CPU.


Send data from the CPU to the GPU (for DL this is done in batches).


Compute result on the GPU.


Send the result back to the CPU.


Depending on the DL framework/LLM pipeline you are using, some of these steps may be automatically done for you."
rc-learning-fork/content/notes/llms-hpc/wrap-up/recap.md,"
We learned…
What an LLM is, the types of problems LLMs can solve, and LLM terminology
How to download and set up LLM software on UVA HPC
The HPC resources (CPU memory, CPU cores, GPU) needed for LLM inference and fine-tuning
How to use GPU Dashboard on UVA HPC
How to select an LLM from Hugging Face for a given task
LLM inference and supervised full fine-tuning on UVA HPC
How to write a Slurm script for LLM code using PyTorch

Research Computing Data Analytics Center
{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_13.png width=80% height=80% >}}
https://www.rc.virginia.edu/service/dac/"
rc-learning-fork/content/notes/llms-hpc/wrap-up/more-help.md,"Office Hours via Zoom
Tuesdays:           3 pm - 5 pm
Thursdays:      10 am - noon
Zoom Links are available at https://www.rc.virginia.edu/support/#office-hours
RC Website"
rc-learning-fork/content/notes/llms-hpc/llm-overview/llm-overview.md,"A Large Language Model (LLM) is a deep learning model that generally solves a natural language processing problem.
ChatGPT is an example of an LLM. 
There are many different types of LLMs that are suited to particular tasks.
Hubs such as Hugging Face provide many LLMs for download.
{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_0.png height=90% width=90% caption=""Graphic Source: https://attri.ai/blog/introduction-to-large-language-models"" >}}
You may hear people talk about LLMs for image classification, computer vision, etc.  These are also called VLMs (vision language models) and we are not covering those today."
rc-learning-fork/content/notes/llms-hpc/llm-overview/nlp-problems.md,"{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_1.png height=75% width=75% >}}
Problems:
Classify : An example of classification is sentiment analysis of Tweets. 
Rewrite : An example of rewriting is text translation.
Extract : An example of extraction is finding company names in news articles.
Generate : An example of generation is creating a story.
Search : An example of search is question answering from a text passage.
Applications of solutions to NLP problems include chatbots, virtual assistants, and recommendation systems."
rc-learning-fork/content/notes/llms-hpc/llm-overview/theory-llms.md,"{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_2.png height=50% width=50% >}}
LLMs are based on the transformer architecture.
We are not covering this in today’s workshop.
More Information:
  * Attention Is All You Need by Vaswani et al. (2017)
  * https://jalammar.github.io/illustrated-transformer/"
rc-learning-fork/content/notes/llms-hpc/llm-overview/terminology.md,"LLMs can have billions of  parameters  (unknown quantities in the model).
An LLM is  trained (model parameters are determined) using a very large amount of text data.
A  pre-trained  LLM has already been trained.  This process allows the model to “learn” the language.
A  fine-tuned  LLM is a pre-trained LLM that then is further trained on additional data for a specific task. Model parameters are updated in the fine-tuning process.
Inference  is the process in which a trained (or fine-tuned) LLM makes a prediction for a given input.
The input to an LLM is a  sequence  of  tokens (words, characters, subwords, etc.)
The  batch size for an LLM is the number of sequences passed to the model at once.
Generally, raw text is passed through a  tokenizer  which processes it into tokens and sequences and then numerical data which can be sent to the LLM."
rc-learning-fork/content/notes/llms-hpc/setup/downloading-llms.md,"When you use a transformers LLM for inference, it is downloaded to your home account.
~/.cache/huggingface/hub
Datasets (from the datasets package) are also downloaded to your home account.
~/.cache/huggingface/datasets
Make sure you have enough storage in your home account!
Currently, each user has access to 200GB of home storage."
rc-learning-fork/content/notes/llms-hpc/setup/install-transformers.md,"Using the PyTorch container:
```bash
module load apptainer pytorch/2.4.0
apptainer exec $CONTAINERDIR/pytorch-2.4.0.sif pip install transformers datasets
```
These packages are provided by Hugging Face (more details on Hugging Face in a bit).
For the fine-tuning example we will do later today, we will also need to install the accelerate and evaluate packages.
bash
apptainer exec $CONTAINERDIR/pytorch-2.4.0.sif pip install accelerate evaluate"
rc-learning-fork/content/notes/llms-hpc/setup/exercise_1.md,"Log On, Copy Materials

Log in to Rivanna using the Interactive partition.
2 hours, 4 cores
Allocation: hpc_training
GPU: yes, 1
Show Additional Options: Yes 
Optional Slurm Option: --reservation=llm_workshop
Copy the workshop folder /project/hpc_training/llms_on_hpc to your home or scratch account.

cp –r /project/hpc_training/llms_on_hpc ~/<…>
OR
cp –r /project/hpc_training/llms_on_hpc /scratch/<ID>/<…>

Open a Jupyter Notebook for PyTorch 2.4.0.
In the first cell of the notebook run the command pip list to see a list of software (i.e., packages) available in the PyTorch  2.4.0 kernel.

Do you see a package called “transformers” or “datasets”?"
rc-learning-fork/content/notes/llms-hpc/setup/ood.md,"{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_3.png height=70% width=70% >}}
Click on the kernel to open a Jupyter Notebook.
Packages from the selected kernel will be available for use in the notebook."
rc-learning-fork/content/notes/llms-hpc/setup/modules-containers.md,"Software on Rivanna is accessed via environment  modules  or  containers .
Software Modules:
R, Rstudio, JupyterLab, TensorFlow, and PyTorch are all examples of software modules. 
List of software available on Rivanna
Containers:
Containers bundle an application, the libraries and other executables it may need, and even the data used with the application into portable, self-contained files called images.
Containers simplify installation and management of software with complex dependencies and can also be used to package workflows."
rc-learning-fork/content/notes/llms-hpc/setup/software-llms.md,"We will use Python deep learning libraries, such as PyTorch and TensorFlow/Keras, to run and fine-tune large language models (LLMs).
The transformers package is compatible with both PyTorch and TensorFlow. 
According to its documentation, ""Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models."" 
This package supports various applications, including Natural Language Processing (which is the focus of this workshop), Computer Vision, Audio, and Multimodal tasks.
Note: There are packages in R/Rstudio that can do deep learning and/or use LLMs.  We are not using those in this workshop."
rc-learning-fork/content/notes/llms-hpc/setup/exercise_2.md,"Installing LLM Software


Install the transformers and datasets packages in the PyTorch 2.4.0 container.


Open the ex2.ipynb file from the workshop folder.


Run each cell of this notebook.

"
rc-learning-fork/content/notes/llms-hpc/fine-tuning/general-advice.md,"If you are learning about LLMs and doing tutorials, choose a small LLM.  The GPUs in the Interactive partition are probably ok to use.
You can leave the GPU choice as default on the GPU partition and work on whichever GPU you get or choose a GPU with a smaller amount of memory first.
Fine-tune your model for one epoch and monitor the GPU memory usage using GPU Dashboard.
If you are getting OOM (out of memory) GPU errors, try lowering the batch size.
There are other advanced techniques to reduce the amount of memory used in fine-tuning."
rc-learning-fork/content/notes/llms-hpc/fine-tuning/cpu-allocation-ft.md,"CPU memory
  * Interactive Partition: The interactive partition requests 6GB RAM per core requested.
  * Standard Partition: The standard partition requests 9GB RAM per core requested (no GPU).
  * GPU partition:  You can select the amount of CPU RAM.
You should have enough RAM to comfortably work with your GPU.
In other words, request at least as much RAM as the GPU you select.
If you are using a large dataset and/or want to do extensive preprocessing, more RAM is probably helpful.
CPU cores
Use multiple cores - especially if you are using a dataset from the Datasets package and a GPU (So that DataLoader can utilize multiple cores under the hood).
A good starting point is to use 8 cores.

Notes: 
Check your resource usage with the GPU Dashboard, or use seff for completed jobs and sstat for running jobs.
It may be the case that even if CPU Efficiency is a low percentage, you need all of the requested CPU cores for a specific part of the code, e.g., data preprocessing.
In this case, request the number of CPU cores that you need for the compute intensive part of the code."
rc-learning-fork/content/notes/llms-hpc/fine-tuning/selecting-gpu-ft.md,"For fine-tuning, select a GPU based on how much GPU memory you will need.
But, it is a hard problem to determine how much GPU memory a LLM will need for training  before  training the model.
A training iteration requires a forward and backward pass of the model. In addition to storing the LLM, training also requires additional storage space such as:
  * Optimizer states
  * Gradients
  * Activations
  * Data (how much is determined by the batch size)

According to the Hugging Face Model Memory Calculator, for a batch size of 1,  $$ \text{GPU Memory Estimate for Fine-Tuning (B)} = 4 \times (\text{LLM Memory in B})$$ 
  * While this formula can help ballpark an estimate, I recommend tracking GPU memory using the GPU Dashboard to make a more informed GPU selection.
For a more specific formula, see https://blog.eleuther.ai/transformer-math/ (Note: this blog requires some understanding of transformers).
Determining LLM memory for fine-tuning is an active area of research.  The paper LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs by Kim, et al. (April 2024) presents a method for doing so within 3% of the actual memory required."
rc-learning-fork/content/notes/llms-hpc/fine-tuning/exercise_5.md,"Supervised Full Fine-Tuning
Text Classification

Open the ex4.ipynb file from the workshop folder.
"
rc-learning-fork/content/notes/llms-hpc/fine-tuning/fine-tuning-intro.md,"{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_10.png width=70% height=70% >}}
LLMs are pre-trained on huge amounts of text data to learn general language patterns.
LLMs can be fine-tuned on a much smaller amount of data to excel at a particular task (e.g., classification of financial text).
Note: LLM pre-training is generally unsupervised.
Types of Fine-Tuning
Fine-tuning can be a supervised or unsupervised process and involves:
  * Changing some of the LLM weights,
  * Changing all of the LLM weights (full fine-tuning), or
  * Parameter Efficient Fine-Tuning (PEFT), i.e., keeping the LLM weights the same but updating a small number of additional parameters that will adjust the LLM weights (e.g., LoRA).
The more weights you update, the more computational resources you need."
rc-learning-fork/content/notes/llms-hpc/fine-tuning/why-use-ft.md,"{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_11.png width=65% height=65% >}}
Fine-tuning builds on a pre-trained language model (LLM) by using a smaller, labeled dataset to specialize and improve its performance for a specific task. Pre-training requires a large dataset and is computationally demanding, but fine-tuning is much less resource-intensive and allows models to adapt to domain-specific needs efficiently.
Example:
distilbert/distilbert-base-uncased was pre-trained on BookCorpus and English Wikipedia, ~25GB of data
distilbert/distilbert-base-uncased-finetuned-sst-2-english was fine-tuned on Stanford Sentiment Treebank (sst2), ~5MB of data"
rc-learning-fork/content/notes/llms-hpc/fine-tuning/hf-trainer-class.md,"The Trainer class allows a user to train or fine-tune a model using a convenient function, rather than using native PyTorch.
When training, the Trainer will automatically use the GPU if one is present.
There are  many options that can be set for the TrainingArguments (number of epochs, learning rate, save strategy, etc).
  * More TrainingArguments
The Trainer will not automatically evaluate the LLM, so we will pass it an evaluation metric."
rc-learning-fork/content/notes/llms-hpc/fine-tuning/example-ft.md,"Supervised Full Fine-Tuning
The data for supervised learning includes labels, e.g., a text review and sentiment label (positive or negative).
An LLM is generally pre-trained for the task of masked language modeling.
Through fine-tuning, we can change the task to text classification.
This means that the LLM head (the last layers) will change to text classification layers.
There is a transformers function that will do this for us."
rc-learning-fork/content/notes/llms-hpc/inference/gpu-inference-select.md,"Select a GPU for inference based on how much GPU memory you will need.
The GPU memory will contain the LLM (i.e., the model weights), input and output data, and extra variables for the forward pass (about 20% of the LLM size). $$ (\text{LLM Memory (B)}) = (\text{number of parameters}) \times (\text{number of bytes/parameter}) $$
The number of bytes/parameter depends on the model’s precision, e.g., fp32 is 4 bytes/parameter.
GPU Memory Estimate for Inference (B): $$ 1.2 \times (\text{LLM Memory in B})$$
I have found this formula to underestimate UVA GPU memory.  It is most likely a ballpark estimate, but I recommend tracking GPU memory using the GPU Dashboard to make a more informed GPU selection.

Note: B is for bytes."
rc-learning-fork/content/notes/llms-hpc/inference/exercise_4.md,"Select a GPU for Inference
Suppose you are going to run inference using the model google-bert/bert-base-uncased.  Which UVA GPU would you select and why?
| UVA GPU | Full Name | Year Launched | Memory | # of Tensor Cores |
| :-: | :-: | :-: | :-: | :-: |
| A100 | NVIDIA A100 | 2020 | 40GB or 80GB | 432 (3rd gen) |
| A6000 | NVIDIA RTX A6000 | 2020 | 48GB | 336 (3rd gen) |
| A40 | NVIDIA A40 | 2020 | 48GB | 336 (3rd gen) |
| RTX3090 | NVIDIA GeForce RTX 3090 | 2020 | 24GB | 328 (3rd gen) |
| RTX2080Ti | NVIDIA GeForce RTX 2080 Ti | 2018 | 11GB | 544 (2nd gen) |
| V100 | NVIDIA V100 | 2018 | 32GB | 640 (1st gen) |"
rc-learning-fork/content/notes/llms-hpc/inference/exercise_3c.md,"Batch Size and GPU Memory
As batch_size increases, so does GPU memory usage.
If you get an OOM (out of memory) error while using the GPU, try decreasing the LLM batch size.
Source and more information
Exercise 3c: Batch Size and Num Workers - Text Summarization


Open the ex3c.ipynb file from the workshop folder.


Run each cell of this notebook and complete the EXERCISES as you go.


Watch the GPU memory using GPU Dashboard as you run the cells.

"
rc-learning-fork/content/notes/llms-hpc/inference/cpu-allocation-inference.md,"CPU memory
  * Interactive Partition: 6GB RAM per core is requested.
  * Standard Partition: 9GB RAM per core is requested (no GPU).
  * GPU partition: You can select the amount of CPU RAM.
You should have enough RAM to comfortably work with your GPU.
In other words, request at least as much RAM as the GPU you select.
If you are using a large dataset and/or want to do extensive preprocessing, more RAM is probably helpful.
CPU cores
Use multiple cores - especially if you are using a dataset from the Datasets package and a GPU.  (So that DataLoader can utilize multiple cores under the hood.)
A good starting point is to use 8 cores.

Check your resource usage with the GPU Dashboard, or use seff for completed jobs and sstat for running jobs.
It may be the case that even if CPU Efficiency is a low percentage, you need all of the requested CPU cores for a specific part of the code, e.g., data preprocessing.
In this case, request the number of CPU cores that you need for the compute intensive part of the code."
rc-learning-fork/content/notes/llms-hpc/inference/exercise_3b.md,"Hugging Face Datasets & Debugging
Text Summarization
Open the ex3b.ipynb file from the workshop folder."
rc-learning-fork/content/notes/llms-hpc/inference/transformers-pipeline.md,"The transformers pipeline consists of a tokenizer, model, and post processing for getting model output.
Pros:
  * Easy to use.
  * Efficiently manages data batching and gpu memory for you – good for HPC!
Cons:
  * Harder to debug when something goes wrong.
Recommendation: Use the pipeline first.
If you get errors, you may have to use the model directly to diagnose the problem.
The pipeline ""hides"" details from the programmer, which can be good and bad. 
The tokenizer runs on CPU, and the model runs on GPU.
Source and more information"
rc-learning-fork/content/notes/llms-hpc/inference/batching-pipeline.md,"Data (sequences) are passed in batches to the GPU, instead of one at a time. 
This allows the GPU to stay busy computing without waiting on more data to be passed from the CPU.
Batching can be used if the pipeline is passed a list of data or a dataset from the datasets package.
Batching may or may not speed up your code!  You will need to test it.
The default batch_size for a pipeline is 1.
If a dataset from the datasets package is used, DataLoader is being called under the hood in the pipeline.
Use multiple CPU cores and set the num_workers parameter (default is 8).
Source and more information

Note: Do not use batching on CPU."
rc-learning-fork/content/notes/llms-hpc/inference/exercise_3a.md,"Direct LLM Usage vs Pipeline
Text Summarization


Open the ex3a.ipynb file from the workshop folder.


Run each cell of this notebook and complete the EXERCISES as you go.


Watch the GPU memory using GPU Dashboard as you run the cells.

"
rc-learning-fork/content/notes/llms-hpc/inference/using-llm.md,"LLMs can be used as-is (i.e., out-of-the-box) or after fine-tuning.
Hugging Face model cards will generally provide code for how to get started.
Code may be PyTorch or TensorFlow, “raw” (using the model directly), or pipeline code (using the pipeline from transformers library).

Ex 1
provides “raw” PyTorch code


Ex 2
provides pipeline code



Code for at least loading the model (directly and using the pipeline) is provided by clicking the “Use this model” button on Hugging Face. You may have to dig through the links to find the code you need."
rc-learning-fork/content/notes/llms-hpc/inference/what-is-inference.md,"Inference  is the process in which a trained (or fine-tuned) LLM makes a prediction for a given input.
{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_9.png height=90% width=90% >}}
EOS: end of sequence"
rc-learning-fork/content/notes/llms-hpc/inference/pipeline-debug-tips.md,"Use the CPU. Error messages for code running on the CPU tend to be more helpful than those for code running on the GPU.
Run the pipeline tokenizer and model separately to see where the error is being generated.
Check out the data you are feeding to the pipeline that is causing the error.  Is it somehow different than other pieces of data?
If you get stuck, please submit a ticket to RC.  We can help!"
rc-learning-fork/content/notes/llms-hpc/slurm-scripts/options-slurm.md,"
To request a specific amount of memory per node:
Ex: --mem=64G

Units are given with a suffix (K, M, G, or T).  If no unit is given, megabytes is assumed.
Other options are available at https://slurm.schedmd.com/sbatch.html
For more information, see the RC tutorial Using SLURM from a Terminal.
Tip: if you have a Jupyter notebook file (.ipynb) that you would like to run using a Slurm script, first convert it to a .py file using the following command (make sure you are in the directory that contains the file): 
jupyter nbconvert --to python file_name.ipynb"
rc-learning-fork/content/notes/llms-hpc/slurm-scripts/what-is-slurm.md,"HPC environments are generally shared resources among a group of users.
In order to manage user jobs, we use Slurm, a resource manager for Linux clusters.
This includes deciding which jobs run, when those jobs run, and which node(s) they run on.
A Slurm script provides Slurm with the necessary information to run a job, including details about the required computational resources, the necessary software, and the command(s) needed to execute the code file.
From our website: “Jobs are submitted to the Slurm controller, which queues them until the system is ready to run them. The controller selects which jobs to run, when to run them, and how to place them on the compute node or nodes, according to a predetermined site policy meant to balance competing user needs and to maximize efficient use of cluster resources.”"
rc-learning-fork/content/notes/llms-hpc/slurm-scripts/slurm-ex.md,"Below is an example Slurm script which runs a task using the Pytorch framework and GPUs on the cluster.
```bash
!/bin/bash
SBATCH -A mygroup
SBATCH -p gpu
SBATCH --gres=gpu:1
SBATCH -c 1
SBATCH -t 00:01:00
SBATCH -J pytorchtest
SBATCH -o pytorchtest-%A.out
SBATCH -e pytorchtest-%A.err
load software
module purge
module load apptainer pytorch
run code
apptainer run --nv $CONTAINERDIR/pytorch-2.4.0.sif pytorch_example.py
```
Set up resources:
* -A: allocation
* -p: partition
* --gres=gpu:1 : use 1 gpu
* -c: number of cores
* -t: time limit
* -J: job name
* -o: standard output file (%A is the job #)
* -e: standard error file (%A is the job #)
The default command defined in each container is “python” so using “run” basically executes “python file_name.py”.
The load software and run code lines are what a user would use to run their script at the command line.
TensorFlow example"
rc-learning-fork/content/notes/llms-hpc/model-selection/finding-model-size.md,"{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_7.png height=80% width=80% >}}
Use the information on the model card.
$$ \text{Model Size (B)} = \text{(Number of parameters)} \times \text{(bytes/parameter)} $$

{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_8.png height=60% width=60% >}}
Use information on the Files and versions tab.
Look for the pytorch model in the list of files.  (It will have a .bin extension.)
The size of the model will be given."
rc-learning-fork/content/notes/llms-hpc/model-selection/model-selection.md,{{< figure src=/notes/llms-hpc/img/LLMS_on_HPC_6.png height=50% width=50% >}}
rc-learning-fork/content/notes/llms-hpc/model-selection/benchmarking.md,"Once you have narrowed down your choice of LLMs to a few, benchmarking can help you make a final decision on a model.
Benchmarking results may be given in the model documentation on standard (or other) datasets. It is always good to test models on your data!"
rc-learning-fork/content/notes/llms-hpc/model-selection/choosing-hf.md,"Planning to Choose a Hugging Face Model
Things to consider:
* What type of task will you do?  (text classification, question answering, etc.)
* What type of data will you work with?  Is the text data from a specific domain (financial, scientific, Tweets, etc.)?  What language is it in?
More specific text data will most likely need a fine-tuned model, otherwise a more general LLM may work better.
If you need to fine-tune a model, do you have the computational resources to do so?
Larger models (i.e., models with more parameters) will need more resources.
Source and more information
Choosing the Model
Appropriately use model filters (task, language, license, etc).
Select either a general LLM or a fine-tuned model.
Check number of downloads. While a more popular model isn’t always better, it is good to know what models other people find useful.
Check model size (how to examples on next slides).
Read model card (documentation), including
  * The datasets that the model was trained on and fine-tuned on (if applicable).
  * The model license (does this meet your needs?)
  * Any benchmarking results.
Source and more information"
rc-learning-fork/content/notes/llms-hpc/model-selection/hugging-face.md,"Hugging Face is a machine learning platform that includes models, datasets, and spaces (AI apps).
Information and/or code is provided to show how to use the models and datasets.
Information on how the models were trained, benchmarked, etc. may also be provided.
Models and datasets are filterable by task (e.g., text classification, question answering).
Some models may require you to sign an agreement before using them.
Models and datasets include metrics such as number of downloads and date of last update."
rc-learning-fork/content/tutorials/slurm-from-cli/index.md,
rc-learning-fork/content/tutorials/omero-hands-on/index.md,"{{< figure src=""/img/ome-main-nav.svg"" >}}
OMERO is image management software that allows you to organize, view, annotate, analyze, and share your data from a single centralized repository. 
""This hands-on tutorial will show you how to import images into the OMERO database, organize and tag images, and how to use OMERO with third-party software such as Fiji/ImageJ, MATLAB™, and Python.""
Download the sample dataset and scripts so you can follow along with the
examples."
rc-learning-fork/content/tutorials/benchmark-parallel-programs/index.md,
rc-learning-fork/content/tutorials/matlab-optimization/index.md,"MATLAB is an integrated technical computing environment from the MathWorks that combines array-based numeric computation, advanced graphics and visualization, and a high-level programming language. Separately licensed toolboxes provide additional domain-specific functionality.
The topics of this workshop include:

Running optimization problems in MATLAB
Specifying objective functions
Specifying constraints
Choosing solvers and algorithms
Evaluating results and improving performance
Using global optimization methods
Using parallel computing to speed up optimizations
"
rc-learning-fork/content/tutorials/rapids/index.md,Prerequisites: Python data science/machine learning experience (especially Pandas and Scikit-Learn).
rc-learning-fork/content/tutorials/matlab-deep-learning/index.md,"Deep Learning is a technique that enables machines to learn using multilayered neural
networks. 
Please download the slides and the zip file containing the sample datasets and scripts in order to follow along."
rc-learning-fork/content/tutorials/working-with-files/index.md,"{{< youtube id=""UtfNaYfNHy4"" >}}"
rc-learning-fork/content/tutorials/globus-data-transfer/index.md,
rc-learning-fork/content/tutorials/matlab-parallel/index.md,"By working through common scenarios and workflows, you will gain an understanding of the parallel constructs in MATLAB, their capabilities, and some of the typical issues that arise when using them.
The examples in this workshop will vary in difficulty, starting from simple parallel usage concepts and progressing to more advanced techniques.  Attendees will be able to ask questions and receive feedback regarding workshop exercises.  Users will also have the opportunity to run MATLAB code on the Rivanna cluster if they have login accounts.
Highlights include:

Speeding up programs with parallel computing
Offloading computations and cluster computing
Working with large data sets
GPU Computing

Requirements:
MATLAB experience or basic programming or coding experience is suggested.  Cluster computing and Linux experience is not required for this workshop.

Download the code zip file above
Unzip the folder to a known location
Navigate to the folder location from within MATLAB
Run startWorkshop.m

Parallel Computing Workshop Exercises
Unzip the file you downloaded and work from that folder.
Matlab Parallel Computing on Rivanna
Please see also our documentation for using MATLAB in parallel on Rivanna.
 https://www.rc.virginia.edu/userinfo/rivanna/software/matlab/"
rc-learning-fork/content/tutorials/matlab-data-science/index.md,"The hands-on courses in this workshop cover the use of Matlab for data science applications.
They cover the areas of machine learning, deep learning, and data analytics.
Highlights of machine learning modules include:

Finding Natural Patterns in Data
Classification Methods
Improving Predictive Models
Regression Methods
Neural Networks

Highlights of deep learning modules include:

Classifying Images with Convolutional Networks
Interpreting Network Behavior
Creating Networks
Training Networks
Improving Performance
Performing Regression
Using Deep Learning for Computer Vision
Classifying Sequence Data with Recurrent Networks

A video of using Matlab for data analytics is also included."
rc-learning-fork/content/tutorials/dl-drug-discovery/index.md,"Optionally, this space can be used to introduce your tutorial"
rc-learning-fork/content/tutorials/hpc-intro/index.md,"This tutorial is an introduction to using UVA's high-performance computing (HPC) system.
{{< youtube id=""iPbAc6mnZJ8"" >}}"
rc-learning-fork/content/tutorials/biopython/index.md,"In this workshop participants will learn how to use the biopython package to develop Python scripts for bioinformatics analysis. Participants will learn how to download public datasets from online databases, parse bioinformatics file formats, and implement some sequence analysis workflows."
rc-learning-fork/content/tutorials/databases-intro/index.md,
rc-learning-fork/content/tutorials/python-high-performance/index.md,"My program takes forever!  Can I do something about it? Like most interpreted languages, Python can be slow.  Loops are a particular problem, but there are other pitfalls that may affect the performance of your code.  This workshop will show you tips and tricks to speed up your code.
Participants should have experience programming in Python."
rc-learning-fork/content/tutorials/deep-learning-distributed/index.md,"Deep Learning (DL) is a powerful tool transforming scientific workflow. Because DL training is computationally intensive, it is well suited to HPC systems. Effective use of UVA HPC resources can greatly accelerate your DL workflows. In this tutorial, we will discuss:

When is it appropriate to use a GPU?
How to optimize single-GPU code?
How to convert single-GPU code to Multi-GPU code in different frameworks and run it on UVA HPC?

Please download and unzip the code to follow along with the activities.
{{< file-download file=""notes/deep-learning-distributed/code/distributed_dl.zip"" text=""distributed_dl.zip"" >}}"
rc-learning-fork/content/tutorials/building-running-c-cpp-fortran/index.md,
rc-learning-fork/content/tutorials/unix-tutorial/index.md,
rc-learning-fork/content/tutorials/working-with-jobs-ood/index.md,"{{< youtube id=""n3apBxHpevE"" >}}"
rc-learning-fork/content/tutorials/deep-learning-hpc/index.md,
rc-learning-fork/content/tutorials/llms-hpc/index.md,
rc-learning-fork/content/tutorials/gpu-applications-rivanna/index.md,
rc-learning-fork/content/tutorials/rio-intro/index.md,
rc-learning-fork/content/tutorials/interactive-apps-ood/index.md,"{{< youtube id=""a9ps9ULmPx8"" >}}"
rc-learning-fork/content/tutorials/bioinfo-tools-riv/index.md,
rc-learning-fork/content/tutorials/python-machine-learning/index.md,"This tutorial wil go over Machine Learning concepts using Python discussing the following topics: 
* Overview of Machine Learning
* Decision Trees
    * Coding Decision Trees
* Random Forest
    * Coding Random Forest
* Overview of Neural Networks
    * Coding Neural Networks
* Tensorflow/Keras
    * Coding Tensorflow
* PyTorch
    * Coding PyTorch
* Overview of Parallelizing Deep Learning
    * Coding Multi-GPU Program
As mentioned above, example codes will be provided for respective topics. Please download the code above to follow along."
rc-learning-fork/content/tutorials/connecting-to-hpc/index.md,"{{< youtube id=""jUsGuEbht6U"" >}}"
rc-learning-fork/content/tutorials/git-intro/index.md,
rc-learning-fork/content/tutorials/containers/index.md,In this workshop we will go over how to containerize our apps with Docker and maintain them with GitHub.
rc-learning-fork/content/tutorials/hpc-from-terminal/index.md,
rc-learning-fork/content/tutorials/matlab-image-processing/index.md,Please download the sample image dataset before you begin.
rc-learning-fork/content/tutorials/using-the-shell/index.md,"{{< youtube id=""uALoBtuiTOI"" >}}"
rc-learning-fork/content/tutorials/matlab-statistics/index.md,"This workshop provides hands-on experience for performing statistical data analysis with MATLAB® and the Statistics and Machine Learning Toolbox™. Examples and exercises demonstrate the use of appropriate MATLAB and Statistics and Machine Learning Toolbox functionality throughout the analysis process; from exploratory analysis, to confirmatory analysis and simulation. Topics include:

Calculating summary statistics
Visualizing data
Fitting distributions
Performing tests of significance
Fitting regression models
Interpolating data
Generating random numbers

Participants must have a Mathworks account in order to access their online training materials."
rc-learning-fork/content/tutorials/matlab-data-viz/index.md,
